{
  "python_service/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass SportingLifeAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to SportingLifeAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"a.hr-race-header-course-name__link\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.hr-race-header-time__time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n                race_number = 1\n                if active_link:\n                    all_links = soup.select(\"a.hr-race-header-navigation-link\")\n                    try:\n                        race_number = all_links.index(active_link) + 1\n                    except ValueError:\n                        pass  # Keep default race number if active link not in all links\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from SportingLife, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.hr-racing-runner-horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.hr-racing-runner-saddle-cloth-no\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.select_one(\"span.hr-racing-runner-odds\")\n            odds_str = clean_text(odds_node.get_text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on SportingLife, skipping runner.\")\n            return None\n",
  "python_service/adapters/tab_adapter.py": "# python_service/adapters/tab_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TabAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for tab.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
  "python_service/adapters/template_adapter.py": "# python_service/adapters/template_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TemplateAdapter(BaseAdapterV3):\n    \"\"\"\n    A template for creating new adapters, based on the BaseAdapterV3 pattern.\n    This adapter is a non-functional stub.\n    \"\"\"\n\n    SOURCE_NAME = \"[IMPLEMENT ME] Example Source\"\n    BASE_URL = \"https://api.example.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
  "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for The Racing API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(self.http_client, \"GET\", endpoint, headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in TheRacingAPI response.\")\n            return []\n\n        races = []\n        for race_data in raw_data.get(\"racecards\", []):\n            try:\n                race_id = race_data.get(\"race_id\")\n                off_time = race_data.get(\"off_time\")\n                course = race_data.get(\"course\")\n                race_no = race_data.get(\"race_no\")\n\n                if not all([race_id, off_time, course, race_no]):\n                    continue\n\n                start_time = datetime.fromisoformat(off_time.replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_id}\",\n                    venue=course,\n                    race_number=race_no,\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_data.get(\"race_id\"),\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                horse = runner_data.get(\"horse\")\n                if not horse:\n                    continue\n\n                odds_data = {}\n                odds_list = runner_data.get(\"odds\", [])\n                if odds_list:\n                    odds_decimal_str = odds_list[0].get(\"odds_decimal\")\n                    if odds_decimal_str:\n                        win_odds = Decimal(str(odds_decimal_str))\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=horse,\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI runner\",\n                    runner_name=runner_data.get(\"horse\"),\n                    exc_info=True,\n                )\n        return runners\n",
  "python_service/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        html_pages = []\n        for url_path in links:\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            if response:\n                html_pages.append(response.text)\n            await asyncio.sleep(1)  # Politeness delay\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
  "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US racing data from the TVG API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v2/races/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(self.http_client, \"GET\", summary_url, headers=headers)\n        if not tracks_response:\n            return None\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get(\"tracks\", []):\n            track_id = track.get(\"id\")\n            for race in track.get(\"races\", []):\n                race_id = race.get(\"id\")\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(self.http_client, \"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        # Filter out exceptions and return only successful responses\n        return [resp.json() for resp in race_detail_responses if resp and not isinstance(resp, Exception)]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of detailed race JSON objects into Race models.\"\"\"\n        races = []\n        if not isinstance(raw_data, list):\n            self.logger.warning(\"raw_data is not a list, cannot parse TVG races.\")\n            return races\n\n        for race_detail in raw_data:\n            try:\n                if race := self._parse_race(race_detail):\n                    races.append(race)\n            except AdapterParsingError:\n                self.logger.warning(\n                    \"Failed to parse TVG race detail, skipping.\",\n                    race_detail=race_detail,\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_race(self, race_detail: dict) -> Optional[Race]:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get(\"track\")\n        race_info = race_detail.get(\"race\")\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get(\"runners\", []):\n            if runner_data.get(\"scratched\"):\n                continue\n\n            odds = runner_data.get(\"odds\", {})\n            current_odds = odds.get(\"currentPrice\", {})\n            odds_str = current_odds.get(\"fractional\") or odds.get(\"morningLinePrice\", {}).get(\"fractional\")\n\n            try:\n                number = int(runner_data.get(\"programNumber\", \"0\").replace(\"A\", \"\"))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            odds_data = {}\n            if odds_str:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n\n            runners.append(\n                Runner(\n                    number=number,\n                    name=clean_text(runner_data.get(\"name\")),\n                    odds=odds_data,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        post_time = race_info.get(\"postTime\")\n        if not post_time:\n            raise AdapterParsingError(self.source_name, \"Missing post time.\")\n\n        try:\n            start_time = datetime.fromisoformat(post_time.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name,\n                f\"Could not parse post time: {post_time}\",\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get(\"name\"),\n            race_number=race_info.get(\"number\"),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "python_service/adapters/twinspires_adapter.py": "\"\"\"\nTwinSpires Racing Adapter with production-grade reliability.\n\"\"\"\n\nimport asyncio\nimport os\nimport re\nimport time\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict, List, Optional\nfrom pathlib import Path\nimport logging\n\nfrom scrapling.parser import Selector\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\nlogger = logging.getLogger(__name__)\n\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\"\n    TwinSpires adapter with robust browser handling and retry logic.\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    # Selector strategies (ordered by reliability)\n    RACE_SELECTORS = [\n        'div[class*=\"RaceCard\"]',\n        'div[class*=\"race-card\"]',\n        'section[class*=\"race\"]',\n        '[data-testid*=\"race\"]',\n        '[data-race-id]',\n        'div[class*=\"event-card\"]',\n    ]\n\n    RUNNER_SELECTORS = [\n        'tr[class*=\"runner\"]',\n        'div[class*=\"runner\"]',\n        '[data-runner-id]',\n        'div[class*=\"horse-row\"]',\n        'li[class*=\"entry\"]',\n    ]\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config,\n            enable_cache=True,\n            cache_ttl=180.0,\n            rate_limit=1.5\n        )\n        self._session = None\n        self._session_type = None\n        self._debug_dir = Path(os.environ.get('DEBUG_OUTPUT_DIR', '.'))\n\n    async def _get_session(self):\n        \"\"\"Get or create a browser session.\"\"\"\n        if self._session is not None:\n            return self._session, self._session_type\n\n        # Try StealthySession first (Camoufox)\n        if os.environ.get('CAMOUFOX_AVAILABLE', 'true').lower() == 'true':\n            try:\n                from scrapling.fetchers import AsyncStealthySession\n\n                self._session = AsyncStealthySession(\n                    headless=True,\n                    block_images=True,\n                    block_webrtc=True,\n                    google_search=True,\n                )\n                try:\n                    await self._session.start()\n                except RuntimeError as e:\n                    if \"already has an active browser context\" not in str(e):\n                        raise\n                self._session_type = \"stealthy\"\n                self.logger.info(\"Using StealthySession (Camoufox)\")\n                return self._session, self._session_type\n\n            except Exception as e:\n                self.logger.warning(f\"StealthySession failed: {e}\")\n\n        # Fallback to Playwright\n        try:\n            from scrapling.fetchers import AsyncStealthySession\n\n            self._session = AsyncStealthySession(\n                headless=True,\n                browser_type='chromium',\n            )\n            await self._session.start()\n            self._session_type = \"playwright\"\n            self.logger.info(\"Using PlayWrightSession (Chromium)\")\n            return self._session, self._session_type\n\n        except Exception as e:\n            self.logger.error(f\"All browser backends failed: {e}\")\n            raise RuntimeError(\"No browser backend available\")\n\n    async def _fetch_with_retry(\n        self,\n        url: str,\n        max_retries: int = 3,\n        **kwargs\n    ) -> Optional[Any]:\n        \"\"\"Fetch with exponential backoff retry.\"\"\"\n        last_error = None\n\n        for attempt in range(max_retries):\n            try:\n                session, _ = await self._get_session()\n\n                if attempt > 0:\n                    delay = (2 ** attempt) + random.uniform(0, 1)\n                    self.logger.debug(f\"Retry delay: {delay:.1f}s\")\n                    await asyncio.sleep(delay)\n\n                self.logger.info(f\"Fetching {url} (attempt {attempt + 1})\")\n\n                response = await session.fetch(\n                    url,\n                    timeout=kwargs.get('timeout', 45000),\n                    network_idle=kwargs.get('network_idle', True),\n                )\n\n                if response.status == 200:\n                    # Check for blocks\n                    if self._is_blocked(response.text):\n                        self.logger.warning(\"Blocked response detected\")\n                        last_error = \"Blocked by anti-bot\"\n                        await self._reset_session()\n                        continue\n\n                    return response\n\n                last_error = f\"HTTP {response.status}\"\n\n            except asyncio.TimeoutError:\n                last_error = \"Timeout\"\n            except Exception as e:\n                last_error = str(e)\n                self.logger.warning(f\"Fetch error: {e}\")\n\n        self.logger.error(f\"All retries failed: {last_error}\")\n        return None\n\n    def _is_blocked(self, html: str) -> bool:\n        \"\"\"Check if response indicates blocking.\"\"\"\n        indicators = ['captcha', 'challenge', 'access denied', 'cf-browser']\n        return any(ind in html.lower() for ind in indicators)\n\n    async def _reset_session(self):\n        \"\"\"Reset the browser session.\"\"\"\n        if self._session:\n            try:\n                await self._session.close()\n            except:\n                pass\n            self._session = None\n            self._session_type = None\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetch race data for the given date.\"\"\"\n        self.logger.info(f\"Fetching TwinSpires races for {date}\")\n\n        urls = [\n            f\"{self.BASE_URL}/bet/todays-races/time\",\n            f\"{self.BASE_URL}/racing/entries\",\n        ]\n\n        for url in urls:\n            response = await self._fetch_with_retry(url)\n\n            if response:\n                # Save debug HTML\n                await self._save_debug(response.text, \"twinspires\")\n\n                # Extract races\n                races_data = self._extract_races(response, date)\n\n                if races_data:\n                    self.logger.info(f\"Found {len(races_data)} races from {url}\")\n                    return {\n                        \"races\": races_data,\n                        \"date\": date,\n                        \"source\": \"twinspires\",\n                        \"url\": url,\n                    }\n\n        return None\n\n    def _extract_races(self, response, date: str) -> List[dict]:\n        \"\"\"Extract race data from page response.\"\"\"\n        races = []\n\n        # Try each selector\n        for selector in self.RACE_SELECTORS:\n            try:\n                elements = response.css(selector)\n                if elements:\n                    self.logger.debug(f\"Found {len(elements)} races with: {selector}\")\n\n                    for i, elem in enumerate(elements):\n                        race_data = self._extract_single_race(elem, i + 1, date)\n                        if race_data:\n                            races.append(race_data)\n\n                    if races:\n                        return races\n            except Exception as e:\n                self.logger.debug(f\"Selector {selector} failed: {e}\")\n\n        # No races found - return empty\n        self.logger.warning(\"No race elements found\")\n        return races\n\n    def _extract_single_race(self, elem, default_num: int, date: str) -> Optional[dict]:\n        \"\"\"Extract data from a single race element.\"\"\"\n        try:\n            html = str(elem.html) if hasattr(elem, 'html') else str(elem)\n\n            # Extract track\n            track = \"Unknown\"\n            for sel in ['[class*=\"track\"]', 'h2', 'h3', '[class*=\"venue\"]']:\n                found = elem.css_first(sel)\n                if found and hasattr(found, 'text'):\n                    track = found.text.strip()\n                    break\n\n            # Extract race number\n            race_num = default_num\n            for sel in ['[class*=\"race-num\"]', '[class*=\"number\"]']:\n                found = elem.css_first(sel)\n                if found:\n                    digits = ''.join(filter(str.isdigit, found.text))\n                    if digits:\n                        race_num = int(digits)\n                        break\n\n            return {\n                \"html\": html,\n                \"track\": track,\n                \"race_number\": race_num,\n                \"date\": date,\n            }\n        except Exception as e:\n            self.logger.debug(f\"Extract race error: {e}\")\n            return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parse raw data into Race objects.\"\"\"\n        if not raw_data or \"races\" not in raw_data:\n            return []\n\n        date_str = raw_data.get(\"date\", datetime.now().strftime(\"%Y-%m-%d\"))\n        parsed = []\n\n        for race_data in raw_data[\"races\"]:\n            try:\n                race = self._parse_single_race(race_data, date_str)\n                if race and race.runners:\n                    parsed.append(race)\n            except Exception as e:\n                self.logger.warning(f\"Parse error: {e}\")\n\n        return parsed\n\n    def _parse_single_race(self, race_data: dict, date_str: str) -> Optional[Race]:\n        \"\"\"Parse a single race.\"\"\"\n        html = race_data.get(\"html\", \"\")\n        if not html:\n            return None\n\n        page = Selector(html)\n\n        track = race_data.get(\"track\", \"Unknown\")\n        race_num = race_data.get(\"race_number\", 1)\n\n        # Parse runners\n        runners = self._parse_runners(page)\n\n        if not runners:\n            return None\n\n        # Generate ID\n        track_id = re.sub(r'[^a-z0-9]', '', track.lower())\n        race_id = f\"ts_{track_id}_{date_str.replace('-', '')}_R{race_num}\"\n\n        # Parse time\n        start_time = self._parse_time(page, date_str)\n\n        return Race(\n            id=race_id,\n            venue=track,\n            race_number=race_num,\n            start_time=start_time,\n            discipline=\"Thoroughbred\",\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n    def _parse_runners(self, page) -> List[Runner]:\n        \"\"\"Parse runners from race HTML.\"\"\"\n        runners = []\n\n        for selector in self.RUNNER_SELECTORS:\n            elements = page.css(selector)\n            if elements:\n                for i, elem in enumerate(elements):\n                    runner = self._parse_single_runner(elem, i + 1)\n                    if runner:\n                        runners.append(runner)\n\n                if runners:\n                    return runners\n\n        return runners\n\n    def _parse_single_runner(self, elem, default_num: int) -> Optional[Runner]:\n        \"\"\"Parse a single runner element.\"\"\"\n        try:\n            elem_html = str(elem.html) if hasattr(elem, 'html') else str(elem)\n\n            # Check scratched\n            scratched = 'scratch' in elem_html.lower()\n\n            # Get number\n            number = default_num\n            for sel in ['[class*=\"number\"]', '[class*=\"program\"]']:\n                found = elem.css_first(sel)\n                if found:\n                    digits = ''.join(filter(str.isdigit, found.text))\n                    if digits:\n                        number = int(digits)\n                        break\n\n            # Get name\n            name = None\n            for sel in ['[class*=\"horse\"]', '[class*=\"name\"]', 'a']:\n                found = elem.css_first(sel)\n                if found and hasattr(found, 'text'):\n                    name = found.text.strip()\n                    if name and len(name) > 1:\n                        break\n\n            if not name:\n                return None\n\n            # Get odds\n            odds = {}\n            if not scratched:\n                for sel in ['[class*=\"odds\"]', '[class*=\"ml\"]']:\n                    found = elem.css_first(sel)\n                    if found:\n                        odds_text = found.text.strip()\n                        win_odds = parse_odds_to_decimal(odds_text)\n                        if win_odds and 1.0 < win_odds < 999:\n                            odds[self.SOURCE_NAME] = OddsData(\n                                win=win_odds,\n                                source=self.SOURCE_NAME,\n                                last_updated=datetime.now(),\n                            )\n                            break\n\n            return Runner(\n                number=number,\n                name=name,\n                scratched=scratched,\n                odds=odds,\n            )\n        except Exception:\n            return None\n\n    def _parse_time(self, page, date_str: str) -> Optional[datetime]:\n        \"\"\"Parse post time from page.\"\"\"\n        base_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n        for sel in ['time[datetime]', '[class*=\"post-time\"]', '[class*=\"mtp\"]']:\n            elem = page.css_first(sel)\n            if elem:\n                # Try datetime attribute\n                dt = elem.attrib.get('datetime') if hasattr(elem, 'attrib') else None\n                if dt:\n                    try:\n                        return datetime.fromisoformat(dt.replace('Z', '+00:00'))\n                    except:\n                        pass\n\n                # Try text\n                text = elem.text.strip() if hasattr(elem, 'text') else ''\n                if text:\n                    # Handle MTP (minutes to post)\n                    mtp = re.search(r'(\\d+)\\s*(?:min|mtp)', text, re.I)\n                    if mtp:\n                        return datetime.now() + timedelta(minutes=int(mtp.group(1)))\n\n                    # Try time formats\n                    for fmt in ['%I:%M %p', '%H:%M']:\n                        try:\n                            t = datetime.strptime(text, fmt).time()\n                            return datetime.combine(base_date, t)\n                        except:\n                            pass\n\n        return datetime.combine(base_date, datetime.now().time())\n\n    async def _save_debug(self, html: str, prefix: str):\n        \"\"\"Save debug HTML.\"\"\"\n        try:\n            path = self._debug_dir / f\"{prefix}_debug.html\"\n            path.write_text(html[:500000], encoding='utf-8')  # Limit size\n        except Exception as e:\n            self.logger.debug(f\"Failed to save debug: {e}\")\n\n    async def cleanup(self):\n        \"\"\"Cleanup resources.\"\"\"\n        await self._reset_session()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *args):\n        await self.cleanup()\n",
  "python_service/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
  "python_service/adapters/utils.py": "# python_service/adapters/utils.py\n# Compatibility shim to re-export parse_odds from the centralized location.\n\nfrom ..utils.odds import parse_odds\n\n__all__ = [\"parse_odds\"]\n",
  "python_service/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n"
}