{
  "sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\n\nclass SportingLifeAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        SportingLife requires JavaScript rendering to get the race links,\n        so we must use a full browser engine like Playwright.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=StealthMode.FAST,\n            block_resources=True\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = \"/racing/racecards\"  # The dated URL is causing a 307 redirect\n        index_response = await self.make_request(\n            \"GET\",\n            index_url,\n            headers=self._get_headers(),\n            follow_redirects=True,\n        )\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        parser = HTMLParser(index_response.text)\n        links = {\n            a.attributes[\"href\"]\n            for a in parser.css('li[class^=\"MeetingSummary__LineWrapper\"] a[href*=\"/racecard/\"]')\n            if a.attributes.get(\"href\")\n        }\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.sportinglife.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Not/A)Brand\";v=\"99\", \"Google Chrome\";v=\"115\", \"Chromium\";v=\"115\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n            \"Referer\": \"https://www.sportinglife.com/racing/racecards\",\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to SportingLifeAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                header = parser.css_first('h1[class*=\"RacingRacecardHeader__Title\"]')\n                if not header:\n                    self.logger.warning(\"Could not find race header.\")\n                    continue\n\n                header_text = clean_text(header.text())\n                parts = header_text.split()\n                race_time_str = parts[0]\n                track_name = \" \".join(parts[1:])\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                race_number = 1\n                nav_links = parser.css('a[class*=\"SubNavigation__Link\"]')\n                active_link = parser.css_first('a[class*=\"SubNavigation__Link--active\"]')\n                if active_link and nav_links:\n                    try:\n                        # Find the index of active_link in nav_links\n                        # Selectolax nodes don't support equality easily, so we compare HTML or attributes\n                        for idx, link in enumerate(nav_links):\n                            if link.html == active_link.html:\n                                race_number = idx + 1\n                                break\n                    except Exception:\n                        self.logger.warning(\"Error finding active race link index.\")\n\n                runners = [self._parse_runner(row) for row in parser.css('div[class*=\"RunnerCard\"]')]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError) as e:\n                self.logger.warning(\n                    \"Error parsing a race from SportingLife, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first('a[href*=\"/racing/profiles/horse/\"]')\n            if not name_node:\n                return None\n            name = clean_text(name_node.text()).splitlines()[0].strip()\n\n            num_node = row.css_first('span[class*=\"SaddleCloth__Number\"]')\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.css_first('span[class*=\"Odds__Price\"]')\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on SportingLife, skipping runner.\")\n            return None\n",
  "tab_adapter.py": "# python_service/adapters/tab_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TabAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for tab.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
  "template_adapter.py": "# python_service/adapters/template_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TemplateAdapter(BaseAdapterV3):\n    \"\"\"\n    A template for creating new adapters, based on the BaseAdapterV3 pattern.\n    This adapter is a non-functional stub.\n    \"\"\"\n\n    SOURCE_NAME = \"[IMPLEMENT ME] Example Source\"\n    BASE_URL = \"https://api.example.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
  "the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for The Racing API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(\"GET\", endpoint, headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in TheRacingAPI response.\")\n            return []\n\n        races = []\n        for race_data in raw_data.get(\"racecards\", []):\n            try:\n                race_id = race_data.get(\"race_id\")\n                off_time = race_data.get(\"off_time\")\n                course = race_data.get(\"course\")\n                race_no = race_data.get(\"race_no\")\n\n                if not all([race_id, off_time, course, race_no]):\n                    continue\n\n                start_time = datetime.fromisoformat(off_time.replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_id}\",\n                    venue=course,\n                    race_number=race_no,\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_data.get(\"race_id\"),\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                horse = runner_data.get(\"horse\")\n                if not horse:\n                    continue\n\n                odds_data = {}\n                odds_list = runner_data.get(\"odds\", [])\n                if odds_list:\n                    odds_decimal_str = odds_list[0].get(\"odds_decimal\")\n                    if odds_decimal_str:\n                        win_odds = Decimal(str(odds_decimal_str))\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=horse,\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI runner\",\n                    runner_name=runner_data.get(\"horse\"),\n                    exc_info=True,\n                )\n        return runners\n",
  "timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com/horse-racing\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        # Save the raw HTML for debugging in CI\n        try:\n            with open(\"timeform_debug.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write(index_response.text)\n        except Exception as e:\n            self.logger.warning(\"Failed to save debug HTML for Timeform\", error=str(e))\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.timeform.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
  "tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US racing data from the TVG API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v2/races/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(\"GET\", summary_url, headers=headers)\n        if not tracks_response:\n            return None\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get(\"tracks\", []):\n            track_id = track.get(\"id\")\n            for race in track.get(\"races\", []):\n                race_id = race.get(\"id\")\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(\"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        # Filter out exceptions and return only successful responses\n        return [resp.json() for resp in race_detail_responses if resp and not isinstance(resp, Exception)]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of detailed race JSON objects into Race models.\"\"\"\n        races = []\n        if not isinstance(raw_data, list):\n            self.logger.warning(\"raw_data is not a list, cannot parse TVG races.\")\n            return races\n\n        for race_detail in raw_data:\n            try:\n                if race := self._parse_race(race_detail):\n                    races.append(race)\n            except AdapterParsingError:\n                self.logger.warning(\n                    \"Failed to parse TVG race detail, skipping.\",\n                    race_detail=race_detail,\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_race(self, race_detail: dict) -> Optional[Race]:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get(\"track\")\n        race_info = race_detail.get(\"race\")\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get(\"runners\", []):\n            if runner_data.get(\"scratched\"):\n                continue\n\n            odds = runner_data.get(\"odds\", {})\n            current_odds = odds.get(\"currentPrice\", {})\n            odds_str = current_odds.get(\"fractional\") or odds.get(\"morningLinePrice\", {}).get(\"fractional\")\n\n            try:\n                number = int(runner_data.get(\"programNumber\", \"0\").replace(\"A\", \"\"))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            odds_data = {}\n            if odds_str:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n\n            runners.append(\n                Runner(\n                    number=number,\n                    name=clean_text(runner_data.get(\"name\")),\n                    odds=odds_data,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        post_time = race_info.get(\"postTime\")\n        if not post_time:\n            raise AdapterParsingError(self.source_name, \"Missing post time.\")\n\n        try:\n            start_time = datetime.fromisoformat(post_time.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name,\n                f\"Could not parse post time: {post_time}\",\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get(\"name\"),\n            race_number=race_info.get(\"number\"),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "twinspires_adapter.py": "\"\"\"\nTwinSpires Racing Adapter - Production Implementation\n\nUses Scrapling's AsyncStealthySession for anti-bot bypass with:\n- Persistent session pooling\n- Exponential backoff retry logic\n- Comprehensive selector strategies\n- Detailed diagnostics for debugging\n\"\"\"\n\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict, List, Optional, Tuple\nimport re\nimport os\nimport asyncio\nimport logging\nimport random\nfrom pathlib import Path\n\nfrom scrapling.parser import Selector\n\nfrom web_service.backend.models import OddsData, Race, Runner\nfrom web_service.backend.utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\nlogger = logging.getLogger(__name__)\n\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\"\n    Production adapter for TwinSpires racing data.\n\n    Features:\n    - StealthySession with automatic Playwright fallback\n    - Exponential backoff retry logic\n    - Comprehensive selector strategies\n    - Debug HTML capture for failure analysis\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    # Selector strategies - ordered by reliability\n    RACE_CONTAINER_SELECTORS = [\n        'div[class*=\"RaceCard\"]',\n        'div[class*=\"race-card\"]',\n        'div[data-testid*=\"race\"]',\n        'div[data-race-id]',\n        'section[class*=\"race\"]',\n        'article[class*=\"race\"]',\n        '.race-container',\n        '[data-race]',\n        # Broader fallbacks\n        'div[class*=\"card\"][class*=\"race\" i]',\n        'div[class*=\"event\"]',\n    ]\n\n    TRACK_NAME_SELECTORS = [\n        '[class*=\"track-name\"]',\n        '[class*=\"trackName\"]',\n        '[data-track-name]',\n        'h2[class*=\"track\"]',\n        'h3[class*=\"track\"]',\n        '.track-title',\n        '[class*=\"venue\"]',\n    ]\n\n    RACE_NUMBER_SELECTORS = [\n        '[class*=\"race-number\"]',\n        '[class*=\"raceNumber\"]',\n        '[class*=\"race-num\"]',\n        '[data-race-number]',\n        'span[class*=\"number\"]',\n    ]\n\n    POST_TIME_SELECTORS = [\n        'time[datetime]',\n        '[class*=\"post-time\"]',\n        '[class*=\"postTime\"]',\n        '[class*=\"mtp\"]',  # Minutes to post\n        '[data-post-time]',\n        '[class*=\"race-time\"]',\n    ]\n\n    RUNNER_ROW_SELECTORS = [\n        'tr[class*=\"runner\"]',\n        'div[class*=\"runner\"]',\n        'li[class*=\"runner\"]',\n        '[data-runner-id]',\n        'div[class*=\"horse-row\"]',\n        'tr[class*=\"horse\"]',\n        'div[class*=\"entry\"]',\n        '.runner-row',\n        '.horse-entry',\n    ]\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config,\n            enable_cache=True,\n            cache_ttl=180.0,\n            rate_limit=1.5  # Slightly more conservative\n        )\n        self._debug_dir = Path(os.environ.get('DEBUG_OUTPUT_DIR', '.'))\n        self.attempted_url: Optional[str] = None\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        TwinSpires has strong anti-bot protections.\n        Using CAMOUFLAGE stealth mode and blocking non-essential resources.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CAMOUFOX,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            block_resources=True,\n            max_retries=3,\n            timeout=45,\n        )\n\n    def _is_blocked_response(self, html: str) -> bool:\n        \"\"\"Check if response indicates we're blocked.\"\"\"\n        blocked_indicators = [\n            'captcha',\n            'challenge-running',\n            'cf-browser-verification',\n            'access denied',\n            'please verify you are a human',\n            'ray id',  # CloudFlare\n            'checking your browser',\n        ]\n        html_lower = html.lower()\n        return any(indicator in html_lower for indicator in blocked_indicators)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetch race data from TwinSpires for given date.\n\n        Args:\n            date: Date string in YYYY-MM-DD format\n\n        Returns:\n            Dictionary with races data or None on failure\n        \"\"\"\n        self.logger.info(f\"Fetching TwinSpires races for {date}\")\n\n        # Try multiple URL patterns\n        url_patterns = [\n            f\"{self.BASE_URL}/bet/todays-races/time\",\n            f\"{self.BASE_URL}/racing/entries/{date}\",\n            f\"{self.BASE_URL}/races/today\",\n        ]\n\n        for url in url_patterns:\n            self.attempted_url = url\n            self.logger.info(f\"Trying URL pattern: {url}\")\n\n            try:\n                response = await self.make_request(\n                    \"GET\", \n                    url,\n                    network_idle=True,\n                    wait_selector='div[class*=\"race\"], [class*=\"RaceCard\"], [class*=\"track\"]',\n                )\n            except Exception as e:\n                self.logger.warning(f\"Failed to fetch {url}: {e}\")\n                continue\n\n            if response and response.status == 200:\n                # Save debug HTML\n                await self._save_debug_html(response.text, 'twinspires')\n\n                # Extract races\n                races_data = self._extract_races_from_page(response, date)\n\n                if races_data:\n                    self.logger.info(f\"Successfully extracted {len(races_data)} races from {url}\")\n                    return {\n                        \"races\": races_data,\n                        \"date\": date,\n                        \"source\": \"twinspires_live\",\n                        \"url\": url,\n                    }\n                else:\n                    self.logger.warning(f\"No races extracted from {url}, trying next pattern\")\n\n        self.logger.error(\"All URL patterns failed\")\n        return None\n\n    def _extract_races_from_page(self, response, date: str) -> List[dict]:\n        \"\"\"\n        Extract race information from page response.\n\n        Uses multiple selector strategies with fallback.\n        \"\"\"\n        races_data = []\n        page = response  # Response object has Selector methods\n\n        # Try each selector pattern\n        race_elements = []\n        selector_used = None\n\n        for selector in self.RACE_CONTAINER_SELECTORS:\n            try:\n                elements = page.css(selector)\n                if elements and len(elements) > 0:\n                    # Verify these look like race containers\n                    sample = elements[0]\n                    sample_text = str(sample.html) if hasattr(sample, 'html') else str(sample)\n\n                    # Quick sanity check - should have some race-like content\n                    if any(kw in sample_text.lower() for kw in ['race', 'post', 'horse', 'runner', 'odds']):\n                        race_elements = elements\n                        selector_used = selector\n                        break\n            except Exception as e:\n                self.logger.debug(f\"Selector '{selector}' failed: {e}\")\n                continue\n\n        if race_elements:\n            self.logger.info(f\"Found {len(race_elements)} race containers using: '{selector_used}'\")\n        else:\n            self.logger.warning(\"No race containers found with any selector\")\n            # Return full page for further analysis\n            return [{\n                \"html\": response.text,\n                \"track\": \"Unknown\",\n                \"race_number\": 0,\n                \"date\": date,\n                \"full_page\": True,\n            }]\n\n        # Extract data from each race element\n        for i, race_elem in enumerate(race_elements, 1):\n            try:\n                race_data = self._extract_single_race_data(race_elem, i, date)\n                if race_data:\n                    races_data.append(race_data)\n            except Exception as e:\n                self.logger.warning(f\"Failed to extract race {i}: {e}\")\n                continue\n\n        return races_data\n\n    def _extract_single_race_data(self, race_elem, default_num: int, date: str) -> Optional[dict]:\n        \"\"\"Extract data from a single race element.\"\"\"\n        try:\n            # Get HTML string\n            html = str(race_elem.html) if hasattr(race_elem, 'html') else str(race_elem)\n\n            # Extract track name\n            track_name = self._find_with_selectors(race_elem, self.TRACK_NAME_SELECTORS)\n            if not track_name:\n                track_name = f\"Track {default_num}\"\n\n            # Extract race number\n            race_num_text = self._find_with_selectors(race_elem, self.RACE_NUMBER_SELECTORS)\n            race_number = default_num\n            if race_num_text:\n                digits = ''.join(filter(str.isdigit, race_num_text))\n                if digits:\n                    race_number = int(digits)\n\n            # Extract post time\n            post_time_text = self._find_with_selectors(race_elem, self.POST_TIME_SELECTORS)\n\n            return {\n                \"html\": html,\n                \"track\": track_name.strip(),\n                \"race_number\": race_number,\n                \"post_time_text\": post_time_text,\n                \"date\": date,\n                \"full_page\": False,\n            }\n\n        except Exception as e:\n            self.logger.debug(f\"Extract single race error: {e}\")\n            return None\n\n    def _find_with_selectors(self, element, selectors: List[str]) -> Optional[str]:\n        \"\"\"Try multiple selectors and return first matching text.\"\"\"\n        for selector in selectors:\n            try:\n                found = element.css_first(selector)\n                if found:\n                    text = found.text.strip() if hasattr(found, 'text') else str(found).strip()\n                    if text:\n                        return text\n            except Exception:\n                continue\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parse extracted race data into Race objects.\"\"\"\n        if not raw_data or \"races\" not in raw_data:\n            self.logger.warning(\"No races data to parse\")\n            return []\n\n        races_list = raw_data[\"races\"]\n        date_str = raw_data.get(\"date\", datetime.now().strftime(\"%Y-%m-%d\"))\n\n        self.logger.info(f\"Parsing {len(races_list)} races\")\n\n        parsed_races = []\n\n        for race_data in races_list:\n            try:\n                race = self._parse_single_race(race_data, date_str)\n                if race and race.runners:\n                    parsed_races.append(race)\n                    self.logger.debug(\n                        f\"Parsed race\",\n                        track=race.venue,\n                        race=race.race_number,\n                        runners=len(race.runners)\n                    )\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to parse race\",\n                    track=race_data.get(\"track\"),\n                    error=str(e),\n                    exc_info=True\n                )\n                continue\n\n        self.logger.info(f\"Successfully parsed {len(parsed_races)} races with runners\")\n        return parsed_races\n\n    def _parse_single_race(self, race_data: dict, date_str: str) -> Optional[Race]:\n        \"\"\"Parse a single race from extracted data.\"\"\"\n        html = race_data.get(\"html\", \"\")\n        if not html:\n            return None\n\n        page = Selector(html)\n\n        track_name = race_data.get(\"track\", \"Unknown\")\n        race_number = race_data.get(\"race_number\", 1)\n\n        # Parse start time\n        start_time = self._parse_post_time(\n            race_data.get(\"post_time_text\"),\n            page,\n            date_str\n        )\n\n        # Parse runners\n        runners = self._parse_runners(page)\n\n        # Generate race ID\n        track_id = re.sub(r'[^a-z0-9]', '', track_name.lower())\n        date_compact = date_str.replace('-', '')\n        race_id = f\"ts_{track_id}_{date_compact}_R{race_number}\"\n\n        # Determine discipline\n        discipline = self._detect_discipline(page, html)\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            discipline=discipline,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n    def _parse_post_time(\n        self,\n        time_text: Optional[str],\n        page,\n        date_str: str\n    ) -> Optional[datetime]:\n        \"\"\"Parse post time from text or page elements.\"\"\"\n        base_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n        # Try provided time text first\n        if time_text:\n            parsed = self._parse_time_string(time_text, base_date)\n            if parsed:\n                return parsed\n\n        # Try finding time in page\n        for selector in self.POST_TIME_SELECTORS:\n            elem = page.css_first(selector)\n            if not elem:\n                continue\n\n            # Check datetime attribute\n            dt_attr = elem.attrib.get('datetime') if hasattr(elem, 'attrib') else None\n            if dt_attr:\n                try:\n                    return datetime.fromisoformat(dt_attr.replace('Z', '+00:00'))\n                except ValueError:\n                    pass\n\n            # Try text content\n            text = elem.text.strip() if hasattr(elem, 'text') else str(elem).strip()\n            parsed = self._parse_time_string(text, base_date)\n            if parsed:\n                return parsed\n\n        # Default to now + 1 hour if nothing found\n        self.logger.debug(\"Could not determine post time, using default\")\n        return datetime.combine(base_date, datetime.now().time()) + timedelta(hours=1)\n\n    def _parse_time_string(self, time_str: str, base_date) -> Optional[datetime]:\n        \"\"\"Parse various time string formats.\"\"\"\n        if not time_str:\n            return None\n\n        # Clean up string\n        time_clean = re.sub(r'\\s+(EST|EDT|CST|CDT|MST|MDT|PST|PDT|ET|PT|CT|MT)$', '', time_str, flags=re.I)\n        time_clean = time_clean.strip()\n\n        # Handle \"MTP\" (minutes to post) format\n        mtp_match = re.search(r'(\\d+)\\s*(?:min|mtp)', time_clean, re.I)\n        if mtp_match:\n            minutes = int(mtp_match.group(1))\n            return datetime.now() + timedelta(minutes=minutes)\n\n        # Try various time formats\n        formats = [\n            '%I:%M %p',      # 3:45 PM\n            '%I:%M%p',       # 3:45PM\n            '%H:%M',         # 15:45\n            '%I:%M:%S %p',   # 3:45:00 PM\n        ]\n\n        for fmt in formats:\n            try:\n                time_obj = datetime.strptime(time_clean, fmt).time()\n                return datetime.combine(base_date, time_obj)\n            except ValueError:\n                continue\n\n        return None\n\n    def _parse_runners(self, page) -> List[Runner]:\n        \"\"\"Parse runner information from race HTML.\"\"\"\n        runners = []\n\n        # Find runner elements\n        runner_elements = []\n        for selector in self.RUNNER_ROW_SELECTORS:\n            try:\n                elements = page.css(selector)\n                if elements and len(elements) > 0:\n                    runner_elements = elements\n                    self.logger.debug(f\"Found {len(elements)} runners with: {selector}\")\n                    break\n            except Exception:\n                continue\n\n        if not runner_elements:\n            self.logger.debug(\"No runner elements found\")\n            return runners\n\n        for i, elem in enumerate(runner_elements):\n            try:\n                runner = self._parse_single_runner(elem, i + 1)\n                if runner:\n                    runners.append(runner)\n            except Exception as e:\n                self.logger.debug(f\"Failed to parse runner {i + 1}: {e}\")\n                continue\n\n        return runners\n\n    def _parse_single_runner(self, elem, default_number: int) -> Optional[Runner]:\n        \"\"\"Parse a single runner element.\"\"\"\n        # Get element content\n        elem_str = str(elem.html) if hasattr(elem, 'html') else str(elem)\n        elem_lower = elem_str.lower()\n\n        # Check if scratched\n        scratched = any(s in elem_lower for s in ['scratched', 'scr', 'scratch'])\n\n        # Extract program number\n        number_selectors = [\n            '[class*=\"program\"]',\n            '[class*=\"saddle\"]',\n            '[class*=\"post\"]',\n            '[class*=\"number\"]',\n            '[data-program-number]',\n            'td:first-child',\n        ]\n\n        number = None\n        for selector in number_selectors:\n            try:\n                num_elem = elem.css_first(selector)\n                if num_elem:\n                    num_text = num_elem.text.strip() if hasattr(num_elem, 'text') else str(num_elem)\n                    digits = ''.join(filter(str.isdigit, num_text))\n                    if digits:\n                        number = int(digits)\n                        break\n            except Exception:\n                continue\n\n        if number is None:\n            number = default_number\n\n        # Extract horse name\n        name_selectors = [\n            '[class*=\"horse-name\"]',\n            '[class*=\"horseName\"]',\n            '[class*=\"runner-name\"]',\n            'a[class*=\"name\"]',\n            '[data-horse-name]',\n            'td:nth-child(2)',\n        ]\n\n        name = None\n        for selector in name_selectors:\n            try:\n                name_elem = elem.css_first(selector)\n                if name_elem:\n                    name_text = name_elem.text.strip() if hasattr(name_elem, 'text') else None\n                    if name_text and len(name_text) > 1:\n                        # Clean up name\n                        name = re.sub(r'\\([^)]*\\)', '', name_text).strip()\n                        break\n            except Exception:\n                continue\n\n        if not name:\n            return None\n\n        # Extract odds\n        odds = {}\n        if not scratched:\n            odds_selectors = [\n                '[class*=\"odds\"]',\n                '[class*=\"ml\"]',  # Morning line\n                '[class*=\"morning-line\"]',\n                '[data-odds]',\n            ]\n\n            for selector in odds_selectors:\n                try:\n                    odds_elem = elem.css_first(selector)\n                    if odds_elem:\n                        odds_text = odds_elem.text.strip() if hasattr(odds_elem, 'text') else None\n                        if odds_text and odds_text.upper() not in ['SCR', 'SCRATCHED', '--', 'N/A']:\n                            win_odds = parse_odds_to_decimal(odds_text)\n                            if win_odds and 1.0 < win_odds < 999:\n                                odds[self.SOURCE_NAME] = OddsData(\n                                    win=win_odds,\n                                    source=self.SOURCE_NAME,\n                                    last_updated=datetime.now(),\n                                )\n                                break\n                except Exception:\n                    continue\n\n        return Runner(\n            number=number,\n            name=name,\n            scratched=scratched,\n            odds=odds,\n        )\n\n    def _detect_discipline(self, page, html: str) -> str:\n        \"\"\"Detect race discipline (Thoroughbred, Harness, etc).\"\"\"\n        html_lower = html.lower()\n\n        if any(kw in html_lower for kw in ['harness', 'trotter', 'pacer', 'standardbred']):\n            return \"Harness\"\n        elif any(kw in html_lower for kw in ['quarter horse', 'quarterhorse']):\n            return \"Quarter Horse\"\n        elif any(kw in html_lower for kw in ['greyhound', 'dog']):\n            return \"Greyhound\"\n\n        # Try finding breed element\n        breed_selectors = ['[class*=\"breed\"]', '[class*=\"type\"]', '[data-breed]']\n        for selector in breed_selectors:\n            try:\n                elem = page.css_first(selector)\n                if elem:\n                    text = elem.text.strip().lower() if hasattr(elem, 'text') else ''\n                    if 'harness' in text:\n                        return \"Harness\"\n                    elif 'quarter' in text:\n                        return \"Quarter Horse\"\n            except Exception:\n                continue\n\n        return \"Thoroughbred\"\n\n    async def _save_debug_html(self, html: str, prefix: str):\n        \"\"\"Save HTML for debugging purposes.\"\"\"\n        try:\n            debug_file = self._debug_dir / f\"{prefix}_debug.html\"\n            debug_file.write_text(html, encoding='utf-8')\n            self.logger.debug(f\"Saved debug HTML to {debug_file}\")\n        except Exception as e:\n            self.logger.warning(f\"Failed to save debug HTML: {e}\")\n\n    async def cleanup(self):\n        \"\"\"Cleanup resources.\"\"\"\n        await self.close()\n        self.logger.info(\"TwinSpires adapter cleaned up\")\n",
  "universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(\"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
  "utils.py": "# python_service/adapters/utils.py\n# Compatibility shim to re-export parse_odds from the centralized location.\n\nfrom ..utils.odds import parse_odds\n\n__all__ = [\"parse_odds\"]\n",
  "xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n"
}