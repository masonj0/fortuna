# Scrapling Integration Guide for Fortuna Race Scraping

## What is Scrapling?

Scrapling is an undetectable, powerful, flexible, high-performance Python library designed to make web scraping easy. It features its own rapid parsing engine and fetchers to handle web scraping challenges, with 92% test coverage and full type hints coverage.

### **Why Scrapling for Your Project**

Your current issues:
- ‚ùå RacingPost: 406 (Not Acceptable)
- ‚ùå Oddschecker: 403 (Forbidden)  
- ‚ùå Timeform: 500 (Server Error)
- ‚ùå Brisnet: Timeout/503

Scrapling's StealthyFetcher can easily bypass all types of Cloudflare's Turnstile/Interstitial automatically, bypass CDP runtime leaks and WebRTC leaks, isolate JS execution, remove many Playwright fingerprints, and make requests look as if they came from Google's search page of the requested website.

---

## Installation

### Step 1: Install Scrapling with Fetchers

```bash
# Install with fetcher support
pip install "scrapling[fetchers]"

# Install browser dependencies (Chromium + Modified Firefox)
scrapling install
```

The base installation only includes the parser engine without fetchers or commandline dependencies. For actual scraping, you need to install fetchers' dependencies and their browser dependencies.

### Step 2: Add to requirements.txt

```text
# Add to web_service/backend/requirements.txt
scrapling[fetchers]>=0.3.7
```

---

## Three Fetcher Types

Scrapling provides three main fetcher classes: Fetcher for fast HTTP requests with TLS fingerprinting and HTTP/3 support, DynamicFetcher for full browser automation with Playwright's Chromium, and StealthyFetcher for advanced stealth capabilities with fingerprint spoofing to bypass anti-bot protections.

### 1. **Fetcher** - Fast HTTP (Like httpx, but stealthier)
```python
from scrapling.fetchers import Fetcher

# Simple HTTP request with browser TLS fingerprinting
page = Fetcher.fetch('https://www.racingpost.com/racecards/2026-01-24')
print(page.status)  # 200
```

### 2. **DynamicFetcher** - Browser Automation (Like Selenium/Playwright)
```python
from scrapling.fetchers import DynamicFetcher

# Renders JavaScript, waits for content
page = DynamicFetcher.fetch(
    'https://www.timeform.com/horse-racing/racecards/2026-01-24',
    headless=True,
    network_idle=True  # Wait for JS to finish
)
```

### 3. **StealthyFetcher** - Maximum Stealth (Modified Firefox)
```python
from scrapling.fetchers import StealthyFetcher

# Bypasses Cloudflare, bot detection
page = StealthyFetcher.fetch(
    'https://www.oddschecker.com/horse-racing/2026-01-24',
    headless=True,
    solve_cloudflare=True,  # Auto-solve Cloudflare challenges
    humanize=True,          # Add human-like delays
    google_search=True      # Appear to come from Google
)
```

---

## Integration into Fortuna Adapters

### Example: Update RacingPostAdapter

**Current code (failing with 406):**
```python
class RacingPostAdapter(BaseAdapterV3):
    async def _fetch_data(self, date: str) -> Optional[dict]:
        index_url = f"/racecards/{date}"
        index_response = await self.make_request(
            self.http_client, "GET", index_url, headers=self._get_headers()
        )
        # Returns 406 - Not Acceptable
```

**Updated with Scrapling:**
```python
from scrapling.fetchers import StealthyFetcher

class RacingPostAdapter(BaseAdapterV3):
    async def _fetch_data(self, date: str) -> Optional[dict]:
        """Fetch using Scrapling's StealthyFetcher for anti-bot bypass."""
        full_url = f"{self.base_url}/racecards/{date}"
        
        try:
            # Use StealthyFetcher to bypass bot detection
            page = await StealthyFetcher.async_fetch(
                full_url,
                headless=True,
                network_idle=True,
                google_search=True,  # Referrer from Google
                block_webrtc=True,   # Block WebRTC leaks
                humanize=True        # Human-like timing
            )
            
            if page.status != 200:
                self.logger.warning(f"Non-200 status: {page.status}")
                return None
            
            # Save debug HTML
            try:
                with open("racingpost_debug.html", "w", encoding="utf-8") as f:
                    f.write(page.text)
            except Exception as e:
                self.logger.warning("Failed to save debug HTML", error=str(e))
            
            # Parse links - Scrapling has built-in CSS selectors
            links = page.css('a[href*="/racecards/"]')
            race_urls = [link.attrib['href'] for link in links]
            
            # Fetch individual race pages
            html_pages = []
            for race_url in race_urls:
                race_page = await StealthyFetcher.async_fetch(
                    f"{self.base_url}{race_url}",
                    headless=True
                )
                html_pages.append((race_url, race_page.text))
            
            return {"pages": html_pages, "date": date}
            
        except Exception as e:
            self.logger.error(f"StealthyFetcher failed: {e}")
            return None
```

---

## Session Management (Faster for Multiple Requests)

For persistent session support with cookie and state management across requests, use FetcherSession, StealthySession, and DynamicSession classes.

### Using StealthySession for Multiple Races

```python
from scrapling.fetchers import StealthySession

class RacingPostAdapter(BaseAdapterV3):
    async def _fetch_data(self, date: str) -> Optional[dict]:
        """Fetch using persistent session - MUCH faster."""
        full_url = f"{self.base_url}/racecards/{date}"
        
        # Session stays open, browser reused
        async with StealthySession(
            headless=True,
            google_search=True,
            block_webrtc=True
        ) as session:
            # Fetch index page
            index_page = await session.async_fetch(
                full_url,
                network_idle=True
            )
            
            # Extract race URLs
            race_links = index_page.css('a[href*="/racecards/"]')
            race_urls = [link.attrib['href'] for link in race_links]
            
            # Fetch all race pages using SAME browser instance
            html_pages = []
            for race_url in race_urls:
                race_page = await session.async_fetch(
                    f"{self.base_url}{race_url}"
                )
                html_pages.append((race_url, race_page.text))
            
            return {"pages": html_pages, "date": date}
```

**Benefits:**
- ‚úÖ 5-10x faster (browser stays open)
- ‚úÖ Maintains cookies/session state
- ‚úÖ More "human-like" behavior

---

## Adaptive Selectors (Survives Website Changes!)

Scrapling features smart element tracking that automatically relocates elements even after site redesigns. When website structure changes, pass adaptive=True and Scrapling still finds elements.

```python
# First scrape - save element patterns
products = page.css('.product', auto_save=True)

# Later, if website redesigns and changes .product to .race-card
# Scrapling can still find them!
products = page.css('.product', adaptive=True)
```

---

## Complete Adapter Template

Here's a complete adapter using best practices:

```python
# web_service/backend/adapters/racing_post_adapter_v4.py
from scrapling.fetchers import StealthySession
from typing import Optional, List
from ..models import Race
from .base_adapter_v3 import BaseAdapterV3

class RacingPostAdapterV4(BaseAdapterV3):
    """
    RacingPost adapter using Scrapling's StealthyFetcher
    for anti-bot bypass.
    """
    
    SOURCE_NAME = "RacingPost"
    BASE_URL = "https://www.racingpost.com"
    
    def __init__(self, config=None):
        super().__init__(
            source_name=self.SOURCE_NAME,
            base_url=self.BASE_URL,
            config=config
        )
    
    async def _fetch_data(self, date: str) -> Optional[dict]:
        """Fetch with Scrapling StealthySession for maximum success."""
        try:
            async with StealthySession(
                headless=True,
                google_search=True,
                block_webrtc=True,
                humanize=True
            ) as session:
                # Fetch index
                index_url = f"{self.BASE_URL}/racecards/{date}"
                self.logger.info(f"Fetching index: {index_url}")
                
                index_page = await session.async_fetch(
                    index_url,
                    network_idle=True,
                    wait_selector='a[href*="/racecards/"]'  # Wait for links
                )
                
                if index_page.status != 200:
                    self.logger.error(f"Index failed: {index_page.status}")
                    return None
                
                # Debug output
                try:
                    with open("racingpost_debug.html", "w", encoding="utf-8") as f:
                        f.write(index_page.text)
                except Exception as e:
                    self.logger.warning(f"Debug save failed: {e}")
                
                # Extract race URLs using Scrapling's CSS selector
                race_links = index_page.css('a[href*="/racecards/"]')
                race_urls = list(set([
                    link.attrib.get('href', '') 
                    for link in race_links 
                    if '/racecards/' in link.attrib.get('href', '')
                ]))
                
                self.logger.info(f"Found {len(race_urls)} race URLs")
                
                if not race_urls:
                    self.logger.warning("No race URLs found")
                    return None
                
                # Fetch individual races
                html_pages = []
                for race_url in race_urls[:20]:  # Limit to avoid timeout
                    try:
                        race_page = await session.async_fetch(
                            f"{self.BASE_URL}{race_url}"
                        )
                        html_pages.append((race_url, race_page.text))
                    except Exception as e:
                        self.logger.warning(f"Race {race_url} failed: {e}")
                        continue
                
                return {"pages": html_pages, "date": date}
                
        except Exception as e:
            self.logger.error(f"Fetch failed: {e}", exc_info=True)
            return None
    
    def _parse_races(self, raw_data: dict) -> List[Race]:
        """Parse races from HTML - same as before."""
        # Your existing parsing logic here
        pass
```

---

## Configuration Options

### For Sites with Cloudflare:
```python
page = StealthyFetcher.fetch(
    url,
    solve_cloudflare=True,  # Auto-solve Turnstile/Interstitial
    humanize=True,           # Random delays
    headless=True,           # Hide browser
    google_search=True       # Referrer spoofing
)
```

### For Sites with Heavy JavaScript:
```python
page = DynamicFetcher.fetch(
    url,
    network_idle=True,            # Wait for all network requests
    wait_selector='.race-card',   # Wait for specific element
    page_action=lambda page: page.wait_for_timeout(2000)  # Custom wait
)
```

### For Simple Static Sites (Fastest):
```python
page = Fetcher.fetch(
    url,
    impersonate='chrome',    # Chrome TLS fingerprint
    stealthy_headers=True    # Browser-like headers
)
```

---

## Gradual Migration Strategy

### Phase 1: Test on Failing Adapters (Week 1)
1. Start with **RacingPostAdapter** (406 errors)
2. Add **OddscheckerAdapter** (403 errors)
3. Add **TimeformAdapter** (500 errors)

### Phase 2: Expand to Others (Week 2)
4. **BrisnetAdapter** (timeouts)
5. **EquibaseAdapter** (404s)

### Phase 3: Optimize Performance (Week 3)
6. Convert successful adapters to use **Session classes**
7. Enable **adaptive selectors** for future-proofing
8. Add **caching** for repeated requests

---

## Performance Comparison

In benchmarks, Scrapling is on par with Scrapy and slightly faster than lxml for parsing, while being 4 times faster than PyQuery. All benchmarks represent averages of 100+ runs.

**Your current setup (httpx):**
- ‚ùå Blocked by bot detection
- ‚ùå No JavaScript rendering
- ‚ö†Ô∏è Manual header management

**With Scrapling:**
- ‚úÖ Auto-bypasses bot detection
- ‚úÖ Renders JavaScript automatically
- ‚úÖ Manages fingerprints automatically
- ‚úÖ 4-10x faster parsing than BeautifulSoup

---

## Troubleshooting

### If Still Blocked:
```python
# Try these escalating options:
page = StealthyFetcher.fetch(
    url,
    headless=False,          # 1. Try visible browser
    real_chrome=True,        # 2. Use real Chrome instead of Chromium
    hide_canvas=True,        # 3. Hide canvas fingerprinting
    addons=['/path/addon'],  # 4. Add browser extensions
    proxy='http://...'       # 5. Use proxy
)
```

### Check Stealth Effectiveness:
```python
# Test against bot detection
page = StealthyFetcher.fetch(
    'https://www.browserscan.net/bot-detection',
    headless=True
)
print(page.status == 200)  # Should be True if undetected
```

---

## Cost-Benefit Analysis

**Current Status:**
- 4/4 adapters failing
- 0 races collected
- Manual debugging required

**With Scrapling:**
- **Setup time:** 2-3 hours
- **Expected success rate:** 80-95% (based on anti-bot bypass capabilities)
- **Maintenance:** Minimal (adaptive selectors)
- **Cost:** Free, open source

**Alternative (Proxy Services):**
- **Setup time:** 1 hour
- **Expected success rate:** 90-99%
- **Monthly cost:** $50-500
- **Maintenance:** Minimal

---

## Recommended Next Steps

1. **Install Scrapling** in your environment
2. **Convert RacingPostAdapter** first (currently returning 406)
3. **Test in GitHub Actions** - save debug HTML
4. **Measure success rate** - compare before/after
5. **Gradually expand** to other failing adapters
6. **Optimize with Sessions** once working

---

## Additional Resources

- üìö [Official Scrapling Documentation](https://scrapling.readthedocs.io/)
- üíª [GitHub Repository](https://github.com/D4Vinci/Scrapling)
- üéì [Web Scraping Club Tutorial](https://substack.thewebscraping.club/p/scrapling-hands-on-guide)
- üîß [PyPI Package](https://pypi.org/project/scrapling/)

---

## Summary

Scrapling can solve your blocking issues because:

1. ‚úÖ **StealthyFetcher** bypasses Cloudflare, bot detection
2. ‚úÖ **Modified Firefox** with fingerprint spoofing
3. ‚úÖ **Adaptive selectors** survive website changes
4. ‚úÖ **Fast parsing** - outperforms BeautifulSoup
5. ‚úÖ **Async support** - works with your existing async code
6. ‚úÖ **Session management** - maintains state, faster
7. ‚úÖ **Free & open source** - no monthly costs

**Bottom line:** You're getting 406, 403, and 500 errors because websites detect your bot. Scrapling specifically addresses this with advanced stealth capabilities that make your scrapers undetectable.
