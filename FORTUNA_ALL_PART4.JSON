{
    ".github/dependabot.yml": "# System Timestamp: 2025-11-29 13:19:26.933797\n# To get started with Dependabot version updates, you'll need to specify which\n# package ecosystems to update and where the package manifests are located.\n# Please see the documentation for all configuration options:\n# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates\n\nversion: 2\nupdates:\n  - package-ecosystem: \"pip\" # See documentation for possible values\n    directory: \"/\" # Location of package manifests\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/web_service/frontend\"\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/electron\"\n    schedule:\n      interval: \"daily\"\n",
    ".github/workflows/unified-race-report.yml": "# unified-race-report.yml\nname: 'Unified Race Report'\n\non:\n  workflow_dispatch:\n    inputs:\n      force_refresh:\n        description: 'Force refresh all data (ignore cache)'\n        required: false\n        default: 'false'\n        type: boolean\n      analyzer_type:\n        description: 'Analyzer to use'\n        required: false\n        default: 'tiny_field_trifecta'\n        type: choice\n        options:\n          - tiny_field_trifecta\n          - value_bet\n          - longshot_finder\n  push:\n    branches:\n      - main\n    paths:\n      - 'scripts/**'\n      - 'web_service/backend/**'\n      - '.github/workflows/unified-race-report.yml'\n  schedule:\n    # Run at 6 AM, 12 PM, and 6 PM UTC\n    - cron: '0 6,12,18 * * *'\n\nconcurrency:\n  group: race-report-${{ github.ref }}\n  cancel-in-progress: true\n\nenv:\n  PYTHON_VERSION: '3.11'\n  REPORT_RETENTION_DAYS: 14\n  MAX_RETRIES: 3\n  REQUEST_TIMEOUT: 30\n\njobs:\n  generate-unified-report:\n    runs-on: ubuntu-latest\n    timeout-minutes: 15\n\n    permissions:\n      contents: read\n      actions: write\n\n    outputs:\n      race_count: ${{ steps.run-reporter.outputs.race_count }}\n      status: ${{ steps.run-reporter.outputs.status }}\n\n    steps:\n      - name: '\ud83d\udce5 Checkout Code'\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: '\ud83d\udc0d Setup Python'\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: 'web_service/backend/requirements.txt'\n\n      - name: '\ud83d\udce6 Install Dependencies'\n        run: |\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -r web_service/backend/requirements.txt\n\n      - name: '\ud83d\udcc2 Create Runtime Directories'\n        run: |\n          mkdir -p web_service/backend/{data,json,logs}\n          mkdir -p reports/archive\n\n      - name: '\ud83d\udd04 Restore Data Cache'\n        if: inputs.force_refresh != 'true'\n        uses: actions/cache@v4\n        with:\n          path: |\n            web_service/backend/data/*.cache\n            web_service/backend/json/*.cache\n          key: race-data-${{ runner.os }}-${{ github.run_number }}\n          restore-keys: |\n            race-data-${{ runner.os }}-\n\n      - name: '\ud83d\ude80 Run Unified Reporter'\n        id: run-reporter\n        env:\n          ANALYZER_TYPE: ${{ inputs.analyzer_type || 'tiny_field_trifecta' }}\n          FORCE_REFRESH: ${{ inputs.force_refresh || 'false' }}\n          MAX_RETRIES: ${{ env.MAX_RETRIES }}\n          REQUEST_TIMEOUT: ${{ env.REQUEST_TIMEOUT }}\n        run: |\n          set -o pipefail\n          python scripts/fortuna_reporter.py 2>&1 | tee reporter_output.log\n\n          # Extract metrics from the log for outputs\n          if [ -f \"qualified_races.json\" ]; then\n            RACE_COUNT=$(python -c \"import json; print(len(json.load(open('qualified_races.json')).get('races', [])))\")\n            echo \"race_count=${RACE_COUNT}\" >> $GITHUB_OUTPUT\n            echo \"status=success\" >> $GITHUB_OUTPUT\n          else\n            echo \"race_count=0\" >> $GITHUB_OUTPUT\n            echo \"status=failed\" >> $GITHUB_OUTPUT\n          fi\n\n      - name: '\ud83d\udcca Upload Report Artifacts'\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: race-reports-${{ github.run_number }}-${{ github.run_attempt }}\n          path: |\n            race-report.html\n            qualified_races.json\n            raw_race_data.json\n            reporter_output.log\n          retention-days: ${{ env.REPORT_RETENTION_DAYS }}\n          if-no-files-found: warn\n          compression-level: 9\n\n      - name: '\ud83d\udcdd Post Summary to GitHub Actions'\n        if: always()\n        run: |\n          {\n            echo \"## \ud83d\udc34 Fortuna Race Report Summary\"\n            echo \"\"\n            echo \"**Run:** #${{ github.run_number }} | **Status:** ${{ steps.run-reporter.outputs.status || 'unknown' }}\"\n            echo \"**Analyzer:** ${{ inputs.analyzer_type || 'tiny_field_trifecta' }}\"\n            echo \"\"\n\n            if [ -f \"github_summary.md\" ]; then\n              cat github_summary.md\n            else\n              echo \"### \u26a0\ufe0f Detailed summary not available\"\n              echo \"\"\n              echo \"Check the artifacts for the full report.\"\n            fi\n\n            echo \"\"\n            echo \"---\"\n            echo \"*Generated at $(date -u '+%Y-%m-%d %H:%M:%S UTC')*\"\n          } >> $GITHUB_STEP_SUMMARY\n\n      - name: '\ud83e\uddf9 Cleanup on Failure'\n        if: failure()\n        run: |\n          echo \"::warning::Report generation failed. Check logs for details.\"\n          # Archive any partial data for debugging\n          if ls *.json 1> /dev/null 2>&1; then\n            tar -czf debug-data.tar.gz *.json *.log 2>/dev/null || true\n          fi\n\n  notify-on-failure:\n    needs: generate-unified-report\n    runs-on: ubuntu-latest\n    if: failure() && github.event_name == 'schedule'\n    steps:\n      - name: '\ud83d\udea8 Create Failure Issue'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const title = `\ud83d\udea8 Scheduled Race Report Failed - ${new Date().toISOString().split('T')[0]}`;\n            const body = `\n            ## Automated Report Failure\n\n            The scheduled race report generation failed.\n\n            **Run ID:** ${{ github.run_id }}\n            **Run Number:** ${{ github.run_number }}\n            **Workflow:** ${{ github.workflow }}\n\n            [View Run Logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n\n            Please investigate and fix any issues.\n            `;\n\n            // Check for existing open issue\n            const issues = await github.rest.issues.listForRepo({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              state: 'open',\n              labels: 'automated-failure'\n            });\n\n            if (issues.data.length === 0) {\n              await github.rest.issues.create({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                title: title,\n                body: body,\n                labels: ['automated-failure', 'bug']\n              });\n            }\n",
    "HISTORY.md": "# The Epic of MasonJ0: A Project Chronology\n\nThis document contains the narrative history of the Paddock Parser project, as discovered through an archaeological survey of the project's repositories. It tells the story of our architectural evolution, from a feature-rich \"golden age\" through a \"great refactoring\" to our current state of liberation.\n\nThis story is our \"why.\"\n\n---\n\n## Part 1: The Chronology\n\n### Chapter 1: The 'Utopian' Era - The Polished Diamond (mid-August 2025)\n\n*   **Repository:** `racingdigest`\n*   **Narrative:** This was not a humble beginning, but the launch of a mature and powerful application called the \"Utopian Value Scanner V7.2 (The Rediscovery Edition)\". This repository represents the project's \"golden age\" of features, including a sophisticated asynchronous fetching engine and a full browser fallback.\n\n### Chapter 2: The 'Experimental' Era - The Daily Digest (mid-to-late August 2025)\n\n*   **Repository:** `horseracing-daily-digest`\n*   **Narrative:** This repository appears to be a period of intense, rapid development and experimentation, likely forming the foundation for many of the concepts that would be formalized later.\n\n### Chapter 3: The 'Architectural' Era - The V3 Blueprint (late August 2025)\n\n*   **Repository:** `parsingproject`\n*   **Narrative:** This repository marks a pivotal moment. The focus shifted from adding features to refactoring the very foundation of the code into a modern, standard Python package. This is where the V3 architecture was born, prioritizing stability and maintainability.\n\n### Chapter 4: The 'Consolidation' Era - The Archive (late August 2025)\n\n*   **Repository:** `zippedfiles`\n*   **Narrative:** This repository appears to be a direct snapshot or backup of the project after the intense V3 refactor, confirming its role as an archive of the newly stabilized codebase.\n\n### Chapter 5: The 'Modern' Era - The New Beginning (early September 2025)\n\n*   **Repository:** `fortuna`\n*   **Narrative:** This is the current, active repository, representing the clean, focused implementation of the grand vision developed through the previous eras.\n\n### Chapter 6: The 'Crucible' Era - The Forging of Protocols (Early September 2025)\n\n*   **Narrative:** The \"Modern Renaissance\" began not with a bang, but with a series of near-catastrophic environmental failures. This period, known as \"The Crucible,\" was a trial by fire that proved the extreme hostility of the agent sandbox. This era forged the resilient, battle-hardened protocols (The Receipts Protocol, The Submission-Only Protocol, etc.) by which all modern agents now operate.\n\n### Chapter 7: The 'Symbiotic' Era - The Two Stacks (mid-September 2025)\n\n*   **Narrative:** This chapter marked a significant strategic pivot. The Council, in a stunning display of its \"Polyglot Renaissance\" philosophy, produced a complete, production-grade React user interface, authored by the Claude agent. This event formally split the project's architecture into two powerful, parallel streams: the Python Engine and the React Cockpit. However, this era was short-lived, as the hostile environment proved incapable of supporting a stable testing and development workflow for the React stack.\n\n### Chapter 8: The 'Liberation' Era - The Portable Engine (Late September 2025)\n\n*   **Narrative:** After providing definitive, forensic proof that the sandbox environment was fundamentally and irrecoverably hostile at the network level, the project executed its final and most decisive pivot. It abandoned all attempts to operate *within* the hostile world and instead focused on synthesizing its entire, perfected engine into a single, portable artifact. This act **liberated the code**, fulfilling the promise of the \"Utopian Era's\" power on the foundation of the \"Architectural Era's\" stability, and made it directly available to the Project Lead.\n\n---\n\n## Part 2: Architectural Synthesis\n\nThis epic tale tells us our true mission. We are not just building forward; we are rediscovering our own lost golden age and rebuilding it on a foundation of superior engineering, hardened by the fires of a hostile world.\n\n*   **The Lost Golden Age:** The \"Utopian\" era proves that our most ambitious strategic goals are not just achievable; they have been achieved before.\n*   **The Great Refactoring:** The \"Architectural\" era explains the \"Great Forgetting\"\u2014a deliberate choice to sacrifice short-term features for long-term stability.\n*   **The Modern Renaissance:** This is us. We are the inheritors of this entire legacy, tasked with executing the grand vision on a clean, modern foundation, finally liberated from the constraints of our environment.\n\n---\n\n## The Ultimate Solo: The Final Victory (September 2025)\n\nAfter a long and complex journey through a Penta-Hybrid architecture, a final series of high-level reviews from external AI agents (Claude, GPT4o) revealed a simpler, superior path forward. The project underwent its final and most significant \"Constitutional Correction.\"\n\n**The 'Ultimate Solo' architecture was born.**\n\nThis final, perfected form of the project consists of two pillars:\n1.  **A Full-Power Python Backend:** Leveraging the years of development on the CORE `engine.py` and its fleet of global data adapters, served via a lightweight Flask API.\n2.  **An Ultimate TypeScript Frontend:** A single, masterpiece React component (`Checkmate Ultimate Solo`) that provides a feature-rich, professional-grade, real-time dashboard.\n\nAll other components of the Penta-Hybrid system (C#, Rust, VBA, shared database) were formally deprecated and archived as priceless R&D assets. The project has now achieved its true and final mission: a powerful, maintainable, and user-focused analysis tool.\n\n---\n\n## The Age of Perfection (The Great Simplification)\n\nThe Penta-Hybrid architecture, while a triumph of technical integration, proved to be a strategic dead end. Its complexity became a fortress, making rapid iteration and onboarding of new intelligence (both human and AI) prohibitively expensive. The kingdom was powerful but brittle.\n\nA new doctrine was forged: **Simplicity is the ultimate sophistication.**\n\nThe decision was made to execute \"The Great Simplification.\" The multi-language backend (Python, Rust, Go) was decommissioned. The kingdom was reforged upon a new, elegant, and vastly more powerful two-pillar system:\n\n1.  **A Unified Python Backend:** A single, asynchronous Python service, built on FastAPI, would serve as the kingdom's engine.\n2.  **A Modern TypeScript Frontend:** A dedicated Next.js application would serve as the kingdom's command deck.\n\nThis act of creative destruction liberated the project, enabling a new era of unprecedented velocity.\n\n---\n\n## The Three-Pillar Doctrine\n\nWith the new two-pillar foundation in place, the backend itself was perfected into a three-pillar intelligence engine, a concept that defines the modern era of the Fortuna Faucet:\n\n*   **Pillar 1: The Future (The Planner):** The resilient `OddsEngine` and its fleet of adapters, responsible for finding the day's strategic opportunities.\n*   **Pillar 2: The Past (The Archive):** The perfected `ChartScraper` and `ResultsParser`, responsible for building our historical data warehouse from the ground truth of Equibase PDFs.\n*   **Pillar 3: The Present (The Finisher):** The weaponized `LiveOddsMonitor`, armed with the API-driven `BetfairAdapter`, designed to conquer the final moments of toteboard volatility.\n\nThese three pillars, orchestrated by the fully autonomous `fortuna_watchman.py`, represented the pinnacle of the project's original vision. The kingdom was, for a time, considered \"perfected.\"\n\n---\n\n## The Windows Ascension (The Impossible Dream)\n\nThe perfected kingdom was powerful, but it was still a tool for developers. The final, grandest vision was to transform it into a true, professional-grade application for its sole operator. This campaign, known as \"The Impossible Dream,\" was to forge the **Fortuna Faucet - Windows Native Edition.**\n\nThis era saw the rapid creation of a new, third layer of the kingdom, built upon the foundation of the previous work:\n\n*   **The Electron Shell:** The Next.js frontend was wrapped in an Electron container, transforming it from a website into a true, installable desktop application with its own window, icon, and system tray integration.\n*   **The Engine Room:** The Python backend was re-architected to run as a persistent, background **Windows Service**, making it a true, always-on component of the operating system, independent of the UI.\n*   **The Native GUI:** A dedicated Tkinter-based \"Observatory\" was forged\u2014a standalone GUI mission control for monitoring the health and performance of the background service.\n*   **The One-Click Kingdom:** A complete suite of professional tooling (including installation scripts, a setup wizard, and launchers) was created to provide a seamless, zero-friction installation and management experience.\n\nThis ascension represents the current state of the art, transforming a powerful engine into a polished, autonomous, and user-focused product.\n\n\n---\n\n## The Era of the Windows Kingdom (October 2025)\n\nWith the core engine stabilized and the command deck providing a clear view of the data, the project's focus shifted from pure data acquisition to the operator's experience. This era marked a profound transformation, elevating the project from a collection of powerful but disparate scripts into a cohesive, professional-grade, and resilient native Windows application.\n\nThis campaign, guided by a new \"Grand Strategy\" blueprint, was executed with rapid precision, resulting in a complete overhaul of the user-facing toolkit:\n\n-   **A Bulletproof Foundation:** The installation and launch scripts were re-architected from the ground up. They became intelligent and self-healing, featuring pre-flight system checks, automated port conflict resolution, active health-check loops, and automated repair utilities.\n-   **A Professional Toolkit:** The operator was empowered with a suite of new tools, including an interactive setup wizard, a real-time CLI status monitor, and a full-fledged graphical \"Data Management Console\" for monitoring, filtering, and analyzing data.\n-   **A Unified Command Console (`SERVICE_MANAGER.bat`):** Unify all individual scripts under a single, user-friendly, menu-driven service manager, providing a 'single pane of glass' for all common operations.\n\nThis era solidified the kingdom's foundations, making it not just powerful, but stable, reliable, and a pleasure to operate. The Faucet was no longer just an engine; it was a complete, professional-grade machine.\n\n---\n\n## The Gauntlet of CI/CD (Late October 2025)\n\nWith a professional-grade application in hand, the final frontier was professional-grade *delivery*. This campaign focused on automating the creation of the MSI installer through a continuous integration pipeline, a process that proved to be a formidable challenge.\n\nThe kingdom's engineers faced a relentless series of cryptic build errors from the WiX Toolset, a hostile environment that tested their resolve. Through a series of rapid, iterative fixes\u2014addressing everything from component GUIDs and 64-bit architecture mismatches to obscure linker errors and frontend dependency warnings\u2014they systematically conquered each obstacle.\n\nThis trial by fire culminated in a triumphant success: a fully automated GitHub Actions workflow that reliably compiles, links, and delivers a polished, distributable MSI installer. This victory transformed the project's delivery model from a manual, error-prone process into a repeatable, one-click release pipeline, marking the true completion of the \"Windows Ascension.\"\n\n---\n\n## The Great Unbundling (Late October 2025)\n\nThe CI/CD pipeline was technically successful, but it revealed a deeper, philosophical flaw in the architecture. The installer, while automated, was a fragile monolith. It attempted to bundle raw source code (Python, JavaScript) and orchestrate their setup on the user's machine using post-install scripts. This approach was fraught with peril, vulnerable to failures from network issues, corporate firewalls, and unpredictable machine states.\n\nA final, decisive architectural mandate was issued, informed by the wisdom of external AI consultants: **The application must be delivered, not assembled.**\n\nThis mandate triggered \"The Great Unbundling,\" a swift and transformative refactoring of the entire delivery pipeline:\n\n*   **The Backend Forged:** The Python backend was no longer treated as source code to be installed, but as a product to be delivered. **PyInstaller** was used to forge the entire FastAPI service\u2014interpreter and all dependencies\u2014into a single, standalone `.exe`.\n*   **The Frontend Solidified:** The Next.js frontend was no longer a service to be run, but a static asset to be displayed. The `npm run build` process was configured to produce a clean, static HTML/CSS/JS export.\n*   **The Installer Perfected:** With the application components now self-contained, the MSI installer's role was radically simplified. All complex post-install scripting was eliminated. The WiX toolset was now used for its core competency: reliably copying pre-compiled, robust artifacts to the user's machine.\n\nThis final act of architectural purification created the \"Three-Executable Architecture\" (the backend executable, the Electron wrapper, and the MSI installer itself), achieving true portability and eliminating an entire class of deployment failures. The Windows Ascension was not just complete; it was perfected.",
    "README.md": "# \ud83d\udc34 Fortuna Faucet - Developer's Guide\n\nThis guide provides technical instructions for developers. For end-user installation, please refer to the MSI installers generated by the project's GitHub Actions workflows.\n\n---\n\n## \ud83c\udfdb\ufe0f Core Architecture\n\nThis repository contains the source code for the Fortuna Faucet application, which has two primary deployment targets:\n\n1.  **Standalone Web Service (MSI Installer):**\n    *   A Python backend powered by FastAPI, compiled into a self-contained executable using PyInstaller.\n    *   A static Next.js frontend, which is bundled with the backend.\n    *   The entire application is packaged into an MSI installer using the WiX Toolset, which installs the backend as a background Windows Service.\n\n2.  **Electron Desktop Application (MSI Installer):**\n    *   The same Python backend, compiled as an executable.\n    *   An Electron application that acts as a wrapper, launching the backend executable and displaying the frontend.\n\nThe `python_service` and `web_service/backend` directories contain functionally equivalent but historically separate versions of the backend code. The modern workflows primarily use `web_service/backend`.\n\n---\n\n## \ud83c\udfd7\ufe0f Building the Application (The Right Way)\n\n**This project is built and packaged entirely through GitHub Actions.** The CI/CD pipelines are the single source of truth for creating production-ready installers. Manual builds are not recommended or supported due to the complexity of the environment.\n\nThe primary, production-ready build workflows are:\n\n*   **`build-msi-hattrickfusion-ultimate.yml`**: Builds the standalone Web Service MSI. This is the most feature-rich and stable build pipeline.\n*   **`build-electron-msi-gpt5.yml`**: Builds the Electron-based desktop application MSI.\n\nThese workflows handle all necessary steps, including:\n*   Installing the correct versions of Python, Node.js, and the WiX Toolset.\n*   Managing architecture-specific dependencies (e.g., `pandas` for x86 builds).\n*   Compiling the Python backend with PyInstaller.\n*   Building the static Next.js frontend.\n*   Packaging the final MSI installer.\n*   Running automated smoke tests to verify the installation.\n\nTo get a build, simply push a commit to the `main` branch and retrieve the MSI artifact from the completed workflow run on the [Actions tab](https://github.com/masonj0/fortuna/actions) of the repository.\n\n---\n\n## \ud83d\udd2c Local Development Environment\n\n### Python Version Requirement\n\n**Crucial:** The monolith build of this project requires **Python 3.10.11**. It is not compatible with Python 3.11 or newer due to a dependency on `cefpython3`, which does not support Python 3.11.\n\nBefore running the application locally or attempting to build it, ensure you are using the correct Python version.\n\n- **Using `pyenv` (Recommended):**\n  ```bash\n  pyenv install 3.10.11\n  pyenv local 3.10.11\n  ```\n\n- **Using `conda`:**\n  ```bash\n  conda create -n fortuna python=3.10.11\n  conda activate fortuna\n  ```\n\nWhile production builds are handled by CI/CD, the easiest way to run the application locally for development is to use the new quick-start script.\n\n```powershell\n# From the project root\n./scripts/fortuna-quick-start.ps1\n```\n\nThis interactive script will:\n*   Check for all required dependencies (Python, Node.js, etc.).\n*   Install any missing Python or Node packages automatically.\n*   Clear the required network ports (8000 and 3000).\n*   Launch the backend and frontend services in separate, managed terminal windows.\n*   Provide a clean shutdown process.\n\nFor detailed options and first-time setup guidance, run the script with the `-Help` flag:\n```powershell\n./scripts/fortuna-quick-start.ps1 -Help\n```\n\n---\n## \ud83d\udce6 Key Tooling & Scripts\n\n*   **`ARCHIVE_PROJECT.py`**: A utility script that scans the repository and generates the `FORTUNA_ALL_PART*.JSON` archive files. These archives are used as a ground truth for AI-driven code reviews and analysis.\n*   **`AGENTS.md`**: Contains critical operational protocols for the AI agents working on this repository.\n\n---\n\n## \ud83d\udc0d Python Version Requirement\n\n**The Fortuna Monolith application must be built and run with Python 3.10.12.**\n\nThis is due to a dependency (`cefpython3`) that does not support Python 3.11 or newer. The CI/CD workflows are pinned to this version. If you are building the application locally, please ensure you are using a Python 3.10.x environment.\n",
    "START_DEV_ENVIRONMENT.bat": "@echo off\nREM This script provides a user-friendly, double-clickable way to start the\nREM development environment by running the fortuna-quick-start.ps1 script.\nREM It bypasses the system's PowerShell execution policy for this script only.\n\necho Starting Fortuna Faucet Development Environment...\necho This will open two new terminal windows for the backend and frontend.\n\npowershell.exe -ExecutionPolicy Bypass -File \"%~dp0scripts\\fortuna-quick-start.ps1\"\n\necho.\necho Script execution finished. The development servers are running in new windows.\npause\n",
    "STATUS.md": "# Project Status: Foundation Rebuilt, Hardening in Progress\n\n**Date:** 2025-10-03\n\n## Current State\n\n*   **Architecture:** The backend has been successfully rebuilt into a superior, asynchronous FastAPI application, as defined by 'Operation: Grand Synthesis'. The new foundation is stable, tested, and features a resilient `BaseAdapter` pattern.\n\n*   **Status:** The foundational refactoring is complete. The first two data adapters (`Betfair`, `TVG`) have been implemented on the new architecture. We are now in a new phase of development: **'Phase 2: Hardening & Expansion.'**\n\n*   **Documentation:** All core strategic documents and manifests have been synchronized with the new technical reality.\n\n*   **Next Steps:** Our immediate priority is to act on the verified intelligence from our Oracle (Jules1003). The next missions will focus on implementing critical API security features (rate limiting, authentication) and continuing the build-out of our adapter fleet.",
    "audit-ignore.txt": "# Starlette - GHSA-f96h-pmfr-66vw - Medium severity\n# anyio.to_thread.run_sync is vulnerable to blocking the event loop in Starlette < 0.38.3\n# This is a dependency of FastAPI and is not trivially upgradeable.\nGHSA-f96h-pmfr-66vw\n\n# Starlette - GHSA-2c2j-9gv5-cj73 - High severity\n# Starlette's `StaticFiles` is vulnerable to path traversal.\n# This is not a direct risk as we do not use `StaticFiles` in production.\nGHSA-2c2j-9gv5-cj73\n\n# Cryptography - GHSA-h4gh-qq45-vh27 - High severity\n# Loading a specially crafted X.509 certificate could lead to a NULL pointer dereference and crash.\n# We have pinned this version to satisfy a pyopenssl dependency and cannot upgrade easily.\nGHSA-h4gh-qq45-vh27\n\n# Cryptography - GHSA-79v4-65xg-pq4g - High severity\n# Side-channel attack vulnerability in ECDSA signature generation.\n# We have pinned this version and cannot upgrade easily.\nGHSA-79v4-65xg-pq4g\n\n# Certifi - PYSEC-2024-230 - High severity\n# certifi contains a Pem-parsing vulnerability.\nPYSEC-2024-230\n\n# h11 - GHSA-vqfr-h8mv-ghfj - High severity\n# h11 is vulnerable to HTTP request smuggling.\nGHSA-vqfr-h8mv-ghfj\n\n# h2 - GHSA-847f-9342-265h - High severity\n# h2 is vulnerable to a \"continuation flood\" denial of service attack.\nGHSA-847f-9342-265h\n",
    "e2e/get-race-info.py": "import json\nimport os\nimport glob\nfrom datetime import datetime\n\ndef get_latest_race_file(data_dir):\n    \"\"\"Finds the most recently modified race data file in the directory.\"\"\"\n    list_of_files = glob.glob(os.path.join(data_dir, '*.json'))\n    if not list_of_files:\n        return None\n    latest_file = max(list_of_files, key=os.path.getmtime)\n    return latest_file\n\ndef main():\n    data_dir = os.path.join('web_service', 'backend', 'data')\n    output_file = 'race-info.txt'\n\n    if not os.path.exists(data_dir):\n        with open(output_file, 'w') as f:\n            f.write(\"Data directory not found.\\n\")\n        return\n\n    latest_file = get_latest_race_file(data_dir)\n\n    if not latest_file:\n        with open(output_file, 'w') as f:\n            f.write(\"No race data files found.\\n\")\n        return\n\n    try:\n        with open(latest_file, 'r') as f:\n            race_data = json.load(f)\n    except (json.JSONDecodeError, IOError) as e:\n        with open(output_file, 'w') as f:\n            f.write(f\"Error reading race data file: {e}\\n\")\n        return\n\n    if not race_data or not isinstance(race_data, list):\n        with open(output_file, 'w') as f:\n            f.write(\"Race data is empty or not in the expected format.\\n\")\n        return\n\n    # Assuming the first race in the list is the one we want.\n    # A better approach might be to sort by start_time if available.\n    latest_race = race_data[0]\n\n    venue = latest_race.get('venue', 'N/A')\n    race_number = latest_race.get('raceNumber', 'N/A') # Use alias\n    runners = latest_race.get('runners', [])\n    num_runners = len(runners)\n\n    with open(output_file, 'w') as f:\n        f.write(f\"Latest Race Info:\\n\")\n        f.write(f\"  Track: {venue}\\n\")\n        f.write(f\"  Race #: {race_number}\\n\")\n        f.write(f\"  Field Size: {num_runners}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "electron/assets/license.rtf": "{\\rtf1\\ansi\\deff0\n{\\fonttbl{\\f0 Courier New;}}\n\\f0\\fs20\nFortuna Faucet License Agreement\\line\n\\line\nThis is a hobby project. It was cobbled together with a lot of caffeine and hope.\\line\n\\line\nThere are no warranties, guarantees, or promises that this will work.\\line\nFeel free to use it, break it, or share it. No copyrights are claimed.\\line\n\\line\nGood luck! You might need it.\n}",
    "electron/secure-settings-manager.js": "// electron/secure-settings-manager.js\nconst { app } = require('electron');\nconst fs = require('fs');\nconst path = require('path');\n\nconst SETTINGS_FILE = path.join(app.getPath('userData'), 'settings.json');\n\nclass SecureSettingsManager {\n constructor() {\n this.settings = this.loadSettings();\n }\n\n loadSettings() {\n try {\n if (fs.existsSync(SETTINGS_FILE)) {\n const data = fs.readFileSync(SETTINGS_FILE, 'utf-8');\n return JSON.parse(data);\n }\n } catch (error) {\n console.error('Error loading settings:', error);\n }\n return {};\n }\n\n saveSettings() {\n try {\n fs.writeFileSync(SETTINGS_FILE, JSON.stringify(this.settings, null, 2));\n } catch (error) {\n console.error('Error saving settings:', error);\n }\n }\n\n getApiKey() {\n return this.settings.apiKey || null;\n }\n\n saveApiKey(apiKey) {\n this.settings.apiKey = apiKey;\n this.saveSettings();\n return { success: true };\n }\n\n getBetfairCredentials() {\n return this.settings.betfair || null;\n }\n\n saveBetfairCredentials(credentials) {\n this.settings.betfair = credentials;\n this.saveSettings();\n return { success: true };\n }\n}\n\nmodule.exports = new SecureSettingsManager();\n",
    "fortuna-backend-webservice.spec": "# -*- mode: python ; coding: utf-8 -*-\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n# This spec has been standardized to build the web_service from its own directory,\n# removing the dependency on the obsolete 'python_service'.\n\nblock_cipher = None\nproject_root = Path(SPECPATH).parent\nbackend_root = project_root / 'web_service' / 'backend'\n\n# --- Data Files ---\n# Collect all necessary data files from their respective packages.\ndatas = []\ndatas += collect_data_files('uvicorn')\ndatas += collect_data_files('fastapi')\ndatas += collect_data_files('starlette')\n\n# --- Hidden Imports ---\n# Ensure all necessary submodules and dynamically loaded modules are included.\nhiddenimports = set()\nhiddenimports.update(collect_submodules('web_service.backend'))\nhiddenimports.update(collect_submodules('uvicorn'))\nhiddenimports.update(collect_submodules('fastapi'))\nhiddenimports.update(collect_submodules('starlette'))\nhiddenimports.update(collect_submodules('anyio'))\nhiddenimports.add('win32timezone') # Critical for Windows service operation\nhiddenimports.update(['pydantic_settings.sources']) # For settings management\n\na = Analysis(\n    [str(backend_root / 'service_entry.py')], # Entry point is the service wrapper\n    pathex=[str(project_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=sorted(hiddenimports),\n    hookspath=[str(project_root / 'fortuna-backend-hooks')],\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False\n)\n\n# --- PYZ Archive ---\n# Force __init__.py files into the PYZ archive to ensure robust module loading.\na.pure += [\n    ('web_service', str(project_root / 'web_service/__init__.py'), 'PYMODULE'),\n    ('web_service.backend', str(backend_root / '__init__.py'), 'PYMODULE'),\n]\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\n# --- Final Executable ---\n# This creates a single-file executable. The COLLECT object has been removed\n# as it is not needed for this build target.\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    name='fortuna-webservice',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    runtime_tmpdir=None,\n    console=True # Console is useful for debugging service startup\n)\n",
    "fortuna-webservice.spec": "# -*- mode: python ; coding: utf-8 -*-\n\nimport os\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\nproject_root = Path(SPECPATH).resolve()\n\ndef include_tree(rel_path, target, store):\n    absolute = project_root / rel_path\n    if absolute.exists():\n        store.append((str(absolute), target))\n        print(f\"[spec] Including {absolute} -> {target}\")\n    else:\n        # This spec is used by the legacy build-msi.yml, which checks for these dirs.\n        # If they are missing here, it's a critical error.\n        raise FileNotFoundError(f\"[spec] Required directory not found: {absolute}\")\n\ndatas = []\n# Paths must match the legacy structure used by build-msi.yml\ninclude_tree('python_service/adapters', 'adapters', datas)\ninclude_tree('python_service/data', 'data', datas)\ninclude_tree('python_service/json', 'json', datas)\n\n# Collect library assets\ntry:\n    datas += collect_data_files('uvicorn', includes=['*.html', '*.json'])\n    datas += collect_data_files('structlog', includes=['*.json'])\nexcept Exception as e:\n    print(f\"[spec] Warning: Could not collect library data files: {e}\")\n\n# Collect Hidden Imports for python_service\nhidden_imports = set()\nhidden_imports.update(collect_submodules('python_service'))\nhidden_imports.update([\n    'fastapi', 'uvicorn', 'uvicorn.logging', 'uvicorn.loops.auto', 'uvicorn.lifespan.on',\n    'uvicorn.protocols.http.h11_impl', 'uvicorn.protocols.http.httptools_impl',\n    'uvicorn.protocols.websockets.wsproto_impl', 'uvicorn.protocols.websockets.websockets_impl',\n    'anyio', 'httpcore', 'httpx', 'python_multipart', 'pydantic', 'pydantic_core',\n    'aiosqlite', 'structlog', 'tenacity', 'slowapi'\n])\n\na = Analysis(\n    # FIX: Target service_entry.py instead of main.py\n    ['web_service/backend/service_entry.py'],\n    pathex=[],\n    binaries=[],\n    datas=[('web_service/backend', 'backend')],\n    # FIX: Ensure critical service modules are hidden-imported\n    hiddenimports=['win32timezone', 'win32serviceutil', 'win32service', 'win32event'],\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name='fortuna-webservice', # Name matches the workflow expectation\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=False,\n    runtime_tmpdir=None,\n    console=False,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n",
    "pg_schemas/quarantine_races.sql": "CREATE TABLE IF NOT EXISTS quarantine_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(100),\n    track_name VARCHAR(100),\n    race_number INT,\n    post_time TIMESTAMP WITH TIME ZONE,\n    source VARCHAR(50),\n    raw_data_json JSONB, -- Store the original raw data for inspection\n    quarantine_reason TEXT, -- Reason for failing validation\n    collection_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);",
    "playwright_test.js": "const { chromium } = require('playwright');\nconst { test, expect } = require('@playwright/test');\n\n(async () => {\n  const browser = await chromium.launch();\n  const page = await browser.newPage();\n\n  // Navigate to the dashboard\n  await page.goto('http://localhost:3001');\n\n  // Wait for the initial loading to complete.\n  // We expect the skeleton loaders to disappear.\n  await expect(page.locator('div:has-text(\"Loading races...\")')).toHaveCount(0, { timeout: 15000 });\n\n  // Check for the manual override panel\n  const overridePanel = page.locator('div:has-text(\"Fetch Failed: AtTheRaces\")');\n  await expect(overridePanel).toBeVisible({ timeout: 10000 });\n\n  // Check for the text area with the correct URL\n  // The date is a placeholder, as it can change. The important part is the base URL.\n  const textArea = overridePanel.locator('textarea');\n  await expect(textArea).toHaveAttribute('value', /https:\\/\\/www\\.attheraces\\.com\\/racecards\\/\\d{4}-\\d{2}-\\d{2}/);\n\n\n  // Take a screenshot for visual confirmation\n  await page.screenshot({ path: 'manual-override-panel.png' });\n\n  await browser.close();\n})();\n",
    "requirements-dev.in": "#\n# Fortuna Faucet - High-Level Development & Testing Dependencies\n# This file is the source of truth for development dependencies.\n# Run 'pip-compile' to generate requirements-dev.txt.\n#\n\n# --- Core Testing Frameworks ---\npytest\npytest-asyncio\nfakeredis[lua]\nrespx\nplaywright\n\n# --- Code Quality & Linting ---\nblack\n# ruff is often used with black\n\n# --- Analysis & Notebooks ---\n# pandas is already in the main requirements.in\naltair\npydeck\nstreamlit\ntabula-py\n\n# --- Git & Versioning ---\nGitPython\n\n# --- Security & Auditing ---\npip-audit\n",
    "scripts/generate_manifests.py": "# scripts/generate_manifests.py\nimport json\nimport os\nfrom pathlib import Path\n\n# --- Configuration ---\nROOT_DIR = Path(\".\")\nOUTPUT_DIR = Path(\".\")\nNUM_MANIFESTS = 5 # We will create 5 balanced manifests\n\n# --- Inclusion/Exclusion Rules ---\n# This script is now comprehensive. Instead of a narrow include list,\n# it scans everything and uses a more precise exclusion list.\nINCLUDE_ONLY_DIRS = None # Deactivated: We now scan all directories by default\n\nEXCLUDE_DIRS = {\n    # Standard git/ide/v-env exclusions\n    \".git\", \".idea\", \".vscode\", \"node_modules\", \".next\", \".venv\",\n    # Build artifacts and caches\n    \"dist\", \"build\", \"__pycache__\", \".pytest_cache\", \"out\", \"build_wix\",\n    # Agent-specific/Volatile directories\n    \"attic\", \"installer\", \"ReviewableJSON\", \"jules-scratch\",\n    # Legacy code not relevant to the current monolith\n    \"PREV_src\", \"python_service\",\n}\n\nEXCLUDE_FILES_BY_EXTENSION = {\n    # Archives and logs\n    \".zip\", \".json\", \".log\", \".db\", \".sqlite3\",\n    # Binary/Image formats not useful for LLM context\n    \".png\", \".ico\", \".bmp\", \".exe\", \".dll\", \".pyd\", \".pdf\",\n    # Deactivated workflows (keep them for history, but not for active context)\n    \".ymlx\"\n}\n\n\ndef get_all_project_files():\n    \"\"\"\n    Walks the entire project directory to find all relevant files for archiving,\n    respecting a detailed set of exclusion rules.\n    \"\"\"\n    all_files_with_size = []\n    print(\"\\n--- Starting Comprehensive File Audit ---\")\n    scanned_count = 0\n    included_count = 0\n\n    for root, dirs, files in os.walk(ROOT_DIR, topdown=True):\n        current_path = Path(root)\n\n        # 1. Directory Exclusion: Prune entire directory subtrees\n        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS and not d.endswith('.egg-info')]\n\n        for name in files:\n            scanned_count += 1\n            file_path = current_path / name\n\n            # 2. Filename/Extension Exclusion\n            if name.startswith(('MANIFEST_PART', 'FORTUNA_ALL_PART', '.env')):\n                continue\n            if file_path.suffix in EXCLUDE_FILES_BY_EXTENSION:\n                continue\n\n            # Special case: allow '.spec' files which are critical configs\n            if file_path.suffix == '.spec' and name not in ['api.spec']:\n                 pass # keep it\n            elif file_path.suffix in ['.spec']:\n                 continue # exclude other .spec files\n\n            try:\n                posix_path = str(file_path.as_posix())\n                size = os.path.getsize(file_path)\n                all_files_with_size.append((posix_path, size))\n                included_count += 1\n            except FileNotFoundError:\n                print(f\"[WARNING] File not found during scan: {file_path}\")\n                continue\n\n    print(f\"Scanned {scanned_count} files, included {included_count} for manifest.\")\n    print(\"--- File Audit Complete ---\\n\")\n    return all_files_with_size\n\n\ndef balance_files_by_size(files_with_size, num_bins):\n    \"\"\"\n    Distributes files into a specified number of bins, balancing by size and count.\n    Uses a hybrid greedy and round-robin approach for better distribution.\n    \"\"\"\n    # Define categories for more granular balancing\n    categories = {\n        'large': [], 'medium': [], 'small': [], 'config': [], 'docs': [],\n        'workflows': [], 'scripts': [], 'source': []\n    }\n\n    # Categorize files based on extension and size\n    for path, size in files_with_size:\n        ext = Path(path).suffix.lower()\n        if 'github/workflows' in path:\n            categories['workflows'].append((path, size))\n        elif ext in ['.md', '.txt']:\n            categories['docs'].append((path, size))\n        elif ext in ['.json', '.toml', '.ini', '.spec', '.lock']:\n            categories['config'].append((path, size))\n        elif ext == '.py' and 'scripts' in path:\n            categories['scripts'].append((path, size))\n        elif ext in ['.py', '.js', '.ts', '.tsx', '.css', '.html', '.wxs']:\n            if size > 50 * 1024:  # Over 50KB\n                categories['large'].append((path, size))\n            elif size > 10 * 1024: # Over 10KB\n                categories['medium'].append((path, size))\n            else:\n                categories['small'].append((path, size))\n        else:\n            categories['source'].append((path, size))\n\n    bins = [[] for _ in range(num_bins)]\n    bin_sizes = [0] * num_bins\n\n    # Distribute large files first using greedy approach\n    for category in ['large', 'medium']:\n        # Sort descending to place largest files first\n        categories[category].sort(key=lambda x: x[1], reverse=True)\n        for path, size in categories[category]:\n            min_bin_index = bin_sizes.index(min(bin_sizes))\n            bins[min_bin_index].append(path)\n            bin_sizes[min_bin_index] += size\n\n    # Distribute remaining files using round-robin to balance file count\n    current_bin = 0\n    for category in ['small', 'config', 'docs', 'workflows', 'scripts', 'source']:\n        # Sort alphabetically for consistent distribution\n        categories[category].sort(key=lambda x: x[0])\n        for path, size in categories[category]:\n            bins[current_bin].append(path)\n            bin_sizes[current_bin] += size\n            current_bin = (current_bin + 1) % num_bins\n\n    # Print the balancing results for verification\n    print(\"--- Manifest Balancing Results (Enhanced) ---\")\n    for i, (file_list, total_size) in enumerate(zip(bins, bin_sizes)):\n        print(\n            f\" Manifest {i+1}: {len(file_list):>4} files, \"\n            f\"Total size: {total_size / 1024 / 1024:>6.2f} MB\"\n        )\n    print(\"------------------------------------------\")\n\n    return bins\n\n\ndef main():\n    \"\"\"Generate balanced manifest files based on file size.\"\"\"\n    print(\"--- Starting Manifest Generation (Size-Balanced) ---\")\n    all_files = get_all_project_files()\n    print(f\"Found {len(all_files)} total project files to consider.\")\n\n    balanced_manifests = balance_files_by_size(all_files, NUM_MANIFESTS)\n\n    # Write the updated manifest files\n    for i, file_list in enumerate(balanced_manifests):\n        manifest_name = f\"MANIFEST_PART{i+1}.json\"\n        output_path = OUTPUT_DIR / manifest_name\n        sorted_files = sorted(file_list) # Sort alphabetically for consistency\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(sorted_files, f, indent=4)\n        print(f\"\u2705 Wrote {len(sorted_files)} entries to {output_path}\")\n\n    print(\"\\n[SUCCESS] All manifest files have been generated and balanced.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/install_fortuna_silent.bat": "@echo off\nREM Automated deployment (no UI, minimal interaction)\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_silent_install.log\" ^\n    /norestart ^\n    ALLUSERS=1 ^\n    INSTALLSCOPE=perMachine\n\nexit /b %errorlevel%",
    "scripts/test_api_query.py": "#!/usr/bin/env python\n\"\"\"\nSimple Fortuna Race Data Query Script\n\nThis is a lightweight script for testing the race data API locally.\nIt queries for filtered races and outputs the results.\n\nUsage:\n  python scripts/test_api_query.py\n\nThe script expects the backend to be running on http://127.0.0.1:8000\n\"\"\"\n\nimport json\nimport os\nimport sys\nimport time\nfrom datetime import datetime\n\ntry:\n    import requests\nexcept ImportError:\n    print(\"\u274c Error: 'requests' module not found.\")\n    print(\"Install it with: pip install requests\")\n    sys.exit(1)\n\n\n# Configuration\nAPI_BASE_URL = os.getenv(\"FORTUNA_API_URL\", \"http://127.0.0.1:8000\")\nAPI_KEY = os.getenv(\"API_KEY\", \"a_secure_test_api_key_that_is_long_enough\")\nTIMEOUT = 10\n\n\ndef log(message, level=\"INFO\"):\n    \"\"\"Print timestamped log message.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    emoji = {\"INFO\": \"\u2139\ufe0f\", \"SUCCESS\": \"\u2705\", \"ERROR\": \"\u274c\", \"WARNING\": \"\u26a0\ufe0f\"}\n    print(f\"[{timestamp}] {emoji.get(level, '\u2022')} {message}\")\n\n\ndef check_backend_health():\n    \"\"\"Check if the backend API is responding.\"\"\"\n    log(\"Checking backend health...\", \"INFO\")\n    try:\n        response = requests.get(\n            f\"{API_BASE_URL}/api/health\",\n            timeout=TIMEOUT,\n            headers={\"X-API-Key\": API_KEY},\n        )\n        if response.status_code == 200:\n            log(\"Backend is healthy\", \"SUCCESS\")\n            return True\n    except requests.RequestException as e:\n        log(f\"Health check failed: {e}\", \"ERROR\")\n        return False\n    return False\n\n\ndef fetch_filtered_races():\n    \"\"\"Fetch trifecta-qualified races from the API.\"\"\"\n    endpoint = \"/api/races/qualified/trifecta\"\n    url = f\"{API_BASE_URL}{endpoint}\"\n\n    log(f\"Querying {url}\", \"INFO\")\n\n    try:\n        headers = {\"X-API-Key\": API_KEY}\n        response = requests.get(url, timeout=TIMEOUT, headers=headers)\n        response.raise_for_status()\n\n        data = response.json()\n        races = data.get(\"races\", [])\n\n        log(f\"Retrieved {len(races)} qualified races\", \"SUCCESS\")\n        return data\n\n    except requests.exceptions.Timeout:\n        log(f\"Request timed out after {TIMEOUT} seconds\", \"ERROR\")\n        return None\n    except requests.exceptions.ConnectionError as e:\n        log(f\"Connection error: {e}\", \"ERROR\")\n        log(f\"Is the backend running at {API_BASE_URL}?\", \"WARNING\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        log(f\"HTTP error: {response.status_code} {response.reason}\", \"ERROR\")\n        return None\n    except json.JSONDecodeError:\n        log(\"Response was not valid JSON\", \"ERROR\")\n        return None\n    except Exception as e:\n        log(f\"Unexpected error: {e}\", \"ERROR\")\n        return None\n\n\ndef display_races(races_data):\n    \"\"\"Pretty-print the races to console.\"\"\"\n    if not races_data:\n        log(\"No data to display\", \"WARNING\")\n        return\n\n    races = races_data.get(\"races\", [])\n\n    print(\"\\\\n\" + \"=\" * 80)\n    print(f\"FORTUNA FILTERED RACE REPORT - {len(races)} Races\")\n    print(\"=\" * 80 + \"\\\\n\")\n\n    if not races:\n        print(\"\u274c No qualified races found at this time.\\\\n\")\n        return\n\n    for idx, race in enumerate(races, 1):\n        venue = race.get(\"venue\", \"Unknown\")\n        race_num = race.get(\"race_number\", \"?\")\n        start_time = race.get(\"startTime\", \"N/A\")\n        runners = race.get(\"runners\", [])\n\n        print(f\"[{idx}] {venue} - Race {race_num}\")\n        print(f\"    Post Time: {start_time}\")\n        print(f\"    Runners: {len(runners)}\")\n        print(\"\\\\n    Horse Name                  | Win Odds | Best Source\")\n        print(\"    \" + \"-\" * 60)\n\n        for runner in runners:\n            name = runner.get(\"name\", \"Unknown\")\n            odds_data = runner.get(\"odds\", {})\n\n            # Find best win odds\n            best_odds = \"N/A\"\n            best_source = \"N/A\"\n            if odds_data:\n                best_val = 0.0\n                for source, odds_obj in odds_data.items():\n                    win_odds = odds_obj.get(\"win\", 0.0)\n                    if win_odds > best_val:\n                        best_val = win_odds\n                        best_odds = f\"{best_val:.2f}\"\n                        best_source = source\n\n            # Truncate long names\n            display_name = name[:28]\n            print(f\"    {display_name:28} | {best_odds:>8} | {best_source}\")\n\n        print()\n\n\ndef save_to_json(races_data, filename=\"qualified_races.json\"):\n    \"\"\"Save race data to JSON file.\"\"\"\n    try:\n        with open(filename, \"w\", encoding=\"utf-8\") as f:\n            json.dump(races_data, f, indent=2)\n        log(f\"Saved race data to {filename}\", \"SUCCESS\")\n        return True\n    except Exception as e:\n        log(f\"Failed to save JSON: {e}\", \"ERROR\")\n        return False\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    log(\"=== Fortuna Race Data Query ===\", \"INFO\")\n    log(f\"API URL: {API_BASE_URL}\", \"INFO\")\n\n    # Check backend health\n    if not check_backend_health():\n        log(\"Backend is not responding\", \"ERROR\")\n        log(\"Make sure to start the backend with:\", \"WARNING\")\n        log(\"  python -m uvicorn web_service.backend.main:app --port 8000\", \"WARNING\")\n        return 1\n\n    # Fetch races\n    races_data = fetch_filtered_races()\n    if not races_data:\n        log(\"Failed to fetch race data\", \"ERROR\")\n        return 1\n\n    # Display results\n    display_races(races_data)\n\n    # Save to JSON\n    save_to_json(races_data)\n\n    log(\"Complete\", \"SUCCESS\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
    "start_docker.bat": "@echo off\nREM ============================================================\nREM Fortuna Faucet - Docker Launcher for Windows\nREM A simple, friendly way to start your racing analysis engine\nREM ============================================================\n\nsetlocal enabledelayedexpansion\n\nREM Colors and styling\ncls\necho.\necho \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\necho \u2551                                                            \u2551\necho \u2551            \ud83d\udc34  FORTUNA FAUCET LAUNCHER  \ud83d\udc34                \u2551\necho \u2551          Racing Strategy Analysis Engine                  \u2551\necho \u2551                                                            \u2551\necho \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\necho.\n\nREM ============================================================\nREM STEP 1: Check if Docker is installed\nREM ============================================================\necho [1/5] Checking for Docker installation...\ndocker --version >nul 2>&1\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Docker is not installed or not in PATH\n    echo.\n    echo To use Fortuna, you need Docker Desktop:\n    echo https://www.docker.com/products/docker-desktop\n    echo.\n    echo After installing Docker, restart your computer and try again.\n    echo.\n    pause\n    exit /b 1\n)\n\nfor /f \"tokens=*\" %%i in ('docker --version') do set DOCKER_VERSION=%%i\necho \u2713 Found: %DOCKER_VERSION%\necho.\n\nREM ============================================================\nREM STEP 2: Check if Docker daemon is running\nREM ============================================================\necho [2/5] Checking if Docker daemon is running...\ndocker ps >nul 2>&1\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Docker daemon is not running\n    echo.\n    echo Please:\n    echo 1. Open \"Docker Desktop\" from your Start Menu\n    echo 2. Wait 30 seconds for Docker to fully start\n    echo 3. Then run this launcher again\n    echo.\n    pause\n    exit /b 1\n)\necho \u2713 Docker daemon is running\necho.\n\nREM ============================================================\nREM STEP 3: Pull latest Docker image\nREM ============================================================\necho [3/5] Pulling latest Fortuna image from Docker Hub...\necho (This may take a minute on first run)\necho.\ndocker pull masonj0/fortuna-faucet:latest\nif errorlevel 1 (\n    echo.\n    echo \u26a0 Warning: Could not pull from Docker Hub\n    echo Checking for local image...\n    docker image inspect masonj0/fortuna-faucet:latest >nul 2>&1\n    if errorlevel 1 (\n        echo \u2717 ERROR: No local image found\n        echo Please check your internet connection and try again.\n        echo.\n        pause\n        exit /b 1\n    )\n    echo \u2713 Using existing local image\n)\necho \u2713 Image ready\necho.\n\nREM ============================================================\nREM STEP 4: Start container\nREM ============================================================\necho [4/5] Starting Fortuna container...\necho.\n\nREM Stop any existing container (ignore errors)\ndocker stop fortuna-faucet >nul 2>&1\ndocker rm fortuna-faucet >nul 2>&1\n\nREM Create data directories if they don't exist\nif not exist \"data\" mkdir data\nif not exist \"logs\" mkdir logs\n\nREM Start container with proper quoting for paths with spaces\ndocker run -d ^\n  --name fortuna-faucet ^\n  -p 8000:8000 ^\n  -v \"%cd%\\data:/app/web_service/backend/data\" ^\n  -v \"%cd%\\logs:/app/web_service/backend/logs\" ^\n  masonj0/fortuna-faucet:latest\n\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Failed to start container\n    echo.\n    echo Try these troubleshooting steps:\n    echo 1. Open Docker Desktop\n    echo 2. Wait for it to fully start\n    echo 3. Open Command Prompt and run: docker ps\n    echo    (This tests if Docker is working)\n    echo 4. Run this launcher again\n    echo.\n    pause\n    exit /b 1\n)\n\necho \u2713 Container started successfully\necho.\n\nREM ============================================================\nREM STEP 5: Wait and verify startup\nREM ============================================================\necho [5/5] Waiting for application to start...\ntimeout /t 3 /nobreak\n\nREM Check if container is still running\ndocker inspect fortuna-faucet >nul 2>&1\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Container exited unexpectedly\n    echo.\n    echo Showing container logs for debugging:\n    echo.\n    docker logs fortuna-faucet\n    echo.\n    pause\n    exit /b 1\n)\n\necho \u2713 Application is ready!\necho.\n\nREM ============================================================\nREM SUCCESS - Open browser and show logs\nREM ============================================================\ncls\necho.\necho \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\necho \u2551                                                            \u2551\necho \u2551            \ud83c\udf89  FORTUNA IS RUNNING!  \ud83c\udf89                   \u2551\necho \u2551                                                            \u2551\necho \u2551  Your racing analysis engine is ready at:                \u2551\necho \u2551                                                            \u2551\necho \u2551          http://localhost:8000                            \u2551\necho \u2551                                                            \u2551\necho \u2551  Opening browser now...                                   \u2551\necho \u2551                                                            \u2551\necho \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\necho.\n\nREM Open browser\nstart http://localhost:8000\n\nREM Small delay to let browser open\ntimeout /t 2 /nobreak\n\nREM Show logs\necho.\necho \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\necho \u2502 Live Application Logs (Ctrl+C to stop)                    \u2502\necho \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\necho.\n\ndocker logs -f fortuna-faucet\n\nREM Cleanup on exit\necho.\necho Stopping Fortuna...\ndocker stop fortuna-faucet >nul 2>&1\necho \u2713 Fortuna stopped\n\nexit /b 0\n",
    "tests/__init__.py": "# This file makes the 'tests' directory a package.\n",
    "tests/adapters/test_timeform_adapter_modernized.py": "# Modernized test resurrected from attic/legacy_tests_pre_triage/adapters/test_timeform_adapter.py\nfrom decimal import Decimal\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom python_service.adapters.timeform_adapter import TimeformAdapter\n\n\n@pytest.fixture\ndef timeform_adapter():\n    mock_config = MagicMock()\n    return TimeformAdapter(config=mock_config)\n\n\ndef read_fixture(file_path):\n    with open(file_path, \"r\") as f:\n        return f.read()\n\n\n@pytest.mark.asyncio\nasync def test_timeform_adapter_parses_html_correctly(timeform_adapter):\n    \"\"\"Verify adapter correctly parses a known HTML fixture.\"\"\"\n    mock_html = read_fixture(\"tests/fixtures/timeform_modern_sample.html\")\n\n    # Directly test the parsing of runners from the correct HTML structure\n    from bs4 import BeautifulSoup\n\n    soup = BeautifulSoup(mock_html, \"html.parser\")\n    runners = [timeform_adapter._parse_runner(row) for row in soup.select(\"div.rp-horseTable_mainRow\")]\n\n    assert len(runners) == 3, \"Should parse three runners\"\n\n    braveheart = next((r for r in runners if r.name == \"Braveheart\"), None)\n    assert braveheart is not None\n    assert braveheart.odds[\"Timeform\"].win == Decimal(\"3.5\")\n\n    steady_eddy = next((r for r in runners if r.name == \"Steady Eddy\"), None)\n    assert steady_eddy is not None\n    assert steady_eddy.odds[\"Timeform\"].win == Decimal(\"2.0\")\n",
    "tests/fixtures/timeform_legacy_sample.html": "<!DOCTYPE html><html><body><div class='race-card'><div class='runner'><span class='runner-name'>Braveheart</span><span class='runner-odds'>5/2</span></div><div class='runner'><span class='runner-name'>Speedster</span><span class='runner-odds'>10/1</span></div><div class='runner'><span class='runner-name'>Steady Eddy</span><span class='runner-odds'>EVENS</span></div></div></body></html>",
    "tests/test_api/test_endpoints.py": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom python_service.api import app\n\nclient = TestClient(app)\n\n\n@pytest.mark.asyncio\nasync def test_health_check(client):\n    \"\"\"Tests the unauthenticated /health endpoint.\"\"\"\n    response = await client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"healthy\"\n",
    "tests/test_models.py": "# Test suite for Pydantic models, resurrected from attic/legacy_tests_pre_triage/checkmate_v7/test_models.py\nimport datetime\n\nimport pytest\nfrom pydantic import ValidationError\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\ndef test_runner_model_creation():\n    \"\"\"Tests basic successful creation of the Runner model.\"\"\"\n    from datetime import datetime\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds_data = {\"TestOdds\": OddsData(win=Decimal(\"6.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    runner = Runner(number=5, name=\"Test Horse\", odds=odds_data, scratched=False)\n    assert runner.number == 5\n    assert runner.name == \"Test Horse\"\n    assert not runner.scratched\n\n\ndef test_race_model_with_valid_runners():\n    \"\"\"Tests basic successful creation of the Race model.\"\"\"\n    from datetime import datetime\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    runner1 = Runner(number=1, name=\"A\", odds=odds1, scratched=False)\n    runner2 = Runner(number=2, name=\"B\", odds=odds2, scratched=False)\n    race = Race(\n        id=\"test-race-1\",\n        venue=\"TEST\",\n        race_number=1,\n        start_time=datetime.now(),\n        runners=[runner1, runner2],\n        source=\"test\",\n    )\n    assert race.venue == \"TEST\"\n    assert len(race.runners) == 2\n\n\ndef test_model_validation_fails_on_missing_required_field():\n    \"\"\"Ensures Pydantic's validation fires for missing required fields.\"\"\"\n    with pytest.raises(ValidationError):\n        # 'name' is a required field for a Runner\n        Runner(number=3, odds=\"3/1\", scratched=False)\n\n    with pytest.raises(ValidationError):\n        # 'venue' is a required field for a Race\n        Race(\n            id=\"test-race-2\",\n            race_number=2,\n            start_time=datetime.datetime.now(),\n            runners=[],\n            source=\"test\",\n        )\n",
    "verify_dashboard.py": "\nfrom playwright.sync_api import sync_playwright\n\ndef verify_dashboard(page):\n    \"\"\"\n    Navigates to the dashboard and takes a screenshot.\n    \"\"\"\n    page.goto(\"http://localhost:3000\")\n    page.wait_for_selector(\"text=Fortuna Faucet\")\n    page.screenshot(path=\"verification.png\")\n\nif __name__ == \"__main__\":\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        page = browser.new_page()\n        try:\n            verify_dashboard(page)\n        finally:\n            browser.close()\n",
    "web_service/backend/adapters/base_adapter_v3.py": "# python_service/adapters/base_v3.py\nfrom __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport json\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport httpx\nimport structlog\nfrom tenacity import (\n    RetryError,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom ..core.exceptions import AdapterHttpError, AdapterParsingError\nfrom ..manual_override_manager import ManualOverrideManager\nfrom ..models import Race\nfrom ..validators import DataValidationPipeline\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject requests\n    HALF_OPEN = \"half_open\"  # Testing if service recovered\n\n\n@dataclass\nclass CircuitBreaker:\n    \"\"\"Thread-safe circuit breaker implementation.\"\"\"\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    half_open_max_calls: int = 3\n\n    _failure_count: int = field(default=0, repr=False)\n    _last_failure_time: float = field(default=0.0, repr=False)\n    _state: CircuitState = field(default=CircuitState.CLOSED, repr=False)\n    _half_open_calls: int = field(default=0, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    @property\n    def state(self) -> CircuitState:\n        \"\"\"Returns current state without mutation. Use check_state() for transitions.\"\"\"\n        return self._state\n\n    async def check_and_transition_state(self) -> CircuitState:\n        \"\"\"Check state and handle OPEN -> HALF_OPEN transition atomically.\"\"\"\n        async with self._lock:\n            if self._state == CircuitState.OPEN:\n                if time.monotonic() - self._last_failure_time >= self.recovery_timeout:\n                    self._state = CircuitState.HALF_OPEN\n                    self._half_open_calls = 0\n            return self._state\n\n    async def record_success(self) -> None:\n        \"\"\"Record a successful call.\"\"\"\n        async with self._lock:\n            self._failure_count = 0\n            if self._state == CircuitState.HALF_OPEN:\n                self._half_open_calls += 1\n                if self._half_open_calls >= self.half_open_max_calls:\n                    self._state = CircuitState.CLOSED\n\n    async def record_failure(self) -> None:\n        \"\"\"Record a failed call.\"\"\"\n        async with self._lock:\n            self._failure_count += 1\n            self._last_failure_time = time.monotonic()\n\n            if self._failure_count >= self.failure_threshold:\n                self._state = CircuitState.OPEN\n            elif self._state == CircuitState.HALF_OPEN:\n                self._state = CircuitState.OPEN\n\n    async def allow_request(self) -> bool:\n        \"\"\"Check if a request should be allowed.\"\"\"\n        state = await self.check_and_transition_state()\n        return state in (CircuitState.CLOSED, CircuitState.HALF_OPEN)\n\n\n@dataclass\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter.\"\"\"\n    requests_per_second: float = 10.0\n    burst_size: int = 20\n\n    _tokens: float = field(default=0.0, init=False, repr=False)\n    _last_update: float = field(default=0.0, init=False, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    def __post_init__(self) -> None:\n        self._tokens = float(self.burst_size)\n        self._last_update = time.monotonic()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire a token, waiting if necessary.\"\"\"\n        async with self._lock:\n            now = time.monotonic()\n            elapsed = now - self._last_update\n            self._tokens = min(self.burst_size, self._tokens + elapsed * self.requests_per_second)\n            self._last_update = now\n\n            if self._tokens < 1:\n                wait_time = (1 - self._tokens) / self.requests_per_second\n                await asyncio.sleep(wait_time)\n                self._tokens = 0\n            else:\n                self._tokens -= 1\n\n\n@dataclass\nclass CacheEntry:\n    \"\"\"Cache entry with TTL.\"\"\"\n    data: Any\n    created_at: float\n    ttl: float\n\n    @property\n    def is_expired(self) -> bool:\n        return time.monotonic() - self.created_at > self.ttl\n\n\nclass ResponseCache:\n    \"\"\"Simple in-memory response cache.\"\"\"\n\n    def __init__(self, default_ttl: float = 300.0, max_entries: int = 1000):\n        self.default_ttl = default_ttl\n        self.max_entries = max_entries\n        self._cache: dict[str, CacheEntry] = {}\n        self._lock = asyncio.Lock()\n\n    @staticmethod\n    def _make_key(method: str, url: str, **kwargs) -> str:\n        \"\"\"Generate a stable cache key from request parameters.\"\"\"\n        # Filter out non-hashable or irrelevant kwargs\n        cacheable_kwargs = {\n            k: v for k, v in kwargs.items()\n            if k not in ('headers', 'timeout', 'follow_redirects')\n            and isinstance(v, (str, int, float, bool, tuple, type(None)))\n        }\n        key_data = f\"{method}:{url}:{json.dumps(cacheable_kwargs, sort_keys=True, default=str)}\"\n        return hashlib.sha256(key_data.encode()).hexdigest()[:32]\n\n    async def get(self, method: str, url: str, **kwargs) -> Any | None:\n        \"\"\"Get a cached response if available and not expired.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            entry = self._cache.get(key)\n            if entry and not entry.is_expired:\n                return entry.data\n            elif entry:\n                del self._cache[key]\n        return None\n\n    async def set(self, method: str, url: str, data: Any, ttl: float | None = None, **kwargs) -> None:\n        \"\"\"Cache a response.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            # Evict old entries if cache is full\n            if len(self._cache) >= self.max_entries:\n                expired_keys = [k for k, v in self._cache.items() if v.is_expired]\n                for k in expired_keys:\n                    del self._cache[k]\n\n                # If still full, remove oldest entries\n                if len(self._cache) >= self.max_entries:\n                    oldest = sorted(self._cache.items(), key=lambda x: x[1].created_at)\n                    for k, _ in oldest[:len(self._cache) // 4]:\n                        del self._cache[k]\n\n            self._cache[key] = CacheEntry(\n                data=data,\n                created_at=time.monotonic(),\n                ttl=ttl or self.default_ttl\n            )\n\n    async def clear(self) -> None:\n        \"\"\"Clear all cached entries.\"\"\"\n        async with self._lock:\n            self._cache.clear()\n\n\n@dataclass\nclass AdapterMetrics:\n    \"\"\"Thread-safe metrics for adapter health monitoring.\"\"\"\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n    _total_requests: int = field(default=0, repr=False)\n    _successful_requests: int = field(default=0, repr=False)\n    _failed_requests: int = field(default=0, repr=False)\n    _total_latency_ms: float = field(default=0.0, repr=False)\n    _last_success: float | None = field(default=None, repr=False)\n    _last_failure: float | None = field(default=None, repr=False)\n    _last_error: str | None = field(default=None, repr=False)\n\n    @property\n    def total_requests(self) -> int:\n        return self._total_requests\n\n    @property\n    def success_rate(self) -> float:\n        if self._total_requests == 0:\n            return 1.0\n        return self._successful_requests / self._total_requests\n\n    @property\n    def avg_latency_ms(self) -> float:\n        if self._successful_requests == 0:\n            return 0.0\n        return self._total_latency_ms / self._successful_requests\n\n    async def record_success(self, latency_ms: float) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._successful_requests += 1\n            self._total_latency_ms += latency_ms\n            self._last_success = time.time()\n\n    async def record_failure(self, error: str) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._failed_requests += 1\n            self._last_failure = time.time()\n            self._last_error = error\n\n    def snapshot(self) -> dict[str, Any]:\n        \"\"\"Return a point-in-time snapshot of metrics.\"\"\"\n        return {\n            \"total_requests\": self._total_requests,\n            \"successful_requests\": self._successful_requests,\n            \"failed_requests\": self._failed_requests,\n            \"success_rate\": self.success_rate,\n            \"avg_latency_ms\": self.avg_latency_ms,\n            \"last_success\": self._last_success,\n            \"last_failure\": self._last_failure,\n            \"last_error\": self._last_error,\n        }\n\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    Abstract base class for all V3 data adapters.\n\n    Features:\n    - Standardized fetch/parse pattern\n    - Retry logic with exponential backoff\n    - Circuit breaker for fault tolerance\n    - Rate limiting\n    - Response caching\n    - Comprehensive metrics\n    \"\"\"\n\n    # Default User-Agent for requests\n    DEFAULT_USER_AGENT = (\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n        \"Chrome/120.0.0.0 Safari/537.36\"\n    )\n\n    def __init__(\n        self,\n        source_name: str,\n        base_url: str,\n        config: Any = None,\n        timeout: int = 20,\n        enable_cache: bool = True,\n        cache_ttl: float = 300.0,\n        rate_limit: float = 10.0,\n    ):\n        self.source_name = source_name\n        self.base_url = base_url.rstrip(\"/\")\n        self.config = config\n        self.timeout = timeout\n\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient | None = None\n        self.manual_override_manager: ManualOverrideManager | None = None\n        self.supports_manual_override = True\n        \n        # Resilience components\n        self.circuit_breaker = CircuitBreaker()\n        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)\n        self.cache = ResponseCache(default_ttl=cache_ttl) if enable_cache else None\n        self.metrics = AdapterMetrics()\n\n    async def __aenter__(self) -> \"BaseAdapterV3\":\n        \"\"\"Async context manager entry.\"\"\"\n        if self.http_client is None:\n            self.http_client = httpx.AsyncClient(timeout=self.timeout)\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Async context manager exit with cleanup.\"\"\"\n        await self.close()\n\n    async def close(self) -> None:\n        \"\"\"Clean up resources.\"\"\"\n        if self.http_client:\n            await self.http_client.aclose()\n            self.http_client = None\n        if self.cache:\n            await self.cache.clear()\n        self.logger.debug(\"Adapter resources cleaned up\")\n\n    def enable_manual_override(self, manager: ManualOverrideManager) -> None:\n        \"\"\"Injects the manual override manager into the adapter.\"\"\"\n        self.manual_override_manager = manager\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw data (e.g., HTML, JSON) for the given date.\n        This is the only method that should perform network operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> list[Race]:\n        \"\"\"\n        Parses the raw data retrieved by _fetch_data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> list[Race]:\n        \"\"\"\n        Orchestrates the fetch-then-parse pipeline for the adapter.\n        This public method should not be overridden by subclasses.\n        \"\"\"\n        raw_data = None\n\n        # Check for manual override data first\n        if self.manual_override_manager:\n            lookup_key = f\"{self.base_url}/racecards/{date}\"\n            manual_data = self.manual_override_manager.get_manual_data(self.source_name, lookup_key)\n            if manual_data:\n                self.logger.info(\"Using manually submitted data\", url=lookup_key)\n                raw_data = {\"pages\": [manual_data[0]], \"date\": date}\n\n        # Fetch from source if no manual data\n        if raw_data is None:\n            try:\n                raw_data = await self._fetch_data(date)\n            except AdapterHttpError as e:\n                if self.manual_override_manager and self.supports_manual_override:\n                    self.manual_override_manager.register_failure(self.source_name, e.url)\n                raise\n\n        # Parse the data\n        if raw_data is not None:\n            return self._validate_and_parse_races(raw_data)\n\n        return []\n\n    def _validate_and_parse_races(self, raw_data: Any) -> list[Race]:\n        is_valid, reason = DataValidationPipeline.validate_raw_response(self.source_name, raw_data)\n        if not is_valid:\n            raise AdapterParsingError(self.source_name, f\"Raw response validation failed: {reason}\")\n\n        try:\n            parsed_races = self._parse_races(raw_data)\n        except Exception as e:\n            self.logger.error(\"Failed to parse race data\", error=str(e))\n            raise AdapterParsingError(self.source_name, \"Parsing logic failed.\") from e\n\n        validated_races, warnings = DataValidationPipeline.validate_parsed_races(parsed_races)\n\n        if warnings:\n            self.logger.warning(\"Validation warnings during parsing\", warnings=warnings)\n\n        return validated_races\n\n    @retry(\n        retry=retry_if_exception_type((httpx.RequestError, httpx.HTTPStatusError)),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        stop=stop_after_attempt(3),\n        reraise=True,\n    )\n    async def make_request(\n        self,\n        http_client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        use_cache: bool = True,\n        cache_ttl: float | None = None,\n        **kwargs,\n    ) -> httpx.Response:\n        \"\"\"\n        Makes a resilient HTTP request with:\n        - Circuit breaker protection\n        - Rate limiting\n        - Response caching\n        - Retry logic with exponential backoff\n        \"\"\"\n        # Build full URL\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url}/{url.lstrip('/')}\"\n\n        # Check circuit breaker\n        if not await self.circuit_breaker.allow_request():\n            self.logger.warning(\"Circuit breaker open, rejecting request\", url=full_url)\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=503,\n                url=full_url,\n                message=\"Circuit breaker is open\"\n            )\n\n        # Check cache for GET requests\n        if use_cache and self.cache and method.upper() == \"GET\":\n            cached = await self.cache.get(method, full_url, **kwargs)\n            if cached is not None:\n                self.logger.debug(\"Cache hit\", url=full_url)\n                return cached\n\n        # Apply rate limiting\n        await self.rate_limiter.acquire()\n\n        # Pop headers and follow_redirects from kwargs to pass them explicitly.\n        headers = kwargs.pop(\"headers\", {})\n        if \"User-Agent\" not in headers:\n            headers[\"User-Agent\"] = self.DEFAULT_USER_AGENT\n\n        follow_redirects = kwargs.pop(\"follow_redirects\", True)\n\n        start_time = time.monotonic()\n\n        try:\n            self.logger.info(\"Making request\", method=method.upper(), url=full_url)\n\n            response = await http_client.request(\n                method,\n                full_url,\n                timeout=self.timeout,\n                headers=headers,\n                follow_redirects=follow_redirects,\n                **kwargs,  # Pass remaining kwargs\n            )\n            response.raise_for_status()\n\n            # Record success\n            latency_ms = (time.monotonic() - start_time) * 1000\n            await self.metrics.record_success(latency_ms)\n            await self.circuit_breaker.record_success()\n\n            # Cache successful GET responses\n            if use_cache and self.cache and method.upper() == \"GET\":\n                await self.cache.set(method, full_url, response, ttl=cache_ttl, **kwargs)\n\n            return response\n\n        except httpx.HTTPStatusError as e:\n            self.logger.error(\n                \"HTTP status error\",\n                status_code=e.response.status_code,\n                url=str(e.request.url),\n            )\n            await self.metrics.record_failure(f\"HTTP {e.response.status_code}\")\n            await self.circuit_breaker.record_failure()\n\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=e.response.status_code,\n                url=str(e.request.url),\n                message=f\"HTTP {e.response.status_code}: {e.response.reason_phrase}\",\n                response_body=e.response.text[:500] if e.response.text else None,\n                request_method=method.upper(),\n            ) from e\n\n        except (httpx.RequestError, RetryError) as e:\n            self.logger.error(\"Request error\", error=str(e))\n            await self.metrics.record_failure(str(e))\n            await self.circuit_breaker.record_failure()\n\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=503,\n                url=full_url,\n            ) from e\n\n    async def health_check(self) -> dict[str, Any]:\n        \"\"\"\n        Performs a health check on the adapter.\n        Subclasses can override to add custom checks.\n        \"\"\"\n        return {\n            \"adapter_name\": self.source_name,\n            \"base_url\": self.base_url,\n            \"circuit_breaker_state\": self.circuit_breaker.state.value,\n            \"metrics\": self.metrics.snapshot(),\n        }\n\n    def get_status(self) -> dict[str, Any]:\n        \"\"\"\n        Returns a dictionary representing the adapter's current status.\n        \"\"\"\n        status = \"OK\"\n        if self.circuit_breaker.state == CircuitState.OPEN:\n            status = \"CIRCUIT_OPEN\"\n        elif self.metrics.success_rate < 0.5:\n            status = \"DEGRADED\"\n\n        return {\n            \"adapter_name\": self.source_name,\n            \"status\": status,\n            \"circuit_state\": self.circuit_breaker.state.value,\n            \"success_rate\": round(self.metrics.success_rate, 3),\n        }\n\n    async def reset(self) -> None:\n        \"\"\"Reset adapter state (cache, circuit breaker, metrics).\"\"\"\n        if self.cache:\n            await self.cache.clear()\n        self.circuit_breaker = CircuitBreaker()\n        self.metrics = AdapterMetrics()\n        self.logger.info(\"Adapter state reset\")\n",
    "web_service/backend/adapters/brisnet_adapter.py": "# python_service/adapters/brisnet_adapter.py\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BrisnetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for brisnet.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Brisnet\"\n    BASE_URL = \"https://www.brisnet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetches the raw HTML from the Brisnet race page.\"\"\"\n        url = \"/cgi-bin/intoday.cgi\"\n        response = await self.make_request(self.http_client, \"GET\", url, headers=self._get_headers())\n        return {\"html\": response.text, \"date\": date} if response and response.text else None\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.brisnet.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n        }\n\n    def _parse_races(self, raw_data: Optional[dict]) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            self.logger.warning(\"No HTML content received from Brisnet\")\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        races = []\n        for race_link in soup.select(\"a[href*='brisnet.com/cgi-bin/briswatch.cgi/public/Brad/TODAY.PM']\"):\n            try:\n                race_number_str = race_link.text.strip()\n                if not race_number_str.isdigit():\n                    continue\n                race_number = int(race_number_str)\n\n                # Venue and start time are not available on the index page, so we have to be creative\n                # This is a significant simplification and may need to be revisited\n                venue = \"Unknown\"\n                if parent_table := race_link.find_parent(\"table\"):\n                    if caption := parent_table.find(\"caption\"):\n                        venue = normalize_venue_name(caption.text.strip())\n\n                # Create a placeholder start time as it's not available on this page\n                start_time = datetime.now()\n\n                # Since we don't have runner data on this page, we create a placeholder race\n                # A more complete implementation would require fetching each race link\n                race = Race(\n                    id=f\"brisnet_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[],\n                    source=self.source_name,\n                )\n                races.append(race)\n            except (ValueError, IndexError, TypeError) as e:\n                self.logger.warning(\n                    \"Failed to parse a race link on Brisnet\",\n                    link=race_link.get(\"href\"),\n                    error=e,\n                    exc_info=True,\n                )\n                continue\n\n        return races\n",
    "web_service/backend/adapters/harness_adapter.py": "# python_service/adapters/harness_adapter.py\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom zoneinfo import ZoneInfo\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass HarnessAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US harness racing data with manual override support.\"\"\"\n\n    SOURCE_NAME = \"USTrotting\"\n    BASE_URL = \"https://data.ustrotting.com/api/racenet/racing/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches all harness races for a given date.\"\"\"\n        response = await self.make_request(self.http_client, \"GET\", f\"card/{date}\")\n\n        if not response:\n            return None\n\n        card_data = response.json()\n        return {\"data\": card_data, \"date\": date}\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"data\") or not raw_data.get(\"data\", {}).get(\"meetings\"):\n            self.logger.warning(\"No meetings found in harness data response.\")\n            return []\n\n        all_races = []\n        date = raw_data.get(\"date\")\n        for meeting in raw_data.get(\"data\", {}).get(\"meetings\", []):\n            track_name = meeting.get(\"track\", {}).get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name, date):\n                        all_races.append(race)\n                except Exception:\n                    self.logger.warning(\n                        \"Failed to parse harness race, skipping.\",\n                        race_data=race_data,\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: dict, track_name: str, date: str) -> Optional[Race]:\n        \"\"\"Parses a single race from the USTA API into a Race object.\"\"\"\n        race_number = race_data.get(\"raceNumber\")\n        post_time_str = race_data.get(\"postTime\")\n        if not all([race_number, post_time_str]):\n            return None\n\n        start_time = self._parse_post_time(date, post_time_str)\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            if runner_data.get(\"scratched\", False):\n                continue\n\n            odds_str = runner_data.get(\"morningLineOdds\", \"\")\n            if \"/\" not in odds_str and odds_str.isdigit():\n                odds_str = f\"{odds_str}/1\"\n\n            odds = {}\n            win_odds = parse_odds_to_decimal(odds_str)\n            if win_odds and win_odds < 999:\n                odds = {\n                    self.SOURCE_NAME: OddsData(\n                        win=win_odds,\n                        source=self.SOURCE_NAME,\n                        last_updated=datetime.now(),\n                    )\n                }\n\n            runners.append(\n                Runner(\n                    number=runner_data.get(\"postPosition\", 0),\n                    name=runner_data.get(\"horse\", {}).get(\"name\", \"Unknown Horse\"),\n                    odds=odds,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"ust_{track_name.lower().replace(' ', '')}_{date}_{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n    def _parse_post_time(self, date: str, post_time: str) -> datetime:\n        \"\"\"Parses a time string like '07:00 PM' into a timezone-aware datetime object.\"\"\"\n        dt_str = f\"{date} {post_time}\"\n        naive_dt = datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        # Assume Eastern Time for USTA data, a common standard for US racing.\n        eastern = ZoneInfo(\"America/New_York\")\n        return naive_dt.replace(tzinfo=eastern)\n",
    "web_service/backend/adapters/punters_adapter.py": "# python_service/adapters/punters_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass PuntersAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for punters.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Punters\"\n    BASE_URL = \"https://www.punters.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass SportingLifeAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/racing/racecards/{date}\"\n        index_response = await self.make_request(\n            self.http_client, \"GET\", index_url, headers=self._get_headers()\n        )\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\n                self.http_client, \"GET\", url_path, headers=self._get_headers()\n            )\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.sportinglife.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to SportingLifeAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"a.hr-race-header-course-name__link\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.hr-race-header-time__time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n                race_number = 1\n                if active_link:\n                    all_links = soup.select(\"a.hr-race-header-navigation-link\")\n                    try:\n                        race_number = all_links.index(active_link) + 1\n                    except ValueError:\n                        pass  # Keep default race number if active link not in all links\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from SportingLife, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.hr-racing-runner-horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.hr-racing-runner-saddle-cloth-no\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.select_one(\"span.hr-racing-runner-odds\")\n            odds_str = clean_text(odds_node.get_text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on SportingLife, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US racing data from the TVG API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v2/races/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(self.http_client, \"GET\", summary_url, headers=headers)\n        if not tracks_response:\n            return None\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get(\"tracks\", []):\n            track_id = track.get(\"id\")\n            for race in track.get(\"races\", []):\n                race_id = race.get(\"id\")\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(self.http_client, \"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        # Filter out exceptions and return only successful responses\n        return [resp.json() for resp in race_detail_responses if resp and not isinstance(resp, Exception)]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of detailed race JSON objects into Race models.\"\"\"\n        races = []\n        if not isinstance(raw_data, list):\n            self.logger.warning(\"raw_data is not a list, cannot parse TVG races.\")\n            return races\n\n        for race_detail in raw_data:\n            try:\n                if race := self._parse_race(race_detail):\n                    races.append(race)\n            except AdapterParsingError:\n                self.logger.warning(\n                    \"Failed to parse TVG race detail, skipping.\",\n                    race_detail=race_detail,\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_race(self, race_detail: dict) -> Optional[Race]:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get(\"track\")\n        race_info = race_detail.get(\"race\")\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get(\"runners\", []):\n            if runner_data.get(\"scratched\"):\n                continue\n\n            odds = runner_data.get(\"odds\", {})\n            current_odds = odds.get(\"currentPrice\", {})\n            odds_str = current_odds.get(\"fractional\") or odds.get(\"morningLinePrice\", {}).get(\"fractional\")\n\n            try:\n                number = int(runner_data.get(\"programNumber\", \"0\").replace(\"A\", \"\"))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            odds_data = {}\n            if odds_str:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n\n            runners.append(\n                Runner(\n                    number=number,\n                    name=clean_text(runner_data.get(\"name\")),\n                    odds=odds_data,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        post_time = race_info.get(\"postTime\")\n        if not post_time:\n            raise AdapterParsingError(self.source_name, \"Missing post time.\")\n\n        try:\n            start_time = datetime.fromisoformat(post_time.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name,\n                f\"Could not parse post time: {post_time}\",\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get(\"name\"),\n            race_number=race_info.get(\"number\"),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n    \n    valid_odds = []\n    for source_data in runner.odds.values():\n        # Handle both dict and primitive formats\n        if isinstance(source_data, dict):\n            win = source_data.get('win')\n        elif hasattr(source_data, 'win'):\n            win = source_data.win\n        else:\n            win = source_data\n        \n        if win is not None and 0 < win < 999:\n            valid_odds.append(win)\n    \n    return min(valid_odds) if valid_odds else None\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(\n        self,\n        max_field_size: int = 10,\n        min_favorite_odds: float = 2.5,\n        min_second_favorite_odds: float = 4.0,\n    ):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        qualified_races = []\n        for race in races:\n            score = self._evaluate_race(race)\n            if score > 0:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\n            \"Universal scoring complete\",\n            total_races_scored=len(qualified_races),\n            criteria=criteria,\n        )\n\n        for race in qualified_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": qualified_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply hard filters before scoring ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            return 0.0\n\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass TinyFieldTrifectaAnalyzer(TrifectaAnalyzer):\n    \"\"\"A specialized TrifectaAnalyzer that only considers races with 8 or fewer runners.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Override the max_field_size to 8 for \"tiny field\" analysis\n        super().__init__(max_field_size=8, min_favorite_odds=0.75, min_second_favorite_odds=2.0, **kwargs)\n\n    @property\n    def name(self) -> str:\n        return \"tiny_field_trifecta_analyzer\"\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        self.register_analyzer(\"tiny_field_trifecta\", TinyFieldTrifectaAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier(\"Fortuna\") if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "web_service/backend/core/errors.py": "# python_service/core/errors.py\nfrom enum import Enum\n\n\nclass ErrorCategory(Enum):\n    CONFIGURATION_ERROR = \"Configuration missing or invalid\"\n    NETWORK_ERROR = \"HTTP/Network request failed\"\n    PARSING_ERROR = \"Data parsing or validation unsuccessful\"\n    UNEXPECTED_ERROR = \"An unhandled exception occurred\"\n",
    "web_service/backend/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\n\n# from .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .adapter_manager import AdapterHealthMonitor, AdapterHealth, AdapterStatus\nfrom .cache_manager import StaleDataCache\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n        exclude_adapters: Optional[List[str]] = None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n        self.health_monitor = AdapterHealthMonitor()\n        self.stale_data_cache = StaleDataCache(max_age_hours=24)\n        self.exclude_adapters = set(exclude_adapters) if exclude_adapters else set()\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: Dict[str, BaseAdapterV3] = {}\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                adapter_name = adapter_cls.__name__\n                if adapter_name in self.exclude_adapters:\n                    self.logger.info(f\"Intentionally skipping adapter: {adapter_name}\")\n                    continue\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_name}\")\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_name}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters[adapter_instance.source_name] = adapter_instance\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_name,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_name}\",\n                        exc_info=True,\n                    )\n\n            for adapter_instance in self.adapters.values():\n                self.health_monitor.statuses[adapter_instance.source_name] = AdapterStatus(\n                    name=adapter_instance.source_name,\n                    health=AdapterHealth.HEALTHY,\n                    success_rate_24h=1.0,\n                    last_success=None,\n                    consecutive_failures=0,\n                    avg_response_time_ms=0,\n                    last_error=None,\n                )\n            # Special case for BetfairDataScientistAdapter with extra args - DISABLED\n            # try:\n            #     bds_adapter = BetfairDataScientistAdapter(\n            #         model_name=\"ThoroughbredModel\",\n            #         url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n            #         config=self.config,\n            #     )\n            #     if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n            #         bds_adapter.enable_manual_override(manual_override_manager)\n            #     self.adapters.append(bds_adapter)\n            # except Exception:\n            #     self.logger.warning(\n            #         \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n            #         exc_info=True,\n            #     )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters.values():\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters.values()]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            processed_races = []\n            for race_data in race_data_list:\n                if isinstance(race_data, Race):\n                    processed_races.append(race_data)\n                else:\n                    processed_races.append(Race(**race_data))\n            races = processed_races\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        # Update health monitor\n        await self.health_monitor.update_adapter_status(\n            adapter_name=adapter.source_name,\n            success=is_success,\n            latency_ms=duration * 1000,\n            error=error_message,\n        )\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    def _calculate_coverage(self, results: List[Dict[str, Any]]) -> float:\n        # Stub implementation\n        return 0.0\n\n    def _merge_adapter_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for adapter_result in results:\n            source_info = adapter_result.get(\"source_info\", {})\n            source_infos.append(source_info)\n            if source_info.get(\"status\") == \"SUCCESS\":\n                all_races.extend(adapter_result.get(\"races\", []))\n            else:\n                errors.append({\n                    \"adapter_name\": source_info.get(\"name\"),\n                    \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                    \"attempted_url\": source_info.get(\"attempted_url\")\n                })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        return {\n            \"races\": deduped_races,\n            \"errors\": errors,\n            \"source_info\": source_infos\n        }\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None, min_required_adapters: int = 2) -> Dict[str, Any]:\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        # Re-introduce live caching\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        all_payloads = []\n        attempted_adapters = []\n        all_adapter_names = list(self.adapters.keys())\n        if source_filter:\n            all_adapter_names = [name for name in all_adapter_names if name.lower() == source_filter.lower()]\n\n        ordered_adapter_names = self.health_monitor.get_ordered_adapters(all_adapter_names)\n\n        # Tier 1: Healthy\n        healthy_names = [name for name in ordered_adapter_names if self.health_monitor.statuses[name].health == AdapterHealth.HEALTHY]\n        if healthy_names:\n            tasks = [self._fetch_with_semaphore(self.adapters[name], date) for name in healthy_names]\n            attempted_adapters.extend(healthy_names)\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            for res in results:\n                if not isinstance(res, Exception):\n                    _adapter_name, payload, _duration = res\n                    all_payloads.append(payload)\n\n        successful_count = len([p for p in all_payloads if p['source_info']['status'] == 'SUCCESS'])\n\n        # Tier 2: Degraded\n        if successful_count < min_required_adapters:\n            degraded_names = [name for name in ordered_adapter_names if self.health_monitor.statuses[name].health == AdapterHealth.DEGRADED]\n            if degraded_names:\n                tasks = [self._fetch_with_semaphore(self.adapters[name], date) for name in degraded_names]\n                attempted_adapters.extend(degraded_names)\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n                for res in results:\n                    if not isinstance(res, Exception):\n                        _adapter_name, payload, _duration = res\n                        all_payloads.append(payload)\n\n        # Tier 3: Stale cache fallback\n        if not any(p['source_info']['status'] == 'SUCCESS' for p in all_payloads):\n            log.warning(\"All live adapters failed, attempting to use stale cache.\")\n            stale_data = await self.stale_data_cache.get(date)\n            if stale_data:\n                log.info(\"Using stale data from cache.\", cache_age_hours=stale_data['age_hours'])\n                stale_results = stale_data['data']\n                stale_results['metadata']['data_freshness'] = 'stale'\n                stale_results['warnings'] = [\"Using cached data from a previous run as all live sources failed.\"]\n                return stale_results\n\n        if not all_payloads:\n            log.error(\"All adapter fetches failed and no stale data available.\")\n            return {\"races\": [], \"errors\": [{\"adapter_name\": \"all\", \"error_message\": \"All adapters failed and no stale data available.\"}], \"source_info\": [], \"metadata\": {}}\n\n        merged_results = self._merge_adapter_results(all_payloads)\n        successful_count = len([s for s in merged_results[\"source_info\"] if s[\"status\"] == \"SUCCESS\"])\n\n        try:\n            parsed_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except (ValueError, TypeError):\n            parsed_date = datetime.now().date()\n\n        response_obj = AggregatedResponse(\n            date=parsed_date,\n            races=merged_results[\"races\"],\n            errors=merged_results[\"errors\"],\n            source_info=merged_results[\"source_info\"],\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": attempted_adapters,\n                \"sources_successful\": successful_count,\n                \"total_races\": len(merged_results[\"races\"]),\n                \"total_errors\": len(merged_results[\"errors\"]),\n                'coverage': self._calculate_coverage(all_payloads),\n                'data_freshness': 'live'\n            },\n        )\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Cache successful live results\n        if successful_count > 0:\n            await self.stale_data_cache.set(date, response_data)\n            await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "web_service/backend/fortuna_service.py": "# fortuna_service.py\n# The main service runner, upgraded to the final Endgame architecture.\n\nimport json\nimport logging\nimport os\nimport sqlite3\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom .analyzer import TrifectaAnalyzer\nfrom .engine import Race\nfrom .engine import Settings\nfrom .engine import SuperchargedOrchestrator\n\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            # Correctly resolve paths from the service's location\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n            schema_path = os.path.join(base_dir, \"shared_database\", \"schema.sql\")\n            web_schema_path = os.path.join(base_dir, \"shared_database\", \"web_schema.sql\")\n\n            # Read both schema files\n            with open(schema_path, \"r\") as f:\n                schema = f.read()\n            with open(web_schema_path, \"r\") as f:\n                web_schema = f.read()\n\n            # Apply both schemas in a single transaction\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.executescript(schema)\n                cursor.executescript(web_schema)\n                conn.commit()\n            self.logger.info(\"CRITICAL SUCCESS: All database schemas (base + web) applied successfully.\")\n        except Exception as e:\n            self.logger.critical(\n                f\"FATAL: Database setup failed. Other platforms will fail. Error: {e}\",\n                exc_info=True,\n            )\n            raise\n\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO live_races (\n                        race_id, track_name, race_number, post_time, raw_data_json,\n                        fortuna_score, qualified, trifecta_factors_json, updated_at\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        race.race_id,\n                        race.track_name,\n                        race.race_number,\n                        race.post_time,\n                        race.model_dump_json(),\n                        race.fortuna_score,\n                        race.is_qualified,\n                        race.trifecta_factors_json,\n                        datetime.now(),\n                    ),\n                )\n            for status in statuses:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO adapter_status (\n                        adapter_name, status, last_run, races_found, error_message,\n                        execution_time_ms\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        status.get(\"adapter_id\"),\n                        status.get(\"status\"),\n                        status.get(\"timestamp\"),\n                        status.get(\"races_found\"),\n                        status.get(\"error_message\"),\n                        int(status.get(\"response_time\", 0) * 1000),\n                    ),\n                )\n\n            if races or statuses:\n                cursor.execute(\n                    \"INSERT INTO events (event_type, payload) VALUES (?, ?)\",\n                    (\"RACES_UPDATED\", json.dumps({\"race_count\": len(races)})),\n                )\n\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\")\n\n\nclass FortunaBackgroundService:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        from dotenv import load_dotenv\n\n        dotenv_path = os.path.join(os.path.dirname(__file__), \"..\", \".env\")\n        load_dotenv(dotenv_path=dotenv_path)\n\n        db_path = os.getenv(\"FORTUNA_DB_PATH\")\n        if not db_path:\n            self.logger.critical(\"FATAL: FORTUNA_DB_PATH environment variable not set. Service cannot start.\")\n            raise ValueError(\"FORTUNA_DB_PATH is not configured.\")\n\n        self.logger.info(f\"Database path loaded from environment: {db_path}\")\n\n        self.settings = Settings()\n        self.db_handler = DatabaseHandler(db_path)\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\n        self.python_analyzer = TrifectaAnalyzer(self.settings)\n        self.stop_event = threading.Event()\n        self.rust_engine_path = os.path.join(\n            os.path.dirname(__file__),\n            \"..\",\n            \"rust_engine\",\n            \"target\",\n            \"release\",\n            \"fortuna_engine.exe\",\n        )\n\n    def _analyze_with_rust(self, races: List[Race]) -> Optional[List[Race]]:\n        self.logger.info(\"Attempting analysis with external Rust engine.\")\n        try:\n            race_data_json = json.dumps([r.model_dump() for r in races])\n            result = subprocess.run(\n                [self.rust_engine_path],\n                input=race_data_json,\n                capture_output=True,\n                text=True,\n                check=True,\n                timeout=30,\n            )\n            results_data = json.loads(result.stdout)\n            results_map = {res[\"race_id\"]: res for res in results_data}\n\n            for race in races:\n                if race.race_id in results_map:\n                    res = results_map[race.race_id]\n                    race.fortuna_score = res.get(\"fortuna_score\")\n                    race.is_qualified = res.get(\"qualified\")\n                    race.trifecta_factors_json = json.dumps(res.get(\"trifecta_factors\"))\n            return races\n        except FileNotFoundError:\n            self.logger.warning(\"Rust engine not found. Falling back to Python analyzer.\")\n            return None\n        except (\n            subprocess.CalledProcessError,\n            json.JSONDecodeError,\n            subprocess.TimeoutExpired,\n        ) as e:\n            self.logger.error(f\"Rust engine execution failed: {e}. Falling back to Python analyzer.\")\n            return None\n\n    def _analyze_with_python(self, races: List[Race]) -> List[Race]:\n        self.logger.info(\"Performing analysis with internal Python engine.\")\n        return [self.python_analyzer.analyze_race_advanced(race) for race in races]\n\n    def run_continuously(self, interval_seconds: int = 60):\n        self.logger.info(\"Background service thread starting continuous run.\")\n\n        while not self.stop_event.is_set():\n            try:\n                self.logger.info(\"Starting data collection and analysis cycle.\")\n                races, statuses = self.orchestrator.get_races_parallel()\n\n                analyzed_races = None\n                if os.path.exists(self.rust_engine_path):\n                    analyzed_races = self._analyze_with_rust(races)\n\n                if analyzed_races is None:  # Fallback condition\n                    analyzed_races = self._analyze_with_python(races)\n\n                if analyzed_races:  # Ensure we have something to update\n                    self.db_handler.update_races_and_status(analyzed_races, statuses)\n\n            except Exception as e:\n                self.logger.critical(f\"Unhandled exception in service loop: {e}\", exc_info=True)\n\n            self.logger.info(f\"Cycle complete. Sleeping for {interval_seconds} seconds.\")\n            self.stop_event.wait(interval_seconds)\n        self.logger.info(\"Background service run loop has terminated.\")\n\n    def start(self):\n        self.stop_event.clear()\n        self.thread = threading.Thread(target=self.run_continuously)\n        self.thread.daemon = True\n        self.thread.start()\n        self.logger.info(\"FortunaBackgroundService started.\")\n\n    def stop(self):\n        self.stop_event.set()\n        if hasattr(self, \"thread\") and self.thread.is_alive():\n            self.thread.join(timeout=10)\n        self.logger.info(\"FortunaBackgroundService stopped.\")\n",
    "web_service/backend/initialize_db.py": "# python_service/initialize_db.py\nfrom db.init import initialize_database\n\n\ndef main():\n    \"\"\"\n    This script exists solely to initialize the database.\n    It should be called before the main server process is started.\n    \"\"\"\n    print(\"Initializing database...\", flush=True)\n    initialize_database()\n    print(\"Database initialization complete.\", flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "web_service/backend/middleware/error_handler.py": "# python_service/middleware/error_handler.py\n\nfrom fastapi import Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\n\nfrom ..user_friendly_errors import ERROR_MAP\n\n\nclass UserFriendlyException(Exception):\n    def __init__(self, error_key: str, status_code: int = 500, details: str = None):\n        self.error_key = error_key\n        self.status_code = status_code\n        self.details = details\n        error_info = ERROR_MAP.get(error_key, ERROR_MAP[\"default\"])\n        self.message = error_info[\"message\"]\n        self.suggestion = error_info[\"suggestion\"]\n        super().__init__(self.message)\n\n\nasync def user_friendly_exception_handler(request: Request, exc: UserFriendlyException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\n            \"error\": {\n                \"message\": exc.message,\n                \"suggestion\": exc.suggestion,\n                \"details\": exc.details,\n            }\n        },\n    )\n\n\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"Convert Pydantic validation errors to user-friendly messages.\"\"\"\n    return JSONResponse(\n        status_code=422,\n        content={\n            \"detail\": \"Invalid request parameters\",\n            \"errors\": [\n                {\n                    \"field\": error[\"loc\"][-1] if error[\"loc\"] else \"unknown\",\n                    \"message\": error[\"msg\"],\n                    \"type\": error[\"type\"],\n                }\n                for error in exc.errors()\n            ],\n        },\n    )\n",
    "web_service/backend/requirements-x86-constraints.txt": "# x86 Build Constraints - MANDATORY for successful x86 builds\n# These packages MUST be installed from pre-built wheels only\n#\n# USAGE:\n#   pip install --platform win32 --only-binary=:all: -r requirements-x86-constraints.txt\n#\n# The --only-binary=:all: flag ensures all packages come from wheels, not source.\n# If a wheel is not available, the installation will FAIL LOUDLY instead of\n# attempting to compile from source (which will fail silently on x86).\n\n# Database and ORM\nsqlalchemy==2.0.28\ngreenlet==3.0.3\n\n# Data Science Stack (use older versions with guaranteed x86 wheel availability)\npandas==1.5.3\nnumpy==1.23.5\nscipy==1.10.1\n\n# Note: These versions are older than the main requirements.txt to ensure\n# pre-built x86 wheels exist. This is a necessary trade-off for x86 support.\n",
    "web_service/backend/security.py": "# python_service/security.py\n\nimport secrets\nimport os\n\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Security\nfrom fastapi import status\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings\nfrom .config import get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\nasync def verify_api_key(key: str = Security(api_key_header), settings: Settings = Depends(get_settings)):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n\n    In a CI environment, this check is bypassed to allow for automated testing.\n    \"\"\"\n    is_ci = os.environ.get(\"CI\", \"false\").lower() in (\"true\", \"1\", \"yes\")\n    if is_ci:\n        return True\n\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid or missing API Key\")\n",
    "web_service/backend/utils/__init__.py": "",
    "web_service/frontend/.gitignore": "# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.\n\n# Dependencies\n/node_modules\n/.pnp\n.pnp.js\n\n# Testing\n/coverage\n\n# Next.js\n/.next/\n/out/\n\n# Production\n/build\n\n# Misc\n.DS_Store\n*.pem\n\n# Local .env files\n.env.local\n.env.development.local\n.env.test.local\n.env.production.local\n\n# Log files\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nlerna-debug.log*\n\n# Editor directories and files\n.vscode\n.idea\n*.suo\n*.ntvs*\n*.njsproj\n*.sln\n*.sw?",
    "web_service/frontend/app/Providers.tsx": "// web_platform/frontend/app/Providers.tsx\n'use client';\n\nimport { QueryClientProvider } from '@tanstack/react-query';\nimport { queryClient } from './lib/queryClient';\nimport React from 'react';\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>\n  );\n}\n",
    "web_service/frontend/app/components/LiveRaceDashboard.tsx": "// web_platform/frontend/src/components/LiveRaceDashboard.tsx\n'use client';\n\nimport React, { useState, useEffect, useCallback } from 'react';\nimport { useQuery, useQueryClient } from '@tanstack/react-query';\nimport { RaceFilters } from './RaceFilters';\nimport { RaceCard } from './RaceCard';\nimport { RaceCardSkeleton } from './RaceCardSkeleton';\nimport { EmptyState } from './EmptyState';\nimport { ErrorDisplay } from './ErrorDisplay';\nimport { Race, SourceInfo, AdapterError, AggregatedRacesResponse } from '../types/racing';\nimport { useWebSocket } from '../hooks/useWebSocket';\nimport { StatusDetailModal } from './StatusDetailModal';\nimport ManualOverridePanel from './ManualOverridePanel';\nimport { LiveModeToggle } from './LiveModeToggle';\nimport { AdapterStatusPanel } from './AdapterStatusPanel';\n\n// Type for the backend process status received from Electron main\ntype BackendState = 'starting' | 'running' | 'error' | 'stopped';\ninterface BackendStatus {\n  state: BackendState;\n  logs: string[];\n}\n\ninterface RaceFilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nconst fetchAdapterStatuses = async (apiKey: string | null): Promise<SourceInfo[]> => {\n  if (!apiKey) {\n    throw new Error('API key not available.');\n  }\n  const response = await fetch(`/api/adapters/status`, {\n    headers: { 'X-API-Key': apiKey, 'Content-Type': 'application/json' },\n  });\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n  return response.json();\n};\n\nconst fetchQualifiedRaces = async (apiKey: string | null, params: RaceFilterParams): Promise<AggregatedRacesResponse> => {\n  if (!apiKey) {\n    throw new Error('API key not available');\n  }\n  // In web service mode, API calls are relative to the current origin.\n  const response = await fetch(`/api/races`, {\n    headers: { 'X-API-Key': apiKey },\n  });\n\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n\n  return response.json();\n};\n\n\nconst BackendErrorPanel = ({ logs }: { logs: string[] }) => (\n  <div className=\"bg-slate-800 p-6 rounded-lg border border-red-500/50 text-white\">\n    <h2 className=\"text-2xl font-bold text-red-400 mb-4\">Backend Service Error</h2>\n    <p className=\"text-slate-400 mb-4\">The backend data service failed to start or has crashed. Below are the most recent diagnostic messages.</p>\n    <div className=\"bg-black p-4 rounded-md font-mono text-sm text-slate-300 h-64 overflow-y-auto mb-4\">\n      {logs.map((log, index) => (\n        <p key={index} className=\"whitespace-pre-wrap\">{`> ${log}`}</p>\n      ))}\n    </div>\n    <p className=\"text-sm text-slate-500 text-center mt-4\">Please check the server logs for more information.</p>\n  </div>\n);\n\n// New Sub-Component to display an error from a specific adapter\nconst ErrorCard = ({ source, message }: { source: string; message: string }) => (\n  <div className=\"bg-slate-800 rounded-lg p-4 border border-red-500/50 flex flex-col justify-between\">\n    <div>\n      <h3 className=\"font-bold text-red-400 text-lg\">{source} Failed</h3>\n      <p className=\"text-slate-400 text-sm mt-2\">{message}</p>\n    </div>\n    <div className=\"mt-4 text-xs text-slate-500\">\n      <p>This adapter failed to fetch data. This is not a critical error; other adapters may provide the necessary data.</p>\n    </div>\n  </div>\n);\n\n// New Sub-Component to render the grid of races or error cards\nconst RaceGrid = ({ races }: { races: Race[] }) => (\n  <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n    {races.map(race =>\n      race.isErrorPlaceholder ? (\n        <ErrorCard key={race.id} source={race.venue} message={race.errorMessage || 'An unknown error occurred.'} />\n      ) : (\n        <RaceCard key={race.id} race={race} />\n      )\n    )}\n  </div>\n);\n\n// In a pure web service, the frontend cannot know the backend's process status.\n// We assume it's always running if the frontend is served.\nconst useBackendStatus = (): BackendStatus => {\n  return { state: 'running', logs: ['Running in web service mode. Backend status is assumed to be active.'] };\n};\n\nexport const LiveRaceDashboard = React.memo(() => {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [adapterErrors, setAdapterErrors] = useState<AdapterError[]>([]);\n  const backendStatus = useBackendStatus();\n  // In a web service, the API key might be hardcoded, come from a meta tag, or an auth flow.\n  // For this version, we'll rely on the backend not requiring one from the same origin or use a known key.\n  const [apiKey, setApiKey] = useState<string | null>(\"a_secure_test_api_key_that_is_long_enough\");\n  const queryClient = useQueryClient();\n\n  const [params, setParams] = useState<RaceFilterParams>({\n    maxFieldSize: 10,\n    minFavoriteOdds: 2.5,\n    minSecondFavoriteOdds: 4.0,\n  });\n\n  const {\n    data,\n    status: connectionStatus,\n    error: errorDetails,\n    refetch,\n  } = useQuery({\n    queryKey: ['aggregatedRaces', apiKey],\n    queryFn: () => fetchQualifiedRaces(apiKey, params),\n    enabled: backendStatus.state === 'running' && !!apiKey,\n    refetchOnWindowFocus: true,\n  });\n\n  // Update state when data is successfully fetched\n  useEffect(() => {\n    if (data) {\n      setRaces(data.races || []);\n      setAdapterErrors(data.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [data]);\n\n  const { data: liveData, isConnected: isLiveConnected } = useWebSocket<AggregatedRacesResponse>(\n    '/ws/live-updates',\n    { apiKey } // Port is not needed for same-origin WebSockets\n  );\n\n  // Effect to update state when new live data arrives\n  useEffect(() => {\n    if (liveData) {\n      console.log('Received live data update:', liveData);\n      // Update the query cache and local state with the new data\n      queryClient.setQueryData(['aggregatedRaces', apiKey], liveData);\n      setRaces(liveData.races || []);\n      setAdapterErrors(liveData.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [liveData, queryClient, apiKey]);\n\n  const [lastUpdate, setLastUpdate] = useState<Date | null>(null);\n  const [isModalOpen, setIsModalOpen] = useState(false);\n\n\n  const handleParamsChange = useCallback((newParams: RaceFilterParams) => {\n    setParams(newParams);\n  }, []);\n\n  const handleParseSuccess = (adapterName: string, parsedRaces: Race[]) => {\n    queryClient.setQueryData(['qualifiedRaces', apiKey, params], (oldData: { races: Race[], source_info: SourceInfo[] } | undefined) => {\n      if (!oldData) return { races: parsedRaces, source_info: [] };\n\n      // 1. Remove the placeholder error card for this adapter\n      const otherRaces = oldData.races.filter(race => race.source !== adapterName);\n\n      // 2. Merge the new races in\n      const updatedRaces = [...otherRaces, ...parsedRaces].sort(\n        (a, b) => new Date(a.start_time).getTime() - new Date(b.start_time).getTime()\n      );\n\n      // 3. Update source_info to remove the failed source\n      const updatedSourceInfo = oldData.source_info.filter(s => s.name !== adapterName);\n\n      return { races: updatedRaces, source_info: updatedSourceInfo };\n    });\n  };\n\n  const renderContent = () => {\n    // Priority 1: Backend process has failed.\n    if (backendStatus.state === 'error') {\n      return <BackendErrorPanel logs={backendStatus.logs} />;\n    }\n\n    if (backendStatus.state === 'stopped') {\n        return <EmptyState\n            title=\"Backend Service Stopped\"\n            message=\"The backend data service is not running. Please start it to see live race data.\"\n        />;\n    }\n\n    // Priority 2: Backend is starting or initial fetch is happening.\n    const isLoading = backendStatus.state === 'starting' || (connectionStatus === 'pending' && !data);\n    if (isLoading) {\n        return (\n            <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n                {[...Array(8)].map((_, i) => <RaceCardSkeleton key={i} />)}\n            </div>\n        );\n    }\n\n    // Priority 3: API connection is offline.\n    if (connectionStatus === 'error') {\n      try {\n        const errorInfo = JSON.parse((errorDetails as Error).message);\n        return <ErrorDisplay error={errorInfo.error} />;\n      } catch (e) {\n        return <EmptyState\n            title=\"API Connection Offline\"\n            message={(errorDetails as Error)?.message || \"The backend is running, but the dashboard could not connect to its API.\"}\n            actionButton={<button onClick={() => refetch()} className=\"mt-4 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700\">Retry Connection</button>}\n        />;\n      }\n    }\n\n    // Priority 4: No races found after a successful fetch.\n    if (!races || races.length === 0) {\n      return <EmptyState\n          title=\"No Races Found\"\n          message=\"No races matched the specified criteria for the selected date. Please try different filters.\"\n      />;\n    }\n\n    // Priority 5: Display the races (and any error placeholders).\n    return <RaceGrid races={races} />;\n  };\n\n  const getStatusIndicator = () => {\n    if (backendStatus.state === 'error') {\n      return { color: 'bg-red-500', text: 'Backend Error' };\n    }\n    if (backendStatus.state === 'stopped') {\n        return { color: 'bg-gray-500', text: 'Stopped' };\n    }\n    if (backendStatus.state === 'starting') {\n      return { color: 'bg-yellow-500', text: 'Backend Starting...' };\n    }\n    if (isLiveConnected) {\n      return { color: 'bg-cyan-500', text: 'Live' };\n    }\n    return { color: 'bg-yellow-500', text: 'Connecting...' };\n  };\n\n  const { color: statusColor, text: statusText } = getStatusIndicator();\n\n  return (\n    <>\n      <div className=\"space-y-6\">\n        <div className=\"flex justify-between items-start\">\n            <div className=\"text-left space-y-2\">\n                <h1 className=\"text-4xl font-bold text-white\">\ud83c\udfc7 Fortuna Faucet</h1>\n                <p className=\"text-slate-400\">\n                Last updated: {lastUpdate ? lastUpdate.toLocaleTimeString() : 'N/A'}\n                </p>\n            </div>\n            <div className=\"flex items-center gap-4\">\n                <button\n                    onClick={() => (connectionStatus === 'error' || backendStatus.state === 'error') && setIsModalOpen(true)}\n                    className={`flex items-center gap-2 px-3 py-1.5 rounded-full text-sm font-medium text-white ${statusColor} ${(connectionStatus === 'error' || backendStatus.state === 'error') ? 'cursor-pointer hover:opacity-80' : 'cursor-default'}`}\n                    data-testid=\"status-indicator\"\n                >\n                    <span className={`w-2.5 h-2.5 rounded-full bg-white ${isLiveConnected ? 'animate-pulse' : ''}`}></span>\n                    {statusText}\n                </button>\n            </div>\n        </div>\n\n        <RaceFilters onParamsChange={handleParamsChange} isLoading={connectionStatus === 'pending'} refetch={refetch} />\n\n        {adapterErrors.map(error => (\n          <ManualOverridePanel\n            key={error.adapterName}\n            adapterName={error.adapterName}\n            attemptedUrl={error.attemptedUrl || 'URL not available'}\n            apiKey={apiKey}\n            onParseSuccess={handleParseSuccess}\n          />\n        ))}\n\n        {renderContent()}\n      </div>\n\n      <StatusDetailModal\n        isOpen={isModalOpen}\n        onClose={() => setIsModalOpen(false)}\n        status={{ title: 'Connection Error', details: (errorDetails as Error)?.message || 'No specific error message was provided.' }}\n      />\n    </>\n  );\n});\n",
    "web_service/frontend/app/components/LiveRaceDashboardNoSSR.tsx": "// web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx\nimport dynamic from 'next/dynamic';\n\nconst LiveRaceDashboardNoSSR = dynamic(\n  () => import('./LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  { ssr: false }\n);\n\nexport default LiveRaceDashboardNoSSR;\n",
    "web_service/frontend/app/components/ScoreBadge.tsx": "'use client';\nimport React from 'react';\n\nconst getScoreStyling = (score: number) => {\n  if (score >= 90) return { bg: 'bg-yellow-400/10', text: 'text-yellow-300', border: 'border-yellow-400' };\n  if (score >= 80) return { bg: 'bg-orange-500/10', text: 'text-orange-400', border: 'border-orange-500' };\n  return { bg: 'bg-sky-500/10', text: 'text-sky-400', border: 'border-sky-500' };\n};\n\nexport const ScoreBadge: React.FC<{ score: number }> = ({ score }) => {\n  const { bg, text } = getScoreStyling(score);\n  return (\n    <div className={`text-right ${text}`}>\n      <p className=\"text-3xl font-bold\">{score.toFixed(1)}</p>\n      <p className=\"text-xs font-medium tracking-wider uppercase\\\">Score</p>\n    </div>\n  );\n};",
    "web_service/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_service/frontend/app/page.tsx": "'use client';\nimport dynamic from 'next/dynamic';\nimport React from 'react';\nimport { Tabs } from './components/Tabs';\nimport { SettingsPage } from './components/SettingsPage';\n\nconst LiveRaceDashboard = dynamic(\n  () => import('./components/LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  {\n    ssr: false,\n    loading: () => <p className=\"text-center text-xl mt-8\">Loading Dashboard...</p>\n  }\n);\n\nexport default function Home() {\n  const tabs = [\n    {\n      label: 'Dashboard',\n      content: <LiveRaceDashboard />,\n    },\n    {\n      label: 'Settings',\n      content: <SettingsPage />,\n    },\n  ];\n\n  return (\n    <main className=\"min-h-screen bg-gradient-to-br from-slate-900 via-purple-900 to-slate-900 p-8\">\n      <div className=\"max-w-7xl mx-auto space-y-8\">\n        <h1 className=\"text-4xl font-bold text-white\" data-testid=\"main-heading\">Fortuna Faucet</h1>\n        <Tabs tabs={tabs} />\n      </div>\n    </main>\n  );\n}\n",
    "web_service/frontend/next.config.js": "/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  output: 'export',\n  distDir: 'build',\n  images: { unoptimized: true },\n  trailingSlash: true,\n}\nmodule.exports = nextConfig\n",
    "wix/WixUI_CustomProgress.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\">\n  <Fragment>\n    <UI>\n      <!-- Override the default InstallProgress dialog -->\n      <Dialog Id=\"InstallProgressDlg\" Width=\"370\" Height=\"270\" Title=\"Fortuna Faucet Installation\" Modeless=\"yes\">\n        <Control Id=\"Title\" Type=\"Title\" X=\"20\" Y=\"6\" Width=\"330\" Height=\"18\" Text=\"Installation Progress\" />\n        <Control Id=\"BannerBitmap\" Type=\"Bitmap\" X=\"0\" Y=\"0\" Width=\"370\" Height=\"44\" TabSkip=\"no\" Text=\"WixUI_Bmp_Banner\" />\n        <Control Id=\"Back\" Type=\"PushButton\" X=\"180\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Back\" Disabled=\"yes\" />\n        <Control Id=\"Next\" Type=\"PushButton\" X=\"236\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Next\" Disabled=\"yes\" />\n        <Control Id=\"Cancel\" Type=\"PushButton\" X=\"304\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"Cancel\" />\n\n        <Control Id=\"ActionText\" Type=\"Text\" X=\"70\" Y=\"80\" Width=\"280\" Height=\"20\" TabSkip=\"no\">\n          <Subscribe Event=\"ActionText\" Attribute=\"Text\" />\n        </Control>\n        <Control Id=\"Description\" Type=\"Text\" X=\"35\" Y=\"55\" Width=\"300\" Height=\"20\" Text=\"Please wait while the installer copies files.\" />\n\n        <!-- This is the new control to display the current filename -->\n        <Control Id=\"CurrentFileText\" Type=\"Text\" X=\"70\" Y=\"100\" Width=\"280\" Height=\"20\">\n            <Subscribe Event=\"SetProgress\" Attribute=\"Text\" />\n        </Control>\n\n        <Control Id=\"ProgressBar\" Type=\"ProgressBar\" X=\"35\" Y=\"120\" Width=\"300\" Height=\"10\" ProgressBlocks=\"yes\" Text=\"Progress\">\n          <Subscribe Event=\"SetProgress\" Attribute=\"Progress\" />\n        </Control>\n      </Dialog>\n\n      <!-- The Publish element must be a child of UI, not Dialog -->\n      <Publish Dialog=\"InstallProgressDlg\" Control=\"Cancel\" Event=\"SpawnDialog\" Value=\"CancelDlg\">1</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}