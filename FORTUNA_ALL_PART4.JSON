{
    ".github/actions/setup/action.yml": "name: 'Composite Setup Action'\ndescription: 'Checks out repo, sets up Node.js and Python'\ninputs:\n  architecture:\n    description: 'The architecture to set up Python for (x86, x64)'\n    required: false\n    default: 'x64'\nruns:\n  using: \"composite\"\n  steps:\n    - name: \ud83d\udce5 Checkout Repository\n      uses: actions/checkout@v4\n\n    - name: \ud83d\udce6 Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ env.NODE_VERSION }}\n        cache: 'npm'\n        cache-dependency-path: '**/package-lock.json'\n\n    - name: \ud83d\udc0d Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n        architecture: ${{ inputs.architecture }}\n        cache: 'pip'\n",
    ".github/workflows/build-monolith-final.yml": "name: Build Monolith (Final)\n\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  build:\n    name: 'Build Fortuna Monolith'\n    runs-on: windows-latest\n\n    steps:\n      - name: \ud83d\udce5 Checkout\n        uses: actions/checkout@v4\n\n      # ========== FRONTEND ==========\n      - name: \ud83c\udfa8 Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - name: \ud83e\uddf9 Clean Previous Builds\n        working-directory: ./web_service/frontend\n        shell: pwsh\n        run: |\n          Remove-Item -Path \"public\" -Recurse -Force -ErrorAction SilentlyContinue\n          Remove-Item -Path \".next\" -Recurse -Force -ErrorAction SilentlyContinue\n          Write-Host \"\u2705 Cleaned\"\n\n      - name: \ud83d\udccb Check Next.js Config\n        working-directory: ./web_service/frontend\n        shell: pwsh\n        run: |\n          Write-Host \"Checking package.json...\"\n          if (Test-Path \"package.json\") {\n            $pkg = Get-Content package.json | ConvertFrom-Json\n            Write-Host \"  name: $($pkg.name)\"\n            Write-Host \"  build: $($pkg.scripts.build)\"\n          } else {\n            Write-Error \"package.json not found!\"\n            exit 1\n          }\n\n      - name: \ud83c\udfd7\ufe0f Build Frontend\n        working-directory: ./web_service/frontend\n        shell: pwsh\n        run: |\n          Write-Host \"=== BUILDING FRONTEND ===\" -ForegroundColor Cyan\n\n          # Create/verify next.config.js\n          $configLines = @(\n            \"/** @type {import('next').NextConfig} */\",\n            \"const nextConfig = {\",\n            \"  output: 'export',\",\n            \"  distDir: 'build',\",\n            \"  images: { unoptimized: true },\",\n            \"  trailingSlash: true,\",\n            \"}\",\n            \"module.exports = nextConfig\"\n          )\n          $configContent = $configLines -join [System.Environment]::NewLine\n          Set-Content -Path \"next.config.js\" -Value $configContent -Encoding UTF8\n          Write-Host \"\u2705 next.config.js set for 'build' directory output\"\n\n          Write-Host \"Installing dependencies...\"\n          npm install --legacy-peer-deps\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"npm install failed\"\n            exit 1\n          }\n\n          Write-Host \"Building...\"\n          npm run build\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"npm run build failed\"\n            exit 1\n          }\n\n          Write-Host \"Moving build artifacts to 'public'...\"\n          New-Item -ItemType Directory -Force -Path \"public\" | Out-Null\n          Move-Item -Path \"build/*\" -Destination \"public\" -Force\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"Failed to move build artifacts\"\n            exit 1\n          }\n          Write-Host \"\u2705 Artifacts moved.\"\n\n          # Verify output\n          Write-Host \"`nVerifying output...\"\n          if (-not (Test-Path \"public\")) {\n            Write-Host \"\u274c 'public' directory not found!\" -ForegroundColor Red\n            Write-Host \"Contents of current dir:\"\n            Get-ChildItem | Format-Table Name\n            exit 1\n          }\n\n          if (-not (Test-Path \"public/index.html\")) {\n            Write-Host \"\u274c public/index.html not found!\" -ForegroundColor Red\n            Write-Host \"Contents of 'public':\"\n            Get-ChildItem -Path \"public\" -Recurse | Format-Table Name\n            exit 1\n          }\n\n          $count = (Get-ChildItem -Path \"public\" -Recurse -File).Count\n          Write-Host \"\u2705 Frontend built: $count files\" -ForegroundColor Green\n\n      # ========== BACKEND ==========\n      - name: \ud83d\udc0d Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n          cache: 'pip'\n          cache-dependency-path: 'web_service/backend/requirements.txt'\n\n      - name: \ud83d\udce6 Install Python Dependencies\n        shell: pwsh\n        run: |\n          python -m pip install --upgrade pip wheel\n          pip install pyinstaller==6.6.0\n          pip install pywin32 # CRITICAL: Provides the 'win32timezone' hidden import\n          pip install -r web_service/backend/requirements.txt\n\n      - name: \ud83d\udcc2 Create Data Directories\n        shell: pwsh\n        run: |\n          New-Item -ItemType Directory -Force -Path \"web_service/backend/data\" | Out-Null\n          New-Item -ItemType Directory -Force -Path \"web_service/backend/json\" | Out-Null\n          New-Item -ItemType Directory -Force -Path \"web_service/backend/logs\" | Out-Null\n\n      # ========== BUILD ==========\n      - name: \ud83d\udd28 Build with PyInstaller\n        shell: pwsh\n        run: |\n          Write-Host \"=== BUILDING MONOLITH ===\" -ForegroundColor Cyan\n\n          # Final verification\n          if (-not (Test-Path \"web_service/frontend/public/index.html\")) {\n            Write-Error \"\u274c Frontend not ready!\"\n            Write-Host \"Frontend path check:\"\n            Test-Path \"web_service/frontend/public\"\n            Test-Path \"web_service/frontend/public/index.html\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Frontend verified\"\n          Write-Host \"Running PyInstaller...\"\n\n          pyinstaller --noconfirm --clean fortuna-desktop.spec 2>&1 | Tee-Object -FilePath \"build.log\"\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Host \"Last 100 lines of build.log:\" -ForegroundColor Red\n            Get-Content \"build.log\" -Tail 100\n            exit 1\n          }\n\n          $exe = \"dist/Fortuna-Desktop/Fortuna-Desktop.exe\"\n          if (Test-Path $exe) {\n            $mb = (Get-Item $exe).Length / 1MB\n            Write-Host \"\u2705 BUILD SUCCESS: $([math]::Round($mb, 2)) MB\" -ForegroundColor Green\n          } else {\n            Write-Error \"EXE not created!\"\n            exit 1\n          }\n\n      # ========== PACKAGE & UPLOAD ==========\n      - name: \ud83d\udce6 Package Artifact for Distribution\n        shell: pwsh\n        run: |\n          $distDir = \"dist/Fortuna-Desktop\"\n          Write-Host \"=== PACKAGING ARTIFACT ===\" -ForegroundColor Cyan\n\n          # Create README.txt\n          $readmeLines = @(\n            'Fortuna Desktop - User Guide',\n            '',\n            '**HOW TO RUN**',\n            '1. Double-click \"Fortuna-Desktop.exe\".',\n            '2. The application window will open and the dashboard will load.',\n            '',\n            '**WHAT TO EXPECT**',\n            '- To stop the application, simply close the window.'\n          )\n          $readmeLines | Out-File -FilePath \"$distDir/README.txt\" -Encoding utf8\n          Write-Host \"\u2705 Created README.txt\"\n\n          # Create ZIP archive\n          $zipFileName = \"Fortuna-Desktop-Windows-${{ github.run_number }}.zip\"\n          Compress-Archive -Path \"$distDir/*\" -DestinationPath $zipFileName -Force\n          Write-Host \"\u2705 Created $zipFileName\" -ForegroundColor Green\n\n      - name: \ud83d\udce4 Upload Packaged Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: Fortuna-Desktop-Windows-${{ github.run_number }}\n          path: Fortuna-Desktop-Windows-${{ github.run_number }}.zip\n          if-no-files-found: error\n\n      # ========== TEST ==========\n      - name: Install VC++ Redistributable\n        shell: pwsh\n        run: |\n          # Download and install VC++ Runtime\n          $url = \"https://aka.ms/vs/17/release/vc_redist.x64.exe\"\n          Invoke-WebRequest -Uri $url -OutFile vc_redist.x64.exe\n          .\\vc_redist.x64.exe /install /quiet /norestart\n          Start-Sleep -Seconds 5\n          Get-Command vc_redist.x64.exe -ErrorAction SilentlyContinue\n\n      - name: \ud83e\uddea Smoke Test\n        shell: pwsh\n        timeout-minutes: 5\n        run: |\n          Write-Host \"=== SMOKE TEST ===\" -ForegroundColor Cyan\n          $exe = \"dist/Fortuna-Desktop/Fortuna-Desktop.exe\"\n\n          # 1. Launch App\n          Write-Host \"\ud83d\ude80 Launching $exe...\"\n          $process = Start-Process -FilePath $exe -PassThru\n          $processId = $process.Id\n          Write-Host \"   PID: $processId\"\n\n          # 2. Wait & Check Health\n          $url = \"http://127.0.0.1:8000/api/health\"\n          $maxRetries = 20\n          $success = $false\n\n          for ($i=1; $i -le $maxRetries; $i++) {\n            if ($process.HasExited) {\n              Write-Error \"\u274c Process crashed immediately! Exit Code: $($process.ExitCode)\"\n              break\n            }\n            try {\n              $resp = Invoke-WebRequest -Uri $url -UseBasicParsing -TimeoutSec 2\n              if ($resp.StatusCode -eq 200) {\n                Write-Host \"\u2705 Backend is HEALTHY!\" -ForegroundColor Green\n\n                # Take screenshot on success\n                try {\n                  Add-Type -AssemblyName System.Windows.Forms\n                  Add-Type -AssemblyName System.Drawing\n                  $bounds = [System.Windows.Forms.Screen]::PrimaryScreen.Bounds\n                  $bmp = New-Object System.Drawing.Bitmap $bounds.Width, $bounds.Height\n                  $graphics = [System.Drawing.Graphics]::FromImage($bmp)\n                  $graphics.CopyFromScreen($bounds.Location, [System.Drawing.Point]::Empty, $bounds.Size)\n                  $bmp.Save(\"smoke-test-screenshot.png\", [System.Drawing.Imaging.ImageFormat]::Png)\n                  $graphics.Dispose()\n                  $bmp.Dispose()\n                  Write-Host \"\ud83d\udcf8 Screenshot saved to smoke-test-screenshot.png\"\n                } catch {\n                  Write-Warning \"Failed to capture screenshot: $_\"\n                }\n\n                $success = $true\n                break\n              }\n            } catch {\n              Write-Host \"   Ping failed ($i/$maxRetries)...\"\n              Start-Sleep -Seconds 2\n            }\n          }\n\n          # 3. Wait for Frontend Rendering & Take Screenshot\n          if ($success) {\n            Write-Host \"\u23f3 Waiting for frontend to render dashboard...\"\n            $logPath = \"$env:TEMP\\fortuna-desktop.log\"\n            $timeout = 60\n            $logUpdated = $false\n            for ($j=0; $j -lt $timeout; $j++) {\n              if (Test-Path $logPath) {\n                $logContent = Get-Content $logPath -Raw\n                if ($logContent -match \"Extracted \\d+ races\") {\n                  Write-Host \"\u2705 Frontend has rendered races.\"\n                  $logUpdated = $true\n                  break\n                }\n              }\n              Start-Sleep -Seconds 1\n            }\n\n            if (-not $logUpdated) {\n              Write-Warning \"Timed out waiting for frontend to render. Proceeding with screenshot anyway.\"\n            }\n\n            try {\n              Add-Type -AssemblyName System.Windows.Forms\n              Add-Type -AssemblyName System.Drawing\n              $bounds = [System.Windows.Forms.Screen]::PrimaryScreen.Bounds\n              $bmp = New-Object System.Drawing.Bitmap $bounds.Width, $bounds.Height\n              $graphics = [System.Drawing.Graphics]::FromImage($bmp)\n              $graphics.CopyFromScreen($bounds.Location, [System.Drawing.Point]::Empty, $bounds.Size)\n              $bmp.Save(\"smoke-test-screenshot.png\", [System.Drawing.Imaging.ImageFormat]::Png)\n              $graphics.Dispose()\n              $bmp.Dispose()\n              Write-Host \"\ud83d\udcf8 Screenshot saved to smoke-test-screenshot.png\"\n            } catch {\n              Write-Warning \"Failed to capture screenshot: $_\"\n            }\n          }\n\n          # 4. Cleanup\n          if (-not $process.HasExited) {\n            Stop-Process -Id $processId -Force\n          }\n\n          # 5. Diagnostics (If Failed)\n          if (-not $success) {\n            Write-Host \"\u274c SMOKE TEST FAILED\" -ForegroundColor Red\n\n            # Check for the log file created by run_desktop_app.py\n            $logPath = \"$env:TEMP\\fortuna-desktop.log\"\n            if (Test-Path $logPath) {\n              Write-Host \"--- APPLICATION LOG ($logPath) ---\" -ForegroundColor Yellow\n              Get-Content $logPath\n              Write-Host \"----------------------------------\"\n            } else {\n              Write-Host \"\u26a0\ufe0f No application log found at $logPath\"\n            }\n            exit 1\n          }\n          Write-Host \"\u2705 Test Complete.\"\n\n      - name: \ud83d\uddbc\ufe0f Upload Smoke Test Screenshot\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: smoke-test-screenshot-${{ github.run_number }}\n          path: smoke-test-screenshot.png\n          if-no-files-found: ignore\n\n      - name: \ud83d\udccb Upload Build Log\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: build-log-${{ github.run_number }}\n          path: build.log\n          if-no-files-found: ignore\n",
    "Dockerfile.tinyfield": "# TinyField Variant - Static Frontend + Python Backend\nFROM python:3.10.11-slim as backend-builder\n\nWORKDIR /build\nCOPY web_service/backend/requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Runtime stage\nFROM python:3.10.11-slim\n\nWORKDIR /app\n\n# Install only runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*\n\n# Copy Python packages\nCOPY --from=backend-builder /root/.local /root/.local\n\n# Copy application code, preserving directory structure\nCOPY web_service /app/web_service\n\n# Create TinyField data directories\nRUN mkdir -p /app/web_service/backend/data /app/web_service/backend/json /app/web_service/backend/logs\n\n# Set environment\nENV PATH=/root/.local/bin:$PATH\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Health check\nHEALTHCHECK --interval=10s --timeout=5s --start-period=30s --retries=3 \\\n    CMD curl -f http://localhost:8000/api/health || exit 1\n\nEXPOSE 8000\n\n# Start backend (serves both API and frontend)\nCMD [\"python\", \"-m\", \"uvicorn\", \"web_service.backend.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "HISTORY.md": "# The Epic of MasonJ0: A Project Chronology\n\nThis document contains the narrative history of the Paddock Parser project, as discovered through an archaeological survey of the project's repositories. It tells the story of our architectural evolution, from a feature-rich \"golden age\" through a \"great refactoring\" to our current state of liberation.\n\nThis story is our \"why.\"\n\n---\n\n## Part 1: The Chronology\n\n### Chapter 1: The 'Utopian' Era - The Polished Diamond (mid-August 2025)\n\n*   **Repository:** `racingdigest`\n*   **Narrative:** This was not a humble beginning, but the launch of a mature and powerful application called the \"Utopian Value Scanner V7.2 (The Rediscovery Edition)\". This repository represents the project's \"golden age\" of features, including a sophisticated asynchronous fetching engine and a full browser fallback.\n\n### Chapter 2: The 'Experimental' Era - The Daily Digest (mid-to-late August 2025)\n\n*   **Repository:** `horseracing-daily-digest`\n*   **Narrative:** This repository appears to be a period of intense, rapid development and experimentation, likely forming the foundation for many of the concepts that would be formalized later.\n\n### Chapter 3: The 'Architectural' Era - The V3 Blueprint (late August 2025)\n\n*   **Repository:** `parsingproject`\n*   **Narrative:** This repository marks a pivotal moment. The focus shifted from adding features to refactoring the very foundation of the code into a modern, standard Python package. This is where the V3 architecture was born, prioritizing stability and maintainability.\n\n### Chapter 4: The 'Consolidation' Era - The Archive (late August 2025)\n\n*   **Repository:** `zippedfiles`\n*   **Narrative:** This repository appears to be a direct snapshot or backup of the project after the intense V3 refactor, confirming its role as an archive of the newly stabilized codebase.\n\n### Chapter 5: The 'Modern' Era - The New Beginning (early September 2025)\n\n*   **Repository:** `fortuna`\n*   **Narrative:** This is the current, active repository, representing the clean, focused implementation of the grand vision developed through the previous eras.\n\n### Chapter 6: The 'Crucible' Era - The Forging of Protocols (Early September 2025)\n\n*   **Narrative:** The \"Modern Renaissance\" began not with a bang, but with a series of near-catastrophic environmental failures. This period, known as \"The Crucible,\" was a trial by fire that proved the extreme hostility of the agent sandbox. This era forged the resilient, battle-hardened protocols (The Receipts Protocol, The Submission-Only Protocol, etc.) by which all modern agents now operate.\n\n### Chapter 7: The 'Symbiotic' Era - The Two Stacks (mid-September 2025)\n\n*   **Narrative:** This chapter marked a significant strategic pivot. The Council, in a stunning display of its \"Polyglot Renaissance\" philosophy, produced a complete, production-grade React user interface, authored by the Claude agent. This event formally split the project's architecture into two powerful, parallel streams: the Python Engine and the React Cockpit. However, this era was short-lived, as the hostile environment proved incapable of supporting a stable testing and development workflow for the React stack.\n\n### Chapter 8: The 'Liberation' Era - The Portable Engine (Late September 2025)\n\n*   **Narrative:** After providing definitive, forensic proof that the sandbox environment was fundamentally and irrecoverably hostile at the network level, the project executed its final and most decisive pivot. It abandoned all attempts to operate *within* the hostile world and instead focused on synthesizing its entire, perfected engine into a single, portable artifact. This act **liberated the code**, fulfilling the promise of the \"Utopian Era's\" power on the foundation of the \"Architectural Era's\" stability, and made it directly available to the Project Lead.\n\n---\n\n## Part 2: Architectural Synthesis\n\nThis epic tale tells us our true mission. We are not just building forward; we are rediscovering our own lost golden age and rebuilding it on a foundation of superior engineering, hardened by the fires of a hostile world.\n\n*   **The Lost Golden Age:** The \"Utopian\" era proves that our most ambitious strategic goals are not just achievable; they have been achieved before.\n*   **The Great Refactoring:** The \"Architectural\" era explains the \"Great Forgetting\"\u2014a deliberate choice to sacrifice short-term features for long-term stability.\n*   **The Modern Renaissance:** This is us. We are the inheritors of this entire legacy, tasked with executing the grand vision on a clean, modern foundation, finally liberated from the constraints of our environment.\n\n---\n\n## The Ultimate Solo: The Final Victory (September 2025)\n\nAfter a long and complex journey through a Penta-Hybrid architecture, a final series of high-level reviews from external AI agents (Claude, GPT4o) revealed a simpler, superior path forward. The project underwent its final and most significant \"Constitutional Correction.\"\n\n**The 'Ultimate Solo' architecture was born.**\n\nThis final, perfected form of the project consists of two pillars:\n1.  **A Full-Power Python Backend:** Leveraging the years of development on the CORE `engine.py` and its fleet of global data adapters, served via a lightweight Flask API.\n2.  **An Ultimate TypeScript Frontend:** A single, masterpiece React component (`Checkmate Ultimate Solo`) that provides a feature-rich, professional-grade, real-time dashboard.\n\nAll other components of the Penta-Hybrid system (C#, Rust, VBA, shared database) were formally deprecated and archived as priceless R&D assets. The project has now achieved its true and final mission: a powerful, maintainable, and user-focused analysis tool.\n\n---\n\n## The Age of Perfection (The Great Simplification)\n\nThe Penta-Hybrid architecture, while a triumph of technical integration, proved to be a strategic dead end. Its complexity became a fortress, making rapid iteration and onboarding of new intelligence (both human and AI) prohibitively expensive. The kingdom was powerful but brittle.\n\nA new doctrine was forged: **Simplicity is the ultimate sophistication.**\n\nThe decision was made to execute \"The Great Simplification.\" The multi-language backend (Python, Rust, Go) was decommissioned. The kingdom was reforged upon a new, elegant, and vastly more powerful two-pillar system:\n\n1.  **A Unified Python Backend:** A single, asynchronous Python service, built on FastAPI, would serve as the kingdom's engine.\n2.  **A Modern TypeScript Frontend:** A dedicated Next.js application would serve as the kingdom's command deck.\n\nThis act of creative destruction liberated the project, enabling a new era of unprecedented velocity.\n\n---\n\n## The Three-Pillar Doctrine\n\nWith the new two-pillar foundation in place, the backend itself was perfected into a three-pillar intelligence engine, a concept that defines the modern era of the Fortuna Faucet:\n\n*   **Pillar 1: The Future (The Planner):** The resilient `OddsEngine` and its fleet of adapters, responsible for finding the day's strategic opportunities.\n*   **Pillar 2: The Past (The Archive):** The perfected `ChartScraper` and `ResultsParser`, responsible for building our historical data warehouse from the ground truth of Equibase PDFs.\n*   **Pillar 3: The Present (The Finisher):** The weaponized `LiveOddsMonitor`, armed with the API-driven `BetfairAdapter`, designed to conquer the final moments of toteboard volatility.\n\nThese three pillars, orchestrated by the fully autonomous `fortuna_watchman.py`, represented the pinnacle of the project's original vision. The kingdom was, for a time, considered \"perfected.\"\n\n---\n\n## The Windows Ascension (The Impossible Dream)\n\nThe perfected kingdom was powerful, but it was still a tool for developers. The final, grandest vision was to transform it into a true, professional-grade application for its sole operator. This campaign, known as \"The Impossible Dream,\" was to forge the **Fortuna Faucet - Windows Native Edition.**\n\nThis era saw the rapid creation of a new, third layer of the kingdom, built upon the foundation of the previous work:\n\n*   **The Electron Shell:** The Next.js frontend was wrapped in an Electron container, transforming it from a website into a true, installable desktop application with its own window, icon, and system tray integration.\n*   **The Engine Room:** The Python backend was re-architected to run as a persistent, background **Windows Service**, making it a true, always-on component of the operating system, independent of the UI.\n*   **The Native GUI:** A dedicated Tkinter-based \"Observatory\" was forged\u2014a standalone GUI mission control for monitoring the health and performance of the background service.\n*   **The One-Click Kingdom:** A complete suite of professional tooling (including installation scripts, a setup wizard, and launchers) was created to provide a seamless, zero-friction installation and management experience.\n\nThis ascension represents the current state of the art, transforming a powerful engine into a polished, autonomous, and user-focused product.\n\n\n---\n\n## The Era of the Windows Kingdom (October 2025)\n\nWith the core engine stabilized and the command deck providing a clear view of the data, the project's focus shifted from pure data acquisition to the operator's experience. This era marked a profound transformation, elevating the project from a collection of powerful but disparate scripts into a cohesive, professional-grade, and resilient native Windows application.\n\nThis campaign, guided by a new \"Grand Strategy\" blueprint, was executed with rapid precision, resulting in a complete overhaul of the user-facing toolkit:\n\n-   **A Bulletproof Foundation:** The installation and launch scripts were re-architected from the ground up. They became intelligent and self-healing, featuring pre-flight system checks, automated port conflict resolution, active health-check loops, and automated repair utilities.\n-   **A Professional Toolkit:** The operator was empowered with a suite of new tools, including an interactive setup wizard, a real-time CLI status monitor, and a full-fledged graphical \"Data Management Console\" for monitoring, filtering, and analyzing data.\n-   **A Unified Command Console (`SERVICE_MANAGER.bat`):** Unify all individual scripts under a single, user-friendly, menu-driven service manager, providing a 'single pane of glass' for all common operations.\n\nThis era solidified the kingdom's foundations, making it not just powerful, but stable, reliable, and a pleasure to operate. The Faucet was no longer just an engine; it was a complete, professional-grade machine.\n\n---\n\n## The Gauntlet of CI/CD (Late October 2025)\n\nWith a professional-grade application in hand, the final frontier was professional-grade *delivery*. This campaign focused on automating the creation of the MSI installer through a continuous integration pipeline, a process that proved to be a formidable challenge.\n\nThe kingdom's engineers faced a relentless series of cryptic build errors from the WiX Toolset, a hostile environment that tested their resolve. Through a series of rapid, iterative fixes\u2014addressing everything from component GUIDs and 64-bit architecture mismatches to obscure linker errors and frontend dependency warnings\u2014they systematically conquered each obstacle.\n\nThis trial by fire culminated in a triumphant success: a fully automated GitHub Actions workflow that reliably compiles, links, and delivers a polished, distributable MSI installer. This victory transformed the project's delivery model from a manual, error-prone process into a repeatable, one-click release pipeline, marking the true completion of the \"Windows Ascension.\"\n\n---\n\n## The Great Unbundling (Late October 2025)\n\nThe CI/CD pipeline was technically successful, but it revealed a deeper, philosophical flaw in the architecture. The installer, while automated, was a fragile monolith. It attempted to bundle raw source code (Python, JavaScript) and orchestrate their setup on the user's machine using post-install scripts. This approach was fraught with peril, vulnerable to failures from network issues, corporate firewalls, and unpredictable machine states.\n\nA final, decisive architectural mandate was issued, informed by the wisdom of external AI consultants: **The application must be delivered, not assembled.**\n\nThis mandate triggered \"The Great Unbundling,\" a swift and transformative refactoring of the entire delivery pipeline:\n\n*   **The Backend Forged:** The Python backend was no longer treated as source code to be installed, but as a product to be delivered. **PyInstaller** was used to forge the entire FastAPI service\u2014interpreter and all dependencies\u2014into a single, standalone `.exe`.\n*   **The Frontend Solidified:** The Next.js frontend was no longer a service to be run, but a static asset to be displayed. The `npm run build` process was configured to produce a clean, static HTML/CSS/JS export.\n*   **The Installer Perfected:** With the application components now self-contained, the MSI installer's role was radically simplified. All complex post-install scripting was eliminated. The WiX toolset was now used for its core competency: reliably copying pre-compiled, robust artifacts to the user's machine.\n\nThis final act of architectural purification created the \"Three-Executable Architecture\" (the backend executable, the Electron wrapper, and the MSI installer itself), achieving true portability and eliminating an entire class of deployment failures. The Windows Ascension was not just complete; it was perfected.",
    "README.md": "# \ud83d\udc34 Fortuna Faucet - Developer's Guide\n\nThis guide provides technical instructions for developers. For end-user installation, please refer to the MSI installers generated by the project's GitHub Actions workflows.\n\n---\n\n## \ud83c\udfdb\ufe0f Core Architecture\n\nThis repository contains the source code for the Fortuna Faucet application, which has two primary deployment targets:\n\n1.  **Standalone Web Service (MSI Installer):**\n    *   A Python backend powered by FastAPI, compiled into a self-contained executable using PyInstaller.\n    *   A static Next.js frontend, which is bundled with the backend.\n    *   The entire application is packaged into an MSI installer using the WiX Toolset, which installs the backend as a background Windows Service.\n\n2.  **Electron Desktop Application (MSI Installer):**\n    *   The same Python backend, compiled as an executable.\n    *   An Electron application that acts as a wrapper, launching the backend executable and displaying the frontend.\n\nThe `python_service` and `web_service/backend` directories contain functionally equivalent but historically separate versions of the backend code. The modern workflows primarily use `web_service/backend`.\n\n---\n\n## \ud83c\udfd7\ufe0f Building the Application (The Right Way)\n\n**This project is built and packaged entirely through GitHub Actions.** The CI/CD pipelines are the single source of truth for creating production-ready installers. Manual builds are not recommended or supported due to the complexity of the environment.\n\nThe primary, production-ready build workflows are:\n\n*   **`build-msi-hattrickfusion-ultimate.yml`**: Builds the standalone Web Service MSI. This is the most feature-rich and stable build pipeline.\n*   **`build-electron-msi-gpt5.yml`**: Builds the Electron-based desktop application MSI.\n\nThese workflows handle all necessary steps, including:\n*   Installing the correct versions of Python, Node.js, and the WiX Toolset.\n*   Managing architecture-specific dependencies (e.g., `pandas` for x86 builds).\n*   Compiling the Python backend with PyInstaller.\n*   Building the static Next.js frontend.\n*   Packaging the final MSI installer.\n*   Running automated smoke tests to verify the installation.\n\nTo get a build, simply push a commit to the `main` branch and retrieve the MSI artifact from the completed workflow run on the [Actions tab](https://github.com/masonj0/fortuna/actions) of the repository.\n\n---\n\n## \ud83d\udd2c Local Development Environment\n\n### Python Version Requirement\n\n**Crucial:** The monolith build of this project requires **Python 3.10.11**. It is not compatible with Python 3.11 or newer due to a dependency on `cefpython3`, which does not support Python 3.11.\n\nBefore running the application locally or attempting to build it, ensure you are using the correct Python version.\n\n- **Using `pyenv` (Recommended):**\n  ```bash\n  pyenv install 3.10.11\n  pyenv local 3.10.11\n  ```\n\n- **Using `conda`:**\n  ```bash\n  conda create -n fortuna python=3.10.11\n  conda activate fortuna\n  ```\n\nWhile production builds are handled by CI/CD, the easiest way to run the application locally for development is to use the new quick-start script.\n\n```powershell\n# From the project root\n./scripts/fortuna-quick-start.ps1\n```\n\nThis interactive script will:\n*   Check for all required dependencies (Python, Node.js, etc.).\n*   Install any missing Python or Node packages automatically.\n*   Clear the required network ports (8000 and 3000).\n*   Launch the backend and frontend services in separate, managed terminal windows.\n*   Provide a clean shutdown process.\n\nFor detailed options and first-time setup guidance, run the script with the `-Help` flag:\n```powershell\n./scripts/fortuna-quick-start.ps1 -Help\n```\n\n---\n## \ud83d\udce6 Key Tooling & Scripts\n\n*   **`ARCHIVE_PROJECT.py`**: A utility script that scans the repository and generates the `FORTUNA_ALL_PART*.JSON` archive files. These archives are used as a ground truth for AI-driven code reviews and analysis.\n*   **`AGENTS.md`**: Contains critical operational protocols for the AI agents working on this repository.\n\n---\n\n## \ud83d\udc0d Python Version Requirement\n\n**The Fortuna Monolith application must be built and run with Python 3.10.12.**\n\nThis is due to a dependency (`cefpython3`) that does not support Python 3.11 or newer. The CI/CD workflows are pinned to this version. If you are building the application locally, please ensure you are using a Python 3.10.x environment.\n",
    "STATUS.md": "# Project Status: Foundation Rebuilt, Hardening in Progress\n\n**Date:** 2025-10-03\n\n## Current State\n\n*   **Architecture:** The backend has been successfully rebuilt into a superior, asynchronous FastAPI application, as defined by 'Operation: Grand Synthesis'. The new foundation is stable, tested, and features a resilient `BaseAdapter` pattern.\n\n*   **Status:** The foundational refactoring is complete. The first two data adapters (`Betfair`, `TVG`) have been implemented on the new architecture. We are now in a new phase of development: **'Phase 2: Hardening & Expansion.'**\n\n*   **Documentation:** All core strategic documents and manifests have been synchronized with the new technical reality.\n\n*   **Next Steps:** Our immediate priority is to act on the verified intelligence from our Oracle (Jules1003). The next missions will focus on implementing critical API security features (rate limiting, authentication) and continuing the build-out of our adapter fleet.",
    "docker-compose.yml": "version: '3.8'\n\nservices:\n  fortuna:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./data:/app/data\n      - ./json:/app/json\n      - ./logs:/app/logs\n    environment:\n      - FORTUNA_MODE=docker\n      - FORTUNA_PORT=8000\n    restart: unless-stopped\n",
    "e2e/get-race-info.py": "import json\nimport os\nimport glob\nfrom datetime import datetime\n\ndef get_latest_race_file(data_dir):\n    \"\"\"Finds the most recently modified race data file in the directory.\"\"\"\n    list_of_files = glob.glob(os.path.join(data_dir, '*.json'))\n    if not list_of_files:\n        return None\n    latest_file = max(list_of_files, key=os.path.getmtime)\n    return latest_file\n\ndef main():\n    data_dir = os.path.join('web_service', 'backend', 'data')\n    output_file = 'race-info.txt'\n\n    if not os.path.exists(data_dir):\n        with open(output_file, 'w') as f:\n            f.write(\"Data directory not found.\\n\")\n        return\n\n    latest_file = get_latest_race_file(data_dir)\n\n    if not latest_file:\n        with open(output_file, 'w') as f:\n            f.write(\"No race data files found.\\n\")\n        return\n\n    try:\n        with open(latest_file, 'r') as f:\n            race_data = json.load(f)\n    except (json.JSONDecodeError, IOError) as e:\n        with open(output_file, 'w') as f:\n            f.write(f\"Error reading race data file: {e}\\n\")\n        return\n\n    if not race_data or not isinstance(race_data, list):\n        with open(output_file, 'w') as f:\n            f.write(\"Race data is empty or not in the expected format.\\n\")\n        return\n\n    # Assuming the first race in the list is the one we want.\n    # A better approach might be to sort by start_time if available.\n    latest_race = race_data[0]\n\n    venue = latest_race.get('venue', 'N/A')\n    race_number = latest_race.get('raceNumber', 'N/A') # Use alias\n    runners = latest_race.get('runners', [])\n    num_runners = len(runners)\n\n    with open(output_file, 'w') as f:\n        f.write(f\"Latest Race Info:\\n\")\n        f.write(f\"  Track: {venue}\\n\")\n        f.write(f\"  Race #: {race_number}\\n\")\n        f.write(f\"  Field Size: {num_runners}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "electron/secure-settings-manager.js": "// electron/secure-settings-manager.js\nconst { app } = require('electron');\nconst fs = require('fs');\nconst path = require('path');\n\nconst SETTINGS_FILE = path.join(app.getPath('userData'), 'settings.json');\n\nclass SecureSettingsManager {\n constructor() {\n this.settings = this.loadSettings();\n }\n\n loadSettings() {\n try {\n if (fs.existsSync(SETTINGS_FILE)) {\n const data = fs.readFileSync(SETTINGS_FILE, 'utf-8');\n return JSON.parse(data);\n }\n } catch (error) {\n console.error('Error loading settings:', error);\n }\n return {};\n }\n\n saveSettings() {\n try {\n fs.writeFileSync(SETTINGS_FILE, JSON.stringify(this.settings, null, 2));\n } catch (error) {\n console.error('Error saving settings:', error);\n }\n }\n\n getApiKey() {\n return this.settings.apiKey || null;\n }\n\n saveApiKey(apiKey) {\n this.settings.apiKey = apiKey;\n this.saveSettings();\n return { success: true };\n }\n\n getBetfairCredentials() {\n return this.settings.betfair || null;\n }\n\n saveBetfairCredentials(credentials) {\n this.settings.betfair = credentials;\n this.saveSettings();\n return { success: true };\n }\n}\n\nmodule.exports = new SecureSettingsManager();\n",
    "fortuna-backend-hooks/hook-tenacity.py.bak": "\"\"\"\nPyInstaller hook for tenacity.\n\nTenacity uses dynamic imports for async support and retry strategies that\nPyInstaller cannot automatically detect. This hook ensures all tenacity\nsubmodules are collected into the bundle.\n\nThis is especially critical for tenacity 8.2.3+ which includes async retry support.\n\"\"\"\n\nfrom PyInstaller.utils.hooks import collect_submodules\n\n# Collect all tenacity submodules recursively\nhiddenimports = collect_submodules('tenacity')\n\n# Explicitly add critical submodules that might be missed\n# These are the modules tenacity dynamically imports for retry strategies and async support\ncritical_submodules = [\n    'tenacity.retry',\n    'tenacity.stop',\n    'tenacity.wait',\n    'tenacity.retry_if_result',\n    'tenacity.retry_if_exception',\n    'tenacity.before_sleep',\n    'tenacity.after',\n    'tenacity.before',\n    'tenacity.retry_error',\n    'tenacity.compat',\n    'tenacity.future',\n    'tenacity.asyncio',  # Critical for async retry support\n]\n\n# Merge and deduplicate\nhiddenimports = list(set(hiddenimports + critical_submodules))\n",
    "fortuna-backend-webservice.spec": "# -*- mode: python ; coding: utf-8 -*-\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n# This spec has been standardized to build the web_service from its own directory,\n# removing the dependency on the obsolete 'python_service'.\n\nblock_cipher = None\nproject_root = Path(SPECPATH).parent\nbackend_root = project_root / 'web_service' / 'backend'\n\n# --- Data Files ---\n# Collect all necessary data files from their respective packages.\ndatas = []\ndatas += collect_data_files('uvicorn')\ndatas += collect_data_files('fastapi')\ndatas += collect_data_files('starlette')\n\n# --- Hidden Imports ---\n# Ensure all necessary submodules and dynamically loaded modules are included.\nhiddenimports = set()\nhiddenimports.update(collect_submodules('web_service.backend'))\nhiddenimports.update(collect_submodules('uvicorn'))\nhiddenimports.update(collect_submodules('fastapi'))\nhiddenimports.update(collect_submodules('starlette'))\nhiddenimports.update(collect_submodules('anyio'))\nhiddenimports.add('win32timezone') # Critical for Windows service operation\nhiddenimports.update(['pydantic_settings.sources']) # For settings management\n\na = Analysis(\n    [str(backend_root / 'service_entry.py')], # Entry point is the service wrapper\n    pathex=[str(project_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=sorted(hiddenimports),\n    hookspath=[str(project_root / 'fortuna-backend-hooks')],\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False\n)\n\n# --- PYZ Archive ---\n# Force __init__.py files into the PYZ archive to ensure robust module loading.\na.pure += [\n    ('web_service', str(project_root / 'web_service/__init__.py'), 'PYMODULE'),\n    ('web_service.backend', str(backend_root / '__init__.py'), 'PYMODULE'),\n]\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\n# --- Final Executable ---\n# This creates a single-file executable. The COLLECT object has been removed\n# as it is not needed for this build target.\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    name='fortuna-webservice',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    runtime_tmpdir=None,\n    console=True # Console is useful for debugging service startup\n)\n",
    "fortuna-webservice.spec": "# -*- mode: python ; coding: utf-8 -*-\n\nimport os\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\nproject_root = Path(SPECPATH).resolve()\n\ndef include_tree(rel_path, target, store):\n    absolute = project_root / rel_path\n    if absolute.exists():\n        store.append((str(absolute), target))\n        print(f\"[spec] Including {absolute} -> {target}\")\n    else:\n        # This spec is used by the legacy build-msi.yml, which checks for these dirs.\n        # If they are missing here, it's a critical error.\n        raise FileNotFoundError(f\"[spec] Required directory not found: {absolute}\")\n\ndatas = []\n# Paths must match the legacy structure used by build-msi.yml\ninclude_tree('python_service/adapters', 'adapters', datas)\ninclude_tree('python_service/data', 'data', datas)\ninclude_tree('python_service/json', 'json', datas)\n\n# Collect library assets\ntry:\n    datas += collect_data_files('uvicorn', includes=['*.html', '*.json'])\n    datas += collect_data_files('structlog', includes=['*.json'])\nexcept Exception as e:\n    print(f\"[spec] Warning: Could not collect library data files: {e}\")\n\n# Collect Hidden Imports for python_service\nhidden_imports = set()\nhidden_imports.update(collect_submodules('python_service'))\nhidden_imports.update([\n    'fastapi', 'uvicorn', 'uvicorn.logging', 'uvicorn.loops.auto', 'uvicorn.lifespan.on',\n    'uvicorn.protocols.http.h11_impl', 'uvicorn.protocols.http.httptools_impl',\n    'uvicorn.protocols.websockets.wsproto_impl', 'uvicorn.protocols.websockets.websockets_impl',\n    'anyio', 'httpcore', 'httpx', 'python_multipart', 'pydantic', 'pydantic_core',\n    'aiosqlite', 'structlog', 'tenacity', 'slowapi'\n])\n\na = Analysis(\n    # FIX: Target service_entry.py instead of main.py\n    ['web_service/backend/service_entry.py'],\n    pathex=[],\n    binaries=[],\n    datas=[('web_service/backend', 'backend')],\n    # FIX: Ensure critical service modules are hidden-imported\n    hiddenimports=['win32timezone', 'win32serviceutil', 'win32service', 'win32event'],\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name='fortuna-webservice', # Name matches the workflow expectation\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=False,\n    runtime_tmpdir=None,\n    console=False,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n",
    "fortuna_launcher.py": "#!/usr/bin/env python3\n\"\"\"\nFortuna Faucet - Enhanced Standalone Launcher for Windows 10 Home\nNo Docker, no special permissions, just pure Python magic\nRun this file and your browser opens automatically with all the bells and whistles\n\"\"\"\n\nimport sys\nimport os\nimport subprocess\nimport threading\nimport time\nimport webbrowser\nimport socket\nimport json\nfrom pathlib import Path\nfrom typing import Optional\nfrom datetime import datetime\n\n# ====================================================================\n# CONFIGURATION\n# ====================================================================\nAPP_NAME = \"Fortuna Faucet\"\nAPP_VERSION = \"3.1.0\"\nDEFAULT_HOST = \"127.0.0.1\"\nDEFAULT_PORT = 8000\nBACKEND_STARTUP_TIMEOUT = 15\nHEALTH_CHECK_ATTEMPTS = 30\nLOG_DIR = Path(\"logs\")\nLOG_DIR.mkdir(exist_ok=True)\n\n# ====================================================================\n# COLORS FOR WINDOWS CONSOLE\n# ====================================================================\nclass Colors:\n    \"\"\"ANSI color codes\"\"\"\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    RESET = '\\033[0m'\n\n# Try to enable ANSI colors on Windows 10\ntry:\n    import ctypes\n    kernel32 = ctypes.windll.kernel32\n    kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)\nexcept:\n    pass\n\n# ====================================================================\n# LOGGING\n# ====================================================================\nclass Logger:\n    \"\"\"Dual logging to console and file\"\"\"\n    def __init__(self):\n        self.log_file = LOG_DIR / f\"fortuna_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\n    def write(self, level: str, message: str):\n        \"\"\"Write to both console and file\"\"\"\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        log_line = f\"[{timestamp}] [{level}] {message}\"\n\n        with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n            f.write(log_line + \"\\n\")\n\nlogger = Logger()\n\n# ====================================================================\n# HELPER FUNCTIONS\n# ====================================================================\ndef print_banner():\n    \"\"\"Print welcome banner\"\"\"\n    banner = f\"\"\"\n{Colors.BOLD}{Colors.OKGREEN}\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                                            \u2551\n\u2551              \ud83d\udc34  {APP_NAME} v{APP_VERSION}  \ud83d\udc34              \u2551\n\u2551         Enhanced Launcher - Windows 10 Home Ready         \u2551\n\u2551                                                            \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n{Colors.ENDC}\n\"\"\"\n    print(banner)\n\ndef print_success(msg: str, icon: str = \"\u2713\"):\n    \"\"\"Print success message\"\"\"\n    output = f\"{Colors.OKGREEN}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"SUCCESS\", msg)\n\ndef print_warning(msg: str, icon: str = \"\u26a0\"):\n    \"\"\"Print warning message\"\"\"\n    output = f\"{Colors.WARNING}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"WARNING\", msg)\n\ndef print_error(msg: str, icon: str = \"\u2717\"):\n    \"\"\"Print error message\"\"\"\n    output = f\"{Colors.FAIL}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"ERROR\", msg)\n\ndef print_info(msg: str, icon: str = \"\u2139\"):\n    \"\"\"Print info message\"\"\"\n    output = f\"{Colors.OKBLUE}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"INFO\", msg)\n\ndef print_step(step_num: int, total: int, msg: str):\n    \"\"\"Print step counter\"\"\"\n    output = f\"\\n{Colors.BOLD}[{step_num}/{total}] {msg}{Colors.ENDC}\"\n    print(output)\n    logger.write(\"STEP\", f\"[{step_num}/{total}] {msg}\")\n\ndef print_section(title: str):\n    \"\"\"Print section divider\"\"\"\n    output = f\"\\n{Colors.BOLD}{Colors.OKCYAN}{'\u2500' * 60}{Colors.ENDC}\"\n    print(output)\n    print(f\"{Colors.BOLD}{Colors.OKCYAN}{title}{Colors.ENDC}\")\n    print(f\"{Colors.BOLD}{Colors.OKCYAN}{'\u2500' * 60}{Colors.ENDC}\\n\")\n\n# ====================================================================\n# ENVIRONMENT CHECKS\n# ====================================================================\ndef check_python_version() -> bool:\n    \"\"\"Check if Python version is compatible\"\"\"\n    if sys.version_info < (3, 10):\n        print_error(f\"Python 3.10+ required, you have {sys.version_info.major}.{sys.version_info.minor}\")\n        return False\n    print_success(f\"Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n    return True\n\ndef check_project_structure() -> bool:\n    \"\"\"Check if we're in the right directory\"\"\"\n    required_dirs = [\n        \"web_service/backend\",\n        \"web_platform/frontend\"\n    ]\n    required_files = [\n        \"web_service/backend/requirements.txt\",\n        \"web_platform/frontend/package.json\"\n    ]\n\n    print_info(\"Checking project structure...\")\n    all_good = True\n    for d in required_dirs:\n        if Path(d).exists():\n            print_success(f\"Found: {d}\")\n        else:\n            print_error(f\"Missing: {d}\")\n            all_good = False\n\n    for f in required_files:\n        if Path(f).exists():\n            print_success(f\"Found: {f}\")\n        else:\n            print_error(f\"Missing: {f}\")\n            all_good = False\n\n    return all_good\n\ndef check_port_available(port: int) -> bool:\n    \"\"\"Check if port is available\"\"\"\n    try:\n        sock = socket.create_connection((\"127.0.0.1\", port), timeout=1)\n        sock.close()\n        print_error(f\"Port {port} is already in use by another application\")\n        return False\n    except (socket.timeout, ConnectionRefusedError, OSError):\n        print_success(f\"Port {port} is available\")\n        return True\n\n# ====================================================================\n# DEPENDENCY CHECK & INSTALL\n# ====================================================================\ndef check_and_install_dependencies() -> bool:\n    \"\"\"Check if dependencies are installed, install if needed\"\"\"\n    print_info(\"Checking Python dependencies...\")\n\n    required_packages = {\n        \"fastapi\": \"FastAPI web framework\",\n        \"uvicorn\": \"ASGI server\",\n        \"pydantic\": \"Data validation\",\n    }\n\n    missing = []\n    for package, description in required_packages.items():\n        try:\n            __import__(package)\n            print_success(f\"{description} (installed)\")\n        except ImportError:\n            print_warning(f\"{description} (NOT installed)\")\n            missing.append(package)\n\n    if not missing:\n        print_success(\"All core dependencies satisfied!\")\n        return True\n\n    print()\n    print_info(f\"Installing {len(missing)} missing package(s)...\")\n    print_info(\"This may take 2-3 minutes on first run...\")\n    print()\n\n    try:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"],\n            check=True,\n            capture_output=True,\n            timeout=120\n        )\n\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + missing,\n            check=True,\n            capture_output=True,\n            timeout=300\n        )\n        print_success(\"Dependencies installed successfully!\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print_error(f\"Failed to install dependencies: {e}\")\n        print_info(\"Try running manually in PowerShell:\")\n        print(f\"  python -m pip install -r web_service/backend/requirements.txt\")\n        logger.write(\"ERROR\", f\"Dependency installation failed: {e}\")\n        return False\n    except subprocess.TimeoutExpired:\n        print_error(\"Installation timed out (took too long)\")\n        return False\n\n# ====================================================================\n# FRONTEND BUILD\n# ====================================================================\ndef build_frontend() -> bool:\n    \"\"\"Build Next.js frontend if needed\"\"\"\n    frontend_dir = Path(\"web_platform/frontend\")\n    build_dir = frontend_dir / \"out\"\n\n    if build_dir.exists() and (build_dir / \"index.html\").exists():\n        print_success(\"Frontend already built\")\n        return True\n\n    print_info(\"Frontend build required...\")\n\n    # Check for Node.js\n    try:\n        subprocess.run([\"npm\", \"--version\"], capture_output=True, timeout=5, check=True)\n    except:\n        print_warning(\"Node.js not found - frontend may not load properly\")\n        print_info(\"To fix: Install Node.js from https://nodejs.org/\")\n        logger.write(\"WARNING\", \"Node.js not found for frontend build\")\n        return True\n\n    print_info(\"Building frontend (this takes ~30 seconds)...\")\n    print_info(\"(Progress shown in logs)\")\n\n    try:\n        subprocess.run(\n            [\"npm\", \"ci\"],\n            cwd=str(frontend_dir),\n            capture_output=True,\n            timeout=120,\n            check=True\n        )\n        subprocess.run(\n            [\"npm\", \"run\", \"build\"],\n            cwd=str(frontend_dir),\n            capture_output=True,\n            timeout=180,\n            check=True\n        )\n        print_success(\"Frontend built successfully\")\n        return True\n    except subprocess.TimeoutExpired:\n        print_warning(\"Frontend build timed out, continuing anyway...\")\n        logger.write(\"WARNING\", \"Frontend build timed out\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print_warning(f\"Frontend build failed: {e}\")\n        logger.write(\"WARNING\", f\"Frontend build failed: {e}\")\n        return True\n    except Exception as e:\n        print_warning(f\"Frontend build error: {e}\")\n        logger.write(\"WARNING\", f\"Frontend build error: {e}\")\n        return True\n\n# ====================================================================\n# BACKEND SERVER\n# ====================================================================\ndef start_backend() -> Optional[subprocess.Popen]:\n    \"\"\"Start the FastAPI backend server\"\"\"\n    print_info(\"Starting FastAPI server...\")\n\n    try:\n        process = subprocess.Popen(\n            [\n                sys.executable,\n                \"-m\", \"uvicorn\",\n                \"web_service.backend.main:app\",\n                \"--host\", DEFAULT_HOST,\n                \"--port\", str(DEFAULT_PORT),\n                \"--log-level\", \"info\"\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            bufsize=1\n        )\n\n        # Give it a moment to start\n        time.sleep(1)\n\n        if process.poll() is not None:\n            # Process exited already\n            stdout, stderr = process.communicate()\n            print_error(f\"Backend failed to start: {stderr[:200]}\")\n            logger.write(\"ERROR\", f\"Backend startup failed: {stderr}\")\n            return None\n\n        print_success(\"Backend server started\")\n        logger.write(\"SUCCESS\", \"Backend server started successfully\")\n        return process\n\n    except Exception as e:\n        print_error(f\"Failed to start backend: {e}\")\n        logger.write(\"ERROR\", f\"Backend start exception: {e}\")\n        return None\n\ndef wait_for_backend_ready(max_retries: int = HEALTH_CHECK_ATTEMPTS) -> bool:\n    \"\"\"Wait for backend to respond to health check\"\"\"\n    import urllib.request\n    import urllib.error\n\n    print_info(\"Waiting for backend to be ready...\")\n\n    for attempt in range(max_retries):\n        try:\n            response = urllib.request.urlopen(\n                f\"http://{DEFAULT_HOST}:{DEFAULT_PORT}/api/health\",\n                timeout=2\n            )\n            if response.status == 200:\n                elapsed = attempt + 1\n                print_success(f\"Backend ready in {elapsed} second(s)\")\n                logger.write(\"SUCCESS\", f\"Backend health check passed in {elapsed}s\")\n                return True\n        except (urllib.error.URLError, urllib.error.HTTPError, Exception):\n            if attempt < max_retries - 1:\n                time.sleep(1)\n\n    print_error(\"Backend did not respond after 30 seconds\")\n    logger.write(\"ERROR\", \"Backend health check failed - no response after 30s\")\n    return False\n\n# ====================================================================\n# BROWSER LAUNCHER\n# ====================================================================\ndef open_browser():\n    \"\"\"Open browser to the application\"\"\"\n    url = f\"http://{DEFAULT_HOST}:{DEFAULT_PORT}\"\n    try:\n        print_info(f\"Opening browser at {url}...\")\n        webbrowser.open(url)\n        time.sleep(1)  # Give browser time to open\n        print_success(\"Browser opened!\")\n        logger.write(\"SUCCESS\", f\"Browser opened at {url}\")\n    except Exception as e:\n        print_warning(f\"Could not open browser automatically: {e}\")\n        print_info(f\"Please manually open: {url}\")\n        logger.write(\"WARNING\", f\"Browser auto-open failed: {e}\")\n\n# ====================================================================\n# SYSTEM INFO\n# ====================================================================\ndef print_system_info():\n    \"\"\"Print system information\"\"\"\n    print_section(\"System Information\")\n    print_success(f\"Python: {sys.version.split()[0]}\")\n    print_success(f\"Platform: {sys.platform}\")\n    print_success(f\"Current Directory: {Path.cwd()}\")\n    print_success(f\"Log Directory: {LOG_DIR.absolute()}\")\n    print()\n\n# ====================================================================\n# MAIN APPLICATION\n# ====================================================================\ndef main():\n    \"\"\"Main entry point\"\"\"\n    print_banner()\n    print_system_info()\n\n    # Step 1: Environment validation\n    print_step(1, 5, \"Validating environment...\")\n    if not check_python_version():\n        return 1\n    if not check_project_structure():\n        return 1\n    if not check_port_available(DEFAULT_PORT):\n        return 1\n    print()\n\n    # Step 2: Dependencies\n    print_step(2, 5, \"Installing dependencies...\")\n    if not check_and_install_dependencies():\n        return 1\n    print()\n\n    # Step 3: Frontend build\n    print_step(3, 5, \"Building frontend...\")\n    build_frontend()\n    print()\n\n    # Step 4: Start backend\n    print_step(4, 5, \"Starting backend server...\")\n    backend_process = start_backend()\n    if not backend_process:\n        return 1\n\n    if not wait_for_backend_ready():\n        backend_process.terminate()\n        logger.write(\"ERROR\", \"Application startup failed - health check timeout\")\n        return 1\n    print()\n\n    # Step 5: Open browser\n    print_step(5, 5, \"Launching browser...\")\n    open_browser()\n    print()\n\n    # Success!\n    print(f\"{Colors.BOLD}{Colors.OKGREEN}\")\n    print(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n    print(\"\u2551                                                            \u2551\")\n    print(\"\u2551          \ud83c\udf89  FORTUNA IS RUNNING!  \ud83c\udf89                     \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(f\"\u2551  Access your app at: http://{DEFAULT_HOST}:{DEFAULT_PORT:<5}                      \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(f\"\u2551  Log file: {LOG_DIR / 'fortuna_*.log':<40}  \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(\"\u2551  Press Ctrl+C to stop the server                          \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n    print(f\"{Colors.ENDC}\")\n    print()\n\n    # Keep running\n    try:\n        while True:\n            time.sleep(0.1)\n    except KeyboardInterrupt:\n        print()\n        print_info(\"Shutting down gracefully...\")\n        backend_process.terminate()\n        try:\n            backend_process.wait(timeout=5)\n        except subprocess.TimeoutExpired:\n            backend_process.kill()\n        print_success(\"Fortuna stopped successfully\")\n        logger.write(\"SUCCESS\", \"Application stopped gracefully by user\")\n        return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
    "playwright_test.js": "const { chromium } = require('playwright');\nconst { test, expect } = require('@playwright/test');\n\n(async () => {\n  const browser = await chromium.launch();\n  const page = await browser.newPage();\n\n  // Navigate to the dashboard\n  await page.goto('http://localhost:3001');\n\n  // Wait for the initial loading to complete.\n  // We expect the skeleton loaders to disappear.\n  await expect(page.locator('div:has-text(\"Loading races...\")')).toHaveCount(0, { timeout: 15000 });\n\n  // Check for the manual override panel\n  const overridePanel = page.locator('div:has-text(\"Fetch Failed: AtTheRaces\")');\n  await expect(overridePanel).toBeVisible({ timeout: 10000 });\n\n  // Check for the text area with the correct URL\n  // The date is a placeholder, as it can change. The important part is the base URL.\n  const textArea = overridePanel.locator('textarea');\n  await expect(textArea).toHaveAttribute('value', /https:\\/\\/www\\.attheraces\\.com\\/racecards\\/\\d{4}-\\d{2}-\\d{2}/);\n\n\n  // Take a screenshot for visual confirmation\n  await page.screenshot({ path: 'manual-override-panel.png' });\n\n  await browser.close();\n})();\n",
    "pre-build-check.ps1": "# pre-build-check.ps1\nWrite-Host \"=== FORTUNA PRE-BUILD VERIFICATION ===\" -ForegroundColor Cyan\n\n# 1. Check all required files exist\nWrite-Host \"`n[1] Checking required files...\"\n$required = @(\n    \"web_service/backend/main.py\",\n    \"web_service/backend/api.py\",\n    \"web_service/backend/config.py\",\n    \"web_service/backend/port_check.py\",\n    \"web_service/backend/requirements.txt\",\n    \"web_service/frontend/package.json\",\n    \"web_service/frontend/next.config.js\",\n    \"fortuna-monolith.spec\"\n)\n\n$missing = @()\nforeach ($file in $required) {\n    if (Test-Path $file) {\n        Write-Host \"  \u2705 $file\"\n    } else {\n        Write-Host \"  \u274c $file\"\n        $missing += $file\n    }\n}\n\nif ($missing.Count -gt 0) {\n    Write-Host \"`n\u274c FATAL: Missing files:\" -ForegroundColor Red\n    $missing | ForEach-Object { Write-Host \"  - $_\" }\n    exit 1\n}\n\n# 2. Test Python imports\nWrite-Host \"`n[2] Testing Python imports...\"\n$testScript = @\"\nimport sys\nsys.path.insert(0, '.')\n\ntry:\n    from web_service.backend.api import app\n    print('\u2705 api.app imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import api.app: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.config import get_settings\n    settings = get_settings()\n    print(f'\u2705 config.get_settings imported (host={settings.UVICORN_HOST}, port={settings.FORTUNA_PORT})')\nexcept ImportError as e:\n    print(f'\u274c Failed to import config: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    print('\u2705 port_check.check_port_and_exit_if_in_use imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import port_check: {e}')\n    sys.exit(1)\n\nprint('\u2705 All imports successful')\n\"@\n\n$testScript | Out-File -FilePath \"test_imports.py\" -Encoding UTF8\npython test_imports.py\nif ($LASTEXITCODE -ne 0) {\n    Write-Host \"\u274c Import test FAILED\" -ForegroundColor Red\n    exit 1\n}\nRemove-Item \"test_imports.py\"\n\n# 3. Check frontend\nWrite-Host \"`n[3] Checking frontend...\"\nif (Test-Path \"web_service/frontend/next.config.js\") {\n    $config = Get-Content \"web_service/frontend/next.config.js\"\n    if ($config -match \"output:\\s*['`\"]export['`\"]\") {\n        Write-Host \"  \u2705 next.config.js has output: 'export'\"\n    } else {\n        Write-Host \"  \u274c next.config.js missing output: 'export'\" -ForegroundColor Red\n        exit 1\n    }\n} else {\n    Write-Host \"  \u26a0\ufe0f  next.config.js will be created during build\"\n}\n\n# 4. Check spec file\nWrite-Host \"`n[4] Checking fortuna-monolith.spec...\"\nif (Test-Path \"fortuna-monolith.spec\") {\n    $spec = Get-Content \"fortuna-monolith.spec\"\n    if ($spec -match \"SPECPATH\") {\n        Write-Host \"  \u2705 spec uses SPECPATH\"\n    } else {\n        Write-Host \"  \u26a0\ufe0f  spec doesn't use SPECPATH (may have path issues)\"\n    }\n} else {\n    Write-Host \"  \u274c fortuna-monolith.spec not found\" -ForegroundColor Red\n    exit 1\n}\n\nWrite-Host \"`n\u2705 ALL CHECKS PASSED - Safe to build!\" -ForegroundColor Green\n",
    "scripts/audit_rebranding.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Rebranding Audit Script\n# ==============================================================================\n# This script performs a comprehensive, read-only audit of the project to\n# identify all files containing legacy branding terms.\n# ==============================================================================\n\nimport os\n\n# --- CONFIGURATION ---\nTARGET_TERMS = [\"checkmate\", \"solo\"]\nEXCLUDED_DIRS = [\n    \".git\",\n    \".venv\",\n    \"node_modules\",\n    \"build\",\n    \"dist\",\n    \"__pycache__\",\n    \"ReviewableJSON\",\n]\nEXCLUDED_FILES = [\"audit_rebranding.py\", \"REBRANDING_AUDIT.md\"]\nOUTPUT_FILE = \"REBRANDING_AUDIT.md\"\n# -------------------\n\n\ndef search_file_for_terms(file_path, terms):\n    \"\"\"Searches a single file for a list of terms, case-insensitively.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read().lower()\n            for term in terms:\n                if term in content:\n                    return True\n    except Exception as e:\n        print(f\"[WARNING] Could not read file {file_path}: {e}\")\n    return False\n\n\ndef main():\n    \"\"\"Main orchestrator for the audit.\"\"\"\n    print(\"--- Starting Rebranding Audit ---\")\n    affected_files = []\n    for root, dirs, files in os.walk(\".\", topdown=True):\n        # Exclude specified directories\n        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]\n\n        for filename in files:\n            if filename in EXCLUDED_FILES:\n                continue\n\n            file_path = os.path.join(root, filename)\n\n            # Check filename itself\n            if any(term in filename.lower() for term in TARGET_TERMS):\n                affected_files.append(file_path)\n                print(f\"[FOUND] Legacy term in filename: {file_path}\")\n                continue  # No need to search content if filename matches\n\n            # Check file content\n            if search_file_for_terms(file_path, TARGET_TERMS):\n                affected_files.append(file_path)\n                print(f\"[FOUND] Legacy term in content: {file_path}\")\n\n    print(f\"\\n--- Audit Complete. Found {len(affected_files)} affected files. ---\")\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"# Fortuna Faucet: Rebranding Audit Report\\n\\n\")\n        f.write(\"This report lists all files containing legacy branding terms (`checkmate`, `solo`).\\n\\n---\\n\\n\")\n        if affected_files:\n            for file_path in sorted(affected_files):\n                f.write(f\"- `{file_path.replace(os.sep, '/')}`\\n\")\n        else:\n            f.write(\"No files with legacy branding were found.\\n\")\n\n    print(f\"[SUCCESS] Report written to {OUTPUT_FILE}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/fortuna_reporter.py": "#!/usr/bin/env python\n\"\"\"\nFortuna Unified Race Reporter\n\nGenerates HTML, JSON, and Markdown summary reports for GitHub Actions\nby directly invoking the OddsEngine and AnalyzerEngine without a live API.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any\n\n# Ensure the project root is in the path to allow for direct imports\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom web_service.backend.engine import OddsEngine\nfrom web_service.backend.analyzer import AnalyzerEngine\nfrom web_service.backend.config import get_settings\nfrom web_service.backend.models import Race\n\n\nclass LogLevel(Enum):\n    \"\"\"Log level enumeration with emoji support.\"\"\"\n    INFO = (\"INFO\", \"\u2139\ufe0f\")\n    SUCCESS = (\"SUCCESS\", \"\u2705\")\n    ERROR = (\"ERROR\", \"\u274c\")\n    WARNING = (\"WARNING\", \"\u26a0\ufe0f\")\n    DEBUG = (\"DEBUG\", \"\ud83d\udd0d\")\n\n    @property\n    def emoji(self) -> str:\n        return self.value[1]\n\n\n@dataclass\nclass ReporterConfig:\n    \"\"\"Configuration for the race reporter.\"\"\"\n    template_path: Path = field(default_factory=lambda: Path(\"scripts/templates/race_report_template.html\"))\n    html_output_path: Path = field(default_factory=lambda: Path(\"race-report.html\"))\n    json_output_path: Path = field(default_factory=lambda: Path(\"qualified_races.json\"))\n    markdown_summary_path: Path = field(default_factory=lambda: Path(\"github_summary.md\"))\n    raw_json_output_path: Path = field(default_factory=lambda: Path(\"raw_race_data.json\"))\n\n    max_retries: int = field(default_factory=lambda: int(os.getenv(\"MAX_RETRIES\", \"3\")))\n    request_timeout: int = field(default_factory=lambda: int(os.getenv(\"REQUEST_TIMEOUT\", \"45\")))\n    analyzer_type: str = field(default_factory=lambda: os.getenv(\"ANALYZER_TYPE\", \"tiny_field_trifecta\"))\n    force_refresh: bool = field(default_factory=lambda: os.getenv(\"FORCE_REFRESH\", \"false\").lower() == \"true\")\n    max_summary_races: int = 25\n\n    # All known adapters for exclusion logic\n    ALL_ADAPTERS: tuple[str, ...] = (\n        \"AtTheRacesAdapter\", \"BetfairAdapter\", \"BetfairGreyhoundAdapter\",\n        \"BrisnetAdapter\", \"EquibaseAdapter\", \"FanDuelAdapter\", \"GbgbApiAdapter\",\n        \"GreyhoundAdapter\", \"HarnessAdapter\", \"HorseRacingNationAdapter\",\n        \"NYRABetsAdapter\", \"OddscheckerAdapter\", \"PuntersAdapter\",\n        \"RacingAndSportsAdapter\", \"RacingAndSportsGreyhoundAdapter\",\n        \"RacingPostAdapter\", \"RacingPostB2BAdapter\", \"RacingTVAdapter\", \"SportingLifeAdapter\",\n        \"TabAdapter\", \"TheRacingApiAdapter\", \"TimeformAdapter\",\n        \"TwinSpiresAdapter\", \"TVGAdapter\", \"XpressbetAdapter\",\n        \"PointsBetGreyhoundAdapter\",\n    )\n\n    # Reliable adapters that don't require API keys.\n    # Note: The following adapters may experience issues and should be monitored:\n    # - Timeform: Occasional 500 errors (redirect to error page)\n    # - Equibase: 404 errors on some dates\n    # - Brisnet: Timeout/503 errors, possible rate limiting\n    # - Oddschecker: 403 Forbidden (bot detection)\n    # - RacingPost: 406 Not Acceptable (user agent issues)\n    RELIABLE_NON_KEYED_ADAPTERS: tuple[str, ...] = (\n        \"AtTheRacesAdapter\",\n        \"RacingPostB2BAdapter\",\n        \"SportingLifeAdapter\",\n        \"TwinSpiresAdapter\",\n    )\n\n    @property\n    def excluded_adapters(self) -> list[str]:\n        \"\"\"No adapters will be excluded.\"\"\"\n        return []\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if self.max_retries < 1:\n            raise ValueError(\"max_retries must be at least 1\")\n        if self.request_timeout < 1:\n            raise ValueError(\"request_timeout must be at least 1 second\")\n        if self.max_summary_races < 1:\n            raise ValueError(\"max_summary_races must be at least 1\")\n\n        # Ensure output directories exist\n        for path in (self.html_output_path, self.json_output_path,\n                     self.markdown_summary_path, self.raw_json_output_path):\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n\n@dataclass\nclass ReportMetrics:\n    \"\"\"Metrics collected during report generation.\"\"\"\n    total_races_fetched: int = 0\n    qualified_races: int = 0\n    adapters_used: list[str] = field(default_factory=list)\n    adapters_failed: list[str] = field(default_factory=list)\n    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    end_time: datetime | None = None\n    errors: list[str] = field(default_factory=list)\n\n    @property\n    def duration_seconds(self) -> float:\n        if self.end_time:\n            return (self.end_time - self.start_time).total_seconds()\n        return 0.0\n\n    def to_dict(self) -> dict[str, Any]:\n        return {\n            \"total_races_fetched\": self.total_races_fetched,\n            \"qualified_races\": self.qualified_races,\n            \"adapters_used\": self.adapters_used,\n            \"adapters_failed\": self.adapters_failed,\n            \"duration_seconds\": self.duration_seconds,\n            \"errors\": self.errors,\n            \"timestamp\": self.start_time.isoformat(),\n        }\n\n\nclass Reporter:\n    \"\"\"Main reporter class for generating race reports.\"\"\"\n\n    def __init__(self, config: ReporterConfig | None = None):\n        self.config = config or ReporterConfig()\n        self.metrics = ReportMetrics()\n\n    def log(self, message: str, level: LogLevel = LogLevel.INFO) -> None:\n        \"\"\"Print a timestamped log message.\"\"\"\n        timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(f\"[{timestamp}] {level.emoji} {message}\", flush=True)\n\n        if level == LogLevel.ERROR:\n            self.metrics.errors.append(message)\n\n    def generate_html_report(self, race_data: dict[str, Any]) -> bool:\n        \"\"\"Generates the HTML report from a template.\"\"\"\n        self.log(\"Generating HTML report...\")\n\n        try:\n            if not self.config.template_path.exists():\n                self.log(f\"Template not found at {self.config.template_path}\", LogLevel.ERROR)\n                return self._generate_fallback_html(race_data)\n\n            template = self.config.template_path.read_text(encoding=\"utf-8\")\n\n            # Inject data and metrics\n            race_data_with_metrics = {\n                **race_data,\n                \"generation_metrics\": self.metrics.to_dict(),\n            }\n\n            report_html = template.replace(\n                \"__RACE_DATA_PLACEHOLDER__\",\n                json.dumps(race_data_with_metrics, default=str)\n            )\n\n            self.config.html_output_path.write_text(report_html, encoding=\"utf-8\")\n            self.log(f\"Generated HTML report at {self.config.html_output_path}\", LogLevel.SUCCESS)\n            return True\n\n        except Exception as e:\n            self.log(f\"Failed to generate HTML report: {e}\", LogLevel.ERROR)\n            return self._generate_fallback_html(race_data)\n\n    def _generate_fallback_html(self, race_data: dict[str, Any]) -> bool:\n        \"\"\"Generate a minimal fallback HTML report if template fails.\"\"\"\n        self.log(\"Generating fallback HTML report...\", LogLevel.WARNING)\n\n        try:\n            races = race_data.get(\"races\", [])\n            html = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Fortuna Race Report (Fallback)</title>\n    <style>\n        body {{ font-family: system-ui, sans-serif; max-width: 800px; margin: 2rem auto; padding: 1rem; }}\n        .race {{ border: 1px solid #ccc; padding: 1rem; margin: 1rem 0; border-radius: 8px; }}\n        .error {{ color: #c00; background: #fee; padding: 1rem; border-radius: 4px; }}\n    </style>\n</head>\n<body>\n    <h1>\ud83d\udc34 Fortuna Race Report</h1>\n    <p class=\"error\">\u26a0\ufe0f This is a fallback report. The main template could not be loaded.</p>\n    <p>Found {len(races)} qualified race(s)</p>\n    <pre>{json.dumps(race_data, indent=2, default=str)}</pre>\n</body>\n</html>\"\"\"\n            self.config.html_output_path.write_text(html, encoding=\"utf-8\")\n            return True\n        except Exception as e:\n            self.log(f\"Failed to generate fallback HTML: {e}\", LogLevel.ERROR)\n            return False\n\n    def generate_markdown_summary(self, races: list[Race]) -> bool:\n        \"\"\"Generates a Markdown summary for the GitHub Actions UI.\"\"\"\n        self.log(\"Generating Markdown summary...\")\n\n        try:\n            lines = [\n                \"# \ud83d\udc34 Fortuna Race Report\",\n                \"\",\n                f\"**Generated:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\",\n                f\"**Analyzer:** `{self.config.analyzer_type}`\",\n                f\"**Duration:** {self.metrics.duration_seconds:.1f}s\",\n                \"\",\n            ]\n\n            if self.metrics.errors:\n                lines.extend([\n                    \"### \u26a0\ufe0f Warnings\",\n                    \"\",\n                    *[f\"- {e}\" for e in self.metrics.errors[:5]],\n                    \"\",\n                ])\n\n            if not races:\n                lines.append(\"### \ud83d\udd2d No races found matching filters.\")\n            else:\n                lines.extend([\n                    f\"### \u26a1 Found {len(races)} Qualified Race(s)\",\n                    \"\",\n                    \"| Score | Time | Venue | Race | Runners |\",\n                    \"|:-----:|:-----|:------|:----:|:-------:|\",\n                ])\n\n                for race in races[:self.config.max_summary_races]:\n                    start_time = race.start_time\n                    if isinstance(start_time, str):\n                        try:\n                            start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n                        except ValueError:\n                            start_time = None\n\n                    time_str = start_time.strftime('%H:%M') if start_time else \"N/A\"\n                    score = f\"{race.qualification_score:.1f}\" if race.qualification_score is not None else \"N/A\"\n                    venue = race.venue or \"Unknown\"\n                    race_num = race.race_number or \"?\"\n                    runners = len(race.runners) if race.runners else 0\n\n                    lines.append(f\"| {score} | {time_str} | **{venue}** | {race_num} | {runners} |\")\n\n                if len(races) > self.config.max_summary_races:\n                    lines.append(f\"\\n*...and {len(races) - self.config.max_summary_races} more races*\")\n\n            lines.extend([\n                \"\",\n                \"---\",\n                \"\",\n                \"<details>\",\n                \"<summary>\ud83d\udcca Generation Metrics</summary>\",\n                \"\",\n                f\"- Total races fetched: {self.metrics.total_races_fetched}\",\n                f\"- Qualified races: {self.metrics.qualified_races}\",\n                f\"- Adapters used: {len(self.metrics.adapters_used)}\",\n                f\"- Adapters failed: {len(self.metrics.adapters_failed)}\",\n                \"\",\n                \"</details>\",\n            ])\n\n            self.config.markdown_summary_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n            self.log(f\"Generated Markdown summary at {self.config.markdown_summary_path}\", LogLevel.SUCCESS)\n            return True\n\n        except Exception as e:\n            self.log(f\"Failed to write Markdown summary: {e}\", LogLevel.ERROR)\n            return False\n\n    def save_json(self, data: dict[str, Any], path: Path, description: str) -> bool:\n        \"\"\"Save JSON data with error handling.\"\"\"\n        try:\n            path.write_text(json.dumps(data, indent=2, default=str), encoding=\"utf-8\")\n            self.log(f\"Saved {description} to {path}\", LogLevel.SUCCESS)\n            return True\n        except Exception as e:\n            self.log(f\"Failed to save {description}: {e}\", LogLevel.ERROR)\n            return False\n\n    async def fetch_with_retry(\n        self,\n        odds_engine: OddsEngine,\n        date_str: str,\n    ) -> dict[str, Any]:\n        \"\"\"Fetch race data with retry logic.\"\"\"\n        last_error = None\n\n        for attempt in range(1, self.config.max_retries + 1):\n            try:\n                self.log(f\"Fetching race data (attempt {attempt}/{self.config.max_retries})...\")\n                data = await asyncio.wait_for(\n                    odds_engine.fetch_all_odds(date_str),\n                    timeout=self.config.request_timeout * 2\n                )\n                return data\n            except asyncio.TimeoutError:\n                last_error = \"Request timed out\"\n                self.log(f\"Attempt {attempt} timed out\", LogLevel.WARNING)\n            except Exception as e:\n                last_error = str(e)\n                self.log(f\"Attempt {attempt} failed: {e}\", LogLevel.WARNING)\n\n            if attempt < self.config.max_retries:\n                wait_time = 2 ** attempt  # Exponential backoff\n                self.log(f\"Waiting {wait_time}s before retry...\")\n                await asyncio.sleep(wait_time)\n\n        raise RuntimeError(f\"All {self.config.max_retries} fetch attempts failed. Last error: {last_error}\")\n\n    def _get_firewalled_adapters(self) -> list[str]:\n        \"\"\"Identify adapters that should be disabled due to repeated failures.\"\"\"\n        stats_path = Path(\"adapter_stats.json\")\n        if not stats_path.exists():\n            return []\n\n        try:\n            stats = json.loads(stats_path.read_text())\n            # stats is usually a list of adapter status dicts\n            firewalled = []\n            for adapter in stats:\n                # If consecutive failures > 5, firewall it\n                if adapter.get('consecutive_failures', 0) > 5:\n                    firewalled.append(adapter.get('name'))\n\n            if firewalled:\n                self.log(f\"\ud83d\udd25 Firewalling chronically failing adapters: {firewalled}\", LogLevel.WARNING)\n            return firewalled\n        except Exception as e:\n            self.log(f\"Failed to read adapter stats for firewall: {e}\", LogLevel.DEBUG)\n            return []\n\n    async def run(self) -> bool:\n        \"\"\"Main entry point with graceful degradation.\"\"\"\n        self.log(\"=== Fortuna Unified Race Reporter ===\")\n        self.log(f\"Analyzer: {self.config.analyzer_type}\")\n\n        # Adapter Firewall\n        firewalled = self._get_firewalled_adapters()\n        excluded = list(set(self.config.excluded_adapters + firewalled))\n        self.log(f\"Excluding {len(excluded)} adapters (including {len(firewalled)} firewalled)\")\n\n        settings = get_settings()\n        odds_engine = OddsEngine(config=settings, exclude_adapters=excluded)\n        analyzer_engine = AnalyzerEngine()\n\n        # Pre-flight health check\n        healthy_adapters = []\n        for name, adapter in odds_engine.adapters.items():\n            try:\n                health = await adapter.health_check()\n                if health.get('circuit_breaker_state') == 'closed':\n                    healthy_adapters.append(name)\n            except Exception as e:\n                self.log(f\"Health check failed for {name}: {e}\", LogLevel.WARNING)\n\n        self.log(f\"Healthy adapters: {len(healthy_adapters)}/{len(odds_engine.adapters)}\")\n\n        if len(healthy_adapters) < 2:\n            self.log(\"Insufficient healthy adapters, report may be degraded\", LogLevel.WARNING)\n\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n\n        outputs_generated = {\n            \"raw_json\": False,\n            \"qualified_json\": False,\n            \"html\": False,\n            \"markdown\": False,\n        }\n\n        try:\n            aggregated_data = await self.fetch_with_retry(odds_engine, today_str)\n            outputs_generated[\"raw_json\"] = self.save_json(\n                aggregated_data, self.config.raw_json_output_path, \"raw race data\"\n            )\n\n            all_races_raw = aggregated_data.get(\"races\", [])\n            self.metrics.total_races_fetched = len(all_races_raw)\n\n            if not all_races_raw:\n                self.log(\"No races returned from OddsEngine. This is a critical failure.\", LogLevel.ERROR)\n                self.metrics.end_time = datetime.now(timezone.utc)\n                self.generate_markdown_summary([]) # Generate a summary showing failure\n                return False\n\n            all_races = []\n            validation_errors = []\n\n            for i, race_data in enumerate(all_races_raw):\n                try:\n                    all_races.append(Race(**race_data))\n                except Exception as e:\n                    error_msg = f\"Race {i} ({race_data.get('venue', 'unknown')}): {str(e)}\"\n                    validation_errors.append(error_msg)\n                    self.log(f\"Failed to validate race {i}: {e}\", LogLevel.WARNING)\n\n            self.log(f\"Validated {len(all_races)}/{len(all_races_raw)} races\")\n\n            if len(all_races) == 0 and len(all_races_raw) > 0:\n                self.log(\"All races failed validation! Check schema compatibility.\", LogLevel.ERROR)\n                self.save_json({\n                    \"error\": \"All races failed validation\",\n                    \"total_raw_races\": len(all_races_raw),\n                    \"validation_errors\": validation_errors[:10]\n                }, Path(\"validation_errors.json\"), \"validation errors\")\n                return False\n\n            if validation_errors:\n                self.save_json({\n                    \"validation_errors\": validation_errors\n                }, Path(\"validation_warnings.json\"), \"validation warnings\")\n\n            self.log(f\"Analyzing with '{self.config.analyzer_type}' analyzer...\")\n            analyzer = analyzer_engine.get_analyzer(self.config.analyzer_type)\n            result = analyzer.qualify_races(all_races)\n\n            qualified_races = result.get(\"races\", [])\n            criteria = result.get(\"criteria\", {})\n            self.metrics.qualified_races = len(qualified_races)\n            self.log(f\"Found {len(qualified_races)} qualified races\", LogLevel.SUCCESS)\n\n            report_data = {\n                \"races\": [r.model_dump(mode='json') for r in qualified_races],\n                \"analysis_metadata\": criteria,\n                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                \"analyzer\": self.config.analyzer_type,\n            }\n\n            outputs_generated[\"qualified_json\"] = self.save_json(\n                report_data, self.config.json_output_path, \"qualified races JSON\"\n            )\n            outputs_generated[\"html\"] = self.generate_html_report(report_data)\n            outputs_generated[\"markdown\"] = self.generate_markdown_summary(qualified_races)\n\n            # Metadata for downstream consumers\n            metadata = {\n                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                \"analyzer_type\": self.config.analyzer_type,\n                \"run_mode\": os.getenv(\"RUN_MODE\", \"full\"),\n                \"canary_health\": os.getenv(\"CANARY_HEALTH\", \"unknown\"),\n                \"race_count\": len(qualified_races),\n                \"adapter_count\": len(odds_engine.adapters),\n                \"version\": \"1.1.0\",\n            }\n            self.save_json(metadata, Path(\"report-metadata.json\"), \"report metadata\")\n\n        except Exception as e:\n            self.log(f\"Critical error: {e}\", LogLevel.ERROR)\n            # Still try to generate a failure report\n            self.generate_markdown_summary([])\n\n        finally:\n            self.metrics.end_time = datetime.now(timezone.utc)\n            self.log(\"Generation complete. Shutting down engines...\")\n\n            # Record adapter metrics\n            if 'odds_engine' in locals():\n                self.metrics.adapters_used = list(odds_engine.adapters.keys())\n                \n                # Capture adapter stats (including last_race_count and last_duration_s)\n                adapter_stats = odds_engine.get_all_adapter_statuses()\n                self.save_json(adapter_stats, Path(\"adapter_stats.json\"), \"adapter stats\")\n\n                # --- SUCCESS SUMMARY ARTIFACT ---\n                # Generate a clean summary even if the reporter partially failed or timed out\n                success_summary = [\n                    {\n                        \"adapter\": s[\"adapter_name\"],\n                        \"race_count\": s.get(\"last_race_count\", 0),\n                        \"duration_s\": s.get(\"last_duration_s\", 0.0)\n                    }\n                    for s in adapter_stats if s.get(\"last_race_count\", 0) > 0\n                ]\n                success_summary.sort(key=lambda x: x[\"race_count\"], reverse=True)\n                self.save_json(success_summary, Path(\"adapter_success_summary.json\"), \"adapter success summary\")\n                \n                if success_summary:\n                    self.log(\"=== \ud83c\udfc6 Success Summary: Race Quantities ===\")\n                    for item in success_summary:\n                        self.log(f\"  - {item['adapter']}: {item['race_count']} races ({item['duration_s']}s)\")\n                    self.log(\"==========================================\")\n\n                # CRITICAL: Use shutdown() to ensure all browser sessions are closed\n                await odds_engine.shutdown()\n\n            # Save metrics.json\n            self.save_json(self.metrics.to_dict(), Path(\"metrics.json\"), \"execution metrics\")\n\n            successful_outputs = sum(outputs_generated.values())\n            self.log(f\"Generated {successful_outputs}/{len(outputs_generated)} outputs\")\n\n        return all(outputs_generated.values())\n\n\nasync def main() -> int:\n    \"\"\"CLI entry point.\"\"\"\n    reporter = Reporter()\n    success = await reporter.run()\n    return 0 if success else 1\n\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)\n",
    "scripts/generate_report.ps1": "# scripts/generate_report.ps1\n\n[CmdletBinding()]\nparam (\n    [string]$JsonInputPath = \"qualified_races.json\",\n    [string]$HtmlOutputPath = \"race-report.html\"\n)\n\nWrite-Host \"Generating HTML report from $JsonInputPath...\" -ForegroundColor Cyan\n\nif (-not (Test-Path $JsonInputPath)) {\n    Write-Host \"\u274c ERROR: Input JSON file not found at $JsonInputPath\" -ForegroundColor Red\n    exit 1\n}\n\n$races = Get-Content $JsonInputPath | ConvertFrom-Json\n$timestamp = Get-Date -Format \"yyyy-MM-dd HH:mm:ss UTC\"\n\n$html = @\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>\ud83d\udc34 Fortuna - Filtered Race Report</title>\n  <style>\n    * { margin: 0; padding: 0; box-sizing: border-box; }\n    html { scroll-behavior: smooth; }\n\n    body {\n      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n      background: linear-gradient(135deg, #0f1419 0%, #1a1f2e 50%, #16213e 100%);\n      color: #e2e8f0;\n      padding: 2rem;\n      line-height: 1.6;\n      min-height: 100vh;\n    }\n\n    .container {\n      max-width: 1400px;\n      margin: 0 auto;\n    }\n\n    header {\n      text-align: center;\n      margin-bottom: 3rem;\n      background: linear-gradient(135deg, rgba(15, 52, 96, 0.5), rgba(0, 255, 136, 0.1));\n      border: 2px solid #00ff88;\n      border-radius: 12px;\n      padding: 3rem 2rem;\n      box-shadow: 0 8px 32px rgba(0, 255, 136, 0.2);\n    }\n\n    h1 {\n      color: #00ff88;\n      font-size: 2.8rem;\n      margin-bottom: 0.5rem;\n      text-shadow: 0 0 10px rgba(0, 255, 136, 0.5);\n    }\n\n    .subtitle {\n      color: #a0aec0;\n      font-size: 1rem;\n      margin-top: 0.5rem;\n    }\n\n    .summary-box {\n      background: rgba(0, 255, 136, 0.1);\n      border-left: 4px solid #00ff88;\n      border-radius: 8px;\n      padding: 1.5rem 2rem;\n      margin: 2rem 0;\n      font-size: 1.1rem;\n      text-align: center;\n      color: #00ff88;\n      font-weight: bold;\n    }\n\n    .race-card {\n      background: linear-gradient(135deg, #0f3460 0%, #1a4d7a 100%);\n      border: 1px solid rgba(0, 255, 136, 0.2);\n      border-left: 4px solid #00ff88;\n      border-radius: 12px;\n      padding: 2rem;\n      margin-bottom: 2rem;\n      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);\n      transition: all 0.3s ease;\n    }\n\n    .race-card:hover {\n      transform: translateY(-4px);\n      box-shadow: 0 8px 30px rgba(0, 255, 136, 0.25);\n      border-color: rgba(0, 255, 136, 0.5);\n    }\n\n    .race-header {\n      display: flex;\n      justify-content: space-between;\n      align-items: flex-start;\n      margin-bottom: 1.5rem;\n      border-bottom: 2px solid #00ff88;\n      padding-bottom: 1rem;\n      flex-wrap: wrap;\n      gap: 1rem;\n    }\n\n    .race-title {\n      font-size: 1.4rem;\n      font-weight: bold;\n      color: #00ff88;\n    }\n\n    .race-meta {\n      color: #a0aec0;\n      font-size: 0.9rem;\n    }\n\n    .runners-table {\n      width: 100%;\n      border-collapse: collapse;\n      margin-top: 1rem;\n    }\n\n    .runners-table thead {\n      background: rgba(0, 255, 136, 0.15);\n      border-bottom: 2px solid #00ff88;\n    }\n\n    .runners-table th {\n      padding: 1rem;\n      text-align: left;\n      font-weight: bold;\n      color: #00ff88;\n      text-transform: uppercase;\n      font-size: 0.85rem;\n      letter-spacing: 1px;\n    }\n\n    .runners-table td {\n      padding: 0.75rem 1rem;\n      border-bottom: 1px solid #1a3a52;\n    }\n\n    .runners-table tbody tr:hover {\n      background: rgba(0, 255, 136, 0.05);\n    }\n\n    .runner-name {\n      font-weight: 600;\n      color: #e2e8f0;\n    }\n\n    .odds {\n      font-family: 'Courier New', monospace;\n      color: #00ff88;\n      font-weight: bold;\n      font-size: 1.1rem;\n    }\n\n    .source {\n      color: #718096;\n      font-size: 0.9rem;\n    }\n\n    .no-races {\n      text-align: center;\n      padding: 3rem;\n      background: #0f3460;\n      border: 2px dashed #ff4444;\n      border-radius: 12px;\n      color: #a0aec0;\n      font-size: 1.1rem;\n    }\n\n    footer {\n      text-align: center;\n      margin-top: 4rem;\n      padding-top: 2rem;\n      border-top: 1px solid #404060;\n      color: #718096;\n      font-size: 0.85rem;\n    }\n\n    footer a {\n      color: #00ff88;\n      text-decoration: none;\n      transition: color 0.2s;\n    }\n\n    footer a:hover {\n      color: #00ff88;\n      text-decoration: underline;\n    }\n\n    @media (max-width: 768px) {\n      h1 { font-size: 1.8rem; }\n      header { padding: 2rem 1rem; }\n      .race-card { padding: 1rem; }\n      .runners-table th, .runners-table td { padding: 0.5rem; }\n    }\n  </style>\n</head>\n<body>\n  <div class=\"container\">\n    <header>\n      <h1>\ud83d\udc34 Fortuna Faucet Race Report</h1>\n      <p class=\"subtitle\">Filtered Trifecta Opportunities</p>\n      <p class=\"subtitle\">Generated: $timestamp</p>\n    </header>\n\n    <div class=\"summary-box\">\n      $($races.races.Count) qualified race(s) found\n    </div>\n\"@\n\nif ($races.races -and $races.races.Count -gt 0) {\n    foreach ($race in $races.races) {\n        $venue = $race.venue ?? \"Unknown\"\n        $raceNum = $race.race_number ?? \"?\"\n        $startTime = $race.startTime ?? \"N/A\"\n\n        $html += @\"\n<div class=\"race-card\">\n<div class=\"race-header\">\n  <div>\n    <div class=\"race-title\">$venue - Race $raceNum</div>\n    <div class=\"race-meta\">Post Time: $startTime</div>\n  </div>\n</div>\n<table class=\"runners-table\">\n  <thead>\n    <tr>\n      <th>Horse Name</th>\n      <th>Win Odds</th>\n      <th>Best Source</th>\n    </tr>\n  </thead>\n  <tbody>\n\"@\n\n        foreach ($runner in $race.runners) {\n            $name = $runner.name ?? \"Unknown\"\n            $odds = \"N/A\"\n            $source = \"N/A\"\n\n            if ($runner.odds) {\n                $bestVal = 0\n                foreach ($src in $runner.odds.PSObject.Properties) {\n                    if ($src.Value.win -gt $bestVal) {\n                        $bestVal = $src.Value.win\n                        $odds = [math]::Round($bestVal, 2)\n                        $source = $src.Name\n                    }\n                }\n            }\n\n            $html += @\"\n    <tr>\n      <td class=\"runner-name\">$name</td>\n      <td class=\"odds\">$odds</td>\n      <td class=\"source\">$source</td>\n    </tr>\n\"@\n        }\n\n        $html += @\"\n  </tbody>\n</table>\n</div>\n\"@\n    }\n} else {\n    $html += @\"\n<div class=\"no-races\">\n\u274c No qualified races found at this time.\n</div>\n\"@\n}\n\n$github_repository = $env:GITHUB_REPOSITORY\n$html += @\"\n    <footer>\n      <p>This report was automatically generated by Fortuna Faucet via GitHub Actions.</p>\n      <p>Data sources: Multiple racing exchanges and bookmakers.</p>\n      <p>\ud83d\udd17 <a href=\"https://github.com/$github_repository\">View Repository</a></p>\n    </footer>\n  </div>\n</body>\n</html>\n\"@\n\n$html | Out-File -FilePath $HtmlOutputPath -Encoding utf8\nWrite-Host \"\u2705 HTML report generated successfully at $HtmlOutputPath\" -ForegroundColor Green\n",
    "scripts/get_race_count.py": "import json\nimport sys\n\ndef get_race_count():\n    \"\"\"\n    Reads 'qualified_races.json', counts the number of races, and prints the count.\n    Prints 0 if the file doesn't exist or is invalid.\n    \"\"\"\n    try:\n        with open('qualified_races.json') as f:\n            data = json.load(f)\n        print(len(data.get('races', [])))\n    except Exception:\n        print(0)\n\nif __name__ == \"__main__\":\n    get_race_count()\n",
    "scripts/repair_fortuna.bat": "@echo off\nREM Repair corrupted or missing files\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\necho Repairing Fortuna Faucet installation...\n\nREM /f flag performs repair. Assumes MSI is in the 'dist' folder.\nmsiexec.exe /f \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_repair.log\"\n\nif %errorlevel% equ 0 (\n    echo Repair completed successfully.\n) else (\n    echo Repair failed. Check log: %TEMP%\\fortuna_repair.log\n)\n\nexit /b %errorlevel%",
    "scripts/verify_browsers.py": "#!/usr/bin/env python3\n\"\"\"\nEnhanced Browser Verification Script (Scrapling 0.3.x compatible)\nTests all available browser engines and provides actionable recommendations\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nimport traceback\nimport shutil\n\n# Results structure\nresults = {\n    \"timestamp\": datetime.utcnow().isoformat(),\n    \"environment\": {\n        \"display\": os.environ.get(\"DISPLAY\", \"not set\"),\n        \"python_version\": sys.version.split()[0],\n        \"ci\": os.environ.get(\"CI\", \"false\"),\n        \"headless\": os.environ.get(\"SCRAPLING_HEADLESS\", \"true\"),\n        \"xvfb_running\": False,\n    },\n    \"tests\": {},\n    \"recommendations\": [],\n    \"summary\": {\n        \"total_tests\": 0,\n        \"passed\": 0,\n        \"failed\": 0,\n        \"operational_engines\": []\n    }\n}\n\ndef check_display():\n    \"\"\"Check if display server is available\"\"\"\n    display = os.environ.get(\"DISPLAY\")\n    if not display:\n        return False, \"DISPLAY not set\"\n\n    # Try to connect to display\n    try:\n        import subprocess\n        # Check if xdpyinfo exists\n        if not shutil.which(\"xdpyinfo\"):\n            return False, \"xdpyinfo not found\"\n\n        result = subprocess.run(\n            ['xdpyinfo', '-display', display],\n            capture_output=True,\n            timeout=5\n        )\n        if result.returncode == 0:\n            results[\"environment\"][\"xvfb_running\"] = True\n            return True, f\"Display {display} accessible\"\n        return False, f\"Display {display} not accessible\"\n    except Exception as e:\n        return False, f\"Could not check display: {e}\"\n\nasync def test_httpx_fallback():\n    \"\"\"Test basic HTTPX fetcher (lightweight fallback)\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST 1/4: HTTPX Fallback (Lightweight)\")\n    print(\"=\" * 60)\n\n    test_name = \"httpx_fallback\"\n    results[\"tests\"][test_name] = {\n        \"started\": datetime.utcnow().isoformat(),\n        \"passed\": False,\n        \"duration_ms\": 0,\n        \"details\": {}\n    }\n\n    start = datetime.now()\n\n    try:\n        from scrapling import Fetcher\n        print(\"\u2713 Scrapling imported\")\n\n        print(\"\u2192 Initializing HTTPX fetcher...\")\n        # Use httpx backend explicitly for stability\n        fetcher = Fetcher()\n        try:\n            fetcher.configure(backend='httpx')\n        except:\n            pass\n        print(\"\u2713 Fetcher initialized\")\n\n        print(\"\u2192 Testing basic fetch...\")\n        # In scrapling 0.3.x, use .get()\n        response = fetcher.get('https://httpbin.org/get')\n\n        duration_ms = (datetime.now() - start).total_seconds() * 1000\n\n        print(f\"\u2713 Status: {response.status}\")\n        print(f\"\u2713 Content length: {len(response.text)} chars\")\n        print(f\"\u2713 Duration: {duration_ms:.0f}ms\")\n\n        # Verify response content\n        if response.status == 200 and len(response.text) > 100:\n            print(\"\u2705 HTTPX PASSED\")\n            results[\"tests\"][test_name].update({\n                \"passed\": True,\n                \"message\": \"Basic HTTP fetch working\",\n                \"duration_ms\": duration_ms,\n                \"details\": {\n                    \"status\": response.status,\n                    \"content_length\": len(response.text)\n                }\n            })\n            return True, \"Basic HTTP fetch working\"\n\n        return False, f\"Unexpected response: {response.status}\"\n\n    except Exception as e:\n        print(f\"\u274c Error: {str(e)}\")\n        results[\"tests\"][test_name][\"message\"] = str(e)\n        traceback.print_exc()\n        return False, str(e)\n    finally:\n        results[\"tests\"][test_name][\"duration_ms\"] = (datetime.now() - start).total_seconds() * 1000\n\nasync def test_playwright_chromium():\n    \"\"\"Test Playwright Chromium (Recommended for CI)\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST 2/4: Playwright Chromium (Recommended for CI)\")\n    print(\"=\" * 60)\n\n    test_name = \"playwright_chromium\"\n    results[\"tests\"][test_name] = {\n        \"started\": datetime.utcnow().isoformat(),\n        \"passed\": False,\n        \"duration_ms\": 0,\n        \"details\": {}\n    }\n\n    start = datetime.now()\n    session = None\n    try:\n        from scrapling.fetchers import AsyncDynamicSession\n        print(\"\u2713 AsyncDynamicSession imported\")\n\n        print(\"\u2192 Initializing Playwright (Chromium) session...\")\n        session = AsyncDynamicSession(\n            headless=True,\n            disable_resources=True\n        )\n        await session.start()\n        print(\"\u2713 Session started\")\n\n        print(\"\u2192 Testing HTML fetch...\")\n        response = await session.fetch('https://httpbin.org/html')\n        print(f\"\u2713 Status: {response.status}\")\n        print(f\"\u2713 Content: {len(response.text)} chars\")\n\n        duration_ms = (datetime.now() - start).total_seconds() * 1000\n        print(f\"\u2713 Total duration: {duration_ms:.0f}ms\")\n\n        print(\"\u2705 PLAYWRIGHT CHROMIUM PASSED\")\n        results[\"tests\"][test_name].update({\n            \"passed\": True,\n            \"message\": \"All Playwright tests passed\",\n            \"duration_ms\": duration_ms,\n            \"details\": {\n                \"status\": response.status\n            }\n        })\n        return True, \"All Playwright tests passed\"\n\n    except Exception as e:\n        msg = str(e)\n        print(f\"\u274c Error: {msg}\")\n        results[\"tests\"][test_name][\"message\"] = msg\n        return False, msg\n    finally:\n        if session:\n            try:\n                await session.close()\n            except:\n                pass\n        results[\"tests\"][test_name][\"duration_ms\"] = (datetime.now() - start).total_seconds() * 1000\n\nasync def test_playwright_firefox():\n    \"\"\"Test Playwright Firefox (Backup)\"\"\"\n    # Not easily supported via AsyncDynamicSession which defaults to Chromium\n    # We'll skip this for now or implement if needed\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST 3/4: Playwright Firefox (Backup)\")\n    print(\"=\" * 60)\n    print(\"Skipping Firefox specific test as AsyncDynamicSession is Chromium optimized.\")\n    return True, \"Skipped\"\n\nasync def test_async_stealthy():\n    \"\"\"Test AsyncStealthySession (Camoufox - most stealthy)\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST 4/4: AsyncStealthySession (Camoufox - Most Stealthy)\")\n    print(\"=\" * 60)\n\n    test_name = \"async_stealthy\"\n    results[\"tests\"][test_name] = {\n        \"started\": datetime.utcnow().isoformat(),\n        \"passed\": False,\n        \"duration_ms\": 0,\n        \"details\": {}\n    }\n\n    session = None\n    start = datetime.now()\n\n    try:\n        from scrapling.fetchers import AsyncStealthySession\n        print(\"\u2713 AsyncStealthySession imported\")\n\n        print(\"\u2192 Initializing Camoufox session...\")\n        session = AsyncStealthySession(\n            headless=True,\n            disable_resources=True,\n        )\n\n        print(\"\u2192 Starting session (may take 10-30s)...\")\n        await asyncio.wait_for(session.start(), timeout=45)\n        print(\"\u2713 Session started\")\n\n        print(\"\u2192 Testing stealth fetch...\")\n        response = await asyncio.wait_for(\n            session.fetch('https://httpbin.org/headers'),\n            timeout=30\n        )\n\n        duration_ms = (datetime.now() - start).total_seconds() * 1000\n\n        print(f\"\u2713 Status: {response.status}\")\n        print(f\"\u2713 Content: {len(response.text)} chars\")\n        print(f\"\u2713 Duration: {duration_ms:.0f}ms\")\n\n        print(\"\u2705 CAMOUFOX PASSED\")\n        results[\"tests\"][test_name].update({\n            \"passed\": True,\n            \"message\": \"Camoufox stealth working\",\n            \"duration_ms\": duration_ms,\n            \"details\": {\n                \"stealth\": True\n            }\n        })\n        return True, \"Camoufox stealth working\"\n\n    except Exception as e:\n        msg = str(e)\n        print(f\"\u26a0\ufe0f Error: {msg}\")\n        results[\"tests\"][test_name][\"message\"] = msg\n        return False, msg\n    finally:\n        if session:\n            try:\n                await asyncio.wait_for(session.close(), timeout=10)\n                print(\"\u2713 Session closed\")\n            except:\n                pass\n        results[\"tests\"][test_name][\"duration_ms\"] = (datetime.now() - start).total_seconds() * 1000\n\ndef generate_recommendations():\n    \"\"\"Generate actionable recommendations based on test results\"\"\"\n    recs = []\n\n    test_results = results[\"tests\"]\n    passed_tests = [name for name, result in test_results.items() if result.get(\"passed\")]\n\n    # Critical: No browsers available\n    if not passed_tests:\n        recs.append({\n            \"level\": \"CRITICAL\",\n            \"icon\": \"\ud83d\udea8\",\n            \"message\": \"NO BROWSERS AVAILABLE!\",\n            \"action\": \"Install Playwright browsers: playwright install chromium --with-deps\"\n        })\n        return recs\n\n    # Check HTTPX\n    if \"httpx_fallback\" not in passed_tests:\n        recs.append({\n            \"level\": \"WARNING\",\n            \"icon\": \"\u26a0\ufe0f\",\n            \"message\": \"HTTPX fallback not working\",\n            \"action\": \"Check Scrapling installation: pip install 'scrapling[all]'\"\n        })\n\n    # Check Playwright Chromium (most important for CI)\n    if \"playwright_chromium\" not in passed_tests:\n        recs.append({\n            \"level\": \"CRITICAL\",\n            \"icon\": \"\ud83d\udea8\",\n            \"message\": \"Playwright Chromium not working\",\n            \"action\": \"Install: playwright install chromium --with-deps\"\n        })\n    else:\n        recs.append({\n            \"level\": \"SUCCESS\",\n            \"icon\": \"\u2705\",\n            \"message\": \"Playwright Chromium working (recommended for CI)\"\n        })\n\n    # Check Camoufox\n    if \"async_stealthy\" not in passed_tests:\n        recs.append({\n            \"level\": \"INFO\",\n            \"icon\": \"\u2139\ufe0f\",\n            \"message\": \"Camoufox not available (optional)\",\n            \"action\": \"For anti-bot protection, install: pip install camoufox\"\n        })\n    else:\n        recs.append({\n            \"level\": \"SUCCESS\",\n            \"icon\": \"\u2705\",\n            \"message\": \"Camoufox available (best for anti-bot sites)\"\n        })\n\n    # Check display\n    if not results[\"environment\"][\"xvfb_running\"]:\n        recs.append({\n            \"level\": \"WARNING\",\n            \"icon\": \"\u26a0\ufe0f\",\n            \"message\": \"No display server detected\",\n            \"action\": \"For headless browsers, start Xvfb: Xvfb :99 -screen 0 1920x1080x24 &\"\n        })\n\n    return recs\n\nasync def main():\n    \"\"\"Run comprehensive browser verification\"\"\"\n    print(\"=\" * 60)\n    print(\"BROWSER VERIFICATION SUITE\")\n    print(\"=\" * 60)\n    print(f\"Timestamp: {results['timestamp']}\")\n    print(f\"Display: {os.environ.get('DISPLAY', 'not set')}\")\n    print(f\"Python: {sys.version.split()[0]}\")\n    print(f\"CI Mode: {os.environ.get('CI', 'false')}\")\n    print(f\"Headless: {os.environ.get('SCRAPLING_HEADLESS', 'true')}\")\n\n    # Check display\n    display_ok, display_msg = check_display()\n    print(f\"Display Status: {display_msg}\")\n\n    # Check Scrapling\n    try:\n        import scrapling\n        version = scrapling.__version__\n        print(f\"Scrapling: v{version}\")\n        results[\"environment\"][\"scrapling_version\"] = version\n    except Exception as e:\n        print(f\"\u274c FATAL: Scrapling not installed: {e}\")\n        print(\"Install with: pip install 'scrapling[all]'\")\n        results[\"summary\"][\"fatal_error\"] = \"Scrapling not installed\"\n\n        # Save results\n        with open(\"browser_verification.json\", \"w\") as f:\n            json.dump(results, f, indent=2)\n\n        return 1\n\n    # Run tests\n    tests = [\n        (\"httpx_fallback\", test_httpx_fallback),\n        (\"playwright_chromium\", test_playwright_chromium),\n        (\"async_stealthy\", test_async_stealthy),\n    ]\n\n    passed = 0\n    results[\"summary\"][\"total_tests\"] = len(tests)\n\n    for name, test_func in tests:\n        try:\n            success, message = await test_func()\n            if success:\n                passed += 1\n                results[\"summary\"][\"operational_engines\"].append(name)\n        except Exception as e:\n            print(f\"\\n\u274c Test {name} crashed: {e}\")\n            traceback.print_exc()\n\n    results[\"summary\"][\"passed\"] = passed\n    results[\"summary\"][\"failed\"] = len(tests) - passed\n\n    # Generate recommendations\n    results[\"recommendations\"] = generate_recommendations()\n\n    # Print Summary\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SUMMARY\")\n    print(\"=\" * 60)\n\n    for name, result in results[\"tests\"].items():\n        status = \"\u2705\" if result.get(\"passed\") else \"\u274c\"\n        message = result.get(\"message\", \"No message\")\n        duration = result.get(\"duration_ms\", 0)\n        print(f\"  {status} {name}: {message} ({duration:.0f}ms)\")\n\n    print(f\"\\n\ud83d\udcca Results: {passed}/{len(tests)} tests passed\")\n\n    # Print recommendations\n    if results[\"recommendations\"]:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"RECOMMENDATIONS\")\n        print(\"=\" * 60)\n        for rec in results[\"recommendations\"]:\n            icon = rec.get(\"icon\", \"\u2022\")\n            level = rec.get(\"level\", \"INFO\")\n            message = rec.get(\"message\", \"\")\n            action = rec.get(\"action\", \"\")\n\n            print(f\"\\n{icon} [{level}] {message}\")\n            if action:\n                print(f\"   \u2192 {action}\")\n\n    # Save detailed results\n    output_path = Path(\"browser_verification.json\")\n    with open(output_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"\\n\ud83d\udcc4 Detailed results saved to: {output_path}\")\n\n    # Determine exit code\n    print(\"\\n\" + \"=\" * 60)\n    if passed == 0:\n        print(\"\u274c CRITICAL: No browsers available!\")\n        print(\"\ud83d\udd27 Action Required: Install browsers before running scrapers\")\n        return 1\n    elif passed < len(tests):\n        print(\"\u26a0\ufe0f  Some browsers unavailable, but system operational\")\n        print(f\"\u2705 Operational engines: {', '.join(results['summary']['operational_engines'])}\")\n        return 0\n    else:\n        print(\"\u2705 All browsers working perfectly!\")\n        return 0\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)\n",
    "tests/adapters/test_gbgb_api_adapter.py": "# tests/adapters/test_gbgb_api_adapter.py\n\nfrom datetime import date\nfrom decimal import Decimal\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef gbgb_adapter():\n    \"\"\"Returns a GbgbApiAdapter instance for testing.\"\"\"\n    return GbgbApiAdapter(config=get_test_settings())\n\n\n@pytest.mark.asyncio\nasync def test_get_gbgb_races_successfully(gbgb_adapter):\n    \"\"\"\n    SPEC: The GbgbApiAdapter should correctly parse a standard API response,\n    creating Race and Runner objects with the correct data, including fractional odds.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime(\"%Y-%m-%d\")\n    mock_api_response = [\n        {\n            \"trackName\": \"Towcester\",\n            \"races\": [\n                {\n                    \"raceId\": 12345,\n                    \"raceNumber\": 1,\n                    \"raceTime\": f\"{date.today().isoformat()}T18:00:00Z\",\n                    \"raceTitle\": \"The October Sprint\",\n                    \"raceDistance\": 500,\n                    \"traps\": [\n                        {\"trapNumber\": 1, \"dogName\": \"Rapid Rover\", \"sp\": \"5/2\"},\n                        {\"trapNumber\": 2, \"dogName\": \"Speedy Sue\", \"sp\": \"EVS\"},\n                        {\"trapNumber\": 3, \"dogName\": \"Lazy Larry\", \"sp\": \"10/1\"},\n                    ],\n                }\n            ],\n        }\n    ]\n    gbgb_adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = await gbgb_adapter.get_races(mock_date)\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.venue == \"Towcester\"\n    assert race.race_number == 1\n    assert race.race_name == \"The October Sprint\"\n    assert race.distance == \"500m\"\n    assert len(race.runners) == 3\n\n    runner1 = next(r for r in race.runners if r.number == 1)\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.odds[\"GBGB\"].win == Decimal(\"3.5\")\n\n    runner2 = next(r for r in race.runners if r.number == 2)\n    assert runner2.name == \"Speedy Sue\"\n    assert runner2.odds[\"GBGB\"].win == Decimal(\"2.0\")\n\n    runner3 = next(r for r in race.runners if r.number == 3)\n    assert runner3.name == \"Lazy Larry\"\n    assert runner3.odds[\"GBGB\"].win == Decimal(\"11.0\")\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(gbgb_adapter):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime(\"%Y-%m-%d\")\n    gbgb_adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = await gbgb_adapter.get_races(mock_date)\n\n    # ASSERT\n    assert races == []\n",
    "tests/adapters/test_twinspires_adapter.py": "# tests/adapters/test_twinspires_adapter.py\nimport pytest\nfrom python_service.adapters.twinspires_adapter import TwinSpiresAdapter\nfrom unittest.mock import MagicMock, AsyncMock\nfrom pathlib import Path\nfrom python_service.models import Race\nfrom decimal import Decimal\nfrom datetime import datetime, timedelta, timezone\n\n# A mock settings object to satisfy the adapter's config dependency\nclass MockSettings:\n    pass\n\n@pytest.fixture\ndef adapter():\n    return TwinSpiresAdapter(config=MockSettings())\n\n@pytest.mark.asyncio\nasync def test_get_races_from_fixture(adapter, mocker):\n    \"\"\"\n    Test that the adapter can correctly parse a local HTML fixture.\n    This test validates the end-to-end parsing logic, including runner data.\n    \"\"\"\n    # ARRANGE\n    future_date = (datetime.now(timezone.utc) + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    fixture_path = Path(\"tests/fixtures/twinspires_racecard.html\")\n    html_content = fixture_path.read_text()\n    \n    # Mock the internal _fetch_data to return our fixture wrapped in the expected structure\n    mock_raw_data = {\n        \"date\": future_date,\n        \"source\": \"TwinSpires\",\n        \"races\": [\n            {\n                \"html\": html_content,\n                \"track\": \"Churchill Downs\",\n                \"race_number\": 5,\n                \"post_time_text\": f\"{future_date} 17:05\",\n                \"date\": future_date,\n                \"full_page\": False\n            }\n        ]\n    }\n    \n    adapter._fetch_data = AsyncMock(return_value=mock_raw_data)\n\n    # ACT\n    races = await adapter.get_races(date=future_date)\n\n    # ASSERT\n    assert isinstance(races, list)\n    assert len(races) == 1\n\n    # Check the race for correct parsing\n    race = races[0]\n    assert race.venue == \"Churchill Downs\"\n    assert race.race_number == 5\n\n    # Check that runners were parsed correctly\n    assert len(race.runners) == 4\n\n    # Verify a specific runner's details\n    runner_1 = next((r for r in race.runners if r.number == 1), None)\n    assert runner_1 is not None\n    assert runner_1.name == \"Braveheart\"\n    assert not runner_1.scratched\n    # 5/2 = 2.5 + 1 = 3.5\n    assert runner_1.odds[\"TwinSpires\"].win == Decimal(\"3.5\")\n\n    # Verify a scratched runner\n    runner_3 = next((r for r in race.runners if r.number == 3), None)\n    assert runner_3 is not None\n    assert runner_3.name == \"Steady Eddy\"\n    assert runner_3.scratched\n    assert not runner_3.odds\n",
    "tests/fixtures/timeform_modern_sample.html": "<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Braveheart</a>\n  <span class=\"rp-horseTable_horse-number\">(1)</span>\n  <button class=\"rp-bet-placer-btn__odds\">5/2</button>\n</div>\n<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Speedster</a>\n  <span class=\"rp-horseTable_horse-number\">(2)</span>\n  <button class=\"rp-bet-placer-btn__odds\">10/1</button>\n</div>\n<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Steady Eddy</a>\n  <span class=\"rp-horseTable_horse-number\">(3)</span>\n  <button class=\"rp-bet-placer-btn__odds\">EVENS</button>\n</div>\n",
    "tests/test_ci_sanity.py": "def test_ci_pipeline_is_alive():\n    \"\"\"Basic TDD sanity check to ensure pytest is running in CI.\"\"\"\n    assert True",
    "tests/test_models/test_validation.py": "import pytest\nfrom pydantic import ValidationError\n\nfrom python_service.models import Race\n\n\ndef test_race_model_valid_data():\n    \"\"\"Tests that the Race model can be created with valid data.\"\"\"\n    race_data = {\n        \"id\": \"test_race_123\",\n        \"venue\": \"Test Park\",\n        \"race_number\": 1,\n        \"start_time\": \"2025-10-20T12:00:00Z\",\n        \"runners\": [],\n        \"source\": \"test_source\",\n    }\n    race = Race(**race_data)\n    assert race.id == \"test_race_123\"\n    assert race.venue == \"Test Park\"\n\n\ndef test_race_model_invalid_data():\n    \"\"\"Tests that the Race model raises a ValidationError with invalid data.\"\"\"\n    invalid_race_data = {\n        \"id\": \"test_race_456\",\n        \"venue\": 12345,  # Invalid type\n        \"race_number\": \"two\",  # Invalid type\n        \"start_time\": \"not-a-date\",\n        \"runners\": \"not-a-list\",\n        \"source\": \"test_source\",\n    }\n    with pytest.raises(ValidationError):\n        Race(**invalid_race_data)\n",
    "tests/test_msi_installation.ps1": "param([string]$MsiPath = \".\\dist\\Fortuna-Faucet-2.1.0-x64.msi\")\n\nWrite-Host \"Testing MSI Installation...\" -ForegroundColor Cyan\n\n# Test 1: File integrity\nWrite-Host \"\u2022 Verifying MSI structure...\"\nif (Test-Path $MsiPath) {\n    Write-Host \"\u2713 MSI file exists\"\n} else {\n    Write-Error \"MSI file not found\"\n    exit 1\n}\n\n# Test 2: Installation\nWrite-Host \"\u2022 Testing interactive installation...\"\n& msiexec.exe /i $MsiPath /l*v \"test_install.log\"\n\n# Test 3: Verify installation\nWrite-Host \"\u2022 Verifying files were installed...\"\n$programFiles = \"$env:PROGRAMFILES\\Fortuna Faucet\"\nif (Test-Path $programFiles) {\n    Write-Host \"\u2713 Installation successful\"\n} else {\n    Write-Error \"Installation failed\"\n    exit 1\n}\n\n# Test 4: Registry entries\nWrite-Host \"\u2022 Checking registry entries...\"\n$regPath = \"HKLM:\\Software\\Fortuna Faucet\"\nif (Test-Path $regPath) {\n    Write-Host \"\u2713 Registry entries found\"\n} else {\n    Write-Error \"Registry entries missing\"\n    exit 1\n}",
    "web_service/__init__.py": "\"\"\"Web service package for Fortuna Faucet.\"\"\"\n__version__ = \"1.0.0\"\n__all__ = [\"backend\"]\n",
    "web_service/backend/adapters/base_adapter_v3.py": "# python_service/adapters/base_v3.py\nfrom __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport json\nimport random\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport httpx\nimport structlog\nfrom tenacity import (\n    RetryError,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom python_service.core.smart_fetcher import (\n    BrowserEngine,\n    FetchStrategy,\n    SmartFetcher,\n    StealthMode,\n)\n\nfrom ..core.exceptions import AdapterHttpError, AdapterParsingError, ErrorCategory\nfrom ..manual_override_manager import ManualOverrideManager\nfrom ..models import Race\nfrom ..validators import DataValidationPipeline\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject requests\n    HALF_OPEN = \"half_open\"  # Testing if service recovered\n\n\n@dataclass\nclass CircuitBreaker:\n    \"\"\"Thread-safe circuit breaker implementation.\"\"\"\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    half_open_max_calls: int = 3\n\n    _failure_count: int = field(default=0, repr=False)\n    _last_failure_time: float = field(default=0.0, repr=False)\n    _state: CircuitState = field(default=CircuitState.CLOSED, repr=False)\n    _half_open_calls: int = field(default=0, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    @property\n    def state(self) -> CircuitState:\n        \"\"\"Returns current state without mutation. Use check_state() for transitions.\"\"\"\n        return self._state\n\n    async def check_and_transition_state(self) -> CircuitState:\n        \"\"\"Check state and handle OPEN -> HALF_OPEN transition atomically.\"\"\"\n        async with self._lock:\n            if self._state == CircuitState.OPEN:\n                if time.monotonic() - self._last_failure_time >= self.recovery_timeout:\n                    self._state = CircuitState.HALF_OPEN\n                    self._half_open_calls = 0\n            return self._state\n\n    async def record_success(self) -> None:\n        \"\"\"Record a successful call.\"\"\"\n        async with self._lock:\n            self._failure_count = 0\n            if self._state == CircuitState.HALF_OPEN:\n                self._half_open_calls += 1\n                if self._half_open_calls >= self.half_open_max_calls:\n                    self._state = CircuitState.CLOSED\n\n    async def record_failure(self) -> None:\n        \"\"\"Record a failed call.\"\"\"\n        async with self._lock:\n            self._failure_count += 1\n            self._last_failure_time = time.monotonic()\n\n            if self._failure_count >= self.failure_threshold:\n                self._state = CircuitState.OPEN\n            elif self._state == CircuitState.HALF_OPEN:\n                self._state = CircuitState.OPEN\n\n    async def allow_request(self) -> bool:\n        \"\"\"Check if a request should be allowed.\"\"\"\n        state = await self.check_and_transition_state()\n        return state in (CircuitState.CLOSED, CircuitState.HALF_OPEN)\n\n\n@dataclass\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter.\"\"\"\n    requests_per_second: float = 10.0\n    burst_size: int = 20\n\n    _tokens: float = field(default=0.0, init=False, repr=False)\n    _last_update: float = field(default=0.0, init=False, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    def __post_init__(self) -> None:\n        self._tokens = float(self.burst_size)\n        self._last_update = time.monotonic()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire a token, waiting if necessary.\"\"\"\n        async with self._lock:\n            now = time.monotonic()\n            elapsed = now - self._last_update\n            self._tokens = min(self.burst_size, self._tokens + elapsed * self.requests_per_second)\n            self._last_update = now\n\n            if self._tokens < 1:\n                wait_time = (1 - self._tokens) / self.requests_per_second\n                await asyncio.sleep(wait_time)\n                self._tokens = 0\n            else:\n                self._tokens -= 1\n\n\n@dataclass\nclass CacheEntry:\n    \"\"\"Cache entry with TTL.\"\"\"\n    data: Any\n    created_at: float\n    ttl: float\n\n    @property\n    def is_expired(self) -> bool:\n        return time.monotonic() - self.created_at > self.ttl\n\n\nclass ResponseCache:\n    \"\"\"Simple in-memory response cache.\"\"\"\n\n    def __init__(self, default_ttl: float = 300.0, max_entries: int = 1000):\n        self.default_ttl = default_ttl\n        self.max_entries = max_entries\n        self._cache: dict[str, CacheEntry] = {}\n        self._lock = asyncio.Lock()\n\n    @staticmethod\n    def _make_key(method: str, url: str, **kwargs) -> str:\n        \"\"\"Generate a stable cache key from request parameters.\"\"\"\n        # Filter out non-hashable or irrelevant kwargs\n        cacheable_kwargs = {\n            k: v for k, v in kwargs.items()\n            if k not in ('headers', 'timeout', 'follow_redirects')\n            and isinstance(v, (str, int, float, bool, tuple, type(None)))\n        }\n        key_data = f\"{method}:{url}:{json.dumps(cacheable_kwargs, sort_keys=True, default=str)}\"\n        return hashlib.sha256(key_data.encode()).hexdigest()[:32]\n\n    async def get(self, method: str, url: str, **kwargs) -> Any | None:\n        \"\"\"Get a cached response if available and not expired.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            entry = self._cache.get(key)\n            if entry and not entry.is_expired:\n                return entry.data\n            elif entry:\n                del self._cache[key]\n        return None\n\n    async def set(self, method: str, url: str, data: Any, ttl: float | None = None, **kwargs) -> None:\n        \"\"\"Cache a response.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            # Evict old entries if cache is full\n            if len(self._cache) >= self.max_entries:\n                expired_keys = [k for k, v in self._cache.items() if v.is_expired]\n                for k in expired_keys:\n                    del self._cache[k]\n\n                # If still full, remove oldest entries\n                if len(self._cache) >= self.max_entries:\n                    oldest = sorted(self._cache.items(), key=lambda x: x[1].created_at)\n                    for k, _ in oldest[:len(self._cache) // 4]:\n                        del self._cache[k]\n\n            self._cache[key] = CacheEntry(\n                data=data,\n                created_at=time.monotonic(),\n                ttl=ttl or self.default_ttl\n            )\n\n    async def clear(self) -> None:\n        \"\"\"Clear all cached entries.\"\"\"\n        async with self._lock:\n            self._cache.clear()\n\n\n@dataclass\nclass AdapterMetrics:\n    \"\"\"Thread-safe metrics for adapter health monitoring.\"\"\"\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n    _total_requests: int = field(default=0, repr=False)\n    _successful_requests: int = field(default=0, repr=False)\n    _failed_requests: int = field(default=0, repr=False)\n    _consecutive_failures: int = field(default=0, repr=False)\n    _total_latency_ms: float = field(default=0.0, repr=False)\n    _last_success: float | None = field(default=None, repr=False)\n    _last_failure: float | None = field(default=None, repr=False)\n    _last_error: str | None = field(default=None, repr=False)\n\n    @property\n    def total_requests(self) -> int:\n        return self._total_requests\n\n    @property\n    def success_rate(self) -> float:\n        if self._total_requests == 0:\n            return 1.0\n        return self._successful_requests / self._total_requests\n\n    @property\n    def avg_latency_ms(self) -> float:\n        if self._successful_requests == 0:\n            return 0.0\n        return self._total_latency_ms / self._successful_requests\n\n    async def record_success(self, latency_ms: float) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._successful_requests += 1\n            self._consecutive_failures = 0\n            self._total_latency_ms += latency_ms\n            self._last_success = time.time()\n\n    async def record_failure(self, error: str) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._failed_requests += 1\n            self._consecutive_failures += 1\n            self._last_failure = time.time()\n            self._last_error = error\n\n    @property\n    def consecutive_failures(self) -> int:\n        return self._consecutive_failures\n\n    def snapshot(self) -> dict[str, Any]:\n        \"\"\"Return a point-in-time snapshot of metrics.\"\"\"\n        return {\n            \"total_requests\": self._total_requests,\n            \"successful_requests\": self._successful_requests,\n            \"failed_requests\": self._failed_requests,\n            \"success_rate\": self.success_rate,\n            \"avg_latency_ms\": self.avg_latency_ms,\n            \"last_success\": self._last_success,\n            \"last_failure\": self._last_failure,\n            \"last_error\": self._last_error,\n        }\n\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    Abstract base class for all V3 data adapters.\n\n    Features:\n    - Standardized fetch/parse pattern\n    - Retry logic with exponential backoff\n    - Circuit breaker for fault tolerance\n    - Rate limiting\n    - Response caching\n    - Comprehensive metrics\n    \"\"\"\n\n    # List of common User-Agent strings for rotation\n    USER_AGENTS = [\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0\",\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    ]\n\n    @property\n    def DEFAULT_USER_AGENT(self) -> str:\n        \"\"\"Return a randomly selected User-Agent.\"\"\"\n        return random.choice(self.USER_AGENTS)\n\n    def __init__(\n        self,\n        source_name: str,\n        base_url: str,\n        config: Any = None,\n        timeout: int = 20,\n        enable_cache: bool = True,\n        cache_ttl: float = 300.0,\n        rate_limit: float = 10.0,\n    ):\n        self.source_name = source_name\n        self.base_url = base_url.rstrip(\"/\")\n        self.config = config\n        self.timeout = timeout\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient | None = None\n        self.manual_override_manager: ManualOverrideManager | None = None\n        self.supports_manual_override = True\n        self.attempted_url: Optional[str] = None\n        self.last_race_count: int = 0\n        self.last_duration_s: float = 0.0\n        self._initial_rate_limit = rate_limit\n        # \u2705 THESE 4 LINES MUST BE HERE (not in close()):\n        self.circuit_breaker = CircuitBreaker()\n        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)\n        self.cache = ResponseCache(default_ttl=cache_ttl) if enable_cache else None\n        self.metrics = AdapterMetrics()\n\n        # New SmartFetcher integration\n        self.fetch_strategy = self._configure_fetch_strategy()\n        self.smart_fetcher = SmartFetcher(strategy=self.fetch_strategy)\n\n    async def __aenter__(self) -> \"BaseAdapterV3\":\n        \"\"\"Async context manager entry.\"\"\"\n        if self.http_client is None:\n            self.http_client = httpx.AsyncClient(timeout=self.timeout)\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Async context manager exit with cleanup.\"\"\"\n        await self.close()\n\n    async def close(self) -> None:\n        \"\"\"Clean up resources, including the SmartFetcher.\"\"\"\n        if self.http_client:\n            await self.http_client.aclose()\n            self.http_client = None\n        if hasattr(self, \"smart_fetcher\"):\n            await self.smart_fetcher.close()\n        if self.cache:\n            await self.cache.clear()\n        self.logger.debug(\"Adapter resources cleaned up\")\n\n    def enable_manual_override(self, manager: ManualOverrideManager) -> None:\n        \"\"\"Injects the manual override manager into the adapter.\"\"\"\n        self.manual_override_manager = manager\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw data (e.g., HTML, JSON) for the given date.\n        This is the only method that should perform network operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> list[Race]:\n        \"\"\"\n        Parses the raw data retrieved by _fetch_data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> list[Race]:\n        \"\"\"\n        Orchestrates the fetch-then-parse pipeline for the adapter.\n        This public method should not be overridden by subclasses.\n        \"\"\"\n        raw_data = None\n\n        # Check for manual override data first\n        if self.manual_override_manager:\n            lookup_key = f\"{self.base_url}/racecards/{date}\"\n            manual_data = self.manual_override_manager.get_manual_data(self.source_name, lookup_key)\n            if manual_data:\n                self.logger.info(\"Using manually submitted data\", url=lookup_key)\n                raw_data = {\"pages\": [manual_data[0]], \"date\": date}\n\n        # Fetch from source if no manual data\n        if raw_data is None:\n            try:\n                raw_data = await self._fetch_data(date)\n            except AdapterHttpError as e:\n                if self.manual_override_manager and self.supports_manual_override:\n                    self.manual_override_manager.register_failure(self.source_name, e.url)\n                raise\n\n        # Parse the data\n        if raw_data is not None:\n            return self._validate_and_parse_races(raw_data)\n\n        return []\n\n    def _validate_and_parse_races(self, raw_data: Any) -> list[Race]:\n        self.attempted_url = None  # Reset for each new get_races call\n        is_valid, reason = DataValidationPipeline.validate_raw_response(self.source_name, raw_data)\n        if not is_valid:\n            raise AdapterParsingError(self.source_name, f\"Raw response validation failed: {reason}\")\n\n        try:\n            parsed_races = self._parse_races(raw_data)\n        except Exception as e:\n            self.logger.error(\"Failed to parse race data\", error=str(e), exc_info=True)\n            # Save a snapshot of the problematic data on parsing failure\n            self._save_debug_snapshot(\n                content=str(raw_data),\n                context=\"parsing_error\",\n                url=getattr(e, 'url', self.attempted_url)\n            )\n            raise AdapterParsingError(self.source_name, \"Parsing logic failed.\") from e\n\n        validated_races, warnings = DataValidationPipeline.validate_parsed_races(parsed_races)\n        self.last_race_count = len(validated_races)\n\n        if warnings:\n            self.logger.warning(\"Validation warnings during parsing\", warnings=warnings)\n\n        return validated_races\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        Defines the fetching strategy for this adapter. Subclasses should override\n        this method to customize fetching behavior based on the target website's\n        characteristics (e.g., anti-bot measures, JavaScript requirements).\n\n        Example Overrides:\n        - SportingLife: Needs JS rendering -> primary_engine=BrowserEngine.PLAYWRIGHT\n        - AtTheRaces: Simple HTML -> primary_engine=BrowserEngine.HTTPX\n        - RacingPost: Strong anti-bot -> stealth_mode=StealthMode.CAMOUFLAGE\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=StealthMode.FAST,\n            block_resources=True,\n            max_retries=3,\n            timeout=30,\n        )\n\n    async def _adjust_rate_limit(self, success: bool):\n        \"\"\"Dynamically adjust rate limits based on performance.\"\"\"\n        # Target RPS is what was initially configured\n        target_rps = getattr(self, \"_initial_rate_limit\", 5.0)\n        current_rps = self.rate_limiter.requests_per_second\n\n        if not success:\n            # On failure, aggressively reduce rate limit (halve it)\n            new_rps = max(0.1, current_rps * 0.5)\n            if new_rps < current_rps:\n                self.rate_limiter.requests_per_second = new_rps\n                self.logger.warning(\"Backing off: reduced rate limit\",\n                                     new_rps=round(new_rps, 2),\n                                     old_rps=round(current_rps, 2))\n        else:\n            # On success, slowly increase rate limit back to target\n            if current_rps < target_rps:\n                new_rps = min(target_rps, current_rps + 0.05)\n                if new_rps > current_rps:\n                    self.rate_limiter.requests_per_second = new_rps\n                    self.logger.debug(\"Scaling up: increased rate limit\",\n                                      new_rps=round(new_rps, 2))\n\n    async def make_request(self, method: str, url: str, **kwargs) -> httpx.Response:\n        \"\"\"\n        Performs a web request using the SmartFetcher, which intelligently\n        manages browser engines, retries, and stealth capabilities.\n        \"\"\"\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url}/{url.lstrip('/')}\"\n        self.attempted_url = full_url\n\n        # Apply rate limiting before the request\n        await self.rate_limiter.acquire()\n\n        start_time = time.perf_counter()\n        try:\n            # The SmartFetcher handles caching, retries, circuit breaking, etc.\n            response = await self.smart_fetcher.fetch(full_url, method=method, **kwargs)\n\n            latency_ms = (time.perf_counter() - start_time) * 1000\n            self.last_duration_s = latency_ms / 1000.0\n            await self.metrics.record_success(latency_ms)\n            await self.circuit_breaker.record_success()\n            await self._adjust_rate_limit(success=True)\n\n            # Log success with rich metadata from the fetcher\n            self.logger.info(\n                \"Request successful\",\n                url=full_url,\n                status=getattr(response, \"status\", \"N/A\"),\n                size_bytes=len(getattr(response, \"text\", \"\")),\n                engine=getattr(response, \"metadata\", {}).get(\"engine_used\", \"unknown\"),\n                latency_ms=round(latency_ms, 1)\n            )\n            return response\n\n        except Exception as e:\n            category = getattr(e, \"category\", ErrorCategory.UNKNOWN).value\n            await self.metrics.record_failure(str(e))\n            await self.circuit_breaker.record_failure()\n            await self._adjust_rate_limit(success=False)\n            \n            # Log failure with detailed diagnostics from the fetcher\n            self.logger.error(\n                \"Request failed after all retries and engine fallbacks\",\n                url=full_url,\n                error=str(e),\n                error_type=type(e).__name__,\n                error_category=category,\n                health_report=self.smart_fetcher.get_health_report(),\n            )\n\n            # Save a snapshot if we have a response body in the error\n            if hasattr(e, 'response') and hasattr(e.response, 'text'):\n                self._save_debug_snapshot(\n                    content=e.response.text,\n                    context=f\"request_failed_{getattr(e.response, 'status', 'unknown')}\",\n                    url=full_url\n                )\n\n            # Re-raise as a standard adapter error for consistent downstream handling\n            status_code = getattr(getattr(e, 'response', None), 'status', 503)\n            raise AdapterHttpError(\n                adapter_name=self.source_name, status_code=status_code, url=full_url\n            ) from e\n\n    def _should_save_debug_html(self) -> bool:\n        \"\"\"Determines if the current environment is suitable for saving debug files.\"\"\"\n        import os\n        return os.getenv(\"CI\") == \"true\" or os.getenv(\"DEBUG_MODE\") == \"true\"\n\n    def _save_debug_snapshot(self, content: str, context: str, url: str | None = None):\n        \"\"\"\n        Saves HTML or other text content to a file for debugging purposes.\n        Enhanced to include metadata and better organization.\n        \"\"\"\n        if not self._should_save_debug_html():\n            return\n\n        import os\n        import re\n        import json\n        from datetime import datetime\n\n        try:\n            debug_dir = os.path.join(\"debug-snapshots\", self.source_name.lower())\n            os.makedirs(debug_dir, exist_ok=True)\n\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n\n            # Sanitize context and URL for a safe filename\n            sanitized_context = re.sub(r'[\\\\/*?:\"<>|]', \"_\", context)\n            sanitized_url = \"\"\n            if url:\n                # Remove protocol and query params for filename\n                clean_url = re.sub(r'https?://(www\\.)?', '', url).split('?')[0]\n                # Avoid backslashes in f-string for Python < 3.12 compatibility\n                url_part = re.sub(r'[\\\\/*?:\\x22<>|]', '_', clean_url)[:60]\n                sanitized_url = f\"_{url_part}\"\n\n            base_filename = f\"{timestamp}_{sanitized_context}{sanitized_url}\"\n\n            # Save the main content (HTML/JSON)\n            content_ext = \".json\" if content.startswith((\"{\", \"[\")) else \".html\"\n            filepath = os.path.join(debug_dir, f\"{base_filename}{content_ext}\")\n\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n\n            # Save metadata for better diagnostic context\n            meta_path = os.path.join(debug_dir, f\"{base_filename}_meta.json\")\n            meta = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"adapter\": self.source_name,\n                \"url\": url or self.attempted_url,\n                \"context\": context,\n                \"engine\": getattr(self.smart_fetcher, 'last_engine', 'unknown'),\n                \"health_report\": self.smart_fetcher.get_health_report()\n            }\n            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(meta, f, indent=2)\n\n            self.logger.info(\"Saved debug snapshot and metadata\",\n                             filepath=filepath, meta_path=meta_path)\n\n            # Prune old snapshots (keep last 50)\n            self._prune_debug_snapshots(debug_dir, max_files=100)\n\n        except Exception as e:\n            self.logger.warning(\"Failed to save debug snapshot\", error=str(e))\n\n    def _prune_debug_snapshots(self, debug_dir: str, max_files: int = 100):\n        \"\"\"Keep the number of debug files under control.\"\"\"\n        import os\n        try:\n            files = [os.path.join(debug_dir, f) for f in os.listdir(debug_dir)]\n            if len(files) <= max_files:\n                return\n\n            # Sort by modification time (oldest first)\n            files.sort(key=os.path.getmtime)\n            for f in files[:-max_files]:\n                os.remove(f)\n        except Exception:\n            pass\n\n    async def health_check(self) -> dict[str, Any]:\n        \"\"\"\n        Performs a health check on the adapter.\n        Subclasses can override to add custom checks.\n        \"\"\"\n        return {\n            \"adapter_name\": self.source_name,\n            \"base_url\": self.base_url,\n            \"circuit_breaker_state\": self.circuit_breaker.state.value,\n            \"metrics\": self.metrics.snapshot(),\n        }\n\n    def get_status(self) -> dict[str, Any]:\n        \"\"\"\n        Returns a dictionary representing the adapter's current status.\n        \"\"\"\n        status = \"OK\"\n        if self.circuit_breaker.state == CircuitState.OPEN:\n            status = \"CIRCUIT_OPEN\"\n        elif self.metrics.success_rate < 0.5:\n            status = \"DEGRADED\"\n\n        return {\n            \"adapter_name\": self.source_name,\n            \"status\": status,\n            \"circuit_state\": self.circuit_breaker.state.value,\n            \"success_rate\": round(self.metrics.success_rate, 3),\n            \"last_race_count\": self.last_race_count,\n            \"last_duration_s\": round(self.last_duration_s, 2),\n        }\n\n    async def reset(self) -> None:\n        \"\"\"Reset adapter state (cache, circuit breaker, metrics).\"\"\"\n        if self.cache:\n            await self.cache.clear()\n        self.circuit_breaker = CircuitBreaker()\n        self.metrics = AdapterMetrics()\n        self.logger.info(\"Adapter state reset\")\n",
    "web_service/backend/adapters/base_stub_adapter.py": "# python_service/adapters/base_stub_adapter.py\n\"\"\"Base class for non-functional stub adapters.\"\"\"\n\nfrom abc import ABC\nfrom typing import Any, List\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BaseStubAdapter(BaseAdapterV3, ABC):\n    \"\"\"\n    Base class for adapters that are not yet implemented.\n\n    Subclasses only need to define SOURCE_NAME and BASE_URL.\n    \"\"\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Stub implementation - logs warning and returns None.\"\"\"\n        self.logger.warning(\n            \"Adapter is a non-functional stub\",\n            adapter=self.source_name,\n            message=\"This adapter has not been implemented and will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Stub implementation - returns empty list.\"\"\"\n        return []\n",
    "web_service/backend/adapters/constants.py": "# python_service/adapters/constants.py\n\"\"\"Shared constants for all adapters.\"\"\"\n\nfrom typing import Final\n\n# Odds thresholds\nMAX_VALID_ODDS: Final[float] = 999.0\nMIN_VALID_ODDS: Final[float] = 1.01\n\n# Common HTTP headers for browser-like requests\nDEFAULT_BROWSER_HEADERS: Final[dict] = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Connection\": \"keep-alive\",\n    \"Pragma\": \"no-cache\",\n    \"Sec-Fetch-Dest\": \"document\",\n    \"Sec-Fetch-Mode\": \"navigate\",\n    \"Sec-Fetch-Site\": \"none\",\n    \"Sec-Fetch-User\": \"?1\",\n    \"Upgrade-Insecure-Requests\": \"1\",\n}\n\nCHROME_USER_AGENT: Final[str] = (\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n)\n\nCHROME_SEC_CH_UA: Final[str] = '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"'\n",
    "web_service/backend/adapters/harness_adapter.py": "# python_service/adapters/harness_adapter.py\n\"\"\"Adapter for US harness racing (USTA).\"\"\"\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\nfrom zoneinfo import ZoneInfo\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass HarnessAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US harness racing data.\"\"\"\n\n    SOURCE_NAME = \"USTrotting\"\n    BASE_URL = \"https://data.ustrotting.com/api/racenet/racing/\"\n\n    # Eastern timezone is standard for US racing\n    TIMEZONE = ZoneInfo(\"America/New_York\")\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetch harness races for a given date.\"\"\"\n        response = await self.make_request(\"GET\", f\"card/{date}\")\n        if not response:\n            return None\n        return {\"data\": response.json(), \"date\": date}\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parse card data into Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        data = raw_data.get(\"data\", {})\n        meetings = data.get(\"meetings\", [])\n\n        if not meetings:\n            self.logger.warning(\"No meetings found in harness data response.\")\n            return []\n\n        races = []\n        date = raw_data.get(\"date\")\n\n        for meeting in meetings:\n            track_name = meeting.get(\"track\", {}).get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name, date):\n                        races.append(race)\n                except Exception:\n                    self.logger.warning(\n                        \"Failed to parse harness race\",\n                        race_data=race_data,\n                        exc_info=True,\n                    )\n\n        return races\n\n    def _parse_race(\n        self, race_data: dict, track_name: str, date: str\n    ) -> Optional[Race]:\n        \"\"\"Parse a single race from USTA API.\"\"\"\n        race_number = race_data.get(\"raceNumber\")\n        post_time_str = race_data.get(\"postTime\")\n\n        if not all([race_number, post_time_str]):\n            return None\n\n        start_time = self._parse_post_time(date, post_time_str)\n        runners = self._parse_runners(race_data.get(\"runners\", []))\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"ust_{track_name.lower().replace(' ', '')}_{date}_{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runners(self, runners_data: List[dict]) -> List[Runner]:\n        \"\"\"Parse runner data into Runner objects.\"\"\"\n        runners = []\n\n        for runner_data in runners_data:\n            if runner_data.get(\"scratched\", False):\n                continue\n\n            odds_str = runner_data.get(\"morningLineOdds\", \"\")\n            # Handle odds like \"5\" -> \"5/1\"\n            if \"/\" not in odds_str and odds_str.isdigit():\n                odds_str = f\"{odds_str}/1\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds[self.source_name] = odds_data\n\n            runners.append(\n                Runner(\n                    number=runner_data.get(\"postPosition\", 0),\n                    name=runner_data.get(\"horse\", {}).get(\"name\", \"Unknown Horse\"),\n                    odds=odds,\n                    scratched=False,\n                )\n            )\n\n        return runners\n\n    def _parse_post_time(self, date: str, post_time: str) -> datetime:\n        \"\"\"Parse time string into timezone-aware datetime.\"\"\"\n        dt_str = f\"{date} {post_time}\"\n        naive_dt = datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        return naive_dt.replace(tzinfo=self.TIMEZONE)\n",
    "web_service/backend/adapters/oddschecker_adapter.py": "# python_service/adapters/oddschecker_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass OddscheckerAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.oddschecker.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        self._save_debug_html(index_response.text, f\"oddschecker_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Find all links to individual race pages\n        race_links = {a.attributes[\"href\"] for a in parser.css(\"a.race-time-link[href]\") if a.attributes.get(\"href\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                race = self._parse_race_page(parser, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        track_name_node = parser.css_first(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.text(strip=True)\n\n        race_time_node = parser.css_first(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = parser.css_first(\"a.race-time-link.active\")\n        race_number = 1\n        if active_link:\n            all_links = parser.css(\"a.race-time-link\")\n            try:\n                for i, link in enumerate(all_links):\n                    if link.html == active_link.html:\n                        race_number = i + 1\n                        break\n            except Exception:\n                pass\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in parser.css(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.text(strip=True)\n\n            odds_node = row.css_first(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.text(strip=True)\n\n            number_node = row.css_first(\"td.runner-number\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds_dict[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nimport json\nimport re\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\n\nclass SportingLifeAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    Hardened with __NEXT_DATA__ JSON parsing for robustness.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        SportingLife often works with HTTPX as it uses SSR with __NEXT_DATA__.\n        If HTTPX fails or returns incomplete data, SmartFetcher will fallback to Playwright.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.HTTPX,\n            enable_js=False,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            block_resources=True,\n            timeout=30\n        )\n\n    def _get_headers(self) -> dict:\n        \"\"\"Get browser-like headers for SportingLife.\"\"\"\n        return self._get_browser_headers(\n            host=\"www.sportinglife.com\",\n            referer=\"https://www.sportinglife.com/racing/racecards\",\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = \"/racing/racecards\"\n        index_response = await self.make_request(\n            \"GET\",\n            index_url,\n            headers=self._get_headers(),\n            follow_redirects=True,\n        )\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"sportinglife_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        \n        # Try to extract links from __NEXT_DATA__ first\n        links = set()\n        next_data_script = parser.css_first(\"script#__NEXT_DATA__\")\n        if next_data_script:\n            try:\n                data = json.loads(next_data_script.text())\n                # Navigate to meetings/races in the JSON structure\n                meetings = data.get(\"props\", {}).get(\"pageProps\", {}).get(\"meetings\", [])\n                for meeting in meetings:\n                    for race in meeting.get(\"races\", []):\n                        if url := race.get(\"racecard_url\"):\n                            links.add(url)\n            except (json.JSONDecodeError, KeyError, TypeError):\n                self.logger.debug(\"Failed to extract links from __NEXT_DATA__ on index page\")\n\n        # Fallback to HTML selectors if __NEXT_DATA__ failed or was missing\n        if not links:\n            links = {\n                a.attributes[\"href\"]\n                for a in parser.css('li[class^=\"MeetingSummary__LineWrapper\"] a[href*=\"/racecard/\"]')\n                if a.attributes.get(\"href\")\n            }\n\n        if not links:\n            links = {\n                a.attributes[\"href\"]\n                for a in parser.css('.meeting-summary a[href*=\"/racecard/\"]')\n                if a.attributes.get(\"href\")\n            }\n\n        if not links:\n            self.logger.warning(\"No race links found on SportingLife index page\", date=date)\n            return None\n\n        self.logger.info(f\"Found {len(links)} race links on SportingLife\")\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": [p for p in html_pages if p], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                \n                # Preferred: Parse from __NEXT_DATA__ JSON\n                race = self._parse_from_next_data(parser, race_date)\n                \n                # Fallback: Parse from HTML\n                if not race:\n                    race = self._parse_from_html(parser, race_date)\n                \n                if race:\n                    all_races.append(race)\n            except Exception as e:\n                self.logger.warning(\"Error parsing SportingLife race\", error=str(e))\n                continue\n        return all_races\n\n    def _parse_from_next_data(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        \"\"\"Extract race data from __NEXT_DATA__ JSON tag.\"\"\"\n        script = parser.css_first(\"script#__NEXT_DATA__\")\n        if not script:\n            return None\n\n        try:\n            data = json.loads(script.text())\n            race_info = data.get(\"props\", {}).get(\"pageProps\", {}).get(\"race\")\n            if not race_info:\n                return None\n\n            track_name = normalize_venue_name(race_info.get(\"meeting_name\", \"Unknown\"))\n            race_time_str = race_info.get(\"time\", \"\")\n            if not race_time_str:\n                return None\n\n            try:\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n            except ValueError:\n                return None\n\n            race_number = race_info.get(\"race_number\", 1)\n            \n            runners = []\n            for runner_data in race_info.get(\"runners\", []):\n                name = clean_text(runner_data.get(\"horse_name\", \"\"))\n                number = runner_data.get(\"saddle_cloth_number\", 0)\n                if not name:\n                    continue\n\n                # Extract odds\n                win_odds = None\n                betting = runner_data.get(\"betting\", {})\n                if betting:\n                    # Try to get live price\n                    price = betting.get(\"current_price\")\n                    if price:\n                        win_odds = parse_odds_to_decimal(price)\n                \n                # Fallback to 'odds' field if present\n                if win_odds is None:\n                    win_odds = parse_odds_to_decimal(runner_data.get(\"odds\", \"\"))\n\n                odds_data = {}\n                if odds_val := create_odds_data(self.source_name, win_odds):\n                    odds_data[self.source_name] = odds_val\n\n                runners.append(Runner(\n                    number=number,\n                    name=name,\n                    scratched=runner_data.get(\"is_non_runner\", False),\n                    odds=odds_data\n                ))\n\n            if not runners:\n                return None\n\n            return Race(\n                id=f\"sl_{track_name.replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n                venue=track_name,\n                race_number=race_number,\n                start_time=start_time,\n                runners=runners,\n                source=self.source_name,\n            )\n\n        except (json.JSONDecodeError, KeyError, TypeError) as e:\n            self.logger.debug(\"Failed to parse from __NEXT_DATA__\", error=str(e))\n            return None\n\n    def _parse_from_html(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        \"\"\"Fallback HTML parsing logic.\"\"\"\n        header = parser.css_first('h1[class*=\"RacingRacecardHeader__Title\"]')\n        if not header:\n            return None\n\n        header_text = clean_text(header.text())\n        parts = header_text.split()\n        if not parts:\n            return None\n            \n        race_time_str = parts[0]\n        track_name = normalize_venue_name(\" \".join(parts[1:]))\n\n        try:\n            start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        except ValueError:\n            return None\n\n        race_number = 1\n        nav_links = parser.css('a[class*=\"SubNavigation__Link\"]')\n        active_link = parser.css_first('a[class*=\"SubNavigation__Link--active\"]')\n        if active_link and nav_links:\n            try:\n                for idx, link in enumerate(nav_links):\n                    if link.text().strip() == active_link.text().strip():\n                        race_number = idx + 1\n                        break\n            except Exception:\n                pass\n\n        runners = [r for row in parser.css('div[class*=\"RunnerCard\"]') if (r := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"sl_{track_name.replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Node) -> Optional[Runner]:\n        \"\"\"Parse a single runner row from HTML.\"\"\"\n        try:\n            name_node = row.css_first('a[href*=\"/racing/profiles/horse/\"]')\n            if not name_node:\n                return None\n            name = clean_text(name_node.text()).splitlines()[0].strip()\n\n            num_node = row.css_first('span[class*=\"SaddleCloth__Number\"]')\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.css_first('span[class*=\"Odds__Price\"]')\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = {}\n            if odds_val := create_odds_data(self.source_name, win_odds):\n                odds_data[self.source_name] = odds_val\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            return None\n",
    "web_service/backend/adapters/stubs/racingtv_adapter.py": "# python_service/adapters/stubs/racingtv_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass RacingTVAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for racingtv.com.\"\"\"\n\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n",
    "web_service/backend/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for TVG API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not getattr(config, \"TVG_API_KEY\", None):\n            raise AdapterConfigError(self.SOURCE_NAME, \"TVG_API_KEY is not configured.\")\n        self.api_key = config.TVG_API_KEY\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw meetings data from the TVG API.\"\"\"\n        headers = {\"X-API-Key\": self.api_key}\n        response = await self.make_request(\"GET\", f\"meetings?date={date}\", headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in TVG response or invalid format.\")\n            return []\n\n        all_races = []\n        for meeting in raw_data.get(\"meetings\", []):\n            venue = meeting.get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, venue):\n                        all_races.append(race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.error(\n                        \"Error parsing TVG race\",\n                        race_id=race_data.get(\"id\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], venue: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"id\")\n        race_number = race_data.get(\"number\")\n        start_time_str = race_data.get(\"startTime\")\n\n        if not all([race_id, race_number, start_time_str]):\n            return None\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            name = runner_data.get(\"name\")\n            number = runner_data.get(\"number\")\n            if not all([name, number]):\n                continue\n\n            odds = {}\n            # Placeholder for TVG odds parsing\n\n            runners.append(\n                Runner(\n                    name=name,\n                    number=number,\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds,\n                )\n            )\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            start_time = datetime.now()\n\n        return Race(\n            id=f\"tvg_{race_id}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    valid_odds = []\n    for source_data in runner.odds.values():\n        # Handle both dict and primitive formats\n        if isinstance(source_data, dict):\n            win = source_data.get('win')\n        elif hasattr(source_data, 'win'):\n            win = source_data.win\n        else:\n            win = source_data\n\n        if win is not None and 0 < win < 999:\n            valid_odds.append(win)\n\n    return min(valid_odds) if valid_odds else None\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(\n        self,\n        max_field_size: int = 14,\n        min_favorite_odds: float = 0.01,\n        min_second_favorite_odds: float = 0.01,\n    ):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        qualified_races = []\n        for race in races:\n            if not self.is_race_qualified(race):\n                continue\n            score = self._evaluate_race(race)\n            if score > 0:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\n            \"Universal scoring complete\",\n            total_races_scored=len(qualified_races),\n            criteria=criteria,\n        )\n\n        for race in qualified_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": qualified_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply hard filters before scoring ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            return 0.0\n\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass TinyFieldTrifectaAnalyzer(TrifectaAnalyzer):\n    \"\"\"A specialized TrifectaAnalyzer that only considers races with 6 or fewer runners.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Override the max_field_size to 6 for \"tiny field\" analysis\n        # Set low odds thresholds to \"let them through\" as per user request\n        super().__init__(max_field_size=6, min_favorite_odds=0.01, min_second_favorite_odds=0.01, **kwargs)\n\n    @property\n    def name(self) -> str:\n        return \"tiny_field_trifecta_analyzer\"\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        self.register_analyzer(\"tiny_field_trifecta\", TinyFieldTrifectaAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier(\"Fortuna\") if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "web_service/backend/core/exceptions.py": "# python_service/core/exceptions.py\n\"\"\"\nCustom, application-specific exceptions for the Fortuna Faucet service.\n\nThis module defines a hierarchy of exception classes to provide standardized\nerror handling, particularly for the data adapter layer. Using these specific\nexceptions instead of generic ones allows for more precise error handling and\nclearer logging throughout the application.\n\"\"\"\n\n\nclass FortunaException(Exception):\n    \"\"\"Base class for all custom exceptions in this application.\"\"\"\n\n    pass\n\n\nfrom enum import Enum\n\nclass ErrorCategory(Enum):\n    BOT_DETECTION = \"bot_detection\"\n    NETWORK = \"network\"\n    STRUCTURE_CHANGE = \"structure_change\"\n    TIMEOUT = \"timeout\"\n    AUTHENTICATION = \"authentication\"\n    CONFIGURATION = \"configuration\"\n    UNKNOWN = \"unknown\"\n\nclass AdapterError(FortunaException):\n    \"\"\"Base class for all adapter-related errors.\"\"\"\n\n    def __init__(self, adapter_name: str, message: str):\n        self.adapter_name = adapter_name\n        super().__init__(f\"[{adapter_name}] {message}\")\n\n\nclass AdapterRequestError(AdapterError):\n    \"\"\"Raised for general network or request-related issues.\"\"\"\n\n    pass\n\n\nclass AdapterHttpError(AdapterRequestError):\n    \"\"\"Raised for unsuccessful HTTP responses (e.g., 4xx or 5xx status codes).\"\"\"\n\n    def __init__(self, adapter_name: str, status_code: int, url: str):\n        self.status_code = status_code\n        self.url = url\n        message = f\"Received HTTP {status_code} from {url}\"\n        super().__init__(adapter_name, message)\n\n\nclass AdapterAuthError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 401/403 errors, indicating an auth failure.\"\"\"\n\n    pass\n\n\nclass AdapterRateLimitError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 429 errors, indicating a rate limit has been hit.\"\"\"\n\n    pass\n\n\nclass AdapterTimeoutError(AdapterRequestError):\n    \"\"\"Raised when a request to an external API times out.\"\"\"\n\n    pass\n\n\nclass AdapterConnectionError(AdapterRequestError):\n    \"\"\"Raised for DNS lookup failures or refused connections.\"\"\"\n\n    pass\n\n\nclass AdapterConfigError(AdapterError):\n    \"\"\"Raised when an adapter is missing necessary configuration (e.g., an API key).\"\"\"\n\n    pass\n\n\nclass AuthenticationError(FortunaException):\n    \"\"\"Raised when authentication fails.\"\"\"\n\n    def __init__(self, source: str, message: str):\n        self.source = source\n        super().__init__(f\"[{source}] Authentication failed: {message}\")\n\n\nclass AdapterParsingError(AdapterError):\n    \"\"\"Raised when an adapter fails to parse the response from an API.\"\"\"\n\n    pass\n",
    "web_service/backend/fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport structlog\n\nfrom .analyzer import AnalyzerEngine\nfrom .config import get_settings\nfrom .engine import OddsEngine\nfrom .etl import run_etl_for_yesterday\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = OddsEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n        try:\n            background_tasks = set()  # Create a dummy set for background tasks\n            aggregated_data = await self.odds_engine.fetch_all_odds(today_str, background_tasks)\n            all_races = aggregated_data.get(\"races\", [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer(\"trifecta\")\n            qualified_races_result = analyzer.qualify_races(all_races)\n            qualified_races_list = qualified_races_result.get(\"races\", [])\n            log.info(\n                \"Watchman: Initial target acquisition and ranking complete\",\n                target_count=len(qualified_races_list),\n            )\n\n            # Log the top targets for better observability\n            for race in qualified_races_list[:5]:\n                log.info(\n                    \"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat(),\n                )\n            return qualified_races_list\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        # active_targets = list(targets)\n\n        # from python_service.adapters.betfair_adapter import BetfairAdapter\n        # async with LiveOddsMonitor(betfair_adapter=BetfairAdapter(config=self.settings)) as live_monitor:\n        #     async with httpx.AsyncClient() as client:\n        #         while active_targets:\n        #             now = datetime.now(timezone.utc)\n\n        #             # Find races that are within the 5-minute monitoring window\n        #             races_to_monitor = [\n        #                 r\n        #                 for r in active_targets\n        #                 if r.start_time.replace(tzinfo=timezone.utc) > now\n        #                 and r.start_time.replace(tzinfo=timezone.utc)\n        #                 < now + timedelta(minutes=5)\n        #             ]\n\n        #             if races_to_monitor:\n        #                 for race in races_to_monitor:\n        #                     log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n        #                         race_id=race.id,\n        #                         venue=race.venue,\n        #                         score=race.qualification_score\n        #                     )\n        #                     updated_race = await live_monitor.monitor_race(race, client)\n        #                     log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n        #                     # Remove from target list to prevent re-monitoring\n        #                     active_targets = [t for t in active_targets if t.id != race.id]\n\n        #             if not active_targets:\n        #                 break # Exit loop if all targets are processed\n\n        #             await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        try:\n            initial_targets = await self.get_initial_targets()\n            if initial_targets:\n                await self.run_tactical_monitoring(initial_targets)\n            else:\n                log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n        finally:\n            await self.odds_engine.close()\n\n        # Run ETL for yesterday's data after all other operations are complete\n        try:\n            log.info(\"Starting daily ETL process for Scribe's Archives...\")\n            run_etl_for_yesterday()\n            log.info(\"Daily ETL process completed successfully.\")\n        except Exception:\n            log.error(\"Daily ETL process failed.\", exc_info=True)\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\n\nasync def main():\n    from .logging_config import configure_logging\n\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "web_service/backend/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    # Keep the processor chain simple for maximum reliability in bundled executables.\n    # More complex processors like StackInfoRenderer can cause issues in\n    # constrained environments.\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "web_service/backend/models.py": "# python_service/models.py\n\nfrom datetime import datetime, date\nfrom decimal import Decimal\nfrom typing import Annotated, Any, Callable, Dict, List, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, WrapSerializer\n\n\ndef decimal_serializer(value: Decimal, handler: Callable[[Decimal], Any]) -> Any:\n    \"\"\"Custom serializer for Decimal to float conversion.\"\"\"\n    return float(value)\n\n\nJsonDecimal = Annotated[Decimal, WrapSerializer(decimal_serializer, when_used=\"json\")]\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[JsonDecimal] = None\n    place: Optional[JsonDecimal] = None\n    show: Optional[JsonDecimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n    is_error_placeholder: bool = Field(False, alias=\"isErrorPlaceholder\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    metadata: Dict[str, Any] = {}\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AdapterError(FortunaBaseModel):\n    adapter_name: str = Field(..., alias=\"adapterName\")\n    error_message: str = Field(..., alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    race_date: Optional[date] = Field(None, alias=\"date\")\n    races: List[Race]\n    errors: List[AdapterError]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n    metadata: Dict[str, Any] = {}\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n\n\nclass ManualParseRequest(FortunaBaseModel):\n    adapter_name: str\n    html_content: str = Field(..., max_length=5_000_000)  # ~5MB limit\n",
    "web_service/backend/requirements-dev.txt": "#\n# Development & Build-Time Dependencies\n# This file should be used for setting up a development or CI/CD environment.\n#\n\n-r requirements.txt\n\n# --- Build Tools ---\npip-tools\n\n# --- Testing Tools ---\npytest\npytest-asyncio\nfakeredis\nrespx\n\n# --- Linting & Auditing ---\nblack\nruff\npip-audit\nsetuptools<81\n",
    "web_service/backend/service_entry.py": "import win32serviceutil\nimport win32service\nimport win32event\nimport servicemanager\nimport socket\nimport sys\nimport os\nimport uvicorn\nimport multiprocessing\nimport threading\nfrom pathlib import Path\n\n# --- Resilient Import Block ---\n# This block is designed to robustly locate the `main` module and its `app` object,\n# whether running from source, as a PyInstaller bundle, or as a Windows Service.\n\nimport asyncio\n\ndef _bootstrap_path():\n    \"\"\"\n    Ensures the application's root directories are on the Python path.\n    This is critical for PyInstaller's frozen executables to find modules.\n    \"\"\"\n    if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n        # We are running in a PyInstaller bundle.\n        # The `_MEIPASS` directory is the root of our bundled files.\n        # In our `--onedir` build, this is where `main.py`'s content is.\n        sys.path.insert(0, sys._MEIPASS)\n    else:\n        # We are running from source.\n        # The entry point is in `web_service/backend`, so we need to add the project root.\n        project_root = str(Path(__file__).parent.parent.parent)\n        sys.path.insert(0, project_root)\n\n_bootstrap_path()\n\ntry:\n    # This is the most direct import path and should work when the CWD\n    # is correctly set to the directory containing the executable.\n    print(f\"[service_entry] Attempting direct import of 'main:app'...\")\n    from main import app\n    print(f\"[service_entry] Direct import successful.\")\nexcept (ImportError, ModuleNotFoundError) as e:\n    print(f\"[service_entry] Direct import failed: {e}. Attempting namespace import...\")\n    try:\n        # This is a fallback for environments where the `web_service` namespace is preserved.\n        from web_service.backend.main import app\n        print(f\"[service_entry] Namespace import successful.\")\n    except (ImportError, ModuleNotFoundError) as e2:\n        print(f\"[service_entry] All import attempts failed: {e2}. Cannot start service.\")\n        sys.exit(1) # Exit if the app cannot be imported, to prevent service start failure.\n\nclass FortunaSvc(win32serviceutil.ServiceFramework):\n    _svc_name_ = 'FortunaWebService'\n    _svc_display_name_ = 'Fortuna Faucet Backend Service'\n    _svc_description_ = 'Data aggregation and analysis engine.'\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.server = None\n        self.server_thread = None\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        win32event.SetEvent(self.hWaitStop)\n        if self.server:\n            self.server.should_exit = True\n\n    def SvcDoRun(self):\n        # When running as a Windows Service, the default working directory is System32,\n        # which can cause issues with relative paths. This fix changes the working\n        # directory to the location of the executable.\n        if getattr(sys, 'frozen', False):\n            os.chdir(os.path.dirname(sys.executable))\n            # \u2622\ufe0f CRITICAL FIX for Windows Services running asyncio \u2622\ufe0f\n            # This policy prevents the notorious \"NotImplementedError\" when uvicorn\n            # tries to create a subprocess in a non-interactive service environment.\n            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n        servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE,\n                              servicemanager.PYS_SERVICE_STARTED,\n                              (self._svc_name_, ''))\n\n        config = uvicorn.Config(app, host='127.0.0.1', port=8102, log_config=None, reload=False)\n        self.server = uvicorn.Server(config)\n\n        # Run the server in a separate thread\n        self.server_thread = threading.Thread(target=self.server.run)\n        self.server_thread.start()\n\n        # Wait for the stop event\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n        # Wait for the server thread to finish\n        self.server_thread.join()\n\nif __name__ == '__main__':\n    multiprocessing.freeze_support()\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaSvc)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaSvc)\n",
    "web_service/backend/utils/odds.py": "\"\"\"Odds parsing utilities with comprehensive format support.\"\"\"\n\nimport re\nfrom typing import Optional\nfrom decimal import Decimal, InvalidOperation\n\n\ndef parse_odds_to_decimal(odds_str: str) -> Optional[float]:\n    \"\"\"\n    Parse various odds formats to decimal odds.\n\n    Supports:\n    - Fractional: \"5/2\", \"9-2\", \"5-1\"\n    - Decimal: \"3.50\", \"3,50\" (European)\n    - American: \"+250\", \"-150\"\n    - Even/Evens: \"EVN\", \"EVEN\", \"evs\"\n    - Morning line: \"5-2 ML\"\n\n    Returns:\n        Decimal odds as float, or None if unparseable\n    \"\"\"\n    if not odds_str:\n        return None\n\n    # Clean input\n    odds_str = odds_str.strip().upper()\n\n    # Remove common suffixes\n    odds_str = re.sub(r'\\s*(ML|MTP|AM|PM)$', '', odds_str)\n\n    # Handle even money\n    if odds_str in ('EVN', 'EVEN', 'EVS', 'EVENS'):\n        return 2.0\n\n    # Handle scratched/invalid\n    if odds_str in ('SCR', 'SCRATCHED', '--', 'N/A', ''):\n        return None\n\n    try:\n        # Try fractional odds (5/2 or 5-2)\n        frac_match = re.match(r'^(\\d+)[/\\-](\\d+)$', odds_str)\n        if frac_match:\n            num = int(frac_match.group(1))\n            den = int(frac_match.group(2))\n            if den > 0:\n                return round((num / den) + 1.0, 2)\n\n        # Try American odds (+250 or -150)\n        american_match = re.match(r'^([+-])(\\d+)$', odds_str)\n        if american_match:\n            sign = american_match.group(1)\n            value = int(american_match.group(2))\n            if sign == '+':\n                return round((value / 100) + 1.0, 2)\n            else:\n                return round((100 / value) + 1.0, 2)\n\n        # Try decimal odds (already in correct format)\n        decimal_str = odds_str.replace(',', '.')\n        decimal_match = re.match(r'^(\\d+\\.?\\d*)$', decimal_str)\n        if decimal_match:\n            value = float(decimal_match.group(1))\n            if value >= 1.0:\n                return round(value, 2)\n\n    except (ValueError, ZeroDivisionError, InvalidOperation):\n        pass\n\n    return None\n\n\ndef format_odds_display(decimal_odds: float, style: str = 'fractional') -> str:\n    \"\"\"\n    Format decimal odds for display.\n\n    Args:\n        decimal_odds: Odds in decimal format\n        style: 'fractional', 'american', or 'decimal'\n\n    Returns:\n        Formatted odds string\n    \"\"\"\n    if not decimal_odds or decimal_odds < 1.01:\n        return \"N/A\"\n\n    if style == 'decimal':\n        return f\"{decimal_odds:.2f}\"\n\n    elif style == 'american':\n        if decimal_odds >= 2.0:\n            american = int((decimal_odds - 1) * 100)\n            return f\"+{american}\"\n        else:\n            american = int(-100 / (decimal_odds - 1))\n            return str(american)\n\n    else:  # fractional\n        # Common fractional odds lookup\n        profit = decimal_odds - 1\n\n        # Check common fractions\n        common_fractions = [\n            (0.5, \"1/2\"), (1.0, \"1/1\"), (1.5, \"3/2\"), (2.0, \"2/1\"),\n            (2.5, \"5/2\"), (3.0, \"3/1\"), (4.0, \"4/1\"), (5.0, \"5/1\"),\n            (6.0, \"6/1\"), (8.0, \"8/1\"), (10.0, \"10/1\"), (12.0, \"12/1\"),\n            (14.0, \"14/1\"), (16.0, \"16/1\"), (20.0, \"20/1\"), (25.0, \"25/1\"),\n            (33.0, \"33/1\"), (50.0, \"50/1\"), (100.0, \"100/1\"),\n        ]\n\n        for value, display in common_fractions:\n            if abs(profit - value) < 0.05:\n                return display\n\n        # Approximate to nearest reasonable fraction\n        if profit < 1:\n            return f\"{int(profit * 2)}/2\"\n        else:\n            return f\"{int(profit)}/1\"\n",
    "web_service/frontend/app/Providers.tsx": "// web_platform/frontend/app/Providers.tsx\n'use client';\n\nimport { QueryClientProvider } from '@tanstack/react-query';\nimport { queryClient } from './lib/queryClient';\nimport React from 'react';\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>\n  );\n}\n",
    "web_service/frontend/app/components/LiveRaceDashboard.tsx": "// web_platform/frontend/src/components/LiveRaceDashboard.tsx\n'use client';\n\nimport React, { useState, useEffect, useCallback } from 'react';\nimport { useQuery, useQueryClient } from '@tanstack/react-query';\nimport { RaceFilters } from './RaceFilters';\nimport { RaceCard } from './RaceCard';\nimport { RaceCardSkeleton } from './RaceCardSkeleton';\nimport { EmptyState } from './EmptyState';\nimport { ErrorDisplay } from './ErrorDisplay';\nimport { Race, SourceInfo, AdapterError, AggregatedRacesResponse } from '../types/racing';\nimport { useWebSocket } from '../hooks/useWebSocket';\nimport { StatusDetailModal } from './StatusDetailModal';\nimport ManualOverridePanel from './ManualOverridePanel';\nimport { LiveModeToggle } from './LiveModeToggle';\nimport { AdapterStatusPanel } from './AdapterStatusPanel';\n\n// Type for the backend process status received from Electron main\ntype BackendState = 'starting' | 'running' | 'error' | 'stopped';\ninterface BackendStatus {\n  state: BackendState;\n  logs: string[];\n}\n\ninterface RaceFilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nconst fetchAdapterStatuses = async (apiKey: string | null): Promise<SourceInfo[]> => {\n  if (!apiKey) {\n    throw new Error('API key not available.');\n  }\n  const response = await fetch(`/api/adapters/status`, {\n    headers: { 'X-API-Key': apiKey, 'Content-Type': 'application/json' },\n  });\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n  return response.json();\n};\n\nconst fetchQualifiedRaces = async (apiKey: string | null, params: RaceFilterParams): Promise<AggregatedRacesResponse> => {\n  if (!apiKey) {\n    throw new Error('API key not available');\n  }\n  // In web service mode, API calls are relative to the current origin.\n  const response = await fetch(`/api/races`, {\n    headers: { 'X-API-Key': apiKey },\n  });\n\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n\n  return response.json();\n};\n\n\nconst BackendErrorPanel = ({ logs }: { logs: string[] }) => (\n  <div className=\"bg-slate-800 p-6 rounded-lg border border-red-500/50 text-white\">\n    <h2 className=\"text-2xl font-bold text-red-400 mb-4\">Backend Service Error</h2>\n    <p className=\"text-slate-400 mb-4\">The backend data service failed to start or has crashed. Below are the most recent diagnostic messages.</p>\n    <div className=\"bg-black p-4 rounded-md font-mono text-sm text-slate-300 h-64 overflow-y-auto mb-4\">\n      {logs.map((log, index) => (\n        <p key={index} className=\"whitespace-pre-wrap\">{`> ${log}`}</p>\n      ))}\n    </div>\n    <p className=\"text-sm text-slate-500 text-center mt-4\">Please check the server logs for more information.</p>\n  </div>\n);\n\n// New Sub-Component to display an error from a specific adapter\nconst ErrorCard = ({ source, message }: { source: string; message: string }) => (\n  <div className=\"bg-slate-800 rounded-lg p-4 border border-red-500/50 flex flex-col justify-between\">\n    <div>\n      <h3 className=\"font-bold text-red-400 text-lg\">{source} Failed</h3>\n      <p className=\"text-slate-400 text-sm mt-2\">{message}</p>\n    </div>\n    <div className=\"mt-4 text-xs text-slate-500\">\n      <p>This adapter failed to fetch data. This is not a critical error; other adapters may provide the necessary data.</p>\n    </div>\n  </div>\n);\n\n// New Sub-Component to render the grid of races or error cards\nconst RaceGrid = ({ races }: { races: Race[] }) => (\n  <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n    {races.map(race =>\n      race.isErrorPlaceholder ? (\n        <ErrorCard key={race.id} source={race.venue} message={race.errorMessage || 'An unknown error occurred.'} />\n      ) : (\n        <RaceCard key={race.id} race={race} />\n      )\n    )}\n  </div>\n);\n\n// In a pure web service, the frontend cannot know the backend's process status.\n// We assume it's always running if the frontend is served.\nconst useBackendStatus = (): BackendStatus => {\n  return { state: 'running', logs: ['Running in web service mode. Backend status is assumed to be active.'] };\n};\n\nexport const LiveRaceDashboard = React.memo(() => {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [adapterErrors, setAdapterErrors] = useState<AdapterError[]>([]);\n  const backendStatus = useBackendStatus();\n  // In a web service, the API key might be hardcoded, come from a meta tag, or an auth flow.\n  // For this version, we'll rely on the backend not requiring one from the same origin or use a known key.\n  const [apiKey, setApiKey] = useState<string | null>(\"a_secure_test_api_key_that_is_long_enough\");\n  const queryClient = useQueryClient();\n\n  const [params, setParams] = useState<RaceFilterParams>({\n    maxFieldSize: 10,\n    minFavoriteOdds: 2.5,\n    minSecondFavoriteOdds: 4.0,\n  });\n\n  const {\n    data,\n    status: connectionStatus,\n    error: errorDetails,\n    refetch,\n  } = useQuery({\n    queryKey: ['aggregatedRaces', apiKey],\n    queryFn: () => fetchQualifiedRaces(apiKey, params),\n    enabled: backendStatus.state === 'running' && !!apiKey,\n    refetchOnWindowFocus: true,\n  });\n\n  // Update state when data is successfully fetched\n  useEffect(() => {\n    if (data) {\n      setRaces(data.races || []);\n      setAdapterErrors(data.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [data]);\n\n  const { data: liveData, isConnected: isLiveConnected } = useWebSocket<AggregatedRacesResponse>(\n    '/ws/live-updates',\n    { apiKey } // Port is not needed for same-origin WebSockets\n  );\n\n  // Effect to update state when new live data arrives\n  useEffect(() => {\n    if (liveData) {\n      console.log('Received live data update:', liveData);\n      // Update the query cache and local state with the new data\n      queryClient.setQueryData(['aggregatedRaces', apiKey], liveData);\n      setRaces(liveData.races || []);\n      setAdapterErrors(liveData.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [liveData, queryClient, apiKey]);\n\n  const [lastUpdate, setLastUpdate] = useState<Date | null>(null);\n  const [isModalOpen, setIsModalOpen] = useState(false);\n\n\n  const handleParamsChange = useCallback((newParams: RaceFilterParams) => {\n    setParams(newParams);\n  }, []);\n\n  const handleParseSuccess = (adapterName: string, parsedRaces: Race[]) => {\n    queryClient.setQueryData(['qualifiedRaces', apiKey, params], (oldData: { races: Race[], source_info: SourceInfo[] } | undefined) => {\n      if (!oldData) return { races: parsedRaces, source_info: [] };\n\n      // 1. Remove the placeholder error card for this adapter\n      const otherRaces = oldData.races.filter(race => race.source !== adapterName);\n\n      // 2. Merge the new races in\n      const updatedRaces = [...otherRaces, ...parsedRaces].sort(\n        (a, b) => new Date(a.start_time).getTime() - new Date(b.start_time).getTime()\n      );\n\n      // 3. Update source_info to remove the failed source\n      const updatedSourceInfo = oldData.source_info.filter(s => s.name !== adapterName);\n\n      return { races: updatedRaces, source_info: updatedSourceInfo };\n    });\n  };\n\n  const renderContent = () => {\n    // Priority 1: Backend process has failed.\n    if (backendStatus.state === 'error') {\n      return <BackendErrorPanel logs={backendStatus.logs} />;\n    }\n\n    if (backendStatus.state === 'stopped') {\n        return <EmptyState\n            title=\"Backend Service Stopped\"\n            message=\"The backend data service is not running. Please start it to see live race data.\"\n        />;\n    }\n\n    // Priority 2: Backend is starting or initial fetch is happening.\n    const isLoading = backendStatus.state === 'starting' || (connectionStatus === 'pending' && !data);\n    if (isLoading) {\n        return (\n            <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n                {[...Array(8)].map((_, i) => <RaceCardSkeleton key={i} />)}\n            </div>\n        );\n    }\n\n    // Priority 3: API connection is offline.\n    if (connectionStatus === 'error') {\n      try {\n        const errorInfo = JSON.parse((errorDetails as Error).message);\n        return <ErrorDisplay error={errorInfo.error} />;\n      } catch (e) {\n        return <EmptyState\n            title=\"API Connection Offline\"\n            message={(errorDetails as Error)?.message || \"The backend is running, but the dashboard could not connect to its API.\"}\n            actionButton={<button onClick={() => refetch()} className=\"mt-4 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700\">Retry Connection</button>}\n        />;\n      }\n    }\n\n    // Priority 4: No races found after a successful fetch.\n    if (!races || races.length === 0) {\n      return <EmptyState\n          title=\"No Races Found\"\n          message=\"No races matched the specified criteria for the selected date. Please try different filters.\"\n      />;\n    }\n\n    // Priority 5: Display the races (and any error placeholders).\n    return <RaceGrid races={races} />;\n  };\n\n  const getStatusIndicator = () => {\n    if (backendStatus.state === 'error') {\n      return { color: 'bg-red-500', text: 'Backend Error' };\n    }\n    if (backendStatus.state === 'stopped') {\n        return { color: 'bg-gray-500', text: 'Stopped' };\n    }\n    if (backendStatus.state === 'starting') {\n      return { color: 'bg-yellow-500', text: 'Backend Starting...' };\n    }\n    if (isLiveConnected) {\n      return { color: 'bg-cyan-500', text: 'Live' };\n    }\n    return { color: 'bg-yellow-500', text: 'Connecting...' };\n  };\n\n  const { color: statusColor, text: statusText } = getStatusIndicator();\n\n  return (\n    <>\n      <div className=\"space-y-6\">\n        <div className=\"flex justify-between items-start\">\n            <div className=\"text-left space-y-2\">\n                <h1 className=\"text-4xl font-bold text-white\">\ud83c\udfc7 Fortuna Faucet</h1>\n                <p className=\"text-slate-400\">\n                Last updated: {lastUpdate ? lastUpdate.toLocaleTimeString() : 'N/A'}\n                </p>\n            </div>\n            <div className=\"flex items-center gap-4\">\n                <button\n                    onClick={() => (connectionStatus === 'error' || backendStatus.state === 'error') && setIsModalOpen(true)}\n                    className={`flex items-center gap-2 px-3 py-1.5 rounded-full text-sm font-medium text-white ${statusColor} ${(connectionStatus === 'error' || backendStatus.state === 'error') ? 'cursor-pointer hover:opacity-80' : 'cursor-default'}`}\n                    data-testid=\"status-indicator\"\n                >\n                    <span className={`w-2.5 h-2.5 rounded-full bg-white ${isLiveConnected ? 'animate-pulse' : ''}`}></span>\n                    {statusText}\n                </button>\n            </div>\n        </div>\n\n        <RaceFilters onParamsChange={handleParamsChange} isLoading={connectionStatus === 'pending'} refetch={refetch} />\n\n        {adapterErrors.map(error => (\n          <ManualOverridePanel\n            key={error.adapterName}\n            adapterName={error.adapterName}\n            attemptedUrl={error.attemptedUrl || 'URL not available'}\n            apiKey={apiKey}\n            onParseSuccess={handleParseSuccess}\n          />\n        ))}\n\n        {renderContent()}\n      </div>\n\n      <StatusDetailModal\n        isOpen={isModalOpen}\n        onClose={() => setIsModalOpen(false)}\n        status={{ title: 'Connection Error', details: (errorDetails as Error)?.message || 'No specific error message was provided.' }}\n      />\n    </>\n  );\n});\n",
    "web_service/frontend/app/components/LiveRaceDashboardNoSSR.tsx": "// web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx\nimport dynamic from 'next/dynamic';\n\nconst LiveRaceDashboardNoSSR = dynamic(\n  () => import('./LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  { ssr: false }\n);\n\nexport default LiveRaceDashboardNoSSR;\n",
    "web_service/frontend/app/components/ScoreBadge.tsx": "'use client';\nimport React from 'react';\n\nconst getScoreStyling = (score: number) => {\n  if (score >= 90) return { bg: 'bg-yellow-400/10', text: 'text-yellow-300', border: 'border-yellow-400' };\n  if (score >= 80) return { bg: 'bg-orange-500/10', text: 'text-orange-400', border: 'border-orange-500' };\n  return { bg: 'bg-sky-500/10', text: 'text-sky-400', border: 'border-sky-500' };\n};\n\nexport const ScoreBadge: React.FC<{ score: number }> = ({ score }) => {\n  const { bg, text } = getScoreStyling(score);\n  return (\n    <div className={`text-right ${text}`}>\n      <p className=\"text-3xl font-bold\">{score.toFixed(1)}</p>\n      <p className=\"text-xs font-medium tracking-wider uppercase\\\">Score</p>\n    </div>\n  );\n};",
    "web_service/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_service/frontend/app/page.tsx": "'use client';\nimport dynamic from 'next/dynamic';\nimport React from 'react';\nimport { Tabs } from './components/Tabs';\nimport { SettingsPage } from './components/SettingsPage';\n\nconst LiveRaceDashboard = dynamic(\n  () => import('./components/LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  {\n    ssr: false,\n    loading: () => <p className=\"text-center text-xl mt-8\">Loading Dashboard...</p>\n  }\n);\n\nexport default function Home() {\n  const tabs = [\n    {\n      label: 'Dashboard',\n      content: <LiveRaceDashboard />,\n    },\n    {\n      label: 'Settings',\n      content: <SettingsPage />,\n    },\n  ];\n\n  return (\n    <main className=\"min-h-screen bg-gradient-to-br from-slate-900 via-purple-900 to-slate-900 p-8\">\n      <div className=\"max-w-7xl mx-auto space-y-8\">\n        <h1 className=\"text-4xl font-bold text-white\" data-testid=\"main-heading\">Fortuna Faucet</h1>\n        <Tabs tabs={tabs} />\n      </div>\n    </main>\n  );\n}\n",
    "web_service/frontend/next.config.js": "/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  output: 'export',\n  distDir: 'build',\n  images: { unoptimized: true },\n  trailingSlash: true,\n}\nmodule.exports = nextConfig\n",
    "wix/WixUI_CustomProgress.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\">\n  <Fragment>\n    <UI>\n      <!-- Override the default InstallProgress dialog -->\n      <Dialog Id=\"InstallProgressDlg\" Width=\"370\" Height=\"270\" Title=\"Fortuna Faucet Installation\" Modeless=\"yes\">\n        <Control Id=\"Title\" Type=\"Title\" X=\"20\" Y=\"6\" Width=\"330\" Height=\"18\" Text=\"Installation Progress\" />\n        <Control Id=\"BannerBitmap\" Type=\"Bitmap\" X=\"0\" Y=\"0\" Width=\"370\" Height=\"44\" TabSkip=\"no\" Text=\"WixUI_Bmp_Banner\" />\n        <Control Id=\"Back\" Type=\"PushButton\" X=\"180\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Back\" Disabled=\"yes\" />\n        <Control Id=\"Next\" Type=\"PushButton\" X=\"236\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Next\" Disabled=\"yes\" />\n        <Control Id=\"Cancel\" Type=\"PushButton\" X=\"304\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"Cancel\" />\n\n        <Control Id=\"ActionText\" Type=\"Text\" X=\"70\" Y=\"80\" Width=\"280\" Height=\"20\" TabSkip=\"no\">\n          <Subscribe Event=\"ActionText\" Attribute=\"Text\" />\n        </Control>\n        <Control Id=\"Description\" Type=\"Text\" X=\"35\" Y=\"55\" Width=\"300\" Height=\"20\" Text=\"Please wait while the installer copies files.\" />\n\n        <!-- This is the new control to display the current filename -->\n        <Control Id=\"CurrentFileText\" Type=\"Text\" X=\"70\" Y=\"100\" Width=\"280\" Height=\"20\">\n            <Subscribe Event=\"SetProgress\" Attribute=\"Text\" />\n        </Control>\n\n        <Control Id=\"ProgressBar\" Type=\"ProgressBar\" X=\"35\" Y=\"120\" Width=\"300\" Height=\"10\" ProgressBlocks=\"yes\" Text=\"Progress\">\n          <Subscribe Event=\"SetProgress\" Attribute=\"Progress\" />\n        </Control>\n      </Dialog>\n\n      <!-- The Publish element must be a child of UI, not Dialog -->\n      <Publish Dialog=\"InstallProgressDlg\" Control=\"Cancel\" Event=\"SpawnDialog\" Value=\"CancelDlg\">1</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}