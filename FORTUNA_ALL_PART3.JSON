{
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "pg_schemas/quarantine_races.sql": "CREATE TABLE IF NOT EXISTS quarantine_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(100),\n    track_name VARCHAR(100),\n    race_number INT,\n    post_time TIMESTAMP WITH TIME ZONE,\n    source VARCHAR(50),\n    raw_data_json JSONB, -- Store the original raw data for inspection\n    quarantine_reason TEXT, -- Reason for failing validation\n    collection_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);",
    "pg_schemas/quarantined_races.sql": "-- Schema for storing race data that fails validation\nCREATE TABLE IF NOT EXISTS quarantined_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(255),\n    source VARCHAR(50),\n    payload JSONB NOT NULL,\n    reason VARCHAR(255) NOT NULL,\n    quarantined_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "tests/adapters/test_gbgb_api_adapter.py": "# tests/adapters/test_gbgb_api_adapter.py\n\nimport pytest\nfrom datetime import date\nfrom decimal import Decimal\nfrom unittest.mock import AsyncMock\n\nfrom python_service.config import get_settings\nfrom python_service.adapters.gbgb_api_adapter import GbgbApiAdapter\n\n@pytest.fixture\ndef gbgb_adapter():\n    \"\"\"Returns a GbgbApiAdapter instance for testing.\"\"\"\n    return GbgbApiAdapter(config=get_settings())\n\n@pytest.mark.asyncio\nasync def test_get_gbgb_races_successfully(gbgb_adapter):\n    \"\"\"\n    SPEC: The GbgbApiAdapter should correctly parse a standard API response,\n    creating Race and Runner objects with the correct data, including fractional odds.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime('%Y-%m-%d')\n    mock_api_response = [\n        {\n            \"trackName\": \"Towcester\",\n            \"races\": [\n                {\n                    \"raceId\": 12345,\n                    \"raceNumber\": 1,\n                    \"raceTime\": \"2025-10-09T18:00:00Z\",\n                    \"raceTitle\": \"The October Sprint\",\n                    \"raceDistance\": 500,\n                    \"traps\": [\n                        {\"trapNumber\": 1, \"dogName\": \"Rapid Rover\", \"sp\": \"5/2\"},\n                        {\"trapNumber\": 2, \"dogName\": \"Speedy Sue\", \"sp\": \"EVS\"},\n                        {\"trapNumber\": 3, \"dogName\": \"Lazy Larry\", \"sp\": \"10/1\"},\n                    ],\n                }\n            ],\n        }\n    ]\n    gbgb_adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in gbgb_adapter.get_races(mock_date)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.venue == \"Towcester\"\n    assert race.race_number == 1\n    assert race.race_name == \"The October Sprint\"\n    assert race.distance == \"500m\"\n    assert len(race.runners) == 3\n\n    runner1 = next(r for r in race.runners if r.number == 1)\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.odds['GBGB'].win == Decimal(\"3.5\")\n\n    runner2 = next(r for r in race.runners if r.number == 2)\n    assert runner2.name == \"Speedy Sue\"\n    assert runner2.odds['GBGB'].win == Decimal(\"2.0\")\n\n    runner3 = next(r for r in race.runners if r.number == 3)\n    assert runner3.name == \"Lazy Larry\"\n    assert runner3.odds['GBGB'].win == Decimal(\"11.0\")\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(gbgb_adapter):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime('%Y-%m-%d')\n    gbgb_adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = [race async for race in gbgb_adapter.get_races(mock_date)]\n\n    # ASSERT\n    assert races == []\n",
    "tests/adapters/test_greyhound_adapter.py": "import pytest\nfrom unittest.mock import AsyncMock\nfrom datetime import date, datetime\nfrom python_service.adapters.greyhound_adapter import GreyhoundAdapter\nfrom python_service.config import Settings\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"\n    Provides a mock config object for the adapter.\n    \"\"\"\n    return Settings(GREYHOUND_API_URL=\"https://api.example.com\")\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(mock_config):\n    \"\"\"\n    Tests that the GreyhoundAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n\n    mock_api_response = {\n        \"cards\": [\n            {\n                \"track_name\": \"Test Track\",\n                \"races\": [\n                    {\n                        \"race_id\": \"test_race_123\",\n                        \"race_number\": 1,\n                        \"start_time\": int(datetime.now().timestamp()),\n                        \"runners\": [\n                            {\"dog_name\": \"Rapid Rover\", \"trap_number\": 1, \"odds\": {\"win\": \"2.5\"}},\n                            {\"dog_name\": \"Swift Sprint\", \"trap_number\": 2, \"scratched\": True},\n                            {\"dog_name\": \"Lazy Larry\", \"trap_number\": 3, \"odds\": {\"win\": \"10.0\"}},\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == 'greyhound_test_race_123'\n    assert race.venue == 'Test Track'\n    assert len(race.runners) == 2  # One was scratched\n\n    runner1 = race.runners[0]\n    assert runner1.name == 'Rapid Rover'\n    assert runner1.number == 1\n    assert runner1.odds['Greyhound Racing'].win == 2.5\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(mock_config):\n    \"\"\"\n    Tests that the GreyhoundAdapter handles an empty API response gracefully.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    adapter._fetch_data = AsyncMock(return_value={\"cards\": []})\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(mock_config):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n",
    "tests/adapters/test_racingtv_adapter.py": "# tests/adapters/test_racingtv_adapter.py\nimport pytest\nfrom python_service.adapters.racingtv_adapter import RacingTVAdapter\n\n@pytest.mark.asyncio\nasync def test_racingtv_adapter_is_stub():\n    \"\"\"\n    Tests that the RacingTVAdapter is a non-functional stub that returns no data.\n    \"\"\"\n    # ARRANGE\n    adapter = RacingTVAdapter()\n\n    # ACT\n    races = [race async for race in adapter.get_races(\"2025-10-27\")]\n\n    # ASSERT\n    assert races == []\n",
    "tests/adapters/test_the_racing_api_adapter.py": "import pytest\nfrom unittest.mock import AsyncMock\nfrom datetime import date, datetime, timezone\nfrom decimal import Decimal\n\nfrom python_service.adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom python_service.core.exceptions import AdapterConfigError\nfrom python_service.config import Settings\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"Provides a mock config object with the necessary API key.\"\"\"\n    return Settings(THE_RACING_API_KEY=\"test_racing_api_key\")\n\n@pytest.fixture\ndef mock_config_no_key():\n    \"\"\"Provides a mock config with the API key explicitly set to None.\"\"\"\n    return Settings(THE_RACING_API_KEY=None)\n\ndef test_init_raises_config_error_if_no_key(mock_config_no_key):\n    \"\"\"\n    Tests that the adapter raises an AdapterConfigError if the API key is not set.\n    \"\"\"\n    with pytest.raises(AdapterConfigError) as excinfo:\n        TheRacingApiAdapter(config=mock_config_no_key)\n    assert \"THE_RACING_API_KEY is not configured\" in str(excinfo.value)\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(mock_config):\n    \"\"\"\n    Tests that TheRacingApiAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    off_time = datetime.now(timezone.utc)\n\n    mock_api_response = {\n        \"racecards\": [{\"race_id\": \"12345\", \"course\": \"Newbury\", \"race_no\": 3, \"off_time\": off_time.isoformat().replace('+00:00', 'Z'),\n                       \"race_name\": \"The Great Race\", \"distance_f\": \"1m 2f\", \"runners\": [\n                           {\"horse\": \"Speedy Steed\", \"number\": 1, \"jockey\": \"T. Rider\", \"trainer\": \"A. Trainer\", \"odds\": [{\"odds_decimal\": \"5.50\"}]},\n                           {\"horse\": \"Gallant Gus\", \"number\": 2, \"jockey\": \"J. Jockey\", \"trainer\": \"B. Builder\", \"odds\": [{\"odds_decimal\": \"3.25\"}]}\n                       ]}]\n    }\n\n    # Patch the internal _fetch_data method\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == 'tra_12345'\n    assert race.venue == \"Newbury\"\n    assert len(race.runners) == 2\n    runner1 = race.runners[0]\n    assert runner1.name == \"Speedy Steed\"\n    assert runner1.odds[adapter.source_name].win == Decimal(\"5.50\")\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(mock_config):\n    \"\"\"\n    Tests that the adapter returns an empty list for an API response with no racecards.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    adapter._fetch_data = AsyncMock(return_value={\"racecards\": []})\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n\n@pytest.mark.asyncio\nasync def test_get_races_returns_empty_list_on_api_failure(mock_config):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data fails.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    adapter._fetch_data = AsyncMock(side_effect=Exception(\"API is down\"))\n\n    # ACT\n    result = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert result == []\n",
    "tests/adapters/test_timeform_adapter_modernized.py": "# Modernized test resurrected from attic/legacy_tests_pre_triage/adapters/test_timeform_adapter.py\nimport pytest\nfrom unittest.mock import MagicMock, patch\nimport httpx\nfrom decimal import Decimal\nfrom python_service.adapters.timeform_adapter import TimeformAdapter\nfrom python_service.models import Race, Runner\n\n@pytest.fixture\ndef timeform_adapter():\n    mock_config = MagicMock()\n    return TimeformAdapter(config=mock_config)\n\ndef read_fixture(file_path):\n    with open(file_path, 'r') as f:\n        return f.read()\n\n@pytest.mark.asyncio\nasync def test_timeform_adapter_parses_html_correctly(timeform_adapter):\n    \"\"\"Verify adapter correctly parses a known HTML fixture.\"\"\"\n    mock_html = read_fixture('tests/fixtures/timeform_modern_sample.html')\n\n    # Directly test the parsing of runners from the correct HTML structure\n    from bs4 import BeautifulSoup\n    soup = BeautifulSoup(mock_html, \"html.parser\")\n    runners = [timeform_adapter._parse_runner(row) for row in soup.select(\"div.rp-horseTable_mainRow\")]\n\n    assert len(runners) == 3, 'Should parse three runners'\n\n    braveheart = next((r for r in runners if r.name == 'Braveheart'), None)\n    assert braveheart is not None\n    assert braveheart.odds['Timeform'].win == Decimal('3.5')\n\n    steady_eddy = next((r for r in runners if r.name == 'Steady Eddy'), None)\n    assert steady_eddy is not None\n    assert steady_eddy.odds['Timeform'].win == Decimal('2.0')",
    "tests/adapters/test_twinspires_adapter.py": "# tests/adapters/test_twinspires_adapter.py\nimport pytest\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom unittest.mock import AsyncMock, MagicMock\nfrom python_service.adapters.twinspires_adapter import TwinSpiresAdapter\n\n@pytest.fixture\ndef twinspires_adapter():\n    mock_config = MagicMock()\n    return TwinSpiresAdapter(config=mock_config)\n\ndef read_fixture(file_path):\n    with open(file_path, 'r') as f:\n        return f.read()\n\n@pytest.mark.skip(reason=\"Twinspires adapter is a placeholder and not yet implemented.\")\n@pytest.mark.asyncio\nasync def test_twinspires_adapter_get_races_successfully(twinspires_adapter):\n    \"\"\"Verify adapter correctly fetches and parses data via get_races.\"\"\"\n    mock_html = read_fixture('tests/fixtures/twinspires_sample.html')\n    race_date = \"2025-10-26\"\n\n    # Patch the internal _fetch_data method to return the mock HTML\n    twinspires_adapter._fetch_data = AsyncMock(return_value={\"html\": mock_html, \"date\": race_date})\n\n    races = twinspires_adapter.get_races(race_date)\n\n    assert len(races) == 1\n    race = races[0]\n\n    assert race.venue == \"Churchill Downs\"\n    assert race.race_number == 5\n    assert len(race.runners) == 3  # One runner is scratched\n\n    braveheart = next((r for r in race.runners if r.name == 'Braveheart'), None)\n    assert braveheart is not None\n    assert braveheart.odds['TwinSpires'].win == Decimal('3.5')\n\n    gallant_gus = next((r for r in race.runners if r.name == 'Gallant Gus'), None)\n    assert gallant_gus is not None\n    assert gallant_gus.odds['TwinSpires'].win == Decimal('4.0')\n\n    # Check that the start time was parsed correctly\n    assert race.start_time == datetime(2025, 10, 26, 16, 30)\n\n@pytest.mark.skip(reason=\"Twinspires adapter is a placeholder and not yet implemented.\")\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(twinspires_adapter):\n    \"\"\"Tests that get_races returns an empty list when _fetch_data returns None.\"\"\"\n    race_date = \"2025-10-26\"\n    twinspires_adapter._fetch_data = AsyncMock(return_value=None)\n\n    races = twinspires_adapter.get_races(race_date)\n\n    assert races == []\n",
    "tests/analyzers/test_trifecta_analyzer.py": "# Dedicated test suite for the TrifectaAnalyzer, resurrected and expanded.\nimport pytest\nimport datetime\nfrom python_service.analyzer import TrifectaAnalyzer\nfrom python_service.models import Race, Runner\n\n@pytest.fixture\ndef analyzer():\n    return TrifectaAnalyzer()\n\n@pytest.fixture\ndef create_race(runners):\n    return Race(\n        id='test-race',\n        venue='TEST',\n        race_number=1,\n        start_time=datetime.datetime.now(),\n        runners=runners,\n        source='test'\n    )\n\ndef test_analyzer_name(analyzer):\n    assert analyzer.name == \"trifecta_analyzer\"\n\n# Test cases resurrected from legacy scorer and logic tests\ndef test_qualifies_with_exactly_three_runners(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False),\n        Runner(number=3, name='C', odds='4/1', scratched=False)\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is True\n\ndef test_qualifies_with_more_than_three_runners(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False),\n        Runner(number=3, name='C', odds='4/1', scratched=False),\n        Runner(number=4, name='D', odds='5/1', scratched=False)\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is True\n\n# New test cases for edge-case hardening\ndef test_rejects_with_fewer_than_three_runners(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False)\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is False\n\ndef test_rejects_if_scratched_runners_reduce_field_below_three(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False),\n        Runner(number=3, name='C', odds='4/1', scratched=True) # Scratched\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is False\n\ndef test_handles_empty_runner_list(analyzer, create_race):\n    race = create_race([])\n    assert analyzer.is_race_qualified(race) is False\n\ndef test_handles_none_race_object(analyzer):\n    assert analyzer.is_race_qualified(None) is False",
    "tests/conftest.py": "# tests/conftest.py\nimport pytest\nfrom unittest.mock import patch, Mock\nfrom fastapi.testclient import TestClient\nimport httpx\n\nfrom python_service.config import Settings\nfrom python_service.api import app, get_settings\n\ndef get_test_settings():\n    \"\"\"\n    Returns a comprehensive, test-specific Settings object that satisfies all\n    adapter configuration requirements. This prevents AdapterConfigErrors during\n    app startup in a test environment.\n    \"\"\"\n    return Settings(\n        API_KEY=\"test_api_key\",\n        # Required by TheRacingApiAdapter\n        THE_RACING_API_KEY=\"test_racing_api_key\",\n        # Required by Betfair adapters\n        BETFAIR_APP_KEY=\"test_betfair_key\",\n        # Required by TVGAdapter\n        TVG_API_KEY=\"test_tvg_key\",\n        # Required by RacingAndSports adapters\n        RACING_AND_SPORTS_TOKEN=\"test_ras_token\",\n        # Required by GreyhoundAdapter\n        GREYHOUND_API_URL=\"https://api.example.com/greyhound\"\n    )\n\n@pytest.fixture(scope=\"module\")\ndef client():\n    \"\"\"\n    A TestClient instance for testing the FastAPI app.\n    This fixture handles the setup and teardown of dependency overrides.\n    \"\"\"\n    original_get_settings = app.dependency_overrides.get(get_settings)\n    app.dependency_overrides[get_settings] = get_test_settings\n\n    with patch(\"python_service.credentials_manager.keyring.get_password\", side_effect=lambda s, u: f\"test_{u}\"):\n        with TestClient(app) as c:\n            yield c\n\n    # Clean up the override\n    if original_get_settings:\n        app.dependency_overrides[get_settings] = original_get_settings\n    else:\n        app.dependency_overrides.clear()\n\n\n@pytest.fixture\ndef mock_httpx_client():\n    \"\"\"Mocks the httpx.AsyncClient for testing adapters.\"\"\"\n    return Mock(spec=httpx.AsyncClient)\n",
    "tests/fixtures/timeform_legacy_sample.html": "<!DOCTYPE html><html><body><div class='race-card'><div class='runner'><span class='runner-name'>Braveheart</span><span class='runner-odds'>5/2</span></div><div class='runner'><span class='runner-name'>Speedster</span><span class='runner-odds'>10/1</span></div><div class='runner'><span class='runner-name'>Steady Eddy</span><span class='runner-odds'>EVENS</span></div></div></body></html>",
    "tests/fixtures/timeform_modern_sample.html": "<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Braveheart</a>\n  <span class=\"rp-horseTable_horse-number\">(1)</span>\n  <button class=\"rp-bet-placer-btn__odds\">5/2</button>\n</div>\n<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Speedster</a>\n  <span class=\"rp-horseTable_horse-number\">(2)</span>\n  <button class=\"rp-bet-placer-btn__odds\">10/1</button>\n</div>\n<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Steady Eddy</a>\n  <span class=\"rp-horseTable_horse-number\">(3)</span>\n  <button class=\"rp-bet-placer-btn__odds\">EVENS</button>\n</div>\n",
    "tests/fixtures/twinspires_sample.html": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <title>Race Results - Twinspires</title>\n</head>\n<body>\n    <div id=\"race-card\">\n        <h1>Race 5 - Churchill Downs - 2025-10-26</h1>\n        <div class=\"race-details\">\n            <span class=\"post-time\">Post Time: 04:30 PM</span>\n            <span class=\"distance\">1 Mile</span>\n            <span class=\"surface\">Dirt</span>\n        </div>\n        <ul class=\"runners-list\">\n            <li class=\"runner\">\n                <span class=\"runner-number\">1</span>\n                <span class=\"runner-name\">Braveheart</span>\n                <span class=\"runner-odds\">5/2</span>\n            </li>\n            <li class=\"runner\">\n                <span class=\"runner-number\">2</span>\n                <span class=\"runner-name\">Speedster</span>\n                <span class=\"runner-odds\">10/1</span>\n            </li>\n            <li class=\"runner scratched\">\n                <span class=\"runner-number\">3</span>\n                <span class=\"runner-name\">Steady Eddy</span>\n                <span class=\"runner-odds\">SCR</span>\n            </li>\n             <li class=\"runner\">\n                <span class=\"runner-number\">4</span>\n                <span class=\"runner-name\">Gallant Gus</span>\n                <span class=\"runner-odds\">3/1</span>\n            </li>\n        </ul>\n    </div>\n</body>\n</html>\n",
    "tests/test_analyzer.py": "import pytest\nfrom decimal import Decimal\nfrom datetime import datetime\nfrom python_service.models import Race, Runner, OddsData\nfrom python_service.analyzer import AnalyzerEngine, TrifectaAnalyzer, _get_best_win_odds\n\n# Helper to create runners for tests\ndef create_runner(number, odds_val=None, scratched=False):\n    odds_data = {}\n    if odds_val:\n        odds_data[\"TestOdds\"] = OddsData(win=Decimal(str(odds_val)), source=\"TestOdds\", last_updated=datetime.now())\n    return Runner(number=number, name=f\"Runner {number}\", odds=odds_data, scratched=scratched)\n\n@pytest.fixture\ndef sample_races_for_true_trifecta():\n    \"\"\"Provides a list of sample Race objects for the new 'True Trifecta' logic.\"\"\"\n    return [\n        # Race 1: Should PASS all criteria, will have a lower score\n        Race(\n            id=\"race_pass_1\", venue=\"Test Park\", race_number=1, start_time=datetime.now(), source=\"Test\",\n            runners=[\n                create_runner(1, 3.0), # Favorite\n                create_runner(2, 4.5), # Second Favorite\n                create_runner(3, 5.0),\n            ]\n        ),\n        # Race 2: Should FAIL (Field size too large)\n        Race(\n            id=\"race_fail_field_size\", venue=\"Test Park\", race_number=2, start_time=datetime.now(), source=\"Test\",\n            runners=[create_runner(i, 5.0 + i) for i in range(1, 12)] # 11 runners\n        ),\n        # Race 3: Should FAIL (Favorite odds too low)\n        Race(\n            id=\"race_fail_fav_odds\", venue=\"Test Park\", race_number=3, start_time=datetime.now(), source=\"Test\",\n            runners=[create_runner(1, 2.0), create_runner(2, 4.5)]\n        ),\n        # Race 4: Should FAIL (Second favorite odds too low)\n        Race(\n            id=\"race_fail_2nd_fav_odds\", venue=\"Test Park\", race_number=4, start_time=datetime.now(), source=\"Test\",\n            runners=[create_runner(1, 3.0), create_runner(2, 3.5)]\n        ),\n        # Race 5: Should also PASS and have a higher score than race_pass_1\n        Race(\n            id=\"race_pass_2\", venue=\"Test Park\", race_number=5, start_time=datetime.now(), source=\"Test\",\n            runners=[\n                create_runner(1, 4.0), # Favorite\n                create_runner(2, 6.0), # Second Favorite\n                create_runner(3, 8.0),\n                create_runner(4, 12.0),\n                create_runner(5, 15.0),\n            ]\n        ),\n    ]\n\ndef test_analyzer_engine_discovery():\n    \"\"\"Tests that the AnalyzerEngine correctly discovers the TrifectaAnalyzer.\"\"\"\n    engine = AnalyzerEngine()\n    assert 'trifecta' in engine.analyzers\n    assert engine.analyzers['trifecta'] == TrifectaAnalyzer\n\ndef test_analyzer_engine_get_analyzer():\n    \"\"\"Tests that the AnalyzerEngine can instantiate a specific analyzer.\"\"\"\n    engine = AnalyzerEngine()\n    analyzer = engine.get_analyzer('trifecta', max_field_size=8)\n    assert isinstance(analyzer, TrifectaAnalyzer)\n    assert analyzer.max_field_size == 8\n\ndef test_analyzer_engine_get_nonexistent_analyzer():\n    \"\"\"Tests that requesting a non-existent analyzer raises a ValueError.\"\"\"\n    engine = AnalyzerEngine()\n    with pytest.raises(ValueError, match=\"Analyzer 'nonexistent' not found.\"):\n        engine.get_analyzer('nonexistent')\n\ndef test_trifecta_analyzer_plugin_logic(sample_races_for_true_trifecta):\n    \"\"\"\n    Tests the TrifectaAnalyzer's scoring, sorting, and new response structure.\n    \"\"\"\n    engine = AnalyzerEngine()\n    analyzer = engine.get_analyzer('trifecta')  # Use default criteria\n\n    result = analyzer.qualify_races(sample_races_for_true_trifecta)\n\n    # 1. Verify the new response structure\n    assert isinstance(result, dict)\n    assert \"criteria\" in result\n    assert \"races\" in result\n    assert result['criteria']['max_field_size'] == 10\n\n    qualified_races = result['races']\n\n    # 2. Check that the correct number of races were qualified\n    assert len(qualified_races) == 2\n\n    # 3. Check that the scores have been assigned and are valid numbers\n    assert qualified_races[0].qualification_score is not None\n    assert qualified_races[1].qualification_score is not None\n    assert isinstance(qualified_races[0].qualification_score, float)\n\n    # 4. Check that the races are sorted by score in descending order\n    assert qualified_races[0].qualification_score > qualified_races[1].qualification_score\n    assert qualified_races[0].id == \"race_pass_2\"  # This race should have the higher score\n    assert qualified_races[1].id == \"race_pass_1\"\n\ndef test_get_best_win_odds_helper():\n    \"\"\"Tests the helper function for finding the best odds.\"\"\"\n    runner_with_odds = create_runner(1)\n    runner_with_odds.odds = {\n        \"SourceA\": OddsData(win=Decimal(\"3.0\"), source=\"A\", last_updated=datetime.now()),\n        \"SourceB\": OddsData(win=Decimal(\"2.5\"), source=\"B\", last_updated=datetime.now()),\n    }\n    assert _get_best_win_odds(runner_with_odds) == Decimal(\"2.5\")\n\n    runner_no_odds = create_runner(2)\n    assert _get_best_win_odds(runner_no_odds) is None\n\n    runner_no_win = create_runner(3)\n    runner_no_win.odds = {\"SourceA\": OddsData(win=None, source=\"A\", last_updated=datetime.now())}\n    assert _get_best_win_odds(runner_no_win) is None\n\n# Test case added by Operation: Resurrect and Modernize\nfrom python_service.models import Race, Runner\nimport datetime\n\ndef test_trifecta_analyzer_rejects_races_with_too_few_runners(trifecta_analyzer):\n    \"\"\"Ensure analyzer rejects races with < 3 runners for a trifecta.\"\"\"\n    race_with_two_runners = Race(\n        id='test_race_123',\n        venue='TEST',\n        race_number=1,\n        start_time=datetime.datetime.now(),\n        runners=[\n            Runner(number=1, name='Horse A', odds='2/1', scratched=False),\n            Runner(number=2, name='Horse B', odds='3/1', scratched=False)\n        ],\n        source='test'\n    )\n\n    is_qualified = trifecta_analyzer.is_race_qualified(race_with_two_runners)\n    assert not is_qualified, 'Trifecta analyzer should not qualify a race with only two runners.'\n",
    "tests/test_api.py": "# tests/test_api.py\nimport pytest\nimport aiosqlite\nfrom unittest.mock import patch, AsyncMock\nfrom datetime import datetime, date\nfrom decimal import Decimal\n\nfrom python_service.models import Race, Runner, OddsData, TipsheetRace, AggregatedResponse\nfrom python_service.api import app, get_settings\nfrom python_service.config import Settings\n\n# --- Fixtures ---\n\nfrom fastapi.testclient import TestClient\n\n# The client fixture is now correctly sourced from conftest.py,\n# which handles the settings override globally.\n\n# --- API Tests ---\n\n@pytest.mark.asyncio\n@patch('python_service.engine.FortunaEngine.get_races', new_callable=AsyncMock)\nasync def test_get_races_endpoint_success(mock_get_races, client):\n    \"\"\"\n    SPEC: The /api/races endpoint should return data with a valid API key.\n    \"\"\"\n    # ARRANGE\n    today = date.today()\n    mock_response = AggregatedResponse(\n        date=today,\n        races=[],\n        sources=[],\n        metadata={},\n        # This was the missing field causing the validation error\n        source_info=[]\n    )\n    mock_get_races.return_value = mock_response.model_dump()\n    headers = {\"X-API-Key\": \"test_api_key\"}\n\n    # ACT\n    response = client.get(f\"/api/races?race_date={today.isoformat()}\", headers=headers)\n\n    # ASSERT\n    assert response.status_code == 200\n    mock_get_races.assert_awaited_once()\n\n@pytest.mark.asyncio\nasync def test_get_tipsheet_endpoint_success(tmp_path, client):\n    \"\"\"\n    SPEC: The /api/tipsheet endpoint should return a list of tipsheet races from the database.\n    \"\"\"\n    db_path = tmp_path / \"test.db\"\n    post_time = datetime.now()\n\n    with patch('python_service.api.DB_PATH', db_path):\n        async with aiosqlite.connect(db_path) as db:\n            await db.execute(\"\"\"\n                CREATE TABLE tipsheet (\n                    race_id TEXT PRIMARY KEY,\n                    track_name TEXT,\n                    race_number INTEGER,\n                    post_time TEXT,\n                    score REAL,\n                    factors TEXT\n                )\n            \"\"\")\n            await db.execute(\n                \"INSERT INTO tipsheet VALUES (?, ?, ?, ?, ?, ?)\",\n                (\"test_race_1\", \"Test Park\", 1, post_time.isoformat(), 85.5, \"{}\")\n            )\n            await db.commit()\n\n        # ACT\n        response = client.get(f\"/api/tipsheet?date={post_time.date().isoformat()}\")\n\n        # ASSERT\n        assert response.status_code == 200\n        response_data = response.json()\n        assert len(response_data) == 1\n        # The database returns snake_case, but the Pydantic model is camelCase\n        assert response_data[0][\"raceId\"] == \"test_race_1\"\n        assert response_data[0][\"score\"] == 85.5\n\ndef test_health_check_unauthenticated(client):\n    \"\"\"Ensures the /health endpoint is accessible without an API key.\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    json_response = response.json()\n    assert json_response[\"status\"] == \"ok\"\n    assert \"timestamp\" in json_response\n\ndef test_api_key_authentication_failure(client):\n    \"\"\"Ensures that endpoints are protected and fail with an invalid API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\", headers={\"X-API-KEY\": \"invalid_key\"})\n    assert response.status_code == 403\n    assert \"Invalid or missing API Key\" in response.json()[\"detail\"]\n\n\ndef test_api_key_authentication_missing(client):\n    \"\"\"Ensures that endpoints are protected and fail with a missing API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\")\n    assert response.status_code == 403\n    assert \"Not authenticated\" in response.json()[\"detail\"]\n",
    "tests/test_api/test_endpoints.py": "from fastapi.testclient import TestClient\nfrom python_service.api import app\n\nclient = TestClient(app)\n\ndef test_health_check():\n    \"\"\"Tests the unauthenticated /health endpoint.\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"ok\"}\n",
    "tests/test_engine.py": "import pytest\nfrom unittest.mock import AsyncMock, patch\nfrom datetime import datetime, date\nfrom decimal import Decimal\nimport fakeredis.aioredis\n\nfrom python_service.models import Race, Runner, OddsData\nfrom python_service.engine import OddsEngine\nfrom python_service.config import get_settings\nfrom python_service.adapters.base import BaseAdapter\n\ndef create_mock_race(source: str, venue: str, race_number: int, start_time: datetime, runners_data: list) -> Race:\n    \"\"\"Helper function to create a Race object for testing.\"\"\"\n    runners = []\n    for r_data in runners_data:\n        odds = {source: OddsData(win=Decimal(r_data[\"odds\"]), source=source, last_updated=datetime.now())}\n        runners.append(Runner(number=r_data[\"number\"], name=r_data[\"name\"], odds=odds))\n\n    return Race(\n        id=f\"test_{source}_{race_number}\",\n        venue=venue,\n        race_number=race_number,\n        start_time=start_time,\n        runners=runners,\n        source=source\n    )\n\n@pytest.fixture\ndef mock_engine() -> OddsEngine:\n    \"\"\"Provides an OddsEngine instance with a mock config.\"\"\"\n    return OddsEngine(config=get_settings())\n\n@pytest.mark.asyncio\n@patch('python_service.engine.OddsEngine._time_adapter_fetch', new_callable=AsyncMock)\nasync def test_engine_deduplicates_races_and_merges_odds(mock_time_adapter_fetch, mock_engine):\n    \"\"\"\n    SPEC: The OddsEngine's fetch_all_odds method should identify duplicate races\n    from different sources and merge their runner data, stacking the odds.\n    \"\"\"\n    # ARRANGE\n    test_time = datetime(2025, 10, 9, 14, 30)\n\n    source_a_race = create_mock_race(\"SourceA\", \"Test Park\", 1, test_time, [\n        {\"number\": 1, \"name\": \"Speedy\", \"odds\": \"5.0\"},\n        {\"number\": 2, \"name\": \"Steady\", \"odds\": \"10.0\"},\n    ])\n    source_b_race = create_mock_race(\"SourceB\", \"Test Park\", 1, test_time, [\n        {\"number\": 1, \"name\": \"Speedy\", \"odds\": \"5.5\"},\n        {\"number\": 3, \"name\": \"Newcomer\", \"odds\": \"15.0\"},\n    ])\n    other_race = create_mock_race(\"SourceC\", \"Another Place\", 2, test_time, [\n        {\"number\": 1, \"name\": \"Solo\", \"odds\": \"3.0\"}\n    ])\n\n    mock_time_adapter_fetch.side_effect = [\n        (\"SourceA\", {'races': [source_a_race], 'source_info': {'name': 'SourceA', 'status': 'SUCCESS', 'races_fetched': 1}}, 1.0),\n        (\"SourceB\", {'races': [source_b_race], 'source_info': {'name': 'SourceB', 'status': 'SUCCESS', 'races_fetched': 1}}, 1.0),\n        (\"SourceC\", {'races': [other_race], 'source_info': {'name': 'SourceC', 'status': 'SUCCESS', 'races_fetched': 1}}, 1.0),\n    ]\n\n    # ACT\n    today_str = date.today().strftime('%Y-%m-%d')\n    result = await mock_engine.fetch_all_odds(today_str)\n\n    # ASSERT\n    assert len(result['races']) == 2, \"Engine should have de-duplicated the races.\"\n\n    merged_race = next((r for r in result['races'] if r['venue'] == \"Test Park\"), None)\n    assert merged_race is not None, \"Merged race should be present in the results.\"\n    assert len(merged_race['runners']) == 3, \"Merged race should contain all unique runners.\"\n\n    runner1 = next((r for r in merged_race['runners'] if r['number'] == 1), None)\n    assert runner1 is not None\n    assert \"SourceA\" in runner1['odds']\n    assert \"SourceB\" in runner1['odds']\n    assert runner1['odds']['SourceA']['win'] == Decimal(\"5.0\")\n    assert runner1['odds']['SourceB']['win'] == Decimal(\"5.5\")\n\n    runner2 = next((r for r in merged_race['runners'] if r['number'] == 2), None)\n    assert runner2 is not None\n    assert \"SourceA\" in runner2['odds'] and \"SourceB\" not in runner2['odds']\n\n    runner3 = next((r for r in merged_race['runners'] if r['number'] == 3), None)\n    assert runner3 is not None\n    assert \"SourceB\" in runner3['odds'] and \"SourceA\" not in runner3['odds']\n\n\n@pytest.mark.asyncio\n@patch('python_service.engine.redis.from_url')\nasync def test_engine_caching_logic(mock_redis_from_url):\n    \"\"\"\n    SPEC: The OddsEngine should cache results in Redis.\n    1. On a cache miss, it should fetch from adapters and set the cache.\n    2. On a cache hit, it should return data from the cache without fetching from adapters.\n    \"\"\"\n    # ARRANGE\n    mock_redis_client = fakeredis.aioredis.FakeRedis(decode_responses=True)\n    mock_redis_from_url.return_value = mock_redis_client\n    await mock_redis_client.flushall()\n\n    engine = OddsEngine(config=get_settings())\n\n    today_str = date.today().strftime('%Y-%m-%d')\n    cache_key = f\"fortuna:races:{today_str}\"\n    test_time = datetime(2025, 10, 9, 15, 0)\n\n    mock_race = create_mock_race(\"TestSource\", \"Cache Park\", 1, test_time, [\n        {\"number\": 1, \"name\": \"Cachedy\", \"odds\": \"4.0\"}\n    ])\n\n    # Replace the engine's adapters with a single mock to isolate the test\n    mock_adapter = AsyncMock(spec=BaseAdapter)\n    mock_adapter.source_name = \"TestSource\"\n    mock_adapter.fetch_races.return_value = {\n        'races': [mock_race],\n        'source_info': {'name': 'TestSource', 'status': 'SUCCESS', 'races_fetched': 1}\n    }\n    engine.adapters = [mock_adapter]\n\n\n    # --- ACT 1: Cache Miss ---\n    result_miss = await engine.fetch_all_odds(today_str)\n\n    # --- ASSERT 1: Cache Miss ---\n    mock_adapter.fetch_races.assert_called_once()\n    cached_value = await mock_redis_client.get(cache_key)\n    assert cached_value is not None\n    assert len(result_miss['races']) == 1\n    assert result_miss['races'][0]['venue'] == \"Cache Park\"\n\n\n    # --- ACT 2: Cache Hit ---\n    mock_adapter.fetch_races.reset_mock()\n    result_hit = await engine.fetch_all_odds(today_str)\n\n    # --- ASSERT 2: Cache Hit ---\n    mock_adapter.fetch_races.assert_not_called()\n    assert len(result_hit['races']) == 1\n    assert result_hit['races'][0]['venue'] == \"Cache Park\"\n\n    assert result_hit['races'] == result_miss['races']\n    assert result_hit['sources'] == result_miss['sources']\n\n    await engine.close()",
    "tests/test_engine/test_orchestration.py": "from python_service.engine import OddsEngine\n\ndef test_engine_instantiation():\n    \"\"\"Tests that the core OddsEngine can be instantiated without errors.\"\"\"\n    try:\n        engine = OddsEngine()\n        assert engine is not None\n        assert isinstance(engine, OddsEngine)\n    except Exception as e:\n        assert False, f\"OddsEngine instantiation failed with an exception: {e}\"\n",
    "tests/test_models.py": "# Test suite for Pydantic models, resurrected from attic/legacy_tests_pre_triage/checkmate_v7/test_models.py\nimport pytest\nfrom pydantic import ValidationError\nfrom python_service.models import Race, Runner\nimport datetime\n\ndef test_runner_model_creation():\n    \"\"\"Tests basic successful creation of the Runner model.\"\"\"\n    runner = Runner(number=5, name='Test Horse', odds='5/1', scratched=False)\n    assert runner.number == 5\n    assert runner.name == 'Test Horse'\n    assert not runner.scratched\n\ndef test_race_model_with_valid_runners():\n    \"\"\"Tests basic successful creation of the Race model.\"\"\"\n    runner1 = Runner(number=1, name='A', odds='2/1', scratched=False)\n    runner2 = Runner(number=2, name='B', odds='3/1', scratched=False)\n    race = Race(\n        id='test-race-1',\n        venue='TEST',\n        race_number=1,\n        start_time=datetime.datetime.now(),\n        runners=[runner1, runner2],\n        source='test'\n    )\n    assert race.venue == 'TEST'\n    assert len(race.runners) == 2\n\ndef test_model_validation_fails_on_missing_required_field():\n    \"\"\"Ensures Pydantic's validation fires for missing required fields.\"\"\"\n    with pytest.raises(ValidationError):\n        # 'name' is a required field for a Runner\n        Runner(number=3, odds='3/1', scratched=False)\n\n    with pytest.raises(ValidationError):\n        # 'venue' is a required field for a Race\n        Race(\n            id='test-race-2',\n            race_number=2,\n            start_time=datetime.datetime.now(),\n            runners=[],\n            source='test'\n        )",
    "tests/test_models/test_validation.py": "import pytest\nfrom pydantic import ValidationError\nfrom python_service.models import Race\n\ndef test_race_model_valid_data():\n    \"\"\"Tests that the Race model can be created with valid data.\"\"\"\n    race_data = {\n        'id': 'test_race_123',\n        'venue': 'Test Park',\n        'race_number': 1,\n        'start_time': '2025-10-20T12:00:00Z',\n        'runners': [],\n        'source': 'test_source'\n    }\n    race = Race(**race_data)\n    assert race.id == 'test_race_123'\n    assert race.venue == 'Test Park'\n\ndef test_race_model_invalid_data():\n    \"\"\"Tests that the Race model raises a ValidationError with invalid data.\"\"\"\n    invalid_race_data = {\n        'id': 'test_race_456',\n        'venue': 12345,  # Invalid type\n        'race_number': 'two', # Invalid type\n        'start_time': 'not-a-date',\n        'runners': 'not-a-list',\n        'source': 'test_source'\n    }\n    with pytest.raises(ValidationError):\n        Race(**invalid_race_data)\n",
    "tests/test_msi_installation.ps1": "param([string]$MsiPath = \".\\dist\\Fortuna-Faucet-2.1.0-x64.msi\")\n\nWrite-Host \"Testing MSI Installation...\" -ForegroundColor Cyan\n\n# Test 1: File integrity\nWrite-Host \"\u2022 Verifying MSI structure...\"\nif (Test-Path $MsiPath) {\n    Write-Host \"\u2713 MSI file exists\"\n} else {\n    Write-Error \"MSI file not found\"\n    exit 1\n}\n\n# Test 2: Installation\nWrite-Host \"\u2022 Testing interactive installation...\"\n& msiexec.exe /i $MsiPath /l*v \"test_install.log\"\n\n# Test 3: Verify installation\nWrite-Host \"\u2022 Verifying files were installed...\"\n$programFiles = \"$env:PROGRAMFILES\\Fortuna Faucet\"\nif (Test-Path $programFiles) {\n    Write-Host \"\u2713 Installation successful\"\n} else {\n    Write-Error \"Installation failed\"\n    exit 1\n}\n\n# Test 4: Registry entries\nWrite-Host \"\u2022 Checking registry entries...\"\n$regPath = \"HKLM:\\Software\\Fortuna Faucet\"\nif (Test-Path $regPath) {\n    Write-Host \"\u2713 Registry entries found\"\n} else {\n    Write-Error \"Registry entries missing\"\n    exit 1\n}",
    "tests/test_silent_deployment.ps1": "Write-Host \"Testing silent deployment...\" -ForegroundColor Cyan\n\n& msiexec.exe /i \"Fortuna-Faucet-2.1.0-x64.msi\" `\n    /qn /l*v \"silent_test.log\" `\n    ALLUSERS=1 INSTALLSCOPE=perMachine\n\nif ($LASTEXITCODE -eq 0) {\n    Write-Host \"\u2713 Silent deployment successful\"\n} else {\n    Write-Host \"\u2717 Silent deployment failed\"\n    Write-Host \"Log: silent_test.log\"\n    exit 1\n}",
    "tests/test_uninstall.ps1": "Write-Host \"Testing uninstall...\" -ForegroundColor Cyan\n\n$regPath = 'HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*'\n$product = Get-ItemProperty $regPath | Where-Object { $_.DisplayName -like '*Fortuna*' }\n\nif ($product) {\n    & msiexec.exe /x $product.PSChildName /qn /l*v \"uninstall_test.log\"\n\n    Start-Sleep -Seconds 2\n\n    $programFiles = \"$env:PROGRAMFILES\\Fortuna Faucet\"\n    if (-not (Test-Path $programFiles)) {\n        Write-Host \"\u2713 Uninstall successful\"\n    } else {\n        Write-Host \"\u2717 Uninstall incomplete\"\n        exit 1\n    }\n} else {\n    Write-Host \"\u2717 Product not found in registry\"\n    exit 1\n}",
    "tests/utils/test_odds.py": "# tests/utils/test_odds.py\nimport pytest\nfrom decimal import Decimal\nfrom python_service.utils.odds import parse_odds_to_decimal\n\n@pytest.mark.parametrize(\"input_odds, expected_decimal\", [\n    (\"5/2\", Decimal(\"3.5\")),      # 2.5 + 1 stake\n    (\"10/1\", Decimal(\"11.0\")),     # 10.0 + 1 stake\n    (\"EVENS\", Decimal(\"2.0\")),     # 1.0 + 1 stake\n    (\"EVS\", Decimal(\"2.0\")),       # 1.0 + 1 stake\n    (\"1/2\", Decimal(\"1.5\")),       # 0.5 + 1 stake\n    (\"2.5\", Decimal(\"2.5\")),       # Handles decimal strings\n    (3.5, Decimal(\"3.5\")),         # Handles float input\n    (10, Decimal(\"10\")),           # Handles int input\n    (\"SP\", None),                  # Should handle non-fractional odds gracefully\n    (\"SCR\", None),                 # Should handle scratched runners\n    (\"REF\", None),                 # Should handle other non-numeric values\n    (\"\", None),\n    (None, None)\n])\ndef test_parse_odds_to_decimal(input_odds, expected_decimal):\n    \"\"\"Tests the new centralized odds parsing utility with various formats.\"\"\"\n    assert parse_odds_to_decimal(input_odds) == expected_decimal\n"
}