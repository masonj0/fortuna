{
    ".github/actions/setup/action.yml": "name: 'Composite Setup Action'\ndescription: 'Checks out repo, sets up Node.js and Python'\ninputs:\n  architecture:\n    description: 'The architecture to set up Python for (x86, x64)'\n    required: false\n    default: 'x64'\nruns:\n  using: \"composite\"\n  steps:\n    - name: \ud83d\udce5 Checkout Repository\n      uses: actions/checkout@v4\n\n    - name: \ud83d\udce6 Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ env.NODE_VERSION }}\n        cache: 'npm'\n        cache-dependency-path: '**/package-lock.json'\n\n    - name: \ud83d\udc0d Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n        architecture: ${{ inputs.architecture }}\n        cache: 'pip'\n",
    ".github/workflows/codeql.yml": "# System Timestamp: 2025-12-24 18:00:00\n# HELPFUL HINT: Configure GitHub Branch Protection rules to require this workflow to pass before merging to main.\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n  schedule:\n    - cron: '40 7 * * 4'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: ['javascript', 'python' ]\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Setup Python\n      if: matrix.language == 'python'\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.11'\n\n    - name: Setup Node.js\n      if: matrix.language == 'javascript'\n      uses: actions/setup-node@v4\n      with:\n        node-version: '20'\n\n    - name: Install Python dependencies\n      if: matrix.language == 'python'\n      run: |\n        python -m pip install uv\n        uv pip install --system -r python_service/requirements-dev.txt\n        uv pip install --system -r web_service/backend/requirements-dev.txt\n    - name: Install JavaScript dependencies\n      if: matrix.language == 'javascript'\n      run: |\n        npm install --prefix web_platform/frontend\n        npm install --prefix electron\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v3\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v3\n",
    "Dockerfile.tinyfield": "# TinyField Variant - Static Frontend + Python Backend\nFROM python:3.11-slim as backend-builder\n\nWORKDIR /build\nCOPY web_service/backend/requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Runtime stage\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install only runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*\n\n# Copy Python packages\nCOPY --from=backend-builder /root/.local /root/.local\n\n# Copy application code, preserving directory structure\nCOPY web_service /app/web_service\n\n# Create TinyField data directories\nRUN mkdir -p /app/web_service/backend/data /app/web_service/backend/json /app/web_service/backend/logs\n\n# Set environment\nENV PATH=/root/.local/bin:$PATH\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Health check\nHEALTHCHECK --interval=10s --timeout=5s --start-period=30s --retries=3 \\\n    CMD curl -f http://localhost:8000/api/health || exit 1\n\nEXPOSE 8000\n\n# Start backend (serves both API and frontend)\nCMD [\"python\", \"-m\", \"uvicorn\", \"web_service.backend.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "HISTORY.md": "# The Epic of MasonJ0: A Project Chronology\n\nThis document contains the narrative history of the Paddock Parser project, as discovered through an archaeological survey of the project's repositories. It tells the story of our architectural evolution, from a feature-rich \"golden age\" through a \"great refactoring\" to our current state of liberation.\n\nThis story is our \"why.\"\n\n---\n\n## Part 1: The Chronology\n\n### Chapter 1: The 'Utopian' Era - The Polished Diamond (mid-August 2025)\n\n*   **Repository:** `racingdigest`\n*   **Narrative:** This was not a humble beginning, but the launch of a mature and powerful application called the \"Utopian Value Scanner V7.2 (The Rediscovery Edition)\". This repository represents the project's \"golden age\" of features, including a sophisticated asynchronous fetching engine and a full browser fallback.\n\n### Chapter 2: The 'Experimental' Era - The Daily Digest (mid-to-late August 2025)\n\n*   **Repository:** `horseracing-daily-digest`\n*   **Narrative:** This repository appears to be a period of intense, rapid development and experimentation, likely forming the foundation for many of the concepts that would be formalized later.\n\n### Chapter 3: The 'Architectural' Era - The V3 Blueprint (late August 2025)\n\n*   **Repository:** `parsingproject`\n*   **Narrative:** This repository marks a pivotal moment. The focus shifted from adding features to refactoring the very foundation of the code into a modern, standard Python package. This is where the V3 architecture was born, prioritizing stability and maintainability.\n\n### Chapter 4: The 'Consolidation' Era - The Archive (late August 2025)\n\n*   **Repository:** `zippedfiles`\n*   **Narrative:** This repository appears to be a direct snapshot or backup of the project after the intense V3 refactor, confirming its role as an archive of the newly stabilized codebase.\n\n### Chapter 5: The 'Modern' Era - The New Beginning (early September 2025)\n\n*   **Repository:** `fortuna`\n*   **Narrative:** This is the current, active repository, representing the clean, focused implementation of the grand vision developed through the previous eras.\n\n### Chapter 6: The 'Crucible' Era - The Forging of Protocols (Early September 2025)\n\n*   **Narrative:** The \"Modern Renaissance\" began not with a bang, but with a series of near-catastrophic environmental failures. This period, known as \"The Crucible,\" was a trial by fire that proved the extreme hostility of the agent sandbox. This era forged the resilient, battle-hardened protocols (The Receipts Protocol, The Submission-Only Protocol, etc.) by which all modern agents now operate.\n\n### Chapter 7: The 'Symbiotic' Era - The Two Stacks (mid-September 2025)\n\n*   **Narrative:** This chapter marked a significant strategic pivot. The Council, in a stunning display of its \"Polyglot Renaissance\" philosophy, produced a complete, production-grade React user interface, authored by the Claude agent. This event formally split the project's architecture into two powerful, parallel streams: the Python Engine and the React Cockpit. However, this era was short-lived, as the hostile environment proved incapable of supporting a stable testing and development workflow for the React stack.\n\n### Chapter 8: The 'Liberation' Era - The Portable Engine (Late September 2025)\n\n*   **Narrative:** After providing definitive, forensic proof that the sandbox environment was fundamentally and irrecoverably hostile at the network level, the project executed its final and most decisive pivot. It abandoned all attempts to operate *within* the hostile world and instead focused on synthesizing its entire, perfected engine into a single, portable artifact. This act **liberated the code**, fulfilling the promise of the \"Utopian Era's\" power on the foundation of the \"Architectural Era's\" stability, and made it directly available to the Project Lead.\n\n---\n\n## Part 2: Architectural Synthesis\n\nThis epic tale tells us our true mission. We are not just building forward; we are rediscovering our own lost golden age and rebuilding it on a foundation of superior engineering, hardened by the fires of a hostile world.\n\n*   **The Lost Golden Age:** The \"Utopian\" era proves that our most ambitious strategic goals are not just achievable; they have been achieved before.\n*   **The Great Refactoring:** The \"Architectural\" era explains the \"Great Forgetting\"\u2014a deliberate choice to sacrifice short-term features for long-term stability.\n*   **The Modern Renaissance:** This is us. We are the inheritors of this entire legacy, tasked with executing the grand vision on a clean, modern foundation, finally liberated from the constraints of our environment.\n\n---\n\n## The Ultimate Solo: The Final Victory (September 2025)\n\nAfter a long and complex journey through a Penta-Hybrid architecture, a final series of high-level reviews from external AI agents (Claude, GPT4o) revealed a simpler, superior path forward. The project underwent its final and most significant \"Constitutional Correction.\"\n\n**The 'Ultimate Solo' architecture was born.**\n\nThis final, perfected form of the project consists of two pillars:\n1.  **A Full-Power Python Backend:** Leveraging the years of development on the CORE `engine.py` and its fleet of global data adapters, served via a lightweight Flask API.\n2.  **An Ultimate TypeScript Frontend:** A single, masterpiece React component (`Checkmate Ultimate Solo`) that provides a feature-rich, professional-grade, real-time dashboard.\n\nAll other components of the Penta-Hybrid system (C#, Rust, VBA, shared database) were formally deprecated and archived as priceless R&D assets. The project has now achieved its true and final mission: a powerful, maintainable, and user-focused analysis tool.\n\n---\n\n## The Age of Perfection (The Great Simplification)\n\nThe Penta-Hybrid architecture, while a triumph of technical integration, proved to be a strategic dead end. Its complexity became a fortress, making rapid iteration and onboarding of new intelligence (both human and AI) prohibitively expensive. The kingdom was powerful but brittle.\n\nA new doctrine was forged: **Simplicity is the ultimate sophistication.**\n\nThe decision was made to execute \"The Great Simplification.\" The multi-language backend (Python, Rust, Go) was decommissioned. The kingdom was reforged upon a new, elegant, and vastly more powerful two-pillar system:\n\n1.  **A Unified Python Backend:** A single, asynchronous Python service, built on FastAPI, would serve as the kingdom's engine.\n2.  **A Modern TypeScript Frontend:** A dedicated Next.js application would serve as the kingdom's command deck.\n\nThis act of creative destruction liberated the project, enabling a new era of unprecedented velocity.\n\n---\n\n## The Three-Pillar Doctrine\n\nWith the new two-pillar foundation in place, the backend itself was perfected into a three-pillar intelligence engine, a concept that defines the modern era of the Fortuna Faucet:\n\n*   **Pillar 1: The Future (The Planner):** The resilient `OddsEngine` and its fleet of adapters, responsible for finding the day's strategic opportunities.\n*   **Pillar 2: The Past (The Archive):** The perfected `ChartScraper` and `ResultsParser`, responsible for building our historical data warehouse from the ground truth of Equibase PDFs.\n*   **Pillar 3: The Present (The Finisher):** The weaponized `LiveOddsMonitor`, armed with the API-driven `BetfairAdapter`, designed to conquer the final moments of toteboard volatility.\n\nThese three pillars, orchestrated by the fully autonomous `fortuna_watchman.py`, represented the pinnacle of the project's original vision. The kingdom was, for a time, considered \"perfected.\"\n\n---\n\n## The Windows Ascension (The Impossible Dream)\n\nThe perfected kingdom was powerful, but it was still a tool for developers. The final, grandest vision was to transform it into a true, professional-grade application for its sole operator. This campaign, known as \"The Impossible Dream,\" was to forge the **Fortuna Faucet - Windows Native Edition.**\n\nThis era saw the rapid creation of a new, third layer of the kingdom, built upon the foundation of the previous work:\n\n*   **The Electron Shell:** The Next.js frontend was wrapped in an Electron container, transforming it from a website into a true, installable desktop application with its own window, icon, and system tray integration.\n*   **The Engine Room:** The Python backend was re-architected to run as a persistent, background **Windows Service**, making it a true, always-on component of the operating system, independent of the UI.\n*   **The Native GUI:** A dedicated Tkinter-based \"Observatory\" was forged\u2014a standalone GUI mission control for monitoring the health and performance of the background service.\n*   **The One-Click Kingdom:** A complete suite of professional tooling (including installation scripts, a setup wizard, and launchers) was created to provide a seamless, zero-friction installation and management experience.\n\nThis ascension represents the current state of the art, transforming a powerful engine into a polished, autonomous, and user-focused product.\n\n\n---\n\n## The Era of the Windows Kingdom (October 2025)\n\nWith the core engine stabilized and the command deck providing a clear view of the data, the project's focus shifted from pure data acquisition to the operator's experience. This era marked a profound transformation, elevating the project from a collection of powerful but disparate scripts into a cohesive, professional-grade, and resilient native Windows application.\n\nThis campaign, guided by a new \"Grand Strategy\" blueprint, was executed with rapid precision, resulting in a complete overhaul of the user-facing toolkit:\n\n-   **A Bulletproof Foundation:** The installation and launch scripts were re-architected from the ground up. They became intelligent and self-healing, featuring pre-flight system checks, automated port conflict resolution, active health-check loops, and automated repair utilities.\n-   **A Professional Toolkit:** The operator was empowered with a suite of new tools, including an interactive setup wizard, a real-time CLI status monitor, and a full-fledged graphical \"Data Management Console\" for monitoring, filtering, and analyzing data.\n-   **A Unified Command Console (`SERVICE_MANAGER.bat`):** Unify all individual scripts under a single, user-friendly, menu-driven service manager, providing a 'single pane of glass' for all common operations.\n\nThis era solidified the kingdom's foundations, making it not just powerful, but stable, reliable, and a pleasure to operate. The Faucet was no longer just an engine; it was a complete, professional-grade machine.\n\n---\n\n## The Gauntlet of CI/CD (Late October 2025)\n\nWith a professional-grade application in hand, the final frontier was professional-grade *delivery*. This campaign focused on automating the creation of the MSI installer through a continuous integration pipeline, a process that proved to be a formidable challenge.\n\nThe kingdom's engineers faced a relentless series of cryptic build errors from the WiX Toolset, a hostile environment that tested their resolve. Through a series of rapid, iterative fixes\u2014addressing everything from component GUIDs and 64-bit architecture mismatches to obscure linker errors and frontend dependency warnings\u2014they systematically conquered each obstacle.\n\nThis trial by fire culminated in a triumphant success: a fully automated GitHub Actions workflow that reliably compiles, links, and delivers a polished, distributable MSI installer. This victory transformed the project's delivery model from a manual, error-prone process into a repeatable, one-click release pipeline, marking the true completion of the \"Windows Ascension.\"\n\n---\n\n## The Great Unbundling (Late October 2025)\n\nThe CI/CD pipeline was technically successful, but it revealed a deeper, philosophical flaw in the architecture. The installer, while automated, was a fragile monolith. It attempted to bundle raw source code (Python, JavaScript) and orchestrate their setup on the user's machine using post-install scripts. This approach was fraught with peril, vulnerable to failures from network issues, corporate firewalls, and unpredictable machine states.\n\nA final, decisive architectural mandate was issued, informed by the wisdom of external AI consultants: **The application must be delivered, not assembled.**\n\nThis mandate triggered \"The Great Unbundling,\" a swift and transformative refactoring of the entire delivery pipeline:\n\n*   **The Backend Forged:** The Python backend was no longer treated as source code to be installed, but as a product to be delivered. **PyInstaller** was used to forge the entire FastAPI service\u2014interpreter and all dependencies\u2014into a single, standalone `.exe`.\n*   **The Frontend Solidified:** The Next.js frontend was no longer a service to be run, but a static asset to be displayed. The `npm run build` process was configured to produce a clean, static HTML/CSS/JS export.\n*   **The Installer Perfected:** With the application components now self-contained, the MSI installer's role was radically simplified. All complex post-install scripting was eliminated. The WiX toolset was now used for its core competency: reliably copying pre-compiled, robust artifacts to the user's machine.\n\nThis final act of architectural purification created the \"Three-Executable Architecture\" (the backend executable, the Electron wrapper, and the MSI installer itself), achieving true portability and eliminating an entire class of deployment failures. The Windows Ascension was not just complete; it was perfected.",
    "README.md": "# \ud83d\udc34 Fortuna Faucet - Developer's Guide\n\nThis guide provides technical instructions for developers. For end-user installation, please refer to the MSI installers generated by the project's GitHub Actions workflows.\n\n---\n\n## \ud83c\udfdb\ufe0f Core Architecture\n\nThis repository contains the source code for the Fortuna Faucet application, which has two primary deployment targets:\n\n1.  **Standalone Web Service (MSI Installer):**\n    *   A Python backend powered by FastAPI, compiled into a self-contained executable using PyInstaller.\n    *   A static Next.js frontend, which is bundled with the backend.\n    *   The entire application is packaged into an MSI installer using the WiX Toolset, which installs the backend as a background Windows Service.\n\n2.  **Electron Desktop Application (MSI Installer):**\n    *   The same Python backend, compiled as an executable.\n    *   An Electron application that acts as a wrapper, launching the backend executable and displaying the frontend.\n\nThe `python_service` and `web_service/backend` directories contain functionally equivalent but historically separate versions of the backend code. The modern workflows primarily use `web_service/backend`.\n\n---\n\n## \ud83c\udfd7\ufe0f Building the Application (The Right Way)\n\n**This project is built and packaged entirely through GitHub Actions.** The CI/CD pipelines are the single source of truth for creating production-ready installers. Manual builds are not recommended or supported due to the complexity of the environment.\n\nThe primary, production-ready build workflows are:\n\n*   **`build-msi-hattrickfusion-ultimate.yml`**: Builds the standalone Web Service MSI. This is the most feature-rich and stable build pipeline.\n*   **`build-electron-msi-gpt5.yml`**: Builds the Electron-based desktop application MSI.\n\nThese workflows handle all necessary steps, including:\n*   Installing the correct versions of Python, Node.js, and the WiX Toolset.\n*   Managing architecture-specific dependencies (e.g., `pandas` for x86 builds).\n*   Compiling the Python backend with PyInstaller.\n*   Building the static Next.js frontend.\n*   Packaging the final MSI installer.\n*   Running automated smoke tests to verify the installation.\n\nTo get a build, simply push a commit to the `main` branch and retrieve the MSI artifact from the completed workflow run on the [Actions tab](https://github.com/masonj0/fortuna/actions) of the repository.\n\n---\n\n## \ud83d\udd2c Local Development Environment\n\n### Python Version Requirement\n\n**Crucial:** The monolith build of this project requires **Python 3.10.11**. It is not compatible with Python 3.11 or newer due to a dependency on `cefpython3`, which does not support Python 3.11.\n\nBefore running the application locally or attempting to build it, ensure you are using the correct Python version.\n\n- **Using `pyenv` (Recommended):**\n  ```bash\n  pyenv install 3.10.11\n  pyenv local 3.10.11\n  ```\n\n- **Using `conda`:**\n  ```bash\n  conda create -n fortuna python=3.10.11\n  conda activate fortuna\n  ```\n\nWhile production builds are handled by CI/CD, the easiest way to run the application locally for development is to use the new quick-start script.\n\n```powershell\n# From the project root\n./scripts/fortuna-quick-start.ps1\n```\n\nThis interactive script will:\n*   Check for all required dependencies (Python, Node.js, etc.).\n*   Install any missing Python or Node packages automatically.\n*   Clear the required network ports (8000 and 3000).\n*   Launch the backend and frontend services in separate, managed terminal windows.\n*   Provide a clean shutdown process.\n\nFor detailed options and first-time setup guidance, run the script with the `-Help` flag:\n```powershell\n./scripts/fortuna-quick-start.ps1 -Help\n```\n\n---\n## \ud83d\udce6 Key Tooling & Scripts\n\n*   **`ARCHIVE_PROJECT.py`**: A utility script that scans the repository and generates the `FORTUNA_ALL_PART*.JSON` archive files. These archives are used as a ground truth for AI-driven code reviews and analysis.\n*   **`AGENTS.md`**: Contains critical operational protocols for the AI agents working on this repository.\n\n---\n\n## \ud83d\udc0d Python Version Requirement\n\n**The Fortuna Monolith application must be built and run with Python 3.10.12.**\n\nThis is due to a dependency (`cefpython3`) that does not support Python 3.11 or newer. The CI/CD workflows are pinned to this version. If you are building the application locally, please ensure you are using a Python 3.10.x environment.\n",
    "USER_GUIDE.MD": "# Fortuna Faucet - User Guide\n\nThis document provides instructions for users of the Fortuna Faucet application.\n\n## Installation\n\n1.  Download the `Fortuna-Faucet-Installer.msi` file from the latest release.\n2.  Run the installer and follow the on-screen instructions.\n3.  The application will be installed to `C:\\Program Files\\Fortuna Faucet` by default.\n\n## Usage\n\n1.  Launch the application from the Start Menu or desktop shortcut.\n2.  The main window will display the live race dashboard.\n3.  Use the settings page to configure your preferences.\n",
    "config.ini": "[analysis]\nqualification_score = 75.0\nfield_size_optimal_min = 4\nfield_size_optimal_max = 6\nfield_size_acceptable_min = 7\nfield_size_acceptable_max = 8\nfield_size_optimal_points = 30\nfield_size_acceptable_points = 10\nfield_size_penalty_points = -20\nfav_odds_points = 30\nmax_fav_odds = 3.5\nsecond_fav_odds_points = 40\nmin_2nd_fav_odds = 4.0\n\n[system]\napi_rate_limit = 60",
    "configure_startup.py": "# configure_startup.py\nimport sys\nimport winreg\nfrom pathlib import Path\n\n\nclass StartupManager:\n    \"\"\"Manage Windows startup registry entries for the current user.\"\"\"\n\n    REGISTRY_PATH = r\"Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Run\"\n    APP_NAME = \"FortunaFaucetTray\"\n\n    @classmethod\n    def is_enabled(cls) -> bool:\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_READ)\n            winreg.QueryValueEx(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            return True\n        except FileNotFoundError:\n            return False\n\n    @classmethod\n    def enable(cls):\n        launcher_path = Path(__file__).parent / \"launcher.ps1\"\n        cmd = f'powershell.exe -WindowStyle Hidden -File \"{launcher_path}\"'\n\n        key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n        winreg.SetValueEx(key, cls.APP_NAME, 0, winreg.REG_SZ, cmd)\n        winreg.CloseKey(key)\n        print(\"Startup enabled.\")\n\n    @classmethod\n    def disable(cls):\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n            winreg.DeleteValue(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            print(\"Startup disabled.\")\n        except FileNotFoundError:\n            print(\"Already disabled.\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        if sys.argv[1] == \"enable\":\n            StartupManager.enable()\n        elif sys.argv[1] == \"disable\":\n            StartupManager.disable()\n        elif sys.argv[1] == \"status\":\n            print(f\"Startup is currently {'enabled' if StartupManager.is_enabled() else 'disabled'}\")\n    else:\n        print(\"Usage: python configure_startup.py [enable|disable|status]\")\n",
    "electron/assets/.gitkeep": "# This directory is for application icons (e.g., icon.ico, tray-icon.png)",
    "electron/preload.js": "// electron/preload.js\n// This script runs in a privileged environment with access to Node.js APIs.\n// It's used to securely expose specific functionality to the renderer process (the web UI).\n\nconst { contextBridge, ipcRenderer } = require('electron');\n\n// Expose a safe, limited API to the frontend.\ncontextBridge.exposeInMainWorld('electronAPI', {\n /**\n * Asynchronously fetches the secure API key from the main process.\n * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n */\n getApiKey: () => ipcRenderer.invoke('get-api-key'),\n\n /**\n * Asynchronously generates and saves a new secure API key.\n * @returns {Promise<string>} A promise that resolves with the newly generated API key.\n */\n generateApiKey: () => ipcRenderer.invoke('generate-api-key'),\n\n /**\n * Asynchronously saves a provided API key.\n * @param {string} apiKey - The API key to save.\n * @returns {Promise<{success: boolean}>} A promise that resolves with the result of the save operation.\n */\n saveApiKey: (apiKey) => ipcRenderer.invoke('save-api-key', apiKey),\n\n /**\n * Asynchronously saves Betfair credentials.\n * @param {{username: string, apiKey: string}} credentials - The credentials to save.\n * @returns {Promise<{success: boolean}>} A promise that resolves with the result of the save operation.\n */\n saveBetfairCredentials: (credentials) => ipcRenderer.invoke('save-betfair-credentials', credentials),\n\n /**\n  * Restarts the backend service.\n  */\n restartBackend: () => ipcRenderer.send('restart-backend'),\n\n /**\n  * Stops the backend service.\n  */\n stopBackend: () => ipcRenderer.send('stop-backend'),\n\n /**\n  * Fetches the current status of the backend service.\n  * @returns {Promise<{state: string, logs: string[]}>} A promise that resolves with the backend status.\n  */\n getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n\n /**\n  * Subscribes to backend status updates.\n  * @param {function(event, {state: string, logs: string[]})} callback - The function to call with status updates.\n  */\n onBackendStatusUpdate: (callback) => {\n    // Deliberately strip event sender from callback to avoid security risks\n    const subscription = (event, ...args) => callback(...args);\n    ipcRenderer.on('backend-status-update', subscription);\n\n    // Return a function to unsubscribe\n    return () => {\n      ipcRenderer.removeListener('backend-status-update', subscription);\n    };\n  },\n\n  /**\n   * Gets the port the backend API is running on.\n   * @returns {Promise<number>} A promise that resolves with the port number.\n   */\n  getApiPort: () => ipcRenderer.invoke('get-api-port'),\n});\n",
    "file_version_info.txt": "# UTF-8\n#\n# For more details about fixed file info 'ffi' see:\\\n# http://msdn.microsoft.com/en-us/library/ms646997.aspx\nVSVersionInfo(\n  ffi=FixedFileInfo(\n    # filevers and prodvers should be always a tuple with four items: (1, 2, 3, 4)\n    # Set not needed items to zero 0.\n    filevers=(1, 0, 0, 0),\n    prodvers=(1, 0, 0, 0),\n    # Contains a bitmask that specifies the valid bits 'flags'r\n    mask=0x3f,\n    # Contains a bitmask that specifies the Boolean attributes of the file.\n    flags=0x0,\n    # The operating system for which this file was designed.\n    # 0x4 - NT and there is no need to change it.\n    OS=0x40004,\n    # The general type of file.\n    # 0x1 - application\n    fileType=0x1,\n    # The function of the file.\n    # 0x0 - the function is not defined for this fileType\n    subtype=0x0,\n    # Creation date and time stamp.\n    date=(0, 0)\n    ),\n  kids=[\n    StringFileInfo(\n      [\n      StringTable(\n        '000004b0',\n        [StringStruct('CompanyName', 'Fortuna Technologies'),\n        StringStruct('FileDescription', 'Fortuna Faucet - Racing Analysis Engine'),\n        StringStruct('FileVersion', '1.0.0.0'),\n        StringStruct('InternalName', 'Fortuna-WebService'),\n        StringStruct('LegalCopyright', '\u00a9 2026 Fortuna Technologies. All rights reserved.'),\n        StringStruct('OriginalFilename', 'Fortuna-WebService.exe'),\n        StringStruct('ProductName', 'Fortuna Faucet'),\n        StringStruct('ProductVersion', '1.0.0.0')])\n      ]),\n    VarFileInfo([VarStruct('Translation', [1033, 1200])])\n  ]\n)",
    "fortuna-webservice.spec": "# -*- mode: python ; coding: utf-8 -*-\n\nimport os\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\nproject_root = Path(SPECPATH).resolve()\n\ndef include_tree(rel_path, target, store):\n    absolute = project_root / rel_path\n    if absolute.exists():\n        store.append((str(absolute), target))\n        print(f\"[spec] Including {absolute} -> {target}\")\n    else:\n        # This spec is used by the legacy build-msi.yml, which checks for these dirs.\n        # If they are missing here, it's a critical error.\n        raise FileNotFoundError(f\"[spec] Required directory not found: {absolute}\")\n\ndatas = []\n# Paths must match the legacy structure used by build-msi.yml\ninclude_tree('python_service/adapters', 'adapters', datas)\ninclude_tree('python_service/data', 'data', datas)\ninclude_tree('python_service/json', 'json', datas)\n\n# Collect library assets\ntry:\n    datas += collect_data_files('uvicorn', includes=['*.html', '*.json'])\n    datas += collect_data_files('structlog', includes=['*.json'])\nexcept Exception as e:\n    print(f\"[spec] Warning: Could not collect library data files: {e}\")\n\n# Collect Hidden Imports for python_service\nhidden_imports = set()\nhidden_imports.update(collect_submodules('python_service'))\nhidden_imports.update([\n    'fastapi', 'uvicorn', 'uvicorn.logging', 'uvicorn.loops.auto', 'uvicorn.lifespan.on',\n    'uvicorn.protocols.http.h11_impl', 'uvicorn.protocols.http.httptools_impl',\n    'uvicorn.protocols.websockets.wsproto_impl', 'uvicorn.protocols.websockets.websockets_impl',\n    'anyio', 'httpcore', 'httpx', 'python_multipart', 'pydantic', 'pydantic_core',\n    'aiosqlite', 'structlog', 'tenacity', 'slowapi'\n])\n\na = Analysis(\n    # FIX: Target service_entry.py instead of main.py\n    ['web_service/backend/service_entry.py'],\n    pathex=[],\n    binaries=[],\n    datas=[('web_service/backend', 'backend')],\n    # FIX: Ensure critical service modules are hidden-imported\n    hiddenimports=['win32timezone', 'win32serviceutil', 'win32service', 'win32event'],\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name='fortuna-webservice', # Name matches the workflow expectation\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=False,\n    runtime_tmpdir=None,\n    console=False,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n",
    "manual_override_tool.py": "import argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Manual Override Tool for Checkmate Data Warehouse.\")\n    parser.add_argument(\"--file\", required=True, help=\"Path to the CSV file for ingestion.\")\n    parser.add_argument(\"--user\", required=True, help=\"The user ID performing the override.\")\n    args = parser.parse_args()\n\n    print(f\"Executing manual override by '{args.user}' for file '{args.file}'...\")\n\n    # 1. Connect to PostgreSQL\n    # engine = create_engine('postgresql://user:password@host:port/database')\n\n    # 2. Read and validate the CSV data\n    # race_df = pd.read_csv(args.file)\n    # ... validation logic ...\n\n    # 3. Add the manual_override_by column\n    # race_df['manual_override_by'] = args.user\n\n    # 4. Insert data into the 'historical_races' table\n    # race_df.to_sql('historical_races', engine, if_exists='append', index=False)\n\n    print(\"Manual override completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "pre-build-check.sh": "#!/bin/bash\n\necho -e \"\\e[36m=== FORTUNA PRE-BUILD VERIFICATION ===\\e[0m\"\n\n# 1. Check all required files exist\necho -e \"\\n\\e[1m[1] Checking required files...\\e[0m\"\nrequired_files=(\n    \"web_service/backend/main.py\"\n    \"web_service/backend/api.py\"\n    \"web_service/backend/config.py\"\n    \"web_service/backend/port_check.py\"\n    \"web_service/backend/requirements.txt\"\n    \"web_service/frontend/package.json\"\n    \"web_service/frontend/next.config.js\"\n    \"fortuna-monolith.spec\"\n)\n\nmissing_files=()\nall_found=true\nfor file in \"${required_files[@]}\"; do\n    if [ -f \"$file\" ]; then\n        echo -e \"  \\e[32m\u2705 $file\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c $file\\e[0m\"\n        missing_files+=(\"$file\")\n        all_found=false\n    fi\ndone\n\nif [ \"$all_found\" = false ]; then\n    echo -e \"\\n\\e[31m\u274c FATAL: Missing files:\\e[0m\"\n    for file in \"${missing_files[@]}\"; do\n        echo \"  - $file\"\n    done\n    exit 1\nfi\n\n# 2. Test Python imports\necho -e \"\\n\\e[1m[2] Testing Python imports...\\e[0m\"\ncat > test_imports.py << EOL\nimport sys\nsys.path.insert(0, '.')\n\ntry:\n    from web_service.backend.api import app\n    print('\u2705 api.app imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import api.app: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.config import get_settings\n    settings = get_settings()\n    print(f'\u2705 config.get_settings imported (host={settings.UVICORN_HOST}, port={settings.FORTUNA_PORT})')\nexcept ImportError as e:\n    print(f'\u274c Failed to import config: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    print('\u2705 port_check.check_port_and_exit_if_in_use imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import port_check: {e}')\n    sys.exit(1)\n\nprint('\u2705 All imports successful')\nEOL\n\npython test_imports.py\nif [ $? -ne 0 ]; then\n    echo -e \"\\e[31m\u274c Import test FAILED\\e[0m\"\n    rm test_imports.py\n    exit 1\nfi\nrm test_imports.py\n\n# 3. Check frontend\necho -e \"\\n\\e[1m[3] Checking frontend...\\e[0m\"\nif [ -f \"web_service/frontend/next.config.js\" ]; then\n    if grep -q \"output: 'export'\" \"web_service/frontend/next.config.js\"; then\n        echo -e \"  \\e[32m\u2705 next.config.js has output: 'export'\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c next.config.js missing output: 'export'\\e[0m\"\n        exit 1\n    fi\nelse\n    echo -e \"  \\e[33m\u26a0\ufe0f  next.config.js will be created during build\\e[0m\"\nfi\n\n# 4. Check spec file\necho -e \"\\n\\e[1m[4] Checking fortuna-monolith.spec...\\e[0m\"\nif [ -f \"fortuna-monolith.spec\" ]; then\n    if grep -q \"SPECPATH\" \"fortuna-monolith.spec\"; then\n        echo -e \"  \\e[32m\u2705 spec uses SPECPATH\\e[0m\"\n    else\n        echo -e \"  \\e[33m\u26a0\ufe0f  spec doesn't use SPECPATH (may have path issues)\\e[0m\"\n    fi\nelse\n    echo -e \"  \\e[31m\u274c fortuna-monolith.spec not found\\e[0m\"\n    exit 1\nfi\n\necho -e \"\\n\\e[32m\u2705 ALL CHECKS PASSED - Safe to build!\\e[0m\"\n",
    "scripts/convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom multiprocessing import Queue\n\n# --- Configuration ---\nMANIFEST_FILES = [\n    \"MANIFEST_PART1_BACKEND.json\",\n    \"MANIFEST_PART2_FRONTEND.json\",\n    \"MANIFEST_PART3_SUPPORT.json\",\n    \"MANIFEST_PART4_ROOT.json\",\n]\nOUTPUT_DIR = \"ReviewableJSON\"\nFILE_PROCESSING_TIMEOUT = 10\nEXCLUDED_FILES = [\"package-lock.json\"]\nMAX_FILE_SIZE_MB = 10  # Max file size in megabytes\n\n\ndef read_json_manifest(manifest_path: str) -> list[str]:\n    \"\"\"Reads a JSON manifest file and returns a list of file paths.\"\"\"\n    try:\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return []\n\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\n\ndef convert_file_to_json_sandboxed(file_path):\n    # --- Pre-flight check: File size ---\n    try:\n        file_size = os.path.getsize(file_path)\n        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:\n            return {\"error\": f\"File exceeds {MAX_FILE_SIZE_MB}MB size limit.\"}\n    except FileNotFoundError:\n        return {\"error\": \"File not found.\"}\n    except Exception as e:\n        return {\"error\": f\"Could not check file size: {e}\"}\n\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    try:\n        if p.is_alive():\n            print(f\"    [WARNING] Process for {file_path} timed out. Attempting graceful termination...\")\n            p.terminate()\n            p.join(timeout=2)  # Give it a moment to terminate gracefully\n\n            if p.is_alive():\n                print(f\"    [ERROR] Graceful termination failed. Forcibly killing process...\")\n                p.kill()  # The ultimate \"just die\"\n                p.join()\n            return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n        if not q.empty():\n            return q.get()\n        return {\"error\": \"Unknown error in sandboxed read process.\"}\n    finally:\n        # \u2705 Properly close and flush the queue\n        try:\n            while not q.empty():\n                q.get_nowait()\n        except Exception:\n            pass\n        q.close()\n        q.join_thread()\n\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'=' * 60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'=' * 60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        paths = read_json_manifest(manifest)\n        if paths:\n            all_local_paths.extend(paths)\n            print(f\"    --> Found {len(paths)} valid file paths.\")\n        else:\n            print(f\"    [WARNING] Manifest not found or is empty: {manifest}\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        if os.path.basename(local_path) in EXCLUDED_FILES:\n            print(f\"\\n--> Skipping excluded file: {local_path}\")\n            failed_count += 1\n            continue\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + \".json\")\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Backup process complete.\")\n    print(f\"Successfully processed: {processed_count}/{len(unique_local_paths)}\")\n    print(f\"Failed/Skipped: {failed_count}\")\n    print(f\"{'=' * 60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/import_test.py": "\nimport sys\nsys.path.insert(0, '.')\ntry:\n    from web_service.backend.api import app\n    print(f'\u2705 app object loaded: {type(app).__name__}')\nexcept Exception as e:\n    print(f'\u274c CRITICAL IMPORT ERROR: {e}')\n    import traceback\n    traceback.print_exc()\n    sys.exit(1)\n",
    "scripts/install_fortuna_silent.bat": "@echo off\nREM Automated deployment (no UI, minimal interaction)\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_silent_install.log\" ^\n    /norestart ^\n    ALLUSERS=1 ^\n    INSTALLSCOPE=perMachine\n\nexit /b %errorlevel%",
    "start_docker.bat": "@echo off\nREM ============================================================\nREM Fortuna Faucet - Docker Launcher for Windows\nREM A simple, friendly way to start your racing analysis engine\nREM ============================================================\n\nsetlocal enabledelayedexpansion\n\nREM Colors and styling\ncls\necho.\necho \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\necho \u2551                                                            \u2551\necho \u2551            \ud83d\udc34  FORTUNA FAUCET LAUNCHER  \ud83d\udc34                \u2551\necho \u2551          Racing Strategy Analysis Engine                  \u2551\necho \u2551                                                            \u2551\necho \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\necho.\n\nREM ============================================================\nREM STEP 1: Check if Docker is installed\nREM ============================================================\necho [1/5] Checking for Docker installation...\ndocker --version >nul 2>&1\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Docker is not installed or not in PATH\n    echo.\n    echo To use Fortuna, you need Docker Desktop:\n    echo https://www.docker.com/products/docker-desktop\n    echo.\n    echo After installing Docker, restart your computer and try again.\n    echo.\n    pause\n    exit /b 1\n)\n\nfor /f \"tokens=*\" %%i in ('docker --version') do set DOCKER_VERSION=%%i\necho \u2713 Found: %DOCKER_VERSION%\necho.\n\nREM ============================================================\nREM STEP 2: Check if Docker daemon is running\nREM ============================================================\necho [2/5] Checking if Docker daemon is running...\ndocker ps >nul 2>&1\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Docker daemon is not running\n    echo.\n    echo Please:\n    echo 1. Open \"Docker Desktop\" from your Start Menu\n    echo 2. Wait 30 seconds for Docker to fully start\n    echo 3. Then run this launcher again\n    echo.\n    pause\n    exit /b 1\n)\necho \u2713 Docker daemon is running\necho.\n\nREM ============================================================\nREM STEP 3: Pull latest Docker image\nREM ============================================================\necho [3/5] Pulling latest Fortuna image from Docker Hub...\necho (This may take a minute on first run)\necho.\ndocker pull masonj0/fortuna-faucet:latest\nif errorlevel 1 (\n    echo.\n    echo \u26a0 Warning: Could not pull from Docker Hub\n    echo Checking for local image...\n    docker image inspect masonj0/fortuna-faucet:latest >nul 2>&1\n    if errorlevel 1 (\n        echo \u2717 ERROR: No local image found\n        echo Please check your internet connection and try again.\n        echo.\n        pause\n        exit /b 1\n    )\n    echo \u2713 Using existing local image\n)\necho \u2713 Image ready\necho.\n\nREM ============================================================\nREM STEP 4: Start container\nREM ============================================================\necho [4/5] Starting Fortuna container...\necho.\n\nREM Stop any existing container (ignore errors)\ndocker stop fortuna-faucet >nul 2>&1\ndocker rm fortuna-faucet >nul 2>&1\n\nREM Create data directories if they don't exist\nif not exist \"data\" mkdir data\nif not exist \"logs\" mkdir logs\n\nREM Start container with proper quoting for paths with spaces\ndocker run -d ^\n  --name fortuna-faucet ^\n  -p 8000:8000 ^\n  -v \"%cd%\\data:/app/web_service/backend/data\" ^\n  -v \"%cd%\\logs:/app/web_service/backend/logs\" ^\n  masonj0/fortuna-faucet:latest\n\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Failed to start container\n    echo.\n    echo Try these troubleshooting steps:\n    echo 1. Open Docker Desktop\n    echo 2. Wait for it to fully start\n    echo 3. Open Command Prompt and run: docker ps\n    echo    (This tests if Docker is working)\n    echo 4. Run this launcher again\n    echo.\n    pause\n    exit /b 1\n)\n\necho \u2713 Container started successfully\necho.\n\nREM ============================================================\nREM STEP 5: Wait and verify startup\nREM ============================================================\necho [5/5] Waiting for application to start...\ntimeout /t 3 /nobreak\n\nREM Check if container is still running\ndocker inspect fortuna-faucet >nul 2>&1\nif errorlevel 1 (\n    echo.\n    echo \u2717 ERROR: Container exited unexpectedly\n    echo.\n    echo Showing container logs for debugging:\n    echo.\n    docker logs fortuna-faucet\n    echo.\n    pause\n    exit /b 1\n)\n\necho \u2713 Application is ready!\necho.\n\nREM ============================================================\nREM SUCCESS - Open browser and show logs\nREM ============================================================\ncls\necho.\necho \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\necho \u2551                                                            \u2551\necho \u2551            \ud83c\udf89  FORTUNA IS RUNNING!  \ud83c\udf89                   \u2551\necho \u2551                                                            \u2551\necho \u2551  Your racing analysis engine is ready at:                \u2551\necho \u2551                                                            \u2551\necho \u2551          http://localhost:8000                            \u2551\necho \u2551                                                            \u2551\necho \u2551  Opening browser now...                                   \u2551\necho \u2551                                                            \u2551\necho \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\necho.\n\nREM Open browser\nstart http://localhost:8000\n\nREM Small delay to let browser open\ntimeout /t 2 /nobreak\n\nREM Show logs\necho.\necho \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\necho \u2502 Live Application Logs (Ctrl+C to stop)                    \u2502\necho \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\necho.\n\ndocker logs -f fortuna-faucet\n\nREM Cleanup on exit\necho.\necho Stopping Fortuna...\ndocker stop fortuna-faucet >nul 2>&1\necho \u2713 Fortuna stopped\n\nexit /b 0\n",
    "tests/__init__.py": "# This file makes the 'tests' directory a package.\n",
    "tests/adapters/test_timeform_adapter_modernized.py": "# Modernized test resurrected from attic/legacy_tests_pre_triage/adapters/test_timeform_adapter.py\nfrom decimal import Decimal\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom python_service.adapters.timeform_adapter import TimeformAdapter\n\n\n@pytest.fixture\ndef timeform_adapter():\n    mock_config = MagicMock()\n    return TimeformAdapter(config=mock_config)\n\n\ndef read_fixture(file_path):\n    with open(file_path, \"r\") as f:\n        return f.read()\n\n\n@pytest.mark.asyncio\nasync def test_timeform_adapter_parses_html_correctly(timeform_adapter):\n    \"\"\"Verify adapter correctly parses a known HTML fixture.\"\"\"\n    mock_html = read_fixture(\"tests/fixtures/timeform_modern_sample.html\")\n\n    # Directly test the parsing of runners from the correct HTML structure\n    from bs4 import BeautifulSoup\n\n    soup = BeautifulSoup(mock_html, \"html.parser\")\n    runners = [timeform_adapter._parse_runner(row) for row in soup.select(\"div.rp-horseTable_mainRow\")]\n\n    assert len(runners) == 3, \"Should parse three runners\"\n\n    braveheart = next((r for r in runners if r.name == \"Braveheart\"), None)\n    assert braveheart is not None\n    assert braveheart.odds[\"Timeform\"].win == Decimal(\"3.5\")\n\n    steady_eddy = next((r for r in runners if r.name == \"Steady Eddy\"), None)\n    assert steady_eddy is not None\n    assert steady_eddy.odds[\"Timeform\"].win == Decimal(\"2.0\")\n",
    "tests/fixtures/timeform_legacy_sample.html": "<!DOCTYPE html><html><body><div class='race-card'><div class='runner'><span class='runner-name'>Braveheart</span><span class='runner-odds'>5/2</span></div><div class='runner'><span class='runner-name'>Speedster</span><span class='runner-odds'>10/1</span></div><div class='runner'><span class='runner-name'>Steady Eddy</span><span class='runner-odds'>EVENS</span></div></div></body></html>",
    "tests/test_api/test_endpoints.py": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom python_service.api import app\n\nclient = TestClient(app)\n\n\n@pytest.mark.asyncio\nasync def test_health_check(client):\n    \"\"\"Tests the unauthenticated /health endpoint.\"\"\"\n    response = await client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"healthy\"\n",
    "tests/test_models.py": "# Test suite for Pydantic models, resurrected from attic/legacy_tests_pre_triage/checkmate_v7/test_models.py\nimport datetime\n\nimport pytest\nfrom pydantic import ValidationError\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\ndef test_runner_model_creation():\n    \"\"\"Tests basic successful creation of the Runner model.\"\"\"\n    from datetime import datetime\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds_data = {\"TestOdds\": OddsData(win=Decimal(\"6.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    runner = Runner(number=5, name=\"Test Horse\", odds=odds_data, scratched=False)\n    assert runner.number == 5\n    assert runner.name == \"Test Horse\"\n    assert not runner.scratched\n\n\ndef test_race_model_with_valid_runners():\n    \"\"\"Tests basic successful creation of the Race model.\"\"\"\n    from datetime import datetime\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    runner1 = Runner(number=1, name=\"A\", odds=odds1, scratched=False)\n    runner2 = Runner(number=2, name=\"B\", odds=odds2, scratched=False)\n    race = Race(\n        id=\"test-race-1\",\n        venue=\"TEST\",\n        race_number=1,\n        start_time=datetime.now(),\n        runners=[runner1, runner2],\n        source=\"test\",\n    )\n    assert race.venue == \"TEST\"\n    assert len(race.runners) == 2\n\n\ndef test_model_validation_fails_on_missing_required_field():\n    \"\"\"Ensures Pydantic's validation fires for missing required fields.\"\"\"\n    with pytest.raises(ValidationError):\n        # 'name' is a required field for a Runner\n        Runner(number=3, odds=\"3/1\", scratched=False)\n\n    with pytest.raises(ValidationError):\n        # 'venue' is a required field for a Race\n        Race(\n            id=\"test-race-2\",\n            race_number=2,\n            start_time=datetime.datetime.now(),\n            runners=[],\n            source=\"test\",\n        )\n",
    "verify_dashboard.py": "\nfrom playwright.sync_api import sync_playwright\n\ndef verify_dashboard(page):\n    \"\"\"\n    Navigates to the dashboard and takes a screenshot.\n    \"\"\"\n    page.goto(\"http://localhost:3000\")\n    page.wait_for_selector(\"text=Fortuna Faucet\")\n    page.screenshot(path=\"verification.png\")\n\nif __name__ == \"__main__\":\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        page = browser.new_page()\n        try:\n            verify_dashboard(page)\n        finally:\n            browser.close()\n",
    "web_platform/frontend/.gitignore": "# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.\n\n# Dependencies\n/node_modules\n/.pnp\n.pnp.js\n\n# Testing\n/coverage\n\n# Next.js\n/.next/\n/out/\n\n# Production\n/build\n\n# Misc\n.DS_Store\n*.pem\n\n# Local .env files\n.env.local\n.env.development.local\n.env.test.local\n.env.production.local\n\n# Log files\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nlerna-debug.log*\n\n# Editor directories and files\n.vscode\n.idea\n*.suo\n*.ntvs*\n*.njsproj\n*.sln\n*.sw?",
    "web_platform/frontend/app/layout.tsx": "// web_platform/frontend/app/layout.tsx\nimport './globals.css';\nimport type { Metadata } from 'next';\nimport { Inter } from 'next/font/google';\nimport Providers from './Providers';\n\nconst inter = Inter({ subsets: ['latin'] });\n\nexport const metadata: Metadata = {\n  title: 'Fortuna',\n  description: 'Real-time horse racing analysis.',\n};\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  return (\n    <html lang=\"en\">\n      <body className={`${inter.className} bg-white text-gray-900 dark:bg-gray-900 dark:text-gray-100`}>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  );\n}",
    "web_platform/frontend/public/sw.js": "if(!self.define){let e,s={};const a=(a,n)=>(a=new URL(a+\".js\",n).href,s[a]||new Promise(s=>{if(\"document\"in self){const e=document.createElement(\"script\");e.src=a,e.onload=s,document.head.appendChild(e)}else e=a,importScripts(a),s()}).then(()=>{let e=s[a];if(!e)throw new Error(`Module ${a} didn\u2019t register its module`);return e}));self.define=(n,t)=>{const i=e||(\"document\"in self?document.currentScript.src:\"\")||location.href;if(s[i])return;let c={};const r=e=>a(e,i),o={module:{uri:i},exports:c,require:r};s[i]=Promise.all(n.map(e=>o[e]||r(e))).then(e=>(t(...e),c))}}define([\"./workbox-4754cb34\"],function(e){\"use strict\";importScripts(),self.skipWaiting(),e.clientsClaim(),e.precacheAndRoute([{url:\"/_next/app-build-manifest.json\",revision:\"b6130f23369e5df052a4061c412f24fa\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_buildManifest.js\",revision:\"c155cce658e53418dec34664328b51ac\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_ssgManifest.js\",revision:\"b6652df95db52feb4daf4eca35380933\"},{url:\"/_next/static/chunks/117-6326cd814d964913.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/816-7254031126ac0a96.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/928.d7f641058b89a54a.js\",revision:\"d7f641058b89a54a\"},{url:\"/_next/static/chunks/app/_not-found/page-e7dc36cd5a340c38.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/layout-605479d07717f01e.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/page-a2c385e93bfc2dac.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/fd9d1056-af804af0be509bea.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/framework-f66176bb897dc684.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-8563e00d234bd632.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-app-e0b3e4e952d25145.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_app-72b849fbd24ac258.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_error-7ba65e1336b92748.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/polyfills-42372ed130431b0a.js\",revision:\"846118c33b2c0e922d7b3a7676f81f6f\"},{url:\"/_next/static/chunks/webpack-d92cdde7bb2319ca.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/css/a55e4893d0564dbf.css\",revision:\"a55e4893d0564dbf\"},{url:\"/_next/static/media/19cfc7226ec3afaa-s.woff2\",revision:\"9dda5cfc9a46f256d0e131bb535e46f8\"},{url:\"/_next/static/media/21350d82a1f187e9-s.woff2\",revision:\"4e2553027f1d60eff32898367dd4d541\"},{url:\"/_next/static/media/8e9860b6e62d6359-s.woff2\",revision:\"01ba6c2a184b8cba08b0d57167664d75\"},{url:\"/_next/static/media/ba9851c3c22cd980-s.woff2\",revision:\"9e494903d6b0ffec1a1e14d34427d44d\"},{url:\"/_next/static/media/c5fe6dc8356a8c31-s.woff2\",revision:\"027a89e9ab733a145db70f09b8a18b42\"},{url:\"/_next/static/media/df0a9ae256c0569c-s.woff2\",revision:\"d54db44de5ccb18886ece2fda72bdfe0\"},{url:\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",revision:\"65850a373e258f1c897a2b3d75eb74de\"},{url:\"/manifest.json\",revision:\"23bffdb04aba9b85948642cffa772eae\"}],{ignoreURLParametersMatching:[]}),e.cleanupOutdatedCaches(),e.registerRoute(\"/\",new e.NetworkFirst({cacheName:\"start-url\",plugins:[{cacheWillUpdate:async({request:e,response:s,event:a,state:n})=>s&&\"opaqueredirect\"===s.type?new Response(s.body,{status:200,statusText:\"OK\",headers:s.headers}):s}]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:gstatic)\\.com\\/.*/i,new e.CacheFirst({cacheName:\"google-fonts-webfonts\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:31536e3})]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:googleapis)\\.com\\/.*/i,new e.StaleWhileRevalidate({cacheName:\"google-fonts-stylesheets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:eot|otf|ttc|ttf|woff|woff2|font.css)$/i,new e.StaleWhileRevalidate({cacheName:\"static-font-assets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:jpg|jpeg|gif|png|svg|ico|webp)$/i,new e.StaleWhileRevalidate({cacheName:\"static-image-assets\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/image\\?url=.+$/i,new e.StaleWhileRevalidate({cacheName:\"next-image\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp3|wav|ogg)$/i,new e.CacheFirst({cacheName:\"static-audio-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp4)$/i,new e.CacheFirst({cacheName:\"static-video-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:js)$/i,new e.StaleWhileRevalidate({cacheName:\"static-js-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:css|less)$/i,new e.StaleWhileRevalidate({cacheName:\"static-style-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/data\\/.+\\/.+\\.json$/i,new e.StaleWhileRevalidate({cacheName:\"next-data\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:json|xml|csv)$/i,new e.NetworkFirst({cacheName:\"static-data-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;const s=e.pathname;return!s.startsWith(\"/api/auth/\")&&!!s.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"apis\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:16,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;return!e.pathname.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"others\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>!(self.origin===e.origin),new e.NetworkFirst({cacheName:\"cross-origin\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:3600})]}),\"GET\")});\n",
    "web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx": "// web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx\nimport dynamic from 'next/dynamic';\n\nconst LiveRaceDashboardNoSSR = dynamic(\n  () => import('./LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  { ssr: false }\n);\n\nexport default LiveRaceDashboardNoSSR;\n",
    "web_platform/frontend/src/components/ScoreBadge.tsx": "'use client';\nimport React from 'react';\n\nconst getScoreStyling = (score: number) => {\n  if (score >= 90) return { bg: 'bg-yellow-400/10', text: 'text-yellow-300', border: 'border-yellow-400' };\n  if (score >= 80) return { bg: 'bg-orange-500/10', text: 'text-orange-400', border: 'border-orange-500' };\n  return { bg: 'bg-sky-500/10', text: 'text-sky-400', border: 'border-sky-500' };\n};\n\nexport const ScoreBadge: React.FC<{ score: number }> = ({ score }) => {\n  const { bg, text } = getScoreStyling(score);\n  return (\n    <div className={`text-right ${text}`}>\n      <p className=\"text-3xl font-bold\">{score.toFixed(1)}</p>\n      <p className=\"text-xs font-medium tracking-wider uppercase\\\">Score</p>\n    </div>\n  );\n};",
    "web_platform/frontend/src/hooks/useRealTimeRaces.ts": "import { useState, useEffect } from 'react';\nimport { io, Socket } from 'socket.io-client';\nimport { Race } from '../types/racing';\n\nconst API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8080';\n\nexport function useRealTimeRaces() {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [isConnected, setIsConnected] = useState(false);\n\n  useEffect(() => {\n    const socket: Socket = io(API_URL);\n\n    socket.on('connect', () => setIsConnected(true));\n    socket.on('disconnect', () => setIsConnected(false));\n\n    socket.on('races_update', (data: { races: Race[] }) => {\n      if (data && Array.isArray(data.races)) {\n        setRaces(data.races);\n      }\n    });\n\n    // Cleanup on component unmount\n    return () => {\n      socket.disconnect();\n    };\n  }, []);\n\n  return { races, isConnected };\n}",
    "web_platform/frontend/src/utils/exportManager.ts": "// web_platform/frontend/src/utils/exportManager.ts\n// import { saveAs } from 'file-saver';\n// import * as XLSX from 'xlsx';\n\nexport class ExportManager {\n  static exportToExcel(races: any[], filename: string = 'fortuna_races') {\n    //\n    // [JULES] - NOTE FOR JB AND AI EXPERTS:\n    // This feature has been temporarily disabled because the external dependency (xlsx)\n    // is hosted on a CDN (cdn.sheetjs.com) that is consistently failing during\n    // the CI/CD build process with 500 Internal Server Errors.\n    //\n    // To ensure the main application build is not blocked, I have commented out\n    // the implementation of this function. The 'xlsx' package remains in package.json,\n    // but this code will not be active until the dependency issue is resolved.\n    //\n\n    // const workbook = XLSX.utils.book_new();\n\n    // const summaryData = [\n    //   ['Total Qualified Races', races.length],\n    //   ['Generated At', new Date().toLocaleString()]\n    // ];\n    // const summarySheet = XLSX.utils.aoa_to_sheet(summaryData);\n    // XLSX.utils.book_append_sheet(workbook, summarySheet, 'Summary');\n\n    // const raceData = races.map(race => ({\n    //   'Venue': race.venue,\n    //   'Race Number': race.race_number,\n    //   'Post Time': new Date(race.start_time).toLocaleString(),\n    //   'Qualification Score': race.qualification_score || 0,\n    //   'Field Size': race.runners.filter(r => !r.scratched).length,\n    //   'Source': race.source\n    // }));\n    // const raceSheet = XLSX.utils.json_to_sheet(raceData);\n    // XLSX.utils.book_append_sheet(workbook, raceSheet, 'Races');\n\n    // XLSX.writeFile(workbook, `${filename}_${Date.now()}.xlsx`);\n    console.warn(\"Excel export is temporarily disabled due to an external dependency issue.\");\n    alert(\"The Excel export feature is temporarily disabled due to an unreliable external dependency. Please try again later.\");\n  }\n}\n",
    "web_service/backend/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass AtTheRacesAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for attheraces.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"AtTheRaces\"\n    BASE_URL = \"https://www.attheraces.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch AtTheRaces index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        all_races = []\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to AtTheRacesAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                header_element = soup.select_one(\"h1.heading-racecard-title\")\n                if not header_element:\n                    continue\n                header = header_element.get_text()\n                track_name_raw, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n                track_name = normalize_venue_name(track_name_raw)\n                active_link = soup.select_one(\"a.race-time-link.active\")\n                race_number = 1\n                if active_link:\n                    parent_div = active_link.find_parent(\"div\", \"races\")\n                    if parent_div:\n                        all_links = parent_div.select(\"a.race-time-link\")\n                        race_number = all_links.index(active_link) + 1\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time, \"%H:%M\").time())\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n                race = Race(\n                    id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from AtTheRaces, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_element = row.select_one(\"h3.horse-name a\")\n            if not name_element:\n                return None\n            name = clean_text(name_element.get_text())\n\n            num_element = row.select_one(\"span.horse-number\")\n            if not num_element:\n                return None\n            num_str = clean_text(num_element.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_element = row.select_one(\"button.best-odds\")\n            odds_str = clean_text(odds_element.get_text()) if odds_element else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on AtTheRaces, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairGreyhounds\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Failed to parse a Betfair Greyhound market.\",\n                    exc_info=True,\n                    market=market,\n                )\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Optional[Race]:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "web_service/backend/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, \"GREYHOUND_API_URL\") or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "web_service/backend/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n# NOTE: This is a hypothetical implementation based on a potential API structure.\n\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for the hypothetical PointsBet Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches all greyhound events for a given date.\"\"\"\n        endpoint = f\"sports/greyhound-racing/events/by-date/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json().get(\"events\", []) if response else None\n\n    def _parse_races(self, raw_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw event data into a list of standardized Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for event in raw_data:\n            try:\n                if not event.get(\"competitors\") or not event.get(\"startTime\"):\n                    continue\n\n                runners = []\n                for competitor in event.get(\"competitors\", []):\n                    price = competitor.get(\"price\")\n                    if not price:\n                        continue\n\n                    odds_val = Decimal(str(price))\n                    odds = {\n                        self.source_name: OddsData(\n                            win=odds_val,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n                    runner = Runner(\n                        number=competitor.get(\"number\", 99),\n                        name=competitor.get(\"name\", \"Unknown\"),\n                        odds=odds,\n                    )\n                    runners.append(runner)\n\n                if runners:\n                    race_id = event.get(\"id\")\n                    if not race_id:\n                        continue\n\n                    race = Race(\n                        id=f\"pbg_{race_id}\",\n                        venue=event.get(\"venue\", {}).get(\"name\", \"Unknown Venue\"),\n                        start_time=datetime.fromisoformat(event[\"startTime\"]),\n                        race_number=event.get(\"raceNumber\", 1),\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.warning(\n                    \"Failed to parse PointsBet Greyhound event.\",\n                    event=event,\n                    exc_info=True,\n                )\n                continue\n        return races\n",
    "web_service/backend/adapters/racingtv_adapter.py": "# python_service/adapters/racingtv_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingTVAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping data from racingtv.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/core/__init__.py": "",
    "web_service/backend/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\n\n# from .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_cls.__name__}\")\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_cls.__name__}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters.append(adapter_instance)\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_cls.__name__,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_cls.__name__}\",\n                        exc_info=True,\n                    )\n\n            # Special case for BetfairDataScientistAdapter with extra args - DISABLED\n            # try:\n            #     bds_adapter = BetfairDataScientistAdapter(\n            #         model_name=\"ThoroughbredModel\",\n            #         url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n            #         config=self.config,\n            #     )\n            #     if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n            #         bds_adapter.enable_manual_override(manual_override_manager)\n            #     self.adapters.append(bds_adapter)\n            # except Exception:\n            #     self.logger.warning(\n            #         \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n            #         exc_info=True,\n            #     )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            races = [Race(**race_data) for race_data in race_data_list]\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached and broadcasted via WebSocket.\n        \"\"\"\n        # Construct a cache key\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        log.info(\"Cache miss for fetch_all_odds\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._fetch_with_semaphore(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for i, result in enumerate(results):\n            adapter = target_adapters[i]\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed with an unhandled exception\", adapter=adapter.source_name, error=result)\n                errors.append({\n                    \"adapter_name\": adapter.source_name,\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                    \"attempted_url\": \"Unknown\"\n                })\n                source_infos.append({\n                    \"name\": adapter.source_name,\n                    \"status\": \"FAILED\",\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                })\n            else:\n                _adapter_name, adapter_result, _duration = result\n                source_info = adapter_result.get(\"source_info\", {})\n                source_infos.append(source_info)\n                if source_info.get(\"status\") == \"SUCCESS\":\n                    all_races.extend(adapter_result.get(\"races\", []))\n                else:\n                    errors.append({\n                        \"adapter_name\": adapter.source_name,\n                        \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                        \"attempted_url\": source_info.get(\"attempted_url\")\n                    })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            errors=errors,\n            source_info=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n                \"total_errors\": len(errors),\n            },\n        )\n\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Set the result in the cache\n        await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "web_service/backend/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return (\n                None,\n                \"Missing core fields (id, venue, race_number, start_time, runners)\",\n            )\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\"\n                        )\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "web_service/backend/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"127.0.0.1\", port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "web_service/backend/middleware/__init__.py": "",
    "web_service/backend/port_check.py": "import socket\nimport sys\n\ndef check_port_and_exit_if_in_use(port: int, host: str):\n    \"\"\"Checks if a port is in use at the given host and exits if it is.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.bind((host, port))\n        except OSError:\n            print(f\"\u274c FATAL: Port {port} is already in use. Please close the other application or specify a different port.\")\n            sys.exit(1)\n",
    "web_service/backend/requirements-x86.txt": "#\n# This file is a modified version of requirements.txt for x86 architecture compatibility.\n# Some packages are pinned to older versions that provide x86 wheels.\n#\naiosqlite==0.17.0\n    # via -r web_service/backend/requirements.in\naltgraph==0.17.4\n    # via pyinstaller\nannotated-types==0.7.0\n    # via pydantic\nanyio==3.7.1\n    # via\n    #   httpx\n    #   starlette\n    #   watchfiles\nasync-timeout==5.0.1\n    # via redis\nbeautifulsoup4==4.12.3\n    # via -r web_service/backend/requirements.in\nblack==24.4.2\n    # via -r web_service/backend/requirements.in\nbuild==1.2.1\n    # via pip-tools\ncertifi==2024.7.4\n    # via\n    #   -r web_service/backend/requirements.in\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==1.16.0\n    # via cryptography\ncharset-normalizer==3.3.2\n    # via requests\nclick==8.1.7\n    # via\n    #   black\n    #   pip-tools\n    #   rich-toolkit\n    #   typer\n    #   uvicorn\ncryptography==42.0.8\n    # via\n    #   -r web_service/backend/requirements.in\n    #   secretstorage\ndeprecated==1.2.14\n    # via limits\ndnspython==2.7.0\n    # via email-validator\nemail-validator==2.3.0\n    # via fastapi\nexceptiongroup==1.3.1\n    # via\n    #   anyio\n    #   pytest\nfastapi==0.111.0\n    # via -r web_service/backend/requirements.in\nfastapi-cli==0.0.20\n    # via fastapi\ngreenlet==1.1.2  # x86 PINNED\n    # via\n    #   -r web_service/backend/requirements.in\n    #   sqlalchemy\nh11==0.14.0\n    # via\n    #   httpcore\n    #   uvicorn\nh2==4.1.0\n    # via httpx\nhpack==4.0.0\n    # via h2\nhttpcore==1.0.5\n    # via httpx\nhttptools==0.7.1\n    # via uvicorn\nhttpx[http2]==0.27.0\n    # via\n    #   -r web_service/backend/requirements.in\n    #   fastapi\nhyperframe==6.0.1\n    # via h2\nidna==3.7\n    # via\n    #   anyio\n    #   email-validator\n    #   httpx\n    #   requests\nimportlib-metadata==8.7.1\n    # via\n    #   build\n    #   keyring\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\niniconfig==2.0.0\n    # via pytest\njaraco-classes==3.4.0\n    # via keyring\njaraco-context==4.3.0\n    # via keyring\njaraco-functools==4.3.0\n    # via keyring\njeepney==0.8.0\n    # via\n    #   keyring\n    #   secretstorage\njinja2==3.1.6\n    # via fastapi\nkeyring==25.2.1\n    # via -r web_service/backend/requirements.in\nlimits==3.14.1\n    # via slowapi\nmarkdown-it-py==3.0.0\n    # via rich\nmarkupsafe==3.0.3\n    # via jinja2\nmdurl==0.1.2\n    # via markdown-it-py\nmore-itertools==10.3.0\n    # via\n    #   jaraco-classes\n    #   jaraco-functools\nmypy-extensions==1.0.0\n    # via black\nnumpy==1.23.5  # x86 PINNED\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pandas\n    #   scipy\norjson==3.11.5\n    # via fastapi\npackaging==24.1\n    # via\n    #   black\n    #   build\n    #   limits\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\n    #   pytest\npandas==1.5.3  # x86 PINNED\n    # via -r web_service/backend/requirements.in\npathspec==0.12.1\n    # via black\npip-tools==7.4.1\n    # via -r web_service/backend/requirements.in\nplatformdirs==4.2.2\n    # via black\npluggy==1.5.0\n    # via pytest\npsutil==5.9.8\n    # via -r web_service/backend/requirements.in\npsycopg2-binary==2.9.9\n    # via -r web_service/backend/requirements.in\npycparser==2.22\n    # via cffi\npydantic==2.8.2\n    # via\n    #   fastapi\n    #   pydantic-settings\npydantic-core==2.20.1\n    # via pydantic\npydantic-settings==2.3.4\n    # via -r web_service/backend/requirements.in\npygments==2.18.0\n    # via rich\npyinstaller==6.5.0\n    # via -r web_service/backend/requirements.in\npyinstaller-hooks-contrib==2024.6\n    # via pyinstaller\npyproject-hooks==1.1.0\n    # via\n    #   build\n    #   pip-tools\npytest==8.2.2\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pytest-asyncio\npytest-asyncio==0.23.7\n    # via -r web_service/backend/requirements.in\npython-dateutil==2.9.0.post0\n    # via pandas\npython-dotenv==1.0.1\n    # via\n    #   pydantic-settings\n    #   uvicorn\npython-multipart==0.0.20\n    # via fastapi\npytz==2024.1\n    # via pandas\npyyaml==6.0.3\n    # via uvicorn\nredis==5.0.6\n    # via -r web_service/backend/requirements.in\nrequests==2.32.5\n    # via -r web_service/backend/requirements.in\nrich==14.2.0\n    # via\n    #   rich-toolkit\n    #   typer\nrich-toolkit==0.17.1\n    # via fastapi-cli\nscipy==1.10.1  # x86 PINNED\n    # via -r web_service/backend/requirements.in\nsecretstorage==3.3.3\n    # via keyring\nselectolax==0.4.0\n    # via -r web_service/backend/requirements.in\nshellingham==1.5.4\n    # via typer\nsix==1.16.0\n    # via python-dateutil\nslowapi==0.1.9\n    # via -r web_service/backend/requirements.in\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nsoupsieve==2.5\n    # via beautifulsoup4\nsqlalchemy==1.4.46  # x86 PINNED\n    # via -r web_service/backend/requirements.in\nstarlette==0.37.2\n    # via fastapi\nstructlog==24.2.0\n    # via -r web_service/backend/requirements.in\ntenacity==8.2.3\n    # via -r web_service/backend/requirements.in\ntomli==2.3.0\n    # via\n    #   black\n    #   build\n    #   fastapi-cli\n    #   pip-tools\n    #   pytest\ntyper==0.21.0\n    # via fastapi-cli\ntyping-extensions==4.12.2\n    # via\n    #   aiosqlite\n    #   black\n    #   exceptiongroup\n    #   fastapi\n    #   limits\n    #   pydantic\n    #   pydantic-core\n    #   rich-toolkit\n    #   starlette\n    #   typer\n    #   uvicorn\nujson==5.11.0\n    # via fastapi\nurllib3==2.6.2\n    # via\n    #   -r web_service/backend/requirements.in\n    #   requests\nuvicorn==0.30.1\n    # via\n    #   -r web_service/backend/requirements.in\n    #   fastapi\n    #   fastapi-cli\nhttptools==0.7.1\n    # via uvicorn\nwebsockets==15.0.1\n    # via uvicorn\nwatchfiles==1.1.1\n    # via uvicorn\nwebsockets==15.0.1\n    # via uvicorn\nwheel==0.43.0\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pip-tools\nwrapt==1.16.0\n    # via deprecated\nzipp==3.23.0\n    # via importlib-metadata\n",
    "web_service/backend/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "web_service/backend/windows_compat.py": "\"\"\"\nWindows Compatibility Utilities\n\nCRITICAL: This module MUST be imported and called at the top of EVERY entry point\nthat uses asyncio or uvicorn in a PyInstaller bundle on Windows.\n\nWithout this fix, the asyncio event loop will fail to bind network ports, causing\nsilent failures where uvicorn reports \"Application startup complete\" but the\nservice is actually inaccessible.\n\"\"\"\n\nimport sys\n\n\ndef setup_windows_event_loop():\n    \"\"\"\n    Configure Windows event loop policy for PyInstaller bundles.\n\n    This MUST be called BEFORE any asyncio or uvicorn initialization.\n\n    Context:\n    - PyInstaller bundles on Windows have a broken default event loop policy\n    - The default policy (ProactorEventLoop) silently fails to bind ports\n    - WindowsSelectorEventLoopPolicy is the only policy that works reliably\n\n    This function is idempotent and safe to call multiple times.\n    \"\"\"\n    if sys.platform == 'win32' and getattr(sys, 'frozen', False):\n        import asyncio\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n        print('[BOOT] \u2713 Applied WindowsSelectorEventLoopPolicy for PyInstaller',\n              file=sys.stderr)\n    else:\n        # Not Windows or not frozen - no action needed\n        pass\n",
    "web_service/frontend/app/components/LiveModeToggle.tsx": "// web_platform/frontend/src/components/LiveModeToggle.tsx\n'use client';\n\nimport React from 'react';\n\ninterface LiveModeToggleProps {\n  isLive: boolean;\n  onToggle: (isLive: boolean) => void;\n  isDisabled: boolean;\n}\n\nexport const LiveModeToggle: React.FC<LiveModeToggleProps> = ({ isLive, onToggle, isDisabled }) => {\n  const handleToggle = () => {\n    if (!isDisabled) {\n      onToggle(!isLive);\n    }\n  };\n\n  return (\n    <button\n      onClick={handleToggle}\n      disabled={isDisabled}\n      className={`relative inline-flex items-center h-8 rounded-full w-32 transition-colors duration-300 ease-in-out focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-offset-slate-800 focus:ring-blue-500 ${\n        isDisabled ? 'cursor-not-allowed bg-slate-700' : 'cursor-pointer'\n      } ${isLive ? 'bg-green-600' : 'bg-slate-600'}`}\n    >\n      <span className=\"sr-only\">Toggle Live Mode</span>\n      <span\n        className={`absolute left-1 top-1 inline-block w-6 h-6 rounded-full bg-white transform transition-transform duration-300 ease-in-out ${\n          isLive ? 'translate-x-[104px]' : 'translate-x-0'\n        }`}\n      />\n      <span\n        className={`absolute left-8 transition-opacity duration-200 ease-in-out ${\n          !isLive && !isDisabled ? 'opacity-100' : 'opacity-50'\n        }`}\n      >\n        Poll\n      </span>\n      <span\n        className={`absolute right-4 transition-opacity duration-200 ease-in-out ${\n          isLive && !isDisabled ? 'opacity-100' : 'opacity-50'\n        }`}\n      >\n        \u26a1 Live\n      </span>\n    </button>\n  );\n};\n",
    "web_service/frontend/app/components/RaceFilters.tsx": "// web_platform/frontend/src/components/RaceFilters.tsx\n'use client';\n\nimport { useState, useCallback } from 'react';\nimport { Settings, RotateCcw } from 'lucide-react';\n\ninterface FilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nexport interface RaceFiltersProps {\n  onParamsChange: (params: FilterParams) => void;\n  isLoading: boolean;\n  refetch: () => void;\n}\n\nconst DEFAULT_PARAMS: FilterParams = {\n  maxFieldSize: 10,\n  minFavoriteOdds: 2.5,\n  minSecondFavoriteOdds: 4.0,\n};\n\nexport function RaceFilters({ onParamsChange, isLoading, refetch }: RaceFiltersProps) {\n  const [params, setParams] = useState<FilterParams>(DEFAULT_PARAMS);\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  // Handle individual parameter changes\n  const handleChange = useCallback((key: keyof FilterParams, value: number) => {\n    setParams(prev => {\n      const updated = { ...prev, [key]: value };\n      onParamsChange(updated);\n      return updated;\n    });\n    // Debounce the refetch call\n    const timer = setTimeout(() => {\n      refetch();\n    }, 500);\n    return () => clearTimeout(timer);\n  }, [onParamsChange, refetch]);\n\n  // Reset to defaults\n  const handleReset = useCallback(() => {\n    setParams(DEFAULT_PARAMS);\n    onParamsChange(DEFAULT_PARAMS);\n    refetch();\n  }, [onParamsChange, refetch]);\n\n  return (\n    <div className=\"bg-gradient-to-r from-slate-800 to-slate-900 rounded-lg p-4 mb-6 border border-slate-700\">\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-2\">\n          <Settings className=\"w-5 h-5 text-amber-500\" />\n          <h3 className=\"text-lg font-semibold text-white\">Race Filters</h3>\n        </div>\n        <button\n          onClick={() => setIsExpanded(!isExpanded)}\n          className=\"text-sm text-slate-400 hover:text-slate-200 transition\"\n        >\n          {isExpanded ? 'Hide' : 'Show'}\n        </button>\n      </div>\n\n      {isExpanded && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 pt-4 border-t border-slate-700\">\n          {/* Max Field Size */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Max Field Size\n              <span className=\"text-amber-500 ml-2\">{params.maxFieldSize}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2\"\n              max=\"20\"\n              value={params.maxFieldSize}\n              onChange={(e) => handleChange('maxFieldSize', parseInt(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Filters races with larger fields</p>\n          </div>\n\n          {/* Min Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"1.5\"\n              max=\"5\"\n              step=\"0.1\"\n              value={params.minFavoriteOdds}\n              onChange={(e) => handleChange('minFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = pickier favorites</p>\n          </div>\n\n          {/* Min Second Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min 2nd Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minSecondFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2.0\"\n              max=\"8\"\n              step=\"0.1\"\n              value={params.minSecondFavoriteOdds}\n              onChange={(e) => handleChange('minSecondFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = better odds separation</p>\n          </div>\n\n          {/* Reset Button */}\n          <div className=\"md:col-span-3 flex justify-end pt-4 border-t border-slate-700\">\n            <button\n              onClick={handleReset}\n              disabled={isLoading}\n              className=\"inline-flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 text-slate-200 rounded text-sm font-medium transition disabled:opacity-50\"\n            >\n              <RotateCcw className=\"w-4 h-4\" />\n              Reset to Defaults\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n}\n",
    "web_service/frontend/app/components/TrifectaFactors.tsx": "// TrifectaFactors.tsx - FINAL, DYNAMIC VERSION\n'use client';\nimport React from 'react';\n\ninterface TrifectaFactorsProps {\n  factorsJson: string | null;\n}\n\nexport function TrifectaFactors({ factorsJson }: TrifectaFactorsProps) {\n  if (!factorsJson) {\n    return <div className=\"text-sm text-gray-500\">No analysis factors available.</div>;\n  }\n\n  try {\n    const factors = JSON.parse(factorsJson);\n    const positiveFactors = Object.entries(factors).filter(([key, value]: [string, any]) => value.ok);\n\n    if (positiveFactors.length === 0) {\n      return <div className=\"text-sm text-gray-500\">No positive factors identified.</div>;\n    }\n\n    return (\n      <div className=\"mt-2 text-xs\">\n        <h4 className=\"font-semibold mb-1\">Key Factors:</h4>\n        <ul className=\"list-disc list-inside space-y-1\">\n          {positiveFactors.map(([key, value]: [string, any]) => (\n            <li key={key} className=\"text-gray-700\">\n              <span className=\"font-medium text-green-600\">\u2713</span> {value.reason} ({value.points > 0 ? `+${value.points}` : value.points} pts)\n            </li>\n          ))}\n        </ul>\n      </div>\n    );\n  } catch (error) {\n    console.error(\"Failed to parse trifecta factors:\", error);\n    return <div className=\"text-sm text-red-500\">Error displaying analysis factors.</div>;\n  }\n}",
    "web_service/frontend/app/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n",
    "web_service/frontend/next-env.d.ts": "/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.\n",
    "web_service/frontend/public/workbox-4754cb34.js": "define([\"exports\"],function(t){\"use strict\";try{self[\"workbox:core:6.5.4\"]&&_()}catch(t){}const e=(t,...e)=>{let s=t;return e.length>0&&(s+=` :: ${JSON.stringify(e)}`),s};class s extends Error{constructor(t,s){super(e(t,s)),this.name=t,this.details=s}}try{self[\"workbox:routing:6.5.4\"]&&_()}catch(t){}const n=t=>t&&\"object\"==typeof t?t:{handle:t};class r{constructor(t,e,s=\"GET\"){this.handler=n(e),this.match=t,this.method=s}setCatchHandler(t){this.catchHandler=n(t)}}class i extends r{constructor(t,e,s){super(({url:e})=>{const s=t.exec(e.href);if(s&&(e.origin===location.origin||0===s.index))return s.slice(1)},e,s)}}class a{constructor(){this.t=new Map,this.i=new Map}get routes(){return this.t}addFetchListener(){self.addEventListener(\"fetch\",t=>{const{request:e}=t,s=this.handleRequest({request:e,event:t});s&&t.respondWith(s)})}addCacheListener(){self.addEventListener(\"message\",t=>{if(t.data&&\"CACHE_URLS\"===t.data.type){const{payload:e}=t.data,s=Promise.all(e.urlsToCache.map(e=>{\"string\"==typeof e&&(e=[e]);const s=new Request(...e);return this.handleRequest({request:s,event:t})}));t.waitUntil(s),t.ports&&t.ports[0]&&s.then(()=>t.ports[0].postMessage(!0))}})}handleRequest({request:t,event:e}){const s=new URL(t.url,location.href);if(!s.protocol.startsWith(\"http\"))return;const n=s.origin===location.origin,{params:r,route:i}=this.findMatchingRoute({event:e,request:t,sameOrigin:n,url:s});let a=i&&i.handler;const o=t.method;if(!a&&this.i.has(o)&&(a=this.i.get(o)),!a)return;let c;try{c=a.handle({url:s,request:t,event:e,params:r})}catch(t){c=Promise.reject(t)}const h=i&&i.catchHandler;return c instanceof Promise&&(this.o||h)&&(c=c.catch(async n=>{if(h)try{return await h.handle({url:s,request:t,event:e,params:r})}catch(t){t instanceof Error&&(n=t)}if(this.o)return this.o.handle({url:s,request:t,event:e});throw n})),c}findMatchingRoute({url:t,sameOrigin:e,request:s,event:n}){const r=this.t.get(s.method)||[];for(const i of r){let r;const a=i.match({url:t,sameOrigin:e,request:s,event:n});if(a)return r=a,(Array.isArray(r)&&0===r.length||a.constructor===Object&&0===Object.keys(a).length||\"boolean\"==typeof a)&&(r=void 0),{route:i,params:r}}return{}}setDefaultHandler(t,e=\"GET\"){this.i.set(e,n(t))}setCatchHandler(t){this.o=n(t)}registerRoute(t){this.t.has(t.method)||this.t.set(t.method,[]),this.t.get(t.method).push(t)}unregisterRoute(t){if(!this.t.has(t.method))throw new s(\"unregister-route-but-not-found-with-method\",{method:t.method});const e=this.t.get(t.method).indexOf(t);if(!(e>-1))throw new s(\"unregister-route-route-not-registered\");this.t.get(t.method).splice(e,1)}}let o;const c=()=>(o||(o=new a,o.addFetchListener(),o.addCacheListener()),o);function h(t,e,n){let a;if(\"string\"==typeof t){const s=new URL(t,location.href);a=new r(({url:t})=>t.href===s.href,e,n)}else if(t instanceof RegExp)a=new i(t,e,n);else if(\"function\"==typeof t)a=new r(t,e,n);else{if(!(t instanceof r))throw new s(\"unsupported-route-type\",{moduleName:\"workbox-routing\",funcName:\"registerRoute\",paramName:\"capture\"});a=t}return c().registerRoute(a),a}try{self[\"workbox:strategies:6.5.4\"]&&_()}catch(t){}const u={cacheWillUpdate:async({response:t})=>200===t.status||0===t.status?t:null},l={googleAnalytics:\"googleAnalytics\",precache:\"precache-v2\",prefix:\"workbox\",runtime:\"runtime\",suffix:\"undefined\"!=typeof registration?registration.scope:\"\"},f=t=>[l.prefix,t,l.suffix].filter(t=>t&&t.length>0).join(\"-\"),w=t=>t||f(l.precache),d=t=>t||f(l.runtime);function p(t,e){const s=new URL(t);for(const t of e)s.searchParams.delete(t);return s.href}class y{constructor(){this.promise=new Promise((t,e)=>{this.resolve=t,this.reject=e})}}const g=new Set;function m(t){return\"string\"==typeof t?new Request(t):t}class v{constructor(t,e){this.h={},Object.assign(this,e),this.event=e.event,this.u=t,this.l=new y,this.p=[],this.m=[...t.plugins],this.v=new Map;for(const t of this.m)this.v.set(t,{});this.event.waitUntil(this.l.promise)}async fetch(t){const{event:e}=this;let n=m(t);if(\"navigate\"===n.mode&&e instanceof FetchEvent&&e.preloadResponse){const t=await e.preloadResponse;if(t)return t}const r=this.hasCallback(\"fetchDidFail\")?n.clone():null;try{for(const t of this.iterateCallbacks(\"requestWillFetch\"))n=await t({request:n.clone(),event:e})}catch(t){if(t instanceof Error)throw new s(\"plugin-error-request-will-fetch\",{thrownErrorMessage:t.message})}const i=n.clone();try{let t;t=await fetch(n,\"navigate\"===n.mode?void 0:this.u.fetchOptions);for(const s of this.iterateCallbacks(\"fetchDidSucceed\"))t=await s({event:e,request:i,response:t});return t}catch(t){throw r&&await this.runCallbacks(\"fetchDidFail\",{error:t,event:e,originalRequest:r.clone(),request:i.clone()}),t}}async fetchAndCachePut(t){const e=await this.fetch(t),s=e.clone();return this.waitUntil(this.cachePut(t,s)),e}async cacheMatch(t){const e=m(t);let s;const{cacheName:n,matchOptions:r}=this.u,i=await this.getCacheKey(e,\"read\"),a=Object.assign(Object.assign({},r),{cacheName:n});s=await caches.match(i,a);for(const t of this.iterateCallbacks(\"cachedResponseWillBeUsed\"))s=await t({cacheName:n,matchOptions:r,cachedResponse:s,request:i,event:this.event})||void 0;return s}async cachePut(t,e){const n=m(t);var r;await(r=0,new Promise(t=>setTimeout(t,r)));const i=await this.getCacheKey(n,\"write\");if(!e)throw new s(\"cache-put-with-no-response\",{url:(a=i.url,new URL(String(a),location.href).href.replace(new RegExp(`^${location.origin}`),\"\"))});var a;const o=await this.R(e);if(!o)return!1;const{cacheName:c,matchOptions:h}=this.u,u=await self.caches.open(c),l=this.hasCallback(\"cacheDidUpdate\"),f=l?await async function(t,e,s,n){const r=p(e.url,s);if(e.url===r)return t.match(e,n);const i=Object.assign(Object.assign({},n),{ignoreSearch:!0}),a=await t.keys(e,i);for(const e of a)if(r===p(e.url,s))return t.match(e,n)}(u,i.clone(),[\"__WB_REVISION__\"],h):null;try{await u.put(i,l?o.clone():o)}catch(t){if(t instanceof Error)throw\"QuotaExceededError\"===t.name&&await async function(){for(const t of g)await t()}(),t}for(const t of this.iterateCallbacks(\"cacheDidUpdate\"))await t({cacheName:c,oldResponse:f,newResponse:o.clone(),request:i,event:this.event});return!0}async getCacheKey(t,e){const s=`${t.url} | ${e}`;if(!this.h[s]){let n=t;for(const t of this.iterateCallbacks(\"cacheKeyWillBeUsed\"))n=m(await t({mode:e,request:n,event:this.event,params:this.params}));this.h[s]=n}return this.h[s]}hasCallback(t){for(const e of this.u.plugins)if(t in e)return!0;return!1}async runCallbacks(t,e){for(const s of this.iterateCallbacks(t))await s(e)}*iterateCallbacks(t){for(const e of this.u.plugins)if(\"function\"==typeof e[t]){const s=this.v.get(e),n=n=>{const r=Object.assign(Object.assign({},n),{state:s});return e[t](r)};yield n}}waitUntil(t){return this.p.push(t),t}async doneWaiting(){let t;for(;t=this.p.shift();)await t}destroy(){this.l.resolve(null)}async R(t){let e=t,s=!1;for(const t of this.iterateCallbacks(\"cacheWillUpdate\"))if(e=await t({request:this.request,response:e,event:this.event})||void 0,s=!0,!e)break;return s||e&&200!==e.status&&(e=void 0),e}}class R{constructor(t={}){this.cacheName=d(t.cacheName),this.plugins=t.plugins||[],this.fetchOptions=t.fetchOptions,this.matchOptions=t.matchOptions}handle(t){const[e]=this.handleAll(t);return e}handleAll(t){t instanceof FetchEvent&&(t={event:t,request:t.request});const e=t.event,s=\"string\"==typeof t.request?new Request(t.request):t.request,n=\"params\"in t?t.params:void 0,r=new v(this,{event:e,request:s,params:n}),i=this.q(r,s,e);return[i,this.D(i,r,s,e)]}async q(t,e,n){let r;await t.runCallbacks(\"handlerWillStart\",{event:n,request:e});try{if(r=await this.U(e,t),!r||\"error\"===r.type)throw new s(\"no-response\",{url:e.url})}catch(s){if(s instanceof Error)for(const i of t.iterateCallbacks(\"handlerDidError\"))if(r=await i({error:s,event:n,request:e}),r)break;if(!r)throw s}for(const s of t.iterateCallbacks(\"handlerWillRespond\"))r=await s({event:n,request:e,response:r});return r}async D(t,e,s,n){let r,i;try{r=await t}catch(i){}try{await e.runCallbacks(\"handlerDidRespond\",{event:n,request:s,response:r}),await e.doneWaiting()}catch(t){t instanceof Error&&(i=t)}if(await e.runCallbacks(\"handlerDidComplete\",{event:n,request:s,response:r,error:i}),e.destroy(),i)throw i}}function b(t){t.then(()=>{})}function q(){return q=Object.assign?Object.assign.bind():function(t){for(var e=1;e<arguments.length;e++){var s=arguments[e];for(var n in s)({}).hasOwnProperty.call(s,n)&&(t[n]=s[n])}return t},q.apply(null,arguments)}let D,U;const x=new WeakMap,L=new WeakMap,I=new WeakMap,C=new WeakMap,E=new WeakMap;let N={get(t,e,s){if(t instanceof IDBTransaction){if(\"done\"===e)return L.get(t);if(\"objectStoreNames\"===e)return t.objectStoreNames||I.get(t);if(\"store\"===e)return s.objectStoreNames[1]?void 0:s.objectStore(s.objectStoreNames[0])}return k(t[e])},set:(t,e,s)=>(t[e]=s,!0),has:(t,e)=>t instanceof IDBTransaction&&(\"done\"===e||\"store\"===e)||e in t};function O(t){return t!==IDBDatabase.prototype.transaction||\"objectStoreNames\"in IDBTransaction.prototype?(U||(U=[IDBCursor.prototype.advance,IDBCursor.prototype.continue,IDBCursor.prototype.continuePrimaryKey])).includes(t)?function(...e){return t.apply(B(this),e),k(x.get(this))}:function(...e){return k(t.apply(B(this),e))}:function(e,...s){const n=t.call(B(this),e,...s);return I.set(n,e.sort?e.sort():[e]),k(n)}}function T(t){return\"function\"==typeof t?O(t):(t instanceof IDBTransaction&&function(t){if(L.has(t))return;const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"complete\",r),t.removeEventListener(\"error\",i),t.removeEventListener(\"abort\",i)},r=()=>{e(),n()},i=()=>{s(t.error||new DOMException(\"AbortError\",\"AbortError\")),n()};t.addEventListener(\"complete\",r),t.addEventListener(\"error\",i),t.addEventListener(\"abort\",i)});L.set(t,e)}(t),e=t,(D||(D=[IDBDatabase,IDBObjectStore,IDBIndex,IDBCursor,IDBTransaction])).some(t=>e instanceof t)?new Proxy(t,N):t);var e}function k(t){if(t instanceof IDBRequest)return function(t){const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"success\",r),t.removeEventListener(\"error\",i)},r=()=>{e(k(t.result)),n()},i=()=>{s(t.error),n()};t.addEventListener(\"success\",r),t.addEventListener(\"error\",i)});return e.then(e=>{e instanceof IDBCursor&&x.set(e,t)}).catch(()=>{}),E.set(e,t),e}(t);if(C.has(t))return C.get(t);const e=T(t);return e!==t&&(C.set(t,e),E.set(e,t)),e}const B=t=>E.get(t);const P=[\"get\",\"getKey\",\"getAll\",\"getAllKeys\",\"count\"],M=[\"put\",\"add\",\"delete\",\"clear\"],W=new Map;function j(t,e){if(!(t instanceof IDBDatabase)||e in t||\"string\"!=typeof e)return;if(W.get(e))return W.get(e);const s=e.replace(/FromIndex$/,\"\"),n=e!==s,r=M.includes(s);if(!(s in(n?IDBIndex:IDBObjectStore).prototype)||!r&&!P.includes(s))return;const i=async function(t,...e){const i=this.transaction(t,r?\"readwrite\":\"readonly\");let a=i.store;return n&&(a=a.index(e.shift())),(await Promise.all([a[s](...e),r&&i.done]))[0]};return W.set(e,i),i}N=(t=>q({},t,{get:(e,s,n)=>j(e,s)||t.get(e,s,n),has:(e,s)=>!!j(e,s)||t.has(e,s)}))(N);try{self[\"workbox:expiration:6.5.4\"]&&_()}catch(t){}const S=\"cache-entries\",K=t=>{const e=new URL(t,location.href);return e.hash=\"\",e.href};class A{constructor(t){this._=null,this.L=t}I(t){const e=t.createObjectStore(S,{keyPath:\"id\"});e.createIndex(\"cacheName\",\"cacheName\",{unique:!1}),e.createIndex(\"timestamp\",\"timestamp\",{unique:!1})}C(t){this.I(t),this.L&&function(t,{blocked:e}={}){const s=indexedDB.deleteDatabase(t);e&&s.addEventListener(\"blocked\",t=>e(t.oldVersion,t)),k(s).then(()=>{})}(this.L)}async setTimestamp(t,e){const s={url:t=K(t),timestamp:e,cacheName:this.L,id:this.N(t)},n=(await this.getDb()).transaction(S,\"readwrite\",{durability:\"relaxed\"});await n.store.put(s),await n.done}async getTimestamp(t){const e=await this.getDb(),s=await e.get(S,this.N(t));return null==s?void 0:s.timestamp}async expireEntries(t,e){const s=await this.getDb();let n=await s.transaction(S).store.index(\"timestamp\").openCursor(null,\"prev\");const r=[];let i=0;for(;n;){const s=n.value;s.cacheName===this.L&&(t&&s.timestamp<t||e&&i>=e?r.push(n.value):i++),n=await n.continue()}const a=[];for(const t of r)await s.delete(S,t.id),a.push(t.url);return a}N(t){return this.L+\"|\"+K(t)}async getDb(){return this._||(this._=await function(t,e,{blocked:s,upgrade:n,blocking:r,terminated:i}={}){const a=indexedDB.open(t,e),o=k(a);return n&&a.addEventListener(\"upgradeneeded\",t=>{n(k(a.result),t.oldVersion,t.newVersion,k(a.transaction),t)}),s&&a.addEventListener(\"blocked\",t=>s(t.oldVersion,t.newVersion,t)),o.then(t=>{i&&t.addEventListener(\"close\",()=>i()),r&&t.addEventListener(\"versionchange\",t=>r(t.oldVersion,t.newVersion,t))}).catch(()=>{}),o}(\"workbox-expiration\",1,{upgrade:this.C.bind(this)})),this._}}class F{constructor(t,e={}){this.O=!1,this.T=!1,this.k=e.maxEntries,this.B=e.maxAgeSeconds,this.P=e.matchOptions,this.L=t,this.M=new A(t)}async expireEntries(){if(this.O)return void(this.T=!0);this.O=!0;const t=this.B?Date.now()-1e3*this.B:0,e=await this.M.expireEntries(t,this.k),s=await self.caches.open(this.L);for(const t of e)await s.delete(t,this.P);this.O=!1,this.T&&(this.T=!1,b(this.expireEntries()))}async updateTimestamp(t){await this.M.setTimestamp(t,Date.now())}async isURLExpired(t){if(this.B){const e=await this.M.getTimestamp(t),s=Date.now()-1e3*this.B;return void 0===e||e<s}return!1}async delete(){this.T=!1,await this.M.expireEntries(1/0)}}try{self[\"workbox:range-requests:6.5.4\"]&&_()}catch(t){}async function H(t,e){try{if(206===e.status)return e;const n=t.headers.get(\"range\");if(!n)throw new s(\"no-range-header\");const r=function(t){const e=t.trim().toLowerCase();if(!e.startsWith(\"bytes=\"))throw new s(\"unit-must-be-bytes\",{normalizedRangeHeader:e});if(e.includes(\",\"))throw new s(\"single-range-only\",{normalizedRangeHeader:e});const n=/(\\d*)-(\\d*)/.exec(e);if(!n||!n[1]&&!n[2])throw new s(\"invalid-range-values\",{normalizedRangeHeader:e});return{start:\"\"===n[1]?void 0:Number(n[1]),end:\"\"===n[2]?void 0:Number(n[2])}}(n),i=await e.blob(),a=function(t,e,n){const r=t.size;if(n&&n>r||e&&e<0)throw new s(\"range-not-satisfiable\",{size:r,end:n,start:e});let i,a;return void 0!==e&&void 0!==n?(i=e,a=n+1):void 0!==e&&void 0===n?(i=e,a=r):void 0!==n&&void 0===e&&(i=r-n,a=r),{start:i,end:a}}(i,r.start,r.end),o=i.slice(a.start,a.end),c=o.size,h=new Response(o,{status:206,statusText:\"Partial Content\",headers:e.headers});return h.headers.set(\"Content-Length\",String(c)),h.headers.set(\"Content-Range\",`bytes ${a.start}-${a.end-1}/${i.size}`),h}catch(t){return new Response(\"\",{status:416,statusText:\"Range Not Satisfiable\"})}}function $(t,e){const s=e();return t.waitUntil(s),s}try{self[\"workbox:precaching:6.5.4\"]&&_()}catch(t){}function z(t){if(!t)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(\"string\"==typeof t){const e=new URL(t,location.href);return{cacheKey:e.href,url:e.href}}const{revision:e,url:n}=t;if(!n)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(!e){const t=new URL(n,location.href);return{cacheKey:t.href,url:t.href}}const r=new URL(n,location.href),i=new URL(n,location.href);return r.searchParams.set(\"__WB_REVISION__\",e),{cacheKey:r.href,url:i.href}}class G{constructor(){this.updatedURLs=[],this.notUpdatedURLs=[],this.handlerWillStart=async({request:t,state:e})=>{e&&(e.originalRequest=t)},this.cachedResponseWillBeUsed=async({event:t,state:e,cachedResponse:s})=>{if(\"install\"===t.type&&e&&e.originalRequest&&e.originalRequest instanceof Request){const t=e.originalRequest.url;s?this.notUpdatedURLs.push(t):this.updatedURLs.push(t)}return s}}}class V{constructor({precacheController:t}){this.cacheKeyWillBeUsed=async({request:t,params:e})=>{const s=(null==e?void 0:e.cacheKey)||this.W.getCacheKeyForURL(t.url);return s?new Request(s,{headers:t.headers}):t},this.W=t}}let J,Q;async function X(t,e){let n=null;if(t.url){n=new URL(t.url).origin}if(n!==self.location.origin)throw new s(\"cross-origin-copy-response\",{origin:n});const r=t.clone(),i={headers:new Headers(r.headers),status:r.status,statusText:r.statusText},a=e?e(i):i,o=function(){if(void 0===J){const t=new Response(\"\");if(\"body\"in t)try{new Response(t.body),J=!0}catch(t){J=!1}J=!1}return J}()?r.body:await r.blob();return new Response(o,a)}class Y extends R{constructor(t={}){t.cacheName=w(t.cacheName),super(t),this.j=!1!==t.fallbackToNetwork,this.plugins.push(Y.copyRedirectedCacheableResponsesPlugin)}async U(t,e){const s=await e.cacheMatch(t);return s||(e.event&&\"install\"===e.event.type?await this.S(t,e):await this.K(t,e))}async K(t,e){let n;const r=e.params||{};if(!this.j)throw new s(\"missing-precache-entry\",{cacheName:this.cacheName,url:t.url});{const s=r.integrity,i=t.integrity,a=!i||i===s;n=await e.fetch(new Request(t,{integrity:\"no-cors\"!==t.mode?i||s:void 0})),s&&a&&\"no-cors\"!==t.mode&&(this.A(),await e.cachePut(t,n.clone()))}return n}async S(t,e){this.A();const n=await e.fetch(t);if(!await e.cachePut(t,n.clone()))throw new s(\"bad-precaching-response\",{url:t.url,status:n.status});return n}A(){let t=null,e=0;for(const[s,n]of this.plugins.entries())n!==Y.copyRedirectedCacheableResponsesPlugin&&(n===Y.defaultPrecacheCacheabilityPlugin&&(t=s),n.cacheWillUpdate&&e++);0===e?this.plugins.push(Y.defaultPrecacheCacheabilityPlugin):e>1&&null!==t&&this.plugins.splice(t,1)}}Y.defaultPrecacheCacheabilityPlugin={cacheWillUpdate:async({response:t})=>!t||t.status>=400?null:t},Y.copyRedirectedCacheableResponsesPlugin={cacheWillUpdate:async({response:t})=>t.redirected?await X(t):t};class Z{constructor({cacheName:t,plugins:e=[],fallbackToNetwork:s=!0}={}){this.F=new Map,this.H=new Map,this.$=new Map,this.u=new Y({cacheName:w(t),plugins:[...e,new V({precacheController:this})],fallbackToNetwork:s}),this.install=this.install.bind(this),this.activate=this.activate.bind(this)}get strategy(){return this.u}precache(t){this.addToCacheList(t),this.G||(self.addEventListener(\"install\",this.install),self.addEventListener(\"activate\",this.activate),this.G=!0)}addToCacheList(t){const e=[];for(const n of t){\"string\"==typeof n?e.push(n):n&&void 0===n.revision&&e.push(n.url);const{cacheKey:t,url:r}=z(n),i=\"string\"!=typeof n&&n.revision?\"reload\":\"default\";if(this.F.has(r)&&this.F.get(r)!==t)throw new s(\"add-to-cache-list-conflicting-entries\",{firstEntry:this.F.get(r),secondEntry:t});if(\"string\"!=typeof n&&n.integrity){if(this.$.has(t)&&this.$.get(t)!==n.integrity)throw new s(\"add-to-cache-list-conflicting-integrities\",{url:r});this.$.set(t,n.integrity)}if(this.F.set(r,t),this.H.set(r,i),e.length>0){const t=`Workbox is precaching URLs without revision info: ${e.join(\", \")}\\nThis is generally NOT safe. Learn more at https://bit.ly/wb-precache`;console.warn(t)}}}install(t){return $(t,async()=>{const e=new G;this.strategy.plugins.push(e);for(const[e,s]of this.F){const n=this.$.get(s),r=this.H.get(e),i=new Request(e,{integrity:n,cache:r,credentials:\"same-origin\"});await Promise.all(this.strategy.handleAll({params:{cacheKey:s},request:i,event:t}))}const{updatedURLs:s,notUpdatedURLs:n}=e;return{updatedURLs:s,notUpdatedURLs:n}})}activate(t){return $(t,async()=>{const t=await self.caches.open(this.strategy.cacheName),e=await t.keys(),s=new Set(this.F.values()),n=[];for(const r of e)s.has(r.url)||(await t.delete(r),n.push(r.url));return{deletedURLs:n}})}getURLsToCacheKeys(){return this.F}getCachedURLs(){return[...this.F.keys()]}getCacheKeyForURL(t){const e=new URL(t,location.href);return this.F.get(e.href)}getIntegrityForCacheKey(t){return this.$.get(t)}async matchPrecache(t){const e=t instanceof Request?t.url:t,s=this.getCacheKeyForURL(e);if(s){return(await self.caches.open(this.strategy.cacheName)).match(s)}}createHandlerBoundToURL(t){const e=this.getCacheKeyForURL(t);if(!e)throw new s(\"non-precached-url\",{url:t});return s=>(s.request=new Request(t),s.params=Object.assign({cacheKey:e},s.params),this.strategy.handle(s))}}const tt=()=>(Q||(Q=new Z),Q);class et extends r{constructor(t,e){super(({request:s})=>{const n=t.getURLsToCacheKeys();for(const r of function*(t,{ignoreURLParametersMatching:e=[/^utm_/,/^fbclid$/],directoryIndex:s=\"index.html\",cleanURLs:n=!0,urlManipulation:r}={}){const i=new URL(t,location.href);i.hash=\"\",yield i.href;const a=function(t,e=[]){for(const s of[...t.searchParams.keys()])e.some(t=>t.test(s))&&t.searchParams.delete(s);return t}(i,e);if(yield a.href,s&&a.pathname.endsWith(\"/\")){const t=new URL(a.href);t.pathname+=s,yield t.href}if(n){const t=new URL(a.href);t.pathname+=\".html\",yield t.href}if(r){const t=r({url:i});for(const e of t)yield e.href}}(s.url,e)){const e=n.get(r);if(e){return{cacheKey:e,integrity:t.getIntegrityForCacheKey(e)}}}},t.strategy)}}t.CacheFirst=class extends R{async U(t,e){let n,r=await e.cacheMatch(t);if(!r)try{r=await e.fetchAndCachePut(t)}catch(t){t instanceof Error&&(n=t)}if(!r)throw new s(\"no-response\",{url:t.url,error:n});return r}},t.ExpirationPlugin=class{constructor(t={}){this.cachedResponseWillBeUsed=async({event:t,request:e,cacheName:s,cachedResponse:n})=>{if(!n)return null;const r=this.V(n),i=this.J(s);b(i.expireEntries());const a=i.updateTimestamp(e.url);if(t)try{t.waitUntil(a)}catch(t){}return r?n:null},this.cacheDidUpdate=async({cacheName:t,request:e})=>{const s=this.J(t);await s.updateTimestamp(e.url),await s.expireEntries()},this.X=t,this.B=t.maxAgeSeconds,this.Y=new Map,t.purgeOnQuotaError&&function(t){g.add(t)}(()=>this.deleteCacheAndMetadata())}J(t){if(t===d())throw new s(\"expire-custom-caches-only\");let e=this.Y.get(t);return e||(e=new F(t,this.X),this.Y.set(t,e)),e}V(t){if(!this.B)return!0;const e=this.Z(t);if(null===e)return!0;return e>=Date.now()-1e3*this.B}Z(t){if(!t.headers.has(\"date\"))return null;const e=t.headers.get(\"date\"),s=new Date(e).getTime();return isNaN(s)?null:s}async deleteCacheAndMetadata(){for(const[t,e]of this.Y)await self.caches.delete(t),await e.delete();this.Y=new Map}},t.NetworkFirst=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u),this.tt=t.networkTimeoutSeconds||0}async U(t,e){const n=[],r=[];let i;if(this.tt){const{id:s,promise:a}=this.et({request:t,logs:n,handler:e});i=s,r.push(a)}const a=this.st({timeoutId:i,request:t,logs:n,handler:e});r.push(a);const o=await e.waitUntil((async()=>await e.waitUntil(Promise.race(r))||await a)());if(!o)throw new s(\"no-response\",{url:t.url});return o}et({request:t,logs:e,handler:s}){let n;return{promise:new Promise(e=>{n=setTimeout(async()=>{e(await s.cacheMatch(t))},1e3*this.tt)}),id:n}}async st({timeoutId:t,request:e,logs:s,handler:n}){let r,i;try{i=await n.fetchAndCachePut(e)}catch(t){t instanceof Error&&(r=t)}return t&&clearTimeout(t),!r&&i||(i=await n.cacheMatch(e)),i}},t.RangeRequestsPlugin=class{constructor(){this.cachedResponseWillBeUsed=async({request:t,cachedResponse:e})=>e&&t.headers.has(\"range\")?await H(t,e):e}},t.StaleWhileRevalidate=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u)}async U(t,e){const n=e.fetchAndCachePut(t).catch(()=>{});e.waitUntil(n);let r,i=await e.cacheMatch(t);if(i);else try{i=await n}catch(t){t instanceof Error&&(r=t)}if(!i)throw new s(\"no-response\",{url:t.url,error:r});return i}},t.cleanupOutdatedCaches=function(){self.addEventListener(\"activate\",t=>{const e=w();t.waitUntil((async(t,e=\"-precache-\")=>{const s=(await self.caches.keys()).filter(s=>s.includes(e)&&s.includes(self.registration.scope)&&s!==t);return await Promise.all(s.map(t=>self.caches.delete(t))),s})(e).then(t=>{}))})},t.clientsClaim=function(){self.addEventListener(\"activate\",()=>self.clients.claim())},t.precacheAndRoute=function(t,e){!function(t){tt().precache(t)}(t),function(t){const e=tt();h(new et(e,t))}(e)},t.registerRoute=h});\n",
    "windows_service.py": "# windows_service.py\nimport os\nimport socket\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n\nclass FortunaBackendService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaFaucetBackend\"\n    _svc_display_name_ = \"Fortuna Faucet Racing Analysis Service\"\n    _svc_description_ = \"Background service for continuous racing data monitoring.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.stop_event = win32event.CreateEvent(None, 0, 0, None)\n        self.backend_process = None\n        socket.setdefaulttimeout(60)\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        win32event.SetEvent(self.stop_event)\n        if self.backend_process:\n            self.backend_process.terminate()\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        install_dir = Path(__file__).parent.resolve()\n        venv_python = install_dir / \".venv\" / \"Scripts\" / \"python.exe\"\n        api_module_dir = install_dir / \"python_service\"\n\n        env = os.environ.copy()\n        env_file = install_dir / \".env\"\n        if env_file.exists():\n            with open(env_file) as f:\n                for line in f:\n                    if \"=\" in line and not line.startswith(\"#\"):\n                        key, value = line.strip().split(\"=\", 1)\n                        env[key] = value.strip('\"')\n\n        self.backend_process = subprocess.Popen(\n            [\n                str(venv_python),\n                \"-m\",\n                \"uvicorn\",\n                \"api:app\",\n                \"--host\",\n                \"127.0.0.1\",\n                \"--port\",\n                \"8000\",\n            ],\n            cwd=str(api_module_dir),\n            env=env,\n        )\n\n        win32event.WaitForSingleObject(self.stop_event, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaBackendService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaBackendService)\n"
}