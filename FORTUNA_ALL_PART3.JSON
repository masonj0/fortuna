{
    ".github/actions/setup/action.yml": "name: 'Composite Setup Action'\ndescription: 'Checks out repo, sets up Node.js and Python'\ninputs:\n  architecture:\n    description: 'The architecture to set up Python for (x86, x64)'\n    required: false\n    default: 'x64'\nruns:\n  using: \"composite\"\n  steps:\n    - name: \ud83d\udce5 Checkout Repository\n      uses: actions/checkout@v4\n\n    - name: \ud83d\udce6 Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ env.NODE_VERSION }}\n        cache: 'npm'\n        cache-dependency-path: '**/package-lock.json'\n\n    - name: \ud83d\udc0d Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n        architecture: ${{ inputs.architecture }}\n        cache: 'pip'\n",
    ".github/workflows/codeql.yml": "# System Timestamp: 2025-12-24 18:00:00\n# HELPFUL HINT: Configure GitHub Branch Protection rules to require this workflow to pass before merging to main.\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n  schedule:\n    - cron: '40 7 * * 4'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: ['javascript', 'python' ]\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Setup Python\n      if: matrix.language == 'python'\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.11'\n\n    - name: Setup Node.js\n      if: matrix.language == 'javascript'\n      uses: actions/setup-node@v4\n      with:\n        node-version: '20'\n\n    - name: Install Python dependencies\n      if: matrix.language == 'python'\n      run: |\n        python -m pip install uv\n        uv pip install --system -r web_service/backend/requirements-dev.txt\n    - name: Install JavaScript dependencies\n      if: matrix.language == 'javascript'\n      run: |\n        npm install --prefix web_service/frontend\n        npm install --prefix electron\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v4\n      with:\n        languages: ${{ matrix.language }}\n\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v4\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v4\n",
    "ARCHITECTURAL_MANDATE.md": "# Fortuna Faucet - Architectural Mandate (v3.0)\n\nThis document codifies the architectural laws and philosophical principles that govern the Fortuna Faucet kingdom. Adherence to this mandate is non-negotiable for all development.\n\n---\n\n## The Prime Directive: A Professional, Resilient System\n\nThe ultimate goal of this project is to be a professional-grade, A+ intelligence engine. This is achieved through three core pillars:\n\n1.  **Rigid Standardization:** Code should be consistent and predictable. Shared logic must be centralized. Common patterns must be enforced, not merely suggested.\n2.  **Resilience Engineering:** The system must be self-healing and gracefully handle the failure of its individual components. We do not simply handle errors; we build a system that anticipates and survives them.\n3.  **Developer Clarity:** The codebase must be easy to understand, maintain, and extend. Code should be self-documenting, and its intent should be obvious.\n\n---\n\n## The Law of the Adapters: The `BaseAdapterV3` Pattern\n\nAll new data adapters **MUST** inherit from the `BaseAdapterV3` abstract base class. This is the cornerstone of our standardization and resilience strategy.\n\nThe `BaseAdapterV3` enforces a strict separation of concerns:\n\n1.  **`_fetch_data(self, date)` -> `Any`:** This method's **only** responsibility is to perform network operations and retrieve raw data (e.g., HTML, JSON). It should contain no parsing logic.\n2.  **`_parse_races(self, raw_data)` -> `list[Race]`:** This method's **only** responsibility is to parse the raw data provided by `_fetch_data` into a list of `Race` objects. It must be a pure function with no side effects or network calls.\n\nThe public-facing `get_races()` method is provided by the base class and **MUST NOT** be overridden. It orchestrates the fetch-then-parse pipeline, ensuring that all adapters behave identically from the engine's perspective.\n\nThis pattern guarantees that every adapter in our fleet is consistent, predictable, and easy to test.\n\n---\n\n## The Law of the Engine: Orchestrate, Don't Participate\n\nThe `OddsEngine` is the central orchestrator. Its responsibilities are:\n\n-   To manage the fleet of active adapters.\n-   To execute all adapter fetches in parallel.\n-   To gracefully handle the failure of any individual adapter without halting the entire process.\n-   To perform the deduplication and merging of race data from multiple sources.\n-   To manage the caching layer (Redis).\n\nThe engine should remain agnostic to the internal workings of any specific adapter. It interacts only with the standardized interface provided by `BaseAdapterV3`.\n\n---\n\n## The Law of the Core Texts: Maintain the Truth\n\nThe project's core documentation is not optional. It is the living memory and strategic guide of the kingdom.\n\n-   **`ROADMAP_APPENDICES.MD`:** The Grand Strategy must be kept current. Completed objectives must be marked as such.\n-   **`HISTORY.MD`:** Significant architectural shifts and completed campaigns must be chronicled.\n-   **`PSEUDOCODE.MD`:** The architectural blueprint must be updated to reflect major changes to the system's design.\n-   **Manifests (`MANIFEST*.md`):** All new files must be added to the appropriate manifest to ensure the integrity of the archival system.\n\n\n---\n\n## The Final Law: The Law of the True Scribe\n\n**Effective Date:** 2025-10-15\n\n**Verdict:** The system of manually maintained manifest files (`MANIFEST.md`, `MANIFEST2.md`, `MANIFEST3.md`) is hereby declared a catastrophic failure and is **permanently deprecated**.\n\n**The New Law:** The one and only method for generating the project's `FORTUNA_ALL` archives is the `ARCHIVE_PROJECT.py` script. This 'True Scribe' is the single, automated source of truth. It programmatically scans and categorizes the entire kingdom, ensuring a perfect, complete, and uncorrupted archive is generated every time.\n\nAll previous archival scripts (`create_fortuna_json.py`, `MANAGE_MANIFESTS.py`) are not to be used under any circumstances.",
    "Dockerfile.tinyfield": "# TinyField Variant - Static Frontend + Python Backend\nFROM python:3.10.11-slim as backend-builder\n\nWORKDIR /build\nCOPY web_service/backend/requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Runtime stage\nFROM python:3.10.11-slim\n\nWORKDIR /app\n\n# Install only runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*\n\n# Copy Python packages\nCOPY --from=backend-builder /root/.local /root/.local\n\n# Copy application code, preserving directory structure\nCOPY web_service /app/web_service\n\n# Create TinyField data directories\nRUN mkdir -p /app/web_service/backend/data /app/web_service/backend/json /app/web_service/backend/logs\n\n# Set environment\nENV PATH=/root/.local/bin:$PATH\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Health check\nHEALTHCHECK --interval=10s --timeout=5s --start-period=30s --retries=3 \\\n    CMD curl -f http://localhost:8000/api/health || exit 1\n\nEXPOSE 8000\n\n# Start backend (serves both API and frontend)\nCMD [\"python\", \"-m\", \"uvicorn\", \"web_service.backend.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "PSEUDOCODE2026.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Unified Monolith Architecture\n**Version:** 3.0.0\n**Last Updated:** January 13, 2026\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture: The Unified Monolith\n3.  Backend Engine (Python/FastAPI)\n4.  Frontend Interface (TypeScript/Next.js)\n5.  Data Models & API Specification\n6.  Deployment & Automation (CI/CD)\n7.  End-to-End Workflow\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551      Unified Monolith Architecture for Cross-Platform Use      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve a static frontend and a REST API from a single, self-contained executable.\n  \u2022 Provide cross-platform access via launcher scripts for a Podman container.\n\nCORE TENETS:\n  \u2022 Single Origin: The backend serves the frontend, eliminating all CORS and cross-origin security issues.\n  \u2022 Zero Dependencies: The primary Windows artifact is a single .exe file that requires no external installation or setup.\n  \u2022 Containerization: For non-Windows users, a Podman container provides a consistent, isolated environment.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline (`build-monolith.yml`).\n```\n\n---\n\n## 2. ARCHITECTURE: THE UNIFIED MONOLITH\n\nThe application is a **Unified Monolith**. A single Python executable, built with PyInstaller, contains the complete application. It runs a FastAPI web server that both serves the static Next.js frontend and provides the backend REST API.\n\n```\n+------------------------------------------------------+\n| fortuna-monolith.exe (Single Executable)             |\n|                                                      |\n|  +-------------------------------------------------+ |\n|  | Python Environment                              | |\n|  |                                                 | |\n|  |  +-------------------------------------------+  | |\n|  |  | FastAPI / Uvicorn Server (localhost:8000) |  | |\n|  |  |                                           |  | |\n|  |  |  +------------------+  +----------------+ |  | |\n|  |  |  | API Router       |  | Static Files   | |  | |\n|  |  |  | (/api/*)         |  | (/, /_next/*)  | |  | |\n|  |  |  +------------------+  +----------------+ |  | |\n|  |  |           |                     |         |  | |\n|  |  +-----------|---------------------|---------+  | |\n|  |             |                     |            | |\n|  |  +----------v-----------+  +-------v--------+   | |\n|  |  | OddsEngine (engine.py)|  | Bundled Next.js|   | |\n|  |  +----------------------+  | 'out' directory|   | |\n|  |                            +----------------+   | |\n|  +-------------------------------------------------+ |\n|                                                      |\n+------------------------------------------------------+\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON/FASTAPI)\n\n### 3.1 Entry Point & Server Startup (`web_service/backend/main.py`)\n\nThe entry point directly runs the Uvicorn server, loading the FastAPI app instance from `api.py`.\n\n```pseudocode\n// main.py\nPROCEDURE Main_Python_Entry_Point\n  // Path modifications for PyInstaller compatibility\n  IF running in a frozen (PyInstaller) environment:\n    base_path <- sys._MEIPASS\n  ELSE:\n    base_path <- path to project root\n  ADD base_path to sys.path\n\n  // Programmatically launch the FastAPI application\n  CALL uvicorn.run(\n    app=\"web_service.backend.api:app\",\n    host=\"0.0.0.0\",\n    port=8000\n  )\nEND PROCEDURE\n```\n\n### 3.2 Application & Frontend Serving (`web_service/backend/api.py`)\n\nThe `api.py` file is the core of the backend. It defines the FastAPI app, manages the application lifecycle, and includes the critical logic for serving the static frontend.\n\n```pseudocode\n// api.py\n\n// --- Lifespan Management ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // ON STARTUP:\n  CONFIGURE logging\n  CREATE OddsEngine instance\n  STORE engine in app.state\n  YIELD\n  // ON SHUTDOWN:\n  AWAIT app.state.engine.close() // Gracefully close connections\n\n// --- FastAPI App Initialization ---\napp <- CREATE FastAPI(lifespan=lifespan_manager)\nADD CORS middleware\nADD Rate Limiting middleware\nINCLUDE API router (for /api/* routes)\n\n// --- CRITICAL: Unified Frontend Serving ---\nFUNCTION get_ui_directory():\n  IF running in a frozen (PyInstaller) environment:\n    RETURN path to bundled 'ui' directory (sys._MEIPASS/ui)\n  ELSE:\n    RETURN path to `web_platform/frontend/out`\nEND FUNCTION\n\nUI_DIR <- get_ui_directory()\nVERIFY UI_DIR and index.html exist, otherwise RAISE RuntimeError\n\n// --- SPA Middleware ---\n// All requests that are not for '/api/*' or a known static file type\n// will be served the `index.html` file. This allows the Next.js\n// client-side router to handle all frontend navigation.\napp.add_middleware(SPAMiddleware)\n\n// --- Static File Mount ---\n// This serves the actual .js, .css, and image files from the\n// bundled 'ui' directory.\napp.mount(\"/\", StaticFiles(directory=UI_DIR, html=True), name=\"ui\")\n```\n\n### 3.3 Resilient Fetching Strategy (`web_service/backend/engine.py`)\n\nThe `OddsEngine` is designed for resilience, using a multi-tiered fallback strategy to ensure data is returned even when some sources fail.\n\n```pseudocode\n// engine.py\n\nCLASS OddsEngine:\n  INIT():\n    self.adapters: Dict[str, Adapter]\n    self.health_monitor: AdapterHealthMonitor\n    self.stale_data_cache: StaleDataCache\n    // ... other initializations\n\n  ASYNC FUNCTION fetch_all_odds(date):\n    // Tier 1: Attempt to fetch from healthy adapters\n    healthy_adapters <- self.health_monitor.get_healthy_adapters()\n    live_results <- FETCH_IN_PARALLEL(healthy_adapters)\n\n    IF count(live_results) >= MINIMUM_REQUIRED_SOURCES:\n      MERGE and RETURN live_results\n\n    // Tier 2: Augment with degraded adapters if necessary\n    degraded_adapters <- self.health_monitor.get_degraded_adapters()\n    degraded_results <- FETCH_IN_PARALLEL(degraded_adapters)\n    live_results.extend(degraded_results)\n\n    IF count(live_results) > 0:\n      MERGE, CACHE, and RETURN live_results\n\n    // Tier 3: Fall back to stale data from the last successful run\n    stale_data <- self.stale_data_cache.get(date)\n    IF stale_data IS NOT NULL:\n      LOG \"Using stale data as a last resort.\"\n      RETURN stale_data with a 'stale' freshness flag\n\n    // Final fallback: Return an error response\n    RETURN error_response(\"All live adapters failed and no stale cache was available.\")\n\n```\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/NEXT.JS)\n\n### 4.1 Configuration (`next.config.mjs`)\n\nThe frontend is a standard Next.js application configured for static export. This means it is pre-built into a set of HTML, CSS, and JS files that can be served by any static web server.\n\n```javascript\n// next.config.mjs\nconst nextConfig = {\n  output: 'export',   // CRITICAL: Generates a static site in the 'out' directory\n  distDir: 'out',\n  images: { unoptimized: true }, // Required for static export\n  trailingSlash: true,\n};\n```\n\n### 4.2 Main Dashboard Component (`src/components/LiveRaceDashboard.tsx`)\n\nWith the unified architecture, the frontend no longer needs a complex IPC mechanism. It behaves like a standard web application, making HTTP requests to the same origin that served it.\n\n```pseudocode\n// LiveRaceDashboard.tsx (Simplified)\nCOMPONENT LiveRaceDashboard:\n  STATE:\n    races: Race[] <- []\n    status: 'loading' | 'success' | 'error' <- 'loading'\n    errorMessage: string <- \"\"\n\n  EFFECT on mount:\n    fetchQualifiedRaces() // Fetch data immediately on component load\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard, same-origin API call. No full URL needed.\n      response <- AWAIT fetch(\"/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API Error: ${response.statusText}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n      setStatus('success')\n    CATCH e:\n      setStatus('error')\n      setErrorMessage(e.message)\nEND COMPONENT\n```\n\n---\n\n## 5. DATA MODELS & API SPECIFICATION\n\nThe data models and API endpoints remain largely the same, with the key difference being that they are all served from the `localhost:8000` origin.\n\n```\nENDPOINT GET /api/health\n  Response (200 OK): {\"status\":\"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 6. DEPLOYMENT & AUTOMATION (CI/CD)\n\nThe primary build workflow is `.github/workflows/build-monolith.yml`. It creates the single Windows executable. The `.github/workflows/build-podman.yml` workflow provides a container-based alternative for cross-platform use.\n\n### 6.1 Monolith Build (`build-monolith.yml`)\n\n```pseudocode\n// build-monolith.yml\nWORKFLOW Build_Fortuna_Monolith_EXE:\n  // Phase 1: Build Frontend\n  SETUP Node.js\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY the 'out' directory to a staging area (`frontend_dist`)\n\n  // Phase 2: Build Backend Executable\n  SETUP Python\n  INSTALL Python dependencies from requirements.txt\n  INSTALL PyInstaller\n  CREATE required data/log directories\n\n  // Phase 3: Package with PyInstaller\n  // The spec file (`fortuna-monolith.spec`) is configured to:\n  // 1. Identify `web_service/backend/main.py` as the entry point.\n  // 2. Bundle the `frontend_dist` directory into the .exe at the root 'ui'.\n  // 3. Add application icon and version info.\n  EXECUTE PyInstaller using `fortuna-monolith.spec`\n\n  // Phase 4: Smoke Test\n  START the generated `fortuna-monolith.exe` in the background\n  POLL `http://127.0.0.1:8000/api/health` until it responds with 200 OK or times out\n  IF timeout THEN FAIL build\n  KILL the process\n\n  // Phase 5: Upload Artifact\n  UPLOAD the `fortuna-monolith.exe` as a build artifact\n```\n\n### 6.2 Race Report Generation (`unified-race-report.yml`)\n\nThis workflow runs on a schedule or manually to generate the daily race reports.\n\n```pseudocode\n// unified-race-report.yml\nWORKFLOW Generate_Race_Report:\n  // Phase 1: Setup Environment\n  SETUP Python\n  INSTALL dependencies from requirements.txt\n\n  // Phase 2: Run Reporter Script\n  // The script directly invokes the OddsEngine and AnalyzerEngine\n  // without needing a live web server.\n  EXECUTE `scripts/fortuna_reporter.py`\n\n  // Phase 3: Publish Artifacts\n  // The script generates a comprehensive set of artifacts for observability.\n  UPLOAD the following files:\n    - race-report.html (The primary user-facing report)\n    - qualified_races.json (Data for the HTML report)\n    - raw_race_data.json (Unfiltered data for debugging)\n    - reporter_output.log (Full log of the reporter script)\n    - github_summary.md (For display in the GitHub Actions UI)\n```\n---\n\n## 7. END-TO-END WORKFLOW\n\n### 7.1 Windows User Workflow\n\nThe user downloads and runs a single `.exe` file.\n\n```\nWORKFLOW user_launches_monolith_exe:\n  STEP 1: User executes `fortuna-monolith.exe`.\n  STEP 2: The embedded Python environment starts.\n  STEP 3: The Uvicorn server starts inside the process.\n  STEP 4: The FastAPI application initializes, mounts the bundled 'ui' directory, and opens the API endpoints.\n  STEP 5: The user's default web browser is automatically opened to `http://127.0.0.1:8000`.\n  STEP 6: The browser loads `index.html` from the FastAPI server.\n  STEP 7: The Next.js application hydrates and makes same-origin API calls to `/api/*` to fetch data.\n```\n\n### 7.2 Cross-Platform (Podman) User Workflow\n\nThe user runs a launcher script that manages a Podman container.\n\n```\nWORKFLOW user_launches_podman_script:\n  STEP 1: User executes `launcher.bat` or `./launcher.sh`.\n  STEP 2: The script pulls the latest `ghcr.io/masonj0/fortuna-faucet` image.\n  STEP 3: The script starts a Podman container, mapping port 8000 and volume mounting local `data` and `logs` directories.\n  STEP 4: The container runs the same Python application, which starts the Uvicorn/FastAPI server.\n  STEP 5: The user's default web browser is automatically opened to `http://127.0.0.1:8000`.\n  STEP 6: The workflow proceeds identically to the Windows user workflow from Step 6 onward.\n```\n\n---\n*This concludes the blueprint for the Fortuna Faucet unified monolith architecture.*\n",
    "ROADMAP_APPENDICES.md": "# \ud83d\uddfa\ufe0f Fortuna Faucet - Roadmap & Accomplishments\n\nThis document tracks the strategic evolution of the Fortuna Faucet project.\n\n## Phase 1: Core Engine Development (Complete)\n- **Objective:** Build a robust, scalable data extraction and analysis engine.\n- **Status:** COMPLETE.\n\n## Phase 2: The Golden Path - UX Overhaul (Complete)\n- **Objective:** Transform the developer-centric tool into a seamless, professional-grade Windows application for non-technical users.\n- **Status:** COMPLETE.\n\n## Phase 3: The Turnkey Solution - Professional Release Pipeline (Complete)\n- **Objective:** Eliminate all manual setup steps and create an enterprise-grade, automated build and release system.\n- **Status:** COMPLETE.\n\n### Key Accomplishments & Completed Operations:\n\n1.  **Operation: The Great Housekeeping**\n    - Purified the repository architecture, deprecated legacy codebases and scripts, and established a clean foundation.\n    - Forged a new, programmatic manifest generation system.\n\n2.  **Operation: The Blueprint**\n    - Established the professional directory structure for an enterprise-grade build system.\n    - Implemented the master WiX product definition and the PowerShell build orchestrator.\n\n3.  **Operation: The Assembly Line**\n    - Fully automated the MSI build process with a GitHub Actions CI/CD workflow.\n\n4.  **Operation: The Proving Ground**\n    - Forged a complete suite of automated PowerShell scripts to test and validate the integrity of every installer artifact (install, silent deploy, uninstall).\n\n5.  **Operation: The User's Keys**\n    - Created the final, user-facing toolkit of scripts for easy lifecycle management (install, uninstall, repair).\n\n6.  **Operation: Modernize the Assembly Line**\n    - Performed a surgical upgrade to the CI/CD pipeline to resolve a critical GitHub Actions deprecation, ensuring continued operational readiness.\n\n7.  **Operation: The Forge**\n    - Executed a critical architectural overhaul of the entire release pipeline.\n    - Replaced the fragile, runtime-dependent installer with a robust \"Three-Executable Architecture.\"\n    - The Python backend is now a standalone executable compiled with PyInstaller, and the frontend is a static export, eliminating all runtime dependencies and post-install scripting.\n\n## Phase 4: User Experience & Feature Enhancement (Next Steps)\n- **Objective:** Enhance the core user experience and expand the analytical capabilities of the engine.\n- **Status:** PENDING.\n- **Potential Missions:**\n  - **Operation: The Interpreter:** Implement a user-friendly error-handling system that translates technical errors into simple, actionable advice.\n  - **Data Persistence & Caching:** Implement a local SQLite database to cache race data, improving performance and enabling offline access.\n  - **Operation: The Polisher:** Address technical debt by refactoring backend code to resolve deprecation warnings and align with modern library standards.\n  - **Operation: The Shield:** Improve backend test coverage by adding unit tests for untested data adapters and the Electron main process.\n  - **Operation: The Auditor (Real-Time Verification)**\n    - **Goal:** Implement a 'Last Hour' retrospective dashboard to validate the 'Favorite to Place' strategy.\n    - **Core Logic:**\n      1.  **Snapshot:** Log every 'Qualifier' race prediction to a local DB (SQLite/Redis) with a timestamp and the predicted Favorite.\n      2.  **The Fetcher:** Periodically poll for official results of races that finished in the last 60 minutes.\n      3.  **The Verdict:** Compare the predicted Favorite against the official Finish Order.\n          - *Win:* Did it finish 1st, 2nd, (or 3rd)? -> CASHED.\n          - *Loss:* Did it finish out of the money? -> BURNED.\n      4.  **The 'Tiny Profit' Calc:** Calculate Net Profit based on the standard $2.00 tote unit.\n          - *Formula:* `Net Profit = (Official_Place_Payout - 2.00)`\n          - *Example:* Payout $2.60 -> Profit +$0.60. Payout $0.00 -> Profit -$2.00.\n    - **Data Sources:**\n      - *US Racing:* Scrape `https://www.equibase.com/static/chart/summary/index.html` (Free Summary Charts contain Final Odds & Payoffs).\n      - *UK/Dogs:* Use `The Racing API` or `GBGB` results endpoints.\n    - **UI:** Display a rolling 'Strike Rate' % and 'Net Profit (1H)' ticker.\n",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2)---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "configure_startup.py": "# configure_startup.py\nimport sys\nimport winreg\nfrom pathlib import Path\n\n\nclass StartupManager:\n    \"\"\"Manage Windows startup registry entries for the current user.\"\"\"\n\n    REGISTRY_PATH = r\"Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Run\"\n    APP_NAME = \"FortunaFaucetTray\"\n\n    @classmethod\n    def is_enabled(cls) -> bool:\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_READ)\n            winreg.QueryValueEx(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            return True\n        except FileNotFoundError:\n            return False\n\n    @classmethod\n    def enable(cls):\n        launcher_path = Path(__file__).parent / \"launcher.ps1\"\n        cmd = f'powershell.exe -WindowStyle Hidden -File \"{launcher_path}\"'\n\n        key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n        winreg.SetValueEx(key, cls.APP_NAME, 0, winreg.REG_SZ, cmd)\n        winreg.CloseKey(key)\n        print(\"Startup enabled.\")\n\n    @classmethod\n    def disable(cls):\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n            winreg.DeleteValue(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            print(\"Startup disabled.\")\n        except FileNotFoundError:\n            print(\"Already disabled.\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        if sys.argv[1] == \"enable\":\n            StartupManager.enable()\n        elif sys.argv[1] == \"disable\":\n            StartupManager.disable()\n        elif sys.argv[1] == \"status\":\n            print(f\"Startup is currently {'enabled' if StartupManager.is_enabled() else 'disabled'}\")\n    else:\n        print(\"Usage: python configure_startup.py [enable|disable|status]\")\n",
    "electron/assets/.gitkeep": "# This directory is for application icons (e.g., icon.ico, tray-icon.png)",
    "electron/preload.js": "// electron/preload.js\n// This script runs in a privileged environment with access to Node.js APIs.\n// It's used to securely expose specific functionality to the renderer process (the web UI).\n\nconst { contextBridge, ipcRenderer } = require('electron');\n\n// Expose a safe, limited API to the frontend.\ncontextBridge.exposeInMainWorld('electronAPI', {\n /**\n * Asynchronously fetches the secure API key from the main process.\n * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n */\n getApiKey: () => ipcRenderer.invoke('get-api-key'),\n\n /**\n * Asynchronously generates and saves a new secure API key.\n * @returns {Promise<string>} A promise that resolves with the newly generated API key.\n */\n generateApiKey: () => ipcRenderer.invoke('generate-api-key'),\n\n /**\n * Asynchronously saves a provided API key.\n * @param {string} apiKey - The API key to save.\n * @returns {Promise<{success: boolean}>} A promise that resolves with the result of the save operation.\n */\n saveApiKey: (apiKey) => ipcRenderer.invoke('save-api-key', apiKey),\n\n /**\n * Asynchronously saves Betfair credentials.\n * @param {{username: string, apiKey: string}} credentials - The credentials to save.\n * @returns {Promise<{success: boolean}>} A promise that resolves with the result of the save operation.\n */\n saveBetfairCredentials: (credentials) => ipcRenderer.invoke('save-betfair-credentials', credentials),\n\n /**\n  * Restarts the backend service.\n  */\n restartBackend: () => ipcRenderer.send('restart-backend'),\n\n /**\n  * Stops the backend service.\n  */\n stopBackend: () => ipcRenderer.send('stop-backend'),\n\n /**\n  * Fetches the current status of the backend service.\n  * @returns {Promise<{state: string, logs: string[]}>} A promise that resolves with the backend status.\n  */\n getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n\n /**\n  * Subscribes to backend status updates.\n  * @param {function(event, {state: string, logs: string[]})} callback - The function to call with status updates.\n  */\n onBackendStatusUpdate: (callback) => {\n    // Deliberately strip event sender from callback to avoid security risks\n    const subscription = (event, ...args) => callback(...args);\n    ipcRenderer.on('backend-status-update', subscription);\n\n    // Return a function to unsubscribe\n    return () => {\n      ipcRenderer.removeListener('backend-status-update', subscription);\n    };\n  },\n\n  /**\n   * Gets the port the backend API is running on.\n   * @returns {Promise<number>} A promise that resolves with the port number.\n   */\n  getApiPort: () => ipcRenderer.invoke('get-api-port'),\n});\n",
    "fortuna-backend-electron.spec": "# -*- mode: python ; coding: utf-8 -*-\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\nproject_root = Path(SPECPATH).parent\n# FIXED: Target the consolidated backend\nbackend_root = project_root / 'web_service' / 'backend'\n\n# Collect data folders\ndatas = []\nfor folder in ['data', 'json', 'adapters']:\n    source_path = backend_root / folder\n    if source_path.exists():\n        datas.append((str(source_path), folder))\n\n# CRITICAL: Bundle the frontend assets\nfrontend_dist = project_root / 'web_service' / 'frontend' / 'public'\nif frontend_dist.exists():\n    datas.append((str(frontend_dist), 'public'))\n\n# Collect dependencies\nhiddenimports = [\n    'uvicorn', 'fastapi', 'starlette', 'pydantic', 'structlog',\n    'tenacity', 'redis', 'sqlalchemy', 'greenlet', 'win32timezone'\n] + collect_submodules('web_service.backend')\n\na = Analysis(\n    ['web_service/backend/main.py'], # FIXED: Entry point\n    pathex=[str(project_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz, a.scripts, a.binaries, a.zipfiles, a.datas,\n    name='fortuna-backend',\n    debug=False,\n    strip=False,\n    upx=True,\n    console=True, # Keep console for Electron backend debugging\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n",
    "fortuna-unified.spec": "# -*- mode: python ; coding: utf-8 -*-\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\n\n# Collect data folders\ndatas = [\n    # Python service data\n    ('web_service/backend/data', 'data'),\n    ('web_service/backend/json', 'json'),\n    ('web_service/backend/adapters', 'adapters'),\n    # CRITICAL: Bundle the Next.js static frontend build\n    ('web_platform/frontend/out', 'ui'),\n]\n\n# Collect dependencies\nhiddenimports = [\n    'uvicorn', 'fastapi', 'starlette', 'pydantic', 'structlog',\n    'tenacity', 'redis', 'sqlalchemy', 'greenlet', 'win32timezone'\n] + collect_submodules('web_service.backend')\n\na = Analysis(\n    ['web_service/backend/main.py'],\n    pathex=['.'],\n    binaries=[],\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz, a.scripts, a.binaries, a.zipfiles, a.datas,\n    name='fortuna-webservice',\n    debug=False,\n    strip=False,\n    upx=True,\n    console=True,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon='assets/icon.ico',\n    version='file_version_info.txt'\n)",
    "fortuna_launcher.py": "#!/usr/bin/env python3\n\"\"\"\nFortuna Faucet - Enhanced Standalone Launcher for Windows 10 Home\nNo Docker, no special permissions, just pure Python magic\nRun this file and your browser opens automatically with all the bells and whistles\n\"\"\"\n\nimport sys\nimport os\nimport subprocess\nimport threading\nimport time\nimport webbrowser\nimport socket\nimport json\nfrom pathlib import Path\nfrom typing import Optional\nfrom datetime import datetime\n\n# ====================================================================\n# CONFIGURATION\n# ====================================================================\nAPP_NAME = \"Fortuna Faucet\"\nAPP_VERSION = \"3.1.0\"\nDEFAULT_HOST = \"127.0.0.1\"\nDEFAULT_PORT = 8000\nBACKEND_STARTUP_TIMEOUT = 15\nHEALTH_CHECK_ATTEMPTS = 30\nLOG_DIR = Path(\"logs\")\nLOG_DIR.mkdir(exist_ok=True)\n\n# ====================================================================\n# COLORS FOR WINDOWS CONSOLE\n# ====================================================================\nclass Colors:\n    \"\"\"ANSI color codes\"\"\"\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    RESET = '\\033[0m'\n\n# Try to enable ANSI colors on Windows 10\ntry:\n    import ctypes\n    kernel32 = ctypes.windll.kernel32\n    kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)\nexcept:\n    pass\n\n# ====================================================================\n# LOGGING\n# ====================================================================\nclass Logger:\n    \"\"\"Dual logging to console and file\"\"\"\n    def __init__(self):\n        self.log_file = LOG_DIR / f\"fortuna_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\n    def write(self, level: str, message: str):\n        \"\"\"Write to both console and file\"\"\"\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        log_line = f\"[{timestamp}] [{level}] {message}\"\n\n        with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n            f.write(log_line + \"\\n\")\n\nlogger = Logger()\n\n# ====================================================================\n# HELPER FUNCTIONS\n# ====================================================================\ndef print_banner():\n    \"\"\"Print welcome banner\"\"\"\n    banner = f\"\"\"\n{Colors.BOLD}{Colors.OKGREEN}\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                                                            \u2551\n\u2551              \ud83d\udc34  {APP_NAME} v{APP_VERSION}  \ud83d\udc34              \u2551\n\u2551         Enhanced Launcher - Windows 10 Home Ready         \u2551\n\u2551                                                            \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n{Colors.ENDC}\n\"\"\"\n    print(banner)\n\ndef print_success(msg: str, icon: str = \"\u2713\"):\n    \"\"\"Print success message\"\"\"\n    output = f\"{Colors.OKGREEN}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"SUCCESS\", msg)\n\ndef print_warning(msg: str, icon: str = \"\u26a0\"):\n    \"\"\"Print warning message\"\"\"\n    output = f\"{Colors.WARNING}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"WARNING\", msg)\n\ndef print_error(msg: str, icon: str = \"\u2717\"):\n    \"\"\"Print error message\"\"\"\n    output = f\"{Colors.FAIL}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"ERROR\", msg)\n\ndef print_info(msg: str, icon: str = \"\u2139\"):\n    \"\"\"Print info message\"\"\"\n    output = f\"{Colors.OKBLUE}{icon}{Colors.ENDC} {msg}\"\n    print(output)\n    logger.write(\"INFO\", msg)\n\ndef print_step(step_num: int, total: int, msg: str):\n    \"\"\"Print step counter\"\"\"\n    output = f\"\\n{Colors.BOLD}[{step_num}/{total}] {msg}{Colors.ENDC}\"\n    print(output)\n    logger.write(\"STEP\", f\"[{step_num}/{total}] {msg}\")\n\ndef print_section(title: str):\n    \"\"\"Print section divider\"\"\"\n    output = f\"\\n{Colors.BOLD}{Colors.OKCYAN}{'\u2500' * 60}{Colors.ENDC}\"\n    print(output)\n    print(f\"{Colors.BOLD}{Colors.OKCYAN}{title}{Colors.ENDC}\")\n    print(f\"{Colors.BOLD}{Colors.OKCYAN}{'\u2500' * 60}{Colors.ENDC}\\n\")\n\n# ====================================================================\n# ENVIRONMENT CHECKS\n# ====================================================================\ndef check_python_version() -> bool:\n    \"\"\"Check if Python version is compatible\"\"\"\n    if sys.version_info < (3, 10):\n        print_error(f\"Python 3.10+ required, you have {sys.version_info.major}.{sys.version_info.minor}\")\n        return False\n    print_success(f\"Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n    return True\n\ndef check_project_structure() -> bool:\n    \"\"\"Check if we're in the right directory\"\"\"\n    required_dirs = [\n        \"web_service/backend\",\n        \"web_platform/frontend\"\n    ]\n    required_files = [\n        \"web_service/backend/requirements.txt\",\n        \"web_platform/frontend/package.json\"\n    ]\n\n    print_info(\"Checking project structure...\")\n    all_good = True\n    for d in required_dirs:\n        if Path(d).exists():\n            print_success(f\"Found: {d}\")\n        else:\n            print_error(f\"Missing: {d}\")\n            all_good = False\n\n    for f in required_files:\n        if Path(f).exists():\n            print_success(f\"Found: {f}\")\n        else:\n            print_error(f\"Missing: {f}\")\n            all_good = False\n\n    return all_good\n\ndef check_port_available(port: int) -> bool:\n    \"\"\"Check if port is available\"\"\"\n    try:\n        sock = socket.create_connection((\"127.0.0.1\", port), timeout=1)\n        sock.close()\n        print_error(f\"Port {port} is already in use by another application\")\n        return False\n    except (socket.timeout, ConnectionRefusedError, OSError):\n        print_success(f\"Port {port} is available\")\n        return True\n\n# ====================================================================\n# DEPENDENCY CHECK & INSTALL\n# ====================================================================\ndef check_and_install_dependencies() -> bool:\n    \"\"\"Check if dependencies are installed, install if needed\"\"\"\n    print_info(\"Checking Python dependencies...\")\n\n    required_packages = {\n        \"fastapi\": \"FastAPI web framework\",\n        \"uvicorn\": \"ASGI server\",\n        \"pydantic\": \"Data validation\",\n    }\n\n    missing = []\n    for package, description in required_packages.items():\n        try:\n            __import__(package)\n            print_success(f\"{description} (installed)\")\n        except ImportError:\n            print_warning(f\"{description} (NOT installed)\")\n            missing.append(package)\n\n    if not missing:\n        print_success(\"All core dependencies satisfied!\")\n        return True\n\n    print()\n    print_info(f\"Installing {len(missing)} missing package(s)...\")\n    print_info(\"This may take 2-3 minutes on first run...\")\n    print()\n\n    try:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"],\n            check=True,\n            capture_output=True,\n            timeout=120\n        )\n\n        subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + missing,\n            check=True,\n            capture_output=True,\n            timeout=300\n        )\n        print_success(\"Dependencies installed successfully!\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print_error(f\"Failed to install dependencies: {e}\")\n        print_info(\"Try running manually in PowerShell:\")\n        print(f\"  python -m pip install -r web_service/backend/requirements.txt\")\n        logger.write(\"ERROR\", f\"Dependency installation failed: {e}\")\n        return False\n    except subprocess.TimeoutExpired:\n        print_error(\"Installation timed out (took too long)\")\n        return False\n\n# ====================================================================\n# FRONTEND BUILD\n# ====================================================================\ndef build_frontend() -> bool:\n    \"\"\"Build Next.js frontend if needed\"\"\"\n    frontend_dir = Path(\"web_platform/frontend\")\n    build_dir = frontend_dir / \"out\"\n\n    if build_dir.exists() and (build_dir / \"index.html\").exists():\n        print_success(\"Frontend already built\")\n        return True\n\n    print_info(\"Frontend build required...\")\n\n    # Check for Node.js\n    try:\n        subprocess.run([\"npm\", \"--version\"], capture_output=True, timeout=5, check=True)\n    except:\n        print_warning(\"Node.js not found - frontend may not load properly\")\n        print_info(\"To fix: Install Node.js from https://nodejs.org/\")\n        logger.write(\"WARNING\", \"Node.js not found for frontend build\")\n        return True\n\n    print_info(\"Building frontend (this takes ~30 seconds)...\")\n    print_info(\"(Progress shown in logs)\")\n\n    try:\n        subprocess.run(\n            [\"npm\", \"ci\"],\n            cwd=str(frontend_dir),\n            capture_output=True,\n            timeout=120,\n            check=True\n        )\n        subprocess.run(\n            [\"npm\", \"run\", \"build\"],\n            cwd=str(frontend_dir),\n            capture_output=True,\n            timeout=180,\n            check=True\n        )\n        print_success(\"Frontend built successfully\")\n        return True\n    except subprocess.TimeoutExpired:\n        print_warning(\"Frontend build timed out, continuing anyway...\")\n        logger.write(\"WARNING\", \"Frontend build timed out\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print_warning(f\"Frontend build failed: {e}\")\n        logger.write(\"WARNING\", f\"Frontend build failed: {e}\")\n        return True\n    except Exception as e:\n        print_warning(f\"Frontend build error: {e}\")\n        logger.write(\"WARNING\", f\"Frontend build error: {e}\")\n        return True\n\n# ====================================================================\n# BACKEND SERVER\n# ====================================================================\ndef start_backend() -> Optional[subprocess.Popen]:\n    \"\"\"Start the FastAPI backend server\"\"\"\n    print_info(\"Starting FastAPI server...\")\n\n    try:\n        process = subprocess.Popen(\n            [\n                sys.executable,\n                \"-m\", \"uvicorn\",\n                \"web_service.backend.main:app\",\n                \"--host\", DEFAULT_HOST,\n                \"--port\", str(DEFAULT_PORT),\n                \"--log-level\", \"info\"\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            bufsize=1\n        )\n\n        # Give it a moment to start\n        time.sleep(1)\n\n        if process.poll() is not None:\n            # Process exited already\n            stdout, stderr = process.communicate()\n            print_error(f\"Backend failed to start: {stderr[:200]}\")\n            logger.write(\"ERROR\", f\"Backend startup failed: {stderr}\")\n            return None\n\n        print_success(\"Backend server started\")\n        logger.write(\"SUCCESS\", \"Backend server started successfully\")\n        return process\n\n    except Exception as e:\n        print_error(f\"Failed to start backend: {e}\")\n        logger.write(\"ERROR\", f\"Backend start exception: {e}\")\n        return None\n\ndef wait_for_backend_ready(max_retries: int = HEALTH_CHECK_ATTEMPTS) -> bool:\n    \"\"\"Wait for backend to respond to health check\"\"\"\n    import urllib.request\n    import urllib.error\n\n    print_info(\"Waiting for backend to be ready...\")\n\n    for attempt in range(max_retries):\n        try:\n            response = urllib.request.urlopen(\n                f\"http://{DEFAULT_HOST}:{DEFAULT_PORT}/api/health\",\n                timeout=2\n            )\n            if response.status == 200:\n                elapsed = attempt + 1\n                print_success(f\"Backend ready in {elapsed} second(s)\")\n                logger.write(\"SUCCESS\", f\"Backend health check passed in {elapsed}s\")\n                return True\n        except (urllib.error.URLError, urllib.error.HTTPError, Exception):\n            if attempt < max_retries - 1:\n                time.sleep(1)\n\n    print_error(\"Backend did not respond after 30 seconds\")\n    logger.write(\"ERROR\", \"Backend health check failed - no response after 30s\")\n    return False\n\n# ====================================================================\n# BROWSER LAUNCHER\n# ====================================================================\ndef open_browser():\n    \"\"\"Open browser to the application\"\"\"\n    url = f\"http://{DEFAULT_HOST}:{DEFAULT_PORT}\"\n    try:\n        print_info(f\"Opening browser at {url}...\")\n        webbrowser.open(url)\n        time.sleep(1)  # Give browser time to open\n        print_success(\"Browser opened!\")\n        logger.write(\"SUCCESS\", f\"Browser opened at {url}\")\n    except Exception as e:\n        print_warning(f\"Could not open browser automatically: {e}\")\n        print_info(f\"Please manually open: {url}\")\n        logger.write(\"WARNING\", f\"Browser auto-open failed: {e}\")\n\n# ====================================================================\n# SYSTEM INFO\n# ====================================================================\ndef print_system_info():\n    \"\"\"Print system information\"\"\"\n    print_section(\"System Information\")\n    print_success(f\"Python: {sys.version.split()[0]}\")\n    print_success(f\"Platform: {sys.platform}\")\n    print_success(f\"Current Directory: {Path.cwd()}\")\n    print_success(f\"Log Directory: {LOG_DIR.absolute()}\")\n    print()\n\n# ====================================================================\n# MAIN APPLICATION\n# ====================================================================\ndef main():\n    \"\"\"Main entry point\"\"\"\n    print_banner()\n    print_system_info()\n\n    # Step 1: Environment validation\n    print_step(1, 5, \"Validating environment...\")\n    if not check_python_version():\n        return 1\n    if not check_project_structure():\n        return 1\n    if not check_port_available(DEFAULT_PORT):\n        return 1\n    print()\n\n    # Step 2: Dependencies\n    print_step(2, 5, \"Installing dependencies...\")\n    if not check_and_install_dependencies():\n        return 1\n    print()\n\n    # Step 3: Frontend build\n    print_step(3, 5, \"Building frontend...\")\n    build_frontend()\n    print()\n\n    # Step 4: Start backend\n    print_step(4, 5, \"Starting backend server...\")\n    backend_process = start_backend()\n    if not backend_process:\n        return 1\n\n    if not wait_for_backend_ready():\n        backend_process.terminate()\n        logger.write(\"ERROR\", \"Application startup failed - health check timeout\")\n        return 1\n    print()\n\n    # Step 5: Open browser\n    print_step(5, 5, \"Launching browser...\")\n    open_browser()\n    print()\n\n    # Success!\n    print(f\"{Colors.BOLD}{Colors.OKGREEN}\")\n    print(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n    print(\"\u2551                                                            \u2551\")\n    print(\"\u2551          \ud83c\udf89  FORTUNA IS RUNNING!  \ud83c\udf89                     \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(f\"\u2551  Access your app at: http://{DEFAULT_HOST}:{DEFAULT_PORT:<5}                      \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(f\"\u2551  Log file: {LOG_DIR / 'fortuna_*.log':<40}  \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(\"\u2551  Press Ctrl+C to stop the server                          \u2551\")\n    print(\"\u2551                                                            \u2551\")\n    print(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n    print(f\"{Colors.ENDC}\")\n    print()\n\n    # Keep running\n    try:\n        while True:\n            time.sleep(0.1)\n    except KeyboardInterrupt:\n        print()\n        print_info(\"Shutting down gracefully...\")\n        backend_process.terminate()\n        try:\n            backend_process.wait(timeout=5)\n        except subprocess.TimeoutExpired:\n            backend_process.kill()\n        print_success(\"Fortuna stopped successfully\")\n        logger.write(\"SUCCESS\", \"Application stopped gracefully by user\")\n        return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
    "manual_override_tool.py": "import argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Manual Override Tool for Checkmate Data Warehouse.\")\n    parser.add_argument(\"--file\", required=True, help=\"Path to the CSV file for ingestion.\")\n    parser.add_argument(\"--user\", required=True, help=\"The user ID performing the override.\")\n    args = parser.parse_args()\n\n    print(f\"Executing manual override by '{args.user}' for file '{args.file}'...\")\n\n    # 1. Connect to PostgreSQL\n    # engine = create_engine('postgresql://user:password@host:port/database')\n\n    # 2. Read and validate the CSV data\n    # race_df = pd.read_csv(args.file)\n    # ... validation logic ...\n\n    # 3. Add the manual_override_by column\n    # race_df['manual_override_by'] = args.user\n\n    # 4. Insert data into the 'historical_races' table\n    # race_df.to_sql('historical_races', engine, if_exists='append', index=False)\n\n    print(\"Manual override completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "pre-build-check.sh": "#!/bin/bash\n\necho -e \"\\e[36m=== FORTUNA PRE-BUILD VERIFICATION ===\\e[0m\"\n\n# 1. Check all required files exist\necho -e \"\\n\\e[1m[1] Checking required files...\\e[0m\"\nrequired_files=(\n    \"web_service/backend/main.py\"\n    \"web_service/backend/api.py\"\n    \"web_service/backend/config.py\"\n    \"web_service/backend/port_check.py\"\n    \"web_service/backend/requirements.txt\"\n    \"web_service/frontend/package.json\"\n    \"web_service/frontend/next.config.js\"\n    \"fortuna-monolith.spec\"\n)\n\nmissing_files=()\nall_found=true\nfor file in \"${required_files[@]}\"; do\n    if [ -f \"$file\" ]; then\n        echo -e \"  \\e[32m\u2705 $file\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c $file\\e[0m\"\n        missing_files+=(\"$file\")\n        all_found=false\n    fi\ndone\n\nif [ \"$all_found\" = false ]; then\n    echo -e \"\\n\\e[31m\u274c FATAL: Missing files:\\e[0m\"\n    for file in \"${missing_files[@]}\"; do\n        echo \"  - $file\"\n    done\n    exit 1\nfi\n\n# 2. Test Python imports\necho -e \"\\n\\e[1m[2] Testing Python imports...\\e[0m\"\ncat > test_imports.py << EOL\nimport sys\nsys.path.insert(0, '.')\n\ntry:\n    from web_service.backend.api import app\n    print('\u2705 api.app imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import api.app: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.config import get_settings\n    settings = get_settings()\n    print(f'\u2705 config.get_settings imported (host={settings.UVICORN_HOST}, port={settings.FORTUNA_PORT})')\nexcept ImportError as e:\n    print(f'\u274c Failed to import config: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    print('\u2705 port_check.check_port_and_exit_if_in_use imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import port_check: {e}')\n    sys.exit(1)\n\nprint('\u2705 All imports successful')\nEOL\n\npython test_imports.py\nif [ $? -ne 0 ]; then\n    echo -e \"\\e[31m\u274c Import test FAILED\\e[0m\"\n    rm test_imports.py\n    exit 1\nfi\nrm test_imports.py\n\n# 3. Check frontend\necho -e \"\\n\\e[1m[3] Checking frontend...\\e[0m\"\nif [ -f \"web_service/frontend/next.config.js\" ]; then\n    if grep -q \"output: 'export'\" \"web_service/frontend/next.config.js\"; then\n        echo -e \"  \\e[32m\u2705 next.config.js has output: 'export'\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c next.config.js missing output: 'export'\\e[0m\"\n        exit 1\n    fi\nelse\n    echo -e \"  \\e[33m\u26a0\ufe0f  next.config.js will be created during build\\e[0m\"\nfi\n\n# 4. Check spec file\necho -e \"\\n\\e[1m[4] Checking fortuna-monolith.spec...\\e[0m\"\nif [ -f \"fortuna-monolith.spec\" ]; then\n    if grep -q \"SPECPATH\" \"fortuna-monolith.spec\"; then\n        echo -e \"  \\e[32m\u2705 spec uses SPECPATH\\e[0m\"\n    else\n        echo -e \"  \\e[33m\u26a0\ufe0f  spec doesn't use SPECPATH (may have path issues)\\e[0m\"\n    fi\nelse\n    echo -e \"  \\e[31m\u274c fortuna-monolith.spec not found\\e[0m\"\n    exit 1\nfi\n\necho -e \"\\n\\e[32m\u2705 ALL CHECKS PASSED - Safe to build!\\e[0m\"\n",
    "scripts/fortuna_reporter.py": "#!/usr/bin/env python\n\"\"\"\nFortuna Unified Race Reporter\n\nGenerates HTML, JSON, and Markdown summary reports for GitHub Actions\nby directly invoking the OddsEngine and AnalyzerEngine without a live API.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any\n\n# Ensure the project root is in the path to allow for direct imports\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom web_service.backend.engine import OddsEngine\nfrom web_service.backend.analyzer import AnalyzerEngine\nfrom web_service.backend.config import get_settings\nfrom web_service.backend.models import Race\n\n\nclass LogLevel(Enum):\n    \"\"\"Log level enumeration with emoji support.\"\"\"\n    INFO = (\"INFO\", \"\u2139\ufe0f\")\n    SUCCESS = (\"SUCCESS\", \"\u2705\")\n    ERROR = (\"ERROR\", \"\u274c\")\n    WARNING = (\"WARNING\", \"\u26a0\ufe0f\")\n    DEBUG = (\"DEBUG\", \"\ud83d\udd0d\")\n\n    @property\n    def emoji(self) -> str:\n        return self.value[1]\n\n\n@dataclass\nclass ReporterConfig:\n    \"\"\"Configuration for the race reporter.\"\"\"\n    template_path: Path = field(default_factory=lambda: Path(\"scripts/templates/race_report_template.html\"))\n    html_output_path: Path = field(default_factory=lambda: Path(\"race-report.html\"))\n    json_output_path: Path = field(default_factory=lambda: Path(\"qualified_races.json\"))\n    markdown_summary_path: Path = field(default_factory=lambda: Path(\"github_summary.md\"))\n    raw_json_output_path: Path = field(default_factory=lambda: Path(\"raw_race_data.json\"))\n\n    max_retries: int = field(default_factory=lambda: int(os.getenv(\"MAX_RETRIES\", \"3\")))\n    request_timeout: int = field(default_factory=lambda: int(os.getenv(\"REQUEST_TIMEOUT\", \"30\")))\n    analyzer_type: str = field(default_factory=lambda: os.getenv(\"ANALYZER_TYPE\", \"tiny_field_trifecta\"))\n    force_refresh: bool = field(default_factory=lambda: os.getenv(\"FORCE_REFRESH\", \"false\").lower() == \"true\")\n    max_summary_races: int = 25\n\n    # All known adapters for exclusion logic\n    ALL_ADAPTERS: tuple[str, ...] = (\n        \"AtTheRacesAdapter\", \"BetfairAdapter\", \"BetfairGreyhoundAdapter\",\n        \"BrisnetAdapter\", \"EquibaseAdapter\", \"FanDuelAdapter\", \"GbgbApiAdapter\",\n        \"GreyhoundAdapter\", \"HarnessAdapter\", \"HorseRacingNationAdapter\",\n        \"NYRABetsAdapter\", \"OddscheckerAdapter\", \"PuntersAdapter\",\n        \"RacingAndSportsAdapter\", \"RacingAndSportsGreyhoundAdapter\",\n        \"RacingPostAdapter\", \"RacingTVAdapter\", \"SportingLifeAdapter\",\n        \"TabAdapter\", \"TheRacingApiAdapter\", \"TimeformAdapter\",\n        \"TwinSpiresAdapter\", \"TVGAdapter\", \"XpressbetAdapter\",\n        \"PointsBetGreyhoundAdapter\",\n    )\n\n    # Reliable adapters that don't require API keys.\n    # Note: The following adapters may experience issues and should be monitored:\n    # - Timeform: Occasional 500 errors (redirect to error page)\n    # - Equibase: 404 errors on some dates\n    # - Brisnet: Timeout/503 errors, possible rate limiting\n    # - Oddschecker: 403 Forbidden (bot detection)\n    # - RacingPost: 406 Not Acceptable (user agent issues)\n    RELIABLE_NON_KEYED_ADAPTERS: tuple[str, ...] = (\n        \"AtTheRacesAdapter\", \n        \"SportingLifeAdapter\",\n        # Conditionally reliable - monitor closely:\n        # \"RacingPostAdapter\",  # 406 errors observed\n        # \"OddscheckerAdapter\", # 403 errors observed\n    )\n\n    @property\n    def excluded_adapters(self) -> list[str]:\n        \"\"\"Calculate which adapters to exclude.\"\"\"\n        return [a for a in self.ALL_ADAPTERS if a not in self.RELIABLE_NON_KEYED_ADAPTERS]\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if self.max_retries < 1:\n            raise ValueError(\"max_retries must be at least 1\")\n        if self.request_timeout < 1:\n            raise ValueError(\"request_timeout must be at least 1 second\")\n        if self.max_summary_races < 1:\n            raise ValueError(\"max_summary_races must be at least 1\")\n        \n        # Ensure output directories exist\n        for path in (self.html_output_path, self.json_output_path, \n                     self.markdown_summary_path, self.raw_json_output_path):\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n\n@dataclass\nclass ReportMetrics:\n    \"\"\"Metrics collected during report generation.\"\"\"\n    total_races_fetched: int = 0\n    qualified_races: int = 0\n    adapters_used: list[str] = field(default_factory=list)\n    adapters_failed: list[str] = field(default_factory=list)\n    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    end_time: datetime | None = None\n    errors: list[str] = field(default_factory=list)\n\n    @property\n    def duration_seconds(self) -> float:\n        if self.end_time:\n            return (self.end_time - self.start_time).total_seconds()\n        return 0.0\n\n    def to_dict(self) -> dict[str, Any]:\n        return {\n            \"total_races_fetched\": self.total_races_fetched,\n            \"qualified_races\": self.qualified_races,\n            \"adapters_used\": self.adapters_used,\n            \"adapters_failed\": self.adapters_failed,\n            \"duration_seconds\": self.duration_seconds,\n            \"errors\": self.errors,\n            \"timestamp\": self.start_time.isoformat(),\n        }\n\n\nclass Reporter:\n    \"\"\"Main reporter class for generating race reports.\"\"\"\n\n    def __init__(self, config: ReporterConfig | None = None):\n        self.config = config or ReporterConfig()\n        self.metrics = ReportMetrics()\n\n    def log(self, message: str, level: LogLevel = LogLevel.INFO) -> None:\n        \"\"\"Print a timestamped log message.\"\"\"\n        timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(f\"[{timestamp}] {level.emoji} {message}\", flush=True)\n\n        if level == LogLevel.ERROR:\n            self.metrics.errors.append(message)\n\n    def generate_html_report(self, race_data: dict[str, Any]) -> bool:\n        \"\"\"Generates the HTML report from a template.\"\"\"\n        self.log(\"Generating HTML report...\")\n\n        try:\n            if not self.config.template_path.exists():\n                self.log(f\"Template not found at {self.config.template_path}\", LogLevel.ERROR)\n                return self._generate_fallback_html(race_data)\n\n            template = self.config.template_path.read_text(encoding=\"utf-8\")\n\n            # Inject data and metrics\n            race_data_with_metrics = {\n                **race_data,\n                \"generation_metrics\": self.metrics.to_dict(),\n            }\n\n            report_html = template.replace(\n                \"__RACE_DATA_PLACEHOLDER__\",\n                json.dumps(race_data_with_metrics, default=str)\n            )\n\n            self.config.html_output_path.write_text(report_html, encoding=\"utf-8\")\n            self.log(f\"Generated HTML report at {self.config.html_output_path}\", LogLevel.SUCCESS)\n            return True\n\n        except Exception as e:\n            self.log(f\"Failed to generate HTML report: {e}\", LogLevel.ERROR)\n            return self._generate_fallback_html(race_data)\n\n    def _generate_fallback_html(self, race_data: dict[str, Any]) -> bool:\n        \"\"\"Generate a minimal fallback HTML report if template fails.\"\"\"\n        self.log(\"Generating fallback HTML report...\", LogLevel.WARNING)\n\n        try:\n            races = race_data.get(\"races\", [])\n            html = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Fortuna Race Report (Fallback)</title>\n    <style>\n        body {{ font-family: system-ui, sans-serif; max-width: 800px; margin: 2rem auto; padding: 1rem; }}\n        .race {{ border: 1px solid #ccc; padding: 1rem; margin: 1rem 0; border-radius: 8px; }}\n        .error {{ color: #c00; background: #fee; padding: 1rem; border-radius: 4px; }}\n    </style>\n</head>\n<body>\n    <h1>\ud83d\udc34 Fortuna Race Report</h1>\n    <p class=\"error\">\u26a0\ufe0f This is a fallback report. The main template could not be loaded.</p>\n    <p>Found {len(races)} qualified race(s)</p>\n    <pre>{json.dumps(race_data, indent=2, default=str)}</pre>\n</body>\n</html>\"\"\"\n            self.config.html_output_path.write_text(html, encoding=\"utf-8\")\n            return True\n        except Exception as e:\n            self.log(f\"Failed to generate fallback HTML: {e}\", LogLevel.ERROR)\n            return False\n\n    def generate_markdown_summary(self, races: list[Race]) -> bool:\n        \"\"\"Generates a Markdown summary for the GitHub Actions UI.\"\"\"\n        self.log(\"Generating Markdown summary...\")\n\n        try:\n            lines = [\n                \"# \ud83d\udc34 Fortuna Race Report\",\n                \"\",\n                f\"**Generated:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\",\n                f\"**Analyzer:** `{self.config.analyzer_type}`\",\n                f\"**Duration:** {self.metrics.duration_seconds:.1f}s\",\n                \"\",\n            ]\n\n            if self.metrics.errors:\n                lines.extend([\n                    \"### \u26a0\ufe0f Warnings\",\n                    \"\",\n                    *[f\"- {e}\" for e in self.metrics.errors[:5]],\n                    \"\",\n                ])\n\n            if not races:\n                lines.append(\"### \ud83d\udd2d No races found matching filters.\")\n            else:\n                lines.extend([\n                    f\"### \u26a1 Found {len(races)} Qualified Race(s)\",\n                    \"\",\n                    \"| Score | Time | Venue | Race | Runners |\",\n                    \"|:-----:|:-----|:------|:----:|:-------:|\",\n                ])\n\n                for race in races[:self.config.max_summary_races]:\n                    start_time = race.start_time\n                    if isinstance(start_time, str):\n                        try:\n                            start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n                        except ValueError:\n                            start_time = None\n\n                    time_str = start_time.strftime('%H:%M') if start_time else \"N/A\"\n                    score = f\"{race.qualification_score:.1f}\" if race.qualification_score is not None else \"N/A\"\n                    venue = race.venue or \"Unknown\"\n                    race_num = race.race_number or \"?\"\n                    runners = len(race.runners) if race.runners else 0\n\n                    lines.append(f\"| {score} | {time_str} | **{venue}** | {race_num} | {runners} |\")\n\n                if len(races) > self.config.max_summary_races:\n                    lines.append(f\"\\n*...and {len(races) - self.config.max_summary_races} more races*\")\n\n            lines.extend([\n                \"\",\n                \"---\",\n                \"\",\n                \"<details>\",\n                \"<summary>\ud83d\udcca Generation Metrics</summary>\",\n                \"\",\n                f\"- Total races fetched: {self.metrics.total_races_fetched}\",\n                f\"- Qualified races: {self.metrics.qualified_races}\",\n                f\"- Adapters used: {len(self.metrics.adapters_used)}\",\n                f\"- Adapters failed: {len(self.metrics.adapters_failed)}\",\n                \"\",\n                \"</details>\",\n            ])\n\n            self.config.markdown_summary_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n            self.log(f\"Generated Markdown summary at {self.config.markdown_summary_path}\", LogLevel.SUCCESS)\n            return True\n\n        except Exception as e:\n            self.log(f\"Failed to write Markdown summary: {e}\", LogLevel.ERROR)\n            return False\n\n    def save_json(self, data: dict[str, Any], path: Path, description: str) -> bool:\n        \"\"\"Save JSON data with error handling.\"\"\"\n        try:\n            path.write_text(json.dumps(data, indent=2, default=str), encoding=\"utf-8\")\n            self.log(f\"Saved {description} to {path}\", LogLevel.SUCCESS)\n            return True\n        except Exception as e:\n            self.log(f\"Failed to save {description}: {e}\", LogLevel.ERROR)\n            return False\n\n    async def fetch_with_retry(\n        self,\n        odds_engine: OddsEngine,\n        date_str: str,\n    ) -> dict[str, Any]:\n        \"\"\"Fetch race data with retry logic.\"\"\"\n        last_error = None\n\n        for attempt in range(1, self.config.max_retries + 1):\n            try:\n                self.log(f\"Fetching race data (attempt {attempt}/{self.config.max_retries})...\")\n                data = await asyncio.wait_for(\n                    odds_engine.fetch_all_odds(date_str),\n                    timeout=self.config.request_timeout * 2\n                )\n                return data\n            except asyncio.TimeoutError:\n                last_error = \"Request timed out\"\n                self.log(f\"Attempt {attempt} timed out\", LogLevel.WARNING)\n            except Exception as e:\n                last_error = str(e)\n                self.log(f\"Attempt {attempt} failed: {e}\", LogLevel.WARNING)\n\n            if attempt < self.config.max_retries:\n                wait_time = 2 ** attempt  # Exponential backoff\n                self.log(f\"Waiting {wait_time}s before retry...\")\n                await asyncio.sleep(wait_time)\n\n        raise RuntimeError(f\"All {self.config.max_retries} fetch attempts failed. Last error: {last_error}\")\n\n    async def run(self) -> bool:\n        \"\"\"Main entry point with graceful degradation.\"\"\"\n        self.log(\"=== Fortuna Unified Race Reporter ===\")\n        self.log(f\"Analyzer: {self.config.analyzer_type}\")\n        self.log(f\"Excluding {len(self.config.excluded_adapters)} adapters\")\n\n        settings = get_settings()\n        odds_engine = OddsEngine(config=settings, exclude_adapters=self.config.excluded_adapters)\n        analyzer_engine = AnalyzerEngine()\n\n        # Pre-flight health check\n        healthy_adapters = []\n        for name, adapter in odds_engine.adapters.items():\n            try:\n                health = await adapter.health_check()\n                if health.get('circuit_breaker_state') == 'closed':\n                    healthy_adapters.append(name)\n            except Exception as e:\n                self.log(f\"Health check failed for {name}: {e}\", LogLevel.WARNING)\n        \n        self.log(f\"Healthy adapters: {len(healthy_adapters)}/{len(odds_engine.adapters)}\")\n        \n        if len(healthy_adapters) < 2:\n            self.log(\"Insufficient healthy adapters, report may be degraded\", LogLevel.WARNING)\n\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n        \n        outputs_generated = {\n            \"raw_json\": False,\n            \"qualified_json\": False,\n            \"html\": False,\n            \"markdown\": False,\n        }\n\n        try:\n            aggregated_data = await self.fetch_with_retry(odds_engine, today_str)\n            outputs_generated[\"raw_json\"] = self.save_json(\n                aggregated_data, self.config.raw_json_output_path, \"raw race data\"\n            )\n            \n            all_races_raw = aggregated_data.get(\"races\", [])\n            self.metrics.total_races_fetched = len(all_races_raw)\n\n            if not all_races_raw:\n                self.log(\"No races returned from OddsEngine. This is a critical failure.\", LogLevel.ERROR)\n                self.metrics.end_time = datetime.now(timezone.utc)\n                self.generate_markdown_summary([]) # Generate a summary showing failure\n                return False\n\n            all_races = []\n            validation_errors = []\n\n            for i, race_data in enumerate(all_races_raw):\n                try:\n                    all_races.append(Race(**race_data))\n                except Exception as e:\n                    error_msg = f\"Race {i} ({race_data.get('venue', 'unknown')}): {str(e)}\"\n                    validation_errors.append(error_msg)\n                    self.log(f\"Failed to validate race {i}: {e}\", LogLevel.WARNING)\n\n            self.log(f\"Validated {len(all_races)}/{len(all_races_raw)} races\")\n\n            if len(all_races) == 0 and len(all_races_raw) > 0:\n                self.log(\"All races failed validation! Check schema compatibility.\", LogLevel.ERROR)\n                self.save_json({\n                    \"error\": \"All races failed validation\",\n                    \"total_raw_races\": len(all_races_raw),\n                    \"validation_errors\": validation_errors[:10]\n                }, Path(\"validation_errors.json\"), \"validation errors\")\n                return False\n\n            if validation_errors:\n                self.save_json({\n                    \"validation_errors\": validation_errors\n                }, Path(\"validation_warnings.json\"), \"validation warnings\")\n\n            self.log(f\"Analyzing with '{self.config.analyzer_type}' analyzer...\")\n            analyzer = analyzer_engine.get_analyzer(self.config.analyzer_type)\n            result = analyzer.qualify_races(all_races)\n            \n            qualified_races = result.get(\"races\", [])\n            criteria = result.get(\"criteria\", {})\n            self.metrics.qualified_races = len(qualified_races)\n            self.log(f\"Found {len(qualified_races)} qualified races\", LogLevel.SUCCESS)\n\n            report_data = {\n                \"races\": [r.model_dump(mode='json') for r in qualified_races],\n                \"analysis_metadata\": criteria,\n                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                \"analyzer\": self.config.analyzer_type,\n            }\n\n            outputs_generated[\"qualified_json\"] = self.save_json(\n                report_data, self.config.json_output_path, \"qualified races JSON\"\n            )\n            outputs_generated[\"html\"] = self.generate_html_report(report_data)\n            outputs_generated[\"markdown\"] = self.generate_markdown_summary(qualified_races)\n\n        except Exception as e:\n            self.log(f\"Critical error: {e}\", LogLevel.ERROR)\n            # Still try to generate a failure report\n            self.generate_markdown_summary([])\n            \n        finally:\n            self.metrics.end_time = datetime.now(timezone.utc)\n            await odds_engine.close()\n            \n            successful_outputs = sum(outputs_generated.values())\n            self.log(f\"Generated {successful_outputs}/{len(outputs_generated)} outputs\")\n            \n        return all(outputs_generated.values())\n\n\nasync def main() -> int:\n    \"\"\"CLI entry point.\"\"\"\n    reporter = Reporter()\n    success = await reporter.run()\n    return 0 if success else 1\n\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)\n",
    "scripts/install_fortuna_gui.bat": "@echo off\nREM Interactive MSI installation with standard Windows UI\n\ntitle Fortuna Faucet Installation Wizard\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Administrator privileges required\n    echo Please right-click this file and select \"Run as Administrator\"\n    pause\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" /L*v \"%TEMP%\\fortuna_install.log\"\n\nif %errorlevel% equ 0 (\n    echo Installation completed successfully!\n    echo Access dashboard at: http://localhost:3000\n) else (\n    echo Installation failed. Log: %TEMP%\\fortuna_install.log\n)\npause",
    "scripts/prepare_minimal_build.py": "# scripts/prepare_minimal_build.py\nimport os\nimport shutil\n\n# This script prepares the source tree for a 'minimal' build.\n# A minimal build includes only the core application and a small, curated\n# set of essential data adapters, excluding the larger, more specialized ones.\n\nADAPTERS_TO_KEEP = [\n    \"__init__.py\",\n    \"base_adapter.py\",\n    \"handler_factory.py\",\n    # --- Essential Adapters ---\n    \"betfair_adapter.py\",\n    \"sporting_life_adapter.py\",\n    \"racing_post_adapter.py\",\n]\n\n\ndef main():\n    \"\"\"\n    Removes non-essential adapter files from the python_service/adapters\n    directory to create a minimal build artifact.\n    \"\"\"\n    adapters_dir = os.path.join(\"python_service\", \"adapters\")\n    if not os.path.isdir(adapters_dir):\n        print(f\"[ERROR] Adapters directory not found at: {adapters_dir}\")\n        exit(1)\n\n    print(f\"Scanning adapters directory: {adapters_dir}\")\n    removed_count = 0\n    for filename in os.listdir(adapters_dir):\n        if filename not in ADAPTERS_TO_KEEP:\n            file_path = os.path.join(adapters_dir, filename)\n            try:\n                if os.path.isfile(file_path):\n                    os.remove(file_path)\n                    print(f\"  - Removed file: {filename}\")\n                    removed_count += 1\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n                    print(f\"  - Removed directory: {filename}\")\n                    removed_count += 1\n            except OSError as e:\n                print(f\"[ERROR] Failed to remove {file_path}: {e}\")\n                exit(1)\n\n    print(f\"\\nMinimal build preparation complete. Removed {removed_count} non-essential adapter(s).\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/uninstall_fortuna.bat": "@echo off\nREM Complete removal of Fortuna Faucet\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\necho WARNING: This will remove Fortuna Faucet completely.\nset /p confirm=\"Are you sure? (y/N): \"\n\nif /i not \"%confirm%\"==\"y\" exit /b 0\n\nREM Find and remove MSI by UpgradeCode\nfor /f \"tokens=2 delims=\" %%A in ('wmic product where \"Name like 'Fortuna Faucet%%'\" get IdentifyingNumber /value') do (\n    for /f \"tokens=2 delims==\" %%B in (\"%%A\") do (\n        msiexec.exe /x %%B /qn /l*v \"%TEMP%\\fortuna_uninstall.log\"\n    )\n)\n\nREM Clean up directories\nif exist \"%PROGRAMFILES%\\Fortuna Faucet\" rmdir /s /q \"%PROGRAMFILES%\\Fortuna Faucet\" 2>nul\nif exist \"%APPDATA%\\Fortuna Faucet\" rmdir /s /q \"%APPDATA%\\Fortuna Faucet\" 2>nul\n\necho Uninstall complete.",
    "setup.py": "# setup.py\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nwith open(\"requirements.txt\") as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"fortuna_engine\",\n    version=\"1.0.0\",\n    packages=find_packages(),\n    author=\"Jules\",\n    author_email=\"\",\n    description=\"The Python backend for the Fortuna Faucet application.\",\n    long_description=\"This package contains the FastAPI server and all related data adapters and analysis tools.\",\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"fortuna-engine=python_service.run_api:main\",\n        ],\n    },\n    include_package_data=True,\n    package_data={\n        \"python_service\": [\"*.py\"],\n    },\n)\n",
    "tests/adapters/test_the_racing_api_adapter.py": "from datetime import date\nfrom datetime import datetime\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom python_service.core.exceptions import AdapterConfigError\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef test_settings():\n    \"\"\"Provides a valid Settings object for testing.\"\"\"\n    return get_test_settings()\n\n\ndef test_init_raises_config_error_if_no_key():\n    \"\"\"\n    Tests that the adapter raises an AdapterConfigError if the API key is not set.\n    \"\"\"\n    settings_no_key = get_test_settings()\n    settings_no_key.THE_RACING_API_KEY = None\n    with pytest.raises(AdapterConfigError) as excinfo:\n        TheRacingApiAdapter(config=settings_no_key)\n    assert \"THE_RACING_API_KEY is not configured\" in str(excinfo.value)\n\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(test_settings):\n    \"\"\"\n    Tests that TheRacingApiAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    off_time = datetime.now(timezone.utc)\n\n    mock_api_response = {\n        \"racecards\": [\n            {\n                \"race_id\": \"12345\",\n                \"course\": \"Newbury\",\n                \"race_no\": 3,\n                \"off_time\": off_time.isoformat().replace(\"+00:00\", \"Z\"),\n                \"race_name\": \"The Great Race\",\n                \"distance_f\": \"1m 2f\",\n                \"runners\": [\n                    {\n                        \"horse\": \"Speedy Steed\",\n                        \"number\": 1,\n                        \"jockey\": \"T. Rider\",\n                        \"trainer\": \"A. Trainer\",\n                        \"odds\": [{\"odds_decimal\": \"5.50\"}],\n                    },\n                    {\n                        \"horse\": \"Gallant Gus\",\n                        \"number\": 2,\n                        \"jockey\": \"J. Jockey\",\n                        \"trainer\": \"B. Builder\",\n                        \"odds\": [{\"odds_decimal\": \"3.25\"}],\n                    },\n                ],\n            }\n        ]\n    }\n\n    # Patch the internal _fetch_data method\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == \"tra_12345\"\n    assert race.venue == \"Newbury\"\n    assert len(race.runners) == 2\n    runner1 = race.runners[0]\n    assert runner1.name == \"Speedy Steed\"\n    assert runner1.odds[adapter.source_name].win == Decimal(\"5.50\")\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(test_settings):\n    \"\"\"\n    Tests that the adapter returns an empty list for an API response with no racecards.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value={\"racecards\": []})\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n\n\n@pytest.mark.asyncio\nasync def test_get_races_raises_exception_on_api_failure(test_settings):\n    \"\"\"\n    Tests that get_races propagates the exception when _fetch_data fails.\n    This is the desired behavior for the OddsEngine to handle it.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(side_effect=Exception(\"API is down\"))\n\n    # ACT & ASSERT\n    with pytest.raises(Exception, match=\"API is down\"):\n        _ = [race async for race in adapter.get_races(today)]\n",
    "tests/fixtures/at_the_races_greyhounds.html": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <title>Racecard</title>\n</head>\n<body>\n    <link rel=\"canonical\" href=\"/racecard/GB/Monmore/2025-10-29/1817/1\" />\n    <h1 class=\"heading-racecard-title\">Monmore | 18:17</h1>\n    <div class=\"table-default__row--card-runner\">\n        <div class=\"table-default__cell\">\n            <span class=\"runner-number__no\">1</span>\n            <div class=\"runner-cloth-name\">\n                <span class=\"runner-cloth-name__name\">Crossfield Larry</span>\n            </div>\n            <button class=\"bet-selector__odds\">5/2</button>\n        </div>\n    </div>\n    <div class=\"table-default__row--card-runner\">\n        <div class=\"table-default__cell\">\n            <span class=\"runner-number__no\">2</span>\n            <div class=\"runner-cloth-name\">\n                <span class=\"runner-cloth-name__name\">Stouke A Star</span>\n            </div>\n            <button class=\"bet-selector__odds\">11/4</button>\n        </div>\n    </div>\n</body>\n</html>\n",
    "tests/test_api.py": "# tests/test_api.py\nfrom datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import patch\n\nimport aiosqlite\nimport pytest\n\n# --- Fixtures ---\nfrom python_service.models import AggregatedResponse\n\n# The client fixture is now correctly sourced from conftest.py,\n# which handles the settings override globally.\n\n# --- API Tests ---\n\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine.fetch_all_odds\", new_callable=AsyncMock)\nasync def test_get_races_endpoint_success(mock_fetch_all_odds, client):\n    \"\"\"\n    SPEC: The /api/races endpoint should return data with a valid API key.\n    \"\"\"\n    # ARRANGE\n    today = date.today()\n    mock_response = AggregatedResponse(\n        date=today,\n        races=[],\n            errors=[],\n        sources=[],\n        metadata={},\n        # This was the missing field causing the validation error\n        source_info=[],\n    )\n    mock_fetch_all_odds.return_value = mock_response.model_dump()\n    from tests.conftest import get_test_settings\n    settings = get_test_settings()\n    headers = {\"X-API-Key\": settings.API_KEY}\n\n    # ACT\n    response = await client.get(f\"/api/races?race_date={today.isoformat()}\", headers=headers)\n\n    # ASSERT\n    assert response.status_code == 200\n    mock_fetch_all_odds.assert_awaited_once()\n\n\n@pytest.mark.asyncio\nasync def test_get_tipsheet_endpoint_success(tmp_path, client):\n    \"\"\"\n    SPEC: The /api/tipsheet endpoint should return a list of tipsheet races from the database.\n    \"\"\"\n    db_path = tmp_path / \"test.db\"\n    post_time = datetime.now()\n\n    with patch(\"python_service.api.DB_PATH\", db_path):\n        async with aiosqlite.connect(db_path) as db:\n            await db.execute(\n                \"\"\"\n                CREATE TABLE tipsheet (\n                    race_id TEXT PRIMARY KEY,\n                    track_name TEXT,\n                    race_number INTEGER,\n                    post_time TEXT,\n                    score REAL,\n                    factors TEXT\n                )\n            \"\"\"\n            )\n            await db.execute(\n                \"INSERT INTO tipsheet VALUES (?, ?, ?, ?, ?, ?)\",\n                (\"test_race_1\", \"Test Park\", 1, post_time.isoformat(), 85.5, \"{}\"),\n            )\n            await db.commit()\n\n        # ACT\n        response = await client.get(f\"/api/tipsheet?date={post_time.date().isoformat()}\")\n\n        # ASSERT\n        assert response.status_code == 200\n        response_data = response.json()\n        assert len(response_data) == 1\n        # The database returns snake_case, but the Pydantic model is camelCase\n        assert response_data[0][\"raceId\"] == \"test_race_1\"\n        assert response_data[0][\"score\"] == 85.5\n\n\n@pytest.mark.asyncio\nasync def test_health_check_unauthenticated(client):\n    \"\"\"Ensures the /health endpoint is accessible without an API key.\"\"\"\n    response = await client.get(\"/health\")\n    assert response.status_code == 200\n    json_response = response.json()\n    assert json_response[\"status\"] == \"healthy\"\n\n\n@pytest.mark.asyncio\nasync def test_api_key_authentication_failure(client):\n    \"\"\"Ensures that endpoints are protected and fail with an invalid API key.\"\"\"\n    response = await client.get(\"/api/races/qualified/trifecta\", headers={\"X-API-KEY\": \"invalid_key\"})\n    assert response.status_code == 403\n    assert \"Invalid or missing API Key\" in response.json()[\"detail\"]\n\n\n@pytest.mark.asyncio\nasync def test_api_key_authentication_missing(client):\n    \"\"\"Ensures that endpoints are protected and fail with a missing API key.\"\"\"\n    response = await client.get(\"/api/races/qualified/trifecta\")\n    assert response.status_code == 403\n    assert \"Not authenticated\" in response.json()[\"detail\"]\n",
    "tests/test_manual_override.py": "# tests/test_manual_override.py\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom python_service.api import app\nfrom python_service.api import get_settings\nfrom python_service.manual_override_manager import ManualOverrideManager\nfrom tests.conftest import get_test_settings\n\n# Override settings for tests\napp.dependency_overrides[get_settings] = get_test_settings\n_settings = get_test_settings()\nAPI_KEY = getattr(_settings, \"API_KEY\", \"test-override-key-123\")\n\n\n@pytest.fixture\ndef manager() -> ManualOverrideManager:\n    \"\"\"Provides a clean ManualOverrideManager instance for each test.\"\"\"\n    return ManualOverrideManager()\n\n\ndef test_register_failure(manager: ManualOverrideManager):\n    adapter_name = \"TestAdapter\"\n    url = \"http://test.com/races\"\n    request_id = manager.register_failure(adapter_name, url)\n    assert request_id is not None\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter_name\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.submit_manual_data(request_id, \"<html></html>\", \"html\")\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is not None\n    assert data[0] == \"<html></html>\"\n    assert data[1] == \"html\"\n\n\ndef test_skip_request(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.skip_request(request_id)\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is None\n\n\n@pytest.mark.asyncio\nasync def test_get_pending_overrides_endpoint(app, client):\n    # ARRANGE\n    # Access the manager *after* the TestClient has run the lifespan startup\n    manager = app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)  # Ensure a clean state by clearing all\n    manager.register_failure(\"EndpointAdapter\", \"http://endpoint.com/data\")\n\n    # ACT\n    response = await client.get(\"/api/manual-overrides/pending\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    data = response.json()\n    assert \"pending_requests\" in data\n    assert len(data[\"pending_requests\"]) > 0\n    assert data[\"pending_requests\"][0][\"adapter_name\"] == \"EndpointAdapter\"\n\n\n@pytest.mark.asyncio\nasync def test_submit_manual_data_endpoint(app, client):\n    # ARRANGE\n    manager = app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SubmitAdapter\", \"http://submit.com/data\")\n    submission = {\n        \"request_id\": request_id,\n        \"content\": \"<h1>Hello</h1>\",\n        \"content_type\": \"html\",\n    }\n    response = await client.post(\n        \"/api/manual-overrides/submit\",\n        json=submission,\n        headers={\"X-API-Key\": API_KEY},\n    )\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    data = manager.get_manual_data(\"SubmitAdapter\", \"http://submit.com/data\")\n    assert data is not None\n    assert data[0] == \"<h1>Hello</h1>\"\n\n\n@pytest.mark.asyncio\nasync def test_skip_manual_override_endpoint(app, client):\n    # ARRANGE\n    manager = app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SkipAdapter\", \"http://skip.com/data\")\n    response = await client.post(f\"/api/manual-overrides/skip/{request_id}\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    # Verify the request is no longer pending\n    pending = manager.get_pending_requests()\n    assert not any(p.request_id == request_id for p in pending)\n",
    "tests/test_uninstall.ps1": "Write-Host \"Testing uninstall...\" -ForegroundColor Cyan\n\n$regPath = 'HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*'\n$product = Get-ItemProperty $regPath | Where-Object { $_.DisplayName -like '*Fortuna*' }\n\nif ($product) {\n    & msiexec.exe /x $product.PSChildName /qn /l*v \"uninstall_test.log\"\n\n    Start-Sleep -Seconds 2\n\n    $programFiles = \"$env:PROGRAMFILES\\Fortuna Faucet\"\n    if (-not (Test-Path $programFiles)) {\n        Write-Host \"\u2713 Uninstall successful\"\n    } else {\n        Write-Host \"\u2717 Uninstall incomplete\"\n        exit 1\n    }\n} else {\n    Write-Host \"\u2717 Product not found in registry\"\n    exit 1\n}",
    "verify_connection.py": "# verify_connection.py\n\nimport asyncio\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\n\nfrom playwright.sync_api import sync_playwright\n\n# --- Configuration ---\nLOG_LEVEL = logging.INFO\n# Set paths relative to the script's location\nSCRIPT_DIR = Path(__file__).parent.resolve()\nSCREENSHOT_DIR = SCRIPT_DIR / \"verification\"\nFRONTEND_LOG_PATH = SCRIPT_DIR / \"frontend.log\"\nBACKEND_LOG_PATH = SCRIPT_DIR / \"backend.log\"\nFRONTEND_DIR = SCRIPT_DIR / \"web_platform\" / \"frontend\"\nBACKEND_DIR = SCRIPT_DIR / \"python_service\"\nBACKEND_ENTRYPOINT = BACKEND_DIR / \"api.py\"\n\n# --- Setup Logging ---\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\n\n\ndef start_backend():\n    \"\"\"Starts the FastAPI backend as a background process.\"\"\"\n    logging.info(\"Starting backend server...\")\n    # Ensure environment is set up for backend\n    backend_env = os.environ.copy()\n    backend_env[\"PYTHONPATH\"] = str(SCRIPT_DIR)\n    backend_env[\"API_KEY\"] = \"a_secure_test_api_key_that_is_long_enough\"\n    backend_env[\"ALLOWED_ORIGINS\"] = '[\"http://localhost:3000\", \"http://127.0.0.1:3000\"]'\n\n    # Use shell=True for Windows compatibility if needed, but separate args is better\n    command = [\n        sys.executable,\n        \"-m\",\n        \"uvicorn\",\n        \"python_service.api:app\",\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        \"8000\",\n    ]\n    try:\n        process = subprocess.Popen(\n            command,\n            cwd=SCRIPT_DIR,\n            stdout=open(BACKEND_LOG_PATH, \"w\"),\n            stderr=subprocess.STDOUT,\n            env=backend_env,\n        )\n        logging.info(f\"Backend process started with PID: {process.pid}\")\n        # Give the server a moment to initialize\n        time.sleep(5)\n        return process\n    except FileNotFoundError:\n        logging.error(\n            \"uvicorn command not found. Make sure it's installed in the environment.\"\n        )\n        return None\n    except Exception as e:\n        logging.error(f\"Failed to start backend: {e}\", exc_info=True)\n        return None\n\n\ndef start_frontend():\n    \"\"\"Starts the Next.js frontend dev server as a background process.\"\"\"\n    logging.info(\"Starting frontend development server...\")\n    try:\n        # Check for node_modules and run npm install if not present\n        if not (FRONTEND_DIR / \"node_modules\").exists():\n            logging.info(\"node_modules not found. Running 'npm install'...\")\n            install_process = subprocess.run(\n                [\"npm\", \"install\"],\n                cwd=FRONTEND_DIR,\n                check=True,\n                capture_output=True,\n                text=True,\n            )\n            logging.info(install_process.stdout)\n            if install_process.returncode != 0:\n                logging.error(\"npm install failed!\")\n                logging.error(install_process.stderr)\n                return None\n\n        # Start the dev server\n        process = subprocess.Popen(\n            [\"npm\", \"run\", \"dev\"],\n            cwd=FRONTEND_DIR,\n            stdout=open(FRONTEND_LOG_PATH, \"w\"),\n            stderr=subprocess.STDOUT,\n        )\n        logging.info(f\"Frontend process started with PID: {process.pid}\")\n        return process\n    except FileNotFoundError:\n        logging.error(\n            \"npm command not found. Make sure Node.js and npm are installed and in the PATH.\"\n        )\n        return None\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"npm install failed: {e.stderr}\")\n        return None\n    except Exception as e:\n        logging.error(f\"Failed to start frontend: {e}\", exc_info=True)\n        return None\n\n\ndef get_frontend_port_from_logs(log_path: Path, timeout: int = 30) -> int:\n    \"\"\"Parses the frontend log file to find the port the dev server is running on.\"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        try:\n            if log_path.exists():\n                with open(log_path, \"r\") as f:\n                    for line in f:\n                        if \"Local:\" in line and \"http://localhost:\" in line:\n                            port_str = line.split(\"http://localhost:\")[1].strip()\n                            if port_str.isdigit():\n                                port = int(port_str)\n                                logging.info(f\"Detected frontend port: {port}\")\n                                return port\n        except Exception as e:\n            logging.warning(f\"Could not read frontend log yet, retrying... Error: {e}\")\n        time.sleep(1)\n    logging.error(f\"Could not determine frontend port after {timeout} seconds.\")\n    return 3000 # Fallback\n\ndef verify_connection():\n    \"\"\"\n    Uses Playwright to verify the frontend can connect to the backend.\n    Captures a screenshot for visual confirmation.\n    \"\"\"\n    backend_process = None\n    frontend_process = None\n    success = False\n\n    try:\n        backend_process = start_backend()\n        if not backend_process or backend_process.poll() is not None:\n            logging.error(\"Backend failed to start or crashed.\")\n            return\n\n        frontend_process = start_frontend()\n        if not frontend_process or frontend_process.poll() is not None:\n            logging.error(\"Frontend failed to start or crashed.\")\n            return\n\n        logging.info(\"Waiting for frontend to be ready and getting port...\")\n        port = get_frontend_port_from_logs(FRONTEND_LOG_PATH)\n\n\n        with sync_playwright() as p:\n            logging.info(\"Launching browser...\")\n            browser = p.chromium.launch(headless=True)\n            page = browser.new_page()\n\n            logging.info(f\"Navigating to frontend URL at port {port}...\")\n            page.goto(f\"http://localhost:{port}\")\n\n            # Wait for a specific element that indicates a successful connection\n            # or a definitive disconnected state.\n            logging.info(\"Waiting for connection status indicator...\")\n            try:\n                # Wait up to 30 seconds for either a 'Connected' or 'Failed' state\n                page.wait_for_selector(\n                    'text=/Connecting...|Connected|Connection Failed/',\n                    timeout=30000\n                )\n\n                # Check the current status\n                connection_status = page.locator('//button[contains(@class, \"rounded-full\")]').inner_text()\n                logging.info(f\"Connection status found: {connection_status}\")\n                if \"Connected\" in connection_status:\n                    logging.info(\"Successfully connected to the backend.\")\n                    success = True\n                else:\n                    logging.error(f\"Frontend indicated a connection failure: {connection_status}\")\n\n\n            except Exception as e:\n                logging.error(f\"Failed to find connection status indicator: {e}\", exc_info=True)\n\n            logging.info(\"Capturing screenshot...\")\n            SCREENSHOT_DIR.mkdir(exist_ok=True)\n            screenshot_path = SCREENSHOT_DIR / \"debug_screenshot.png\"\n            page.screenshot(path=str(screenshot_path))\n            logging.info(f\"Screenshot saved to {screenshot_path}\")\n\n            browser.close()\n\n    finally:\n        logging.info(\"Cleaning up processes...\")\n        if frontend_process and frontend_process.poll() is None:\n            logging.info(f\"Terminating frontend process {frontend_process.pid}\")\n            frontend_process.terminate()\n            frontend_process.wait()\n        if backend_process and backend_process.poll() is None:\n            logging.info(f\"Terminating backend process {backend_process.pid}\")\n            backend_process.terminate()\n            backend_process.wait()\n        logging.info(\"Cleanup complete.\")\n        # Exit with success or failure code\n        if not success:\n            sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    verify_connection()\n",
    "web_service/backend/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass AtTheRacesAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for attheraces.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"AtTheRaces\"\n    BASE_URL = \"https://www.attheraces.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(\n            self.http_client, \"GET\", index_url, headers=self._get_headers()\n        )\n        if not index_response:\n            self.logger.warning(\"Failed to fetch AtTheRaces index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\n                self.http_client, \"GET\", url_path, headers=self._get_headers()\n            )\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.attheraces.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        all_races = []\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to AtTheRacesAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                header_element = soup.select_one(\"h1.heading-racecard-title\")\n                if not header_element:\n                    continue\n                header = header_element.get_text()\n                track_name_raw, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n                track_name = normalize_venue_name(track_name_raw)\n                active_link = soup.select_one(\"a.race-time-link.active\")\n                race_number = 1\n                if active_link:\n                    parent_div = active_link.find_parent(\"div\", \"races\")\n                    if parent_div:\n                        all_links = parent_div.select(\"a.race-time-link\")\n                        race_number = all_links.index(active_link) + 1\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time, \"%H:%M\").time())\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n                race = Race(\n                    id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from AtTheRaces, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_element = row.select_one(\"h3.horse-name a\")\n            if not name_element:\n                return None\n            name = clean_text(name_element.get_text())\n\n            num_element = row.select_one(\"span.horse-number\")\n            if not num_element:\n                return None\n            num_str = clean_text(num_element.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_element = row.select_one(\"button.best-odds\")\n            odds_str = clean_text(odds_element.get_text()) if odds_element else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on AtTheRaces, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairGreyhounds\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Failed to parse a Betfair Greyhound market.\",\n                    exc_info=True,\n                    market=market,\n                )\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Optional[Race]:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "web_service/backend/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, \"GREYHOUND_API_URL\") or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "web_service/backend/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n# NOTE: This is a hypothetical implementation based on a potential API structure.\n\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for the hypothetical PointsBet Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches all greyhound events for a given date.\"\"\"\n        endpoint = f\"sports/greyhound-racing/events/by-date/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json().get(\"events\", []) if response else None\n\n    def _parse_races(self, raw_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw event data into a list of standardized Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for event in raw_data:\n            try:\n                if not event.get(\"competitors\") or not event.get(\"startTime\"):\n                    continue\n\n                runners = []\n                for competitor in event.get(\"competitors\", []):\n                    price = competitor.get(\"price\")\n                    if not price:\n                        continue\n\n                    odds_val = Decimal(str(price))\n                    odds = {\n                        self.source_name: OddsData(\n                            win=odds_val,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n                    runner = Runner(\n                        number=competitor.get(\"number\", 99),\n                        name=competitor.get(\"name\", \"Unknown\"),\n                        odds=odds,\n                    )\n                    runners.append(runner)\n\n                if runners:\n                    race_id = event.get(\"id\")\n                    if not race_id:\n                        continue\n\n                    race = Race(\n                        id=f\"pbg_{race_id}\",\n                        venue=event.get(\"venue\", {}).get(\"name\", \"Unknown Venue\"),\n                        start_time=datetime.fromisoformat(event[\"startTime\"]),\n                        race_number=event.get(\"raceNumber\", 1),\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.warning(\n                    \"Failed to parse PointsBet Greyhound event.\",\n                    event=event,\n                    exc_info=True,\n                )\n                continue\n        return races\n",
    "web_service/backend/adapters/racingtv_adapter.py": "# python_service/adapters/racingtv_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingTVAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping data from racingtv.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com/horse-racing\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(\n            self.http_client, \"GET\", index_url, headers=self._get_headers()\n        )\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\n                self.http_client, \"GET\", url_path, headers=self._get_headers()\n            )\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.timeform.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/core/__init__.py": "",
    "web_service/backend/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return (\n                None,\n                \"Missing core fields (id, venue, race_number, start_time, runners)\",\n            )\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\"\n                        )\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "web_service/backend/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"127.0.0.1\", port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "web_service/backend/middleware/__init__.py": "",
    "web_service/backend/port_check.py": "import socket\nimport sys\n\ndef check_port_and_exit_if_in_use(port: int, host: str):\n    \"\"\"Checks if a port is in use at the given host and exits if it is.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.bind((host, port))\n        except OSError:\n            print(f\"\u274c FATAL: Port {port} is already in use. Please close the other application or specify a different port.\")\n            sys.exit(1)\n",
    "web_service/backend/requirements-dev.txt": "#\n# Development & Build-Time Dependencies\n# This file should be used for setting up a development or CI/CD environment.\n#\n\n-r requirements.txt\n\n# --- Build Tools ---\npip-tools\n\n# --- Testing Tools ---\npytest\npytest-asyncio\nfakeredis\nrespx\n\n# --- Linting & Auditing ---\nblack\nruff\npip-audit\nsetuptools<81\n",
    "web_service/backend/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "web_service/backend/windows_compat.py": "\"\"\"\nWindows Compatibility Utilities\n\nCRITICAL: This module MUST be imported and called at the top of EVERY entry point\nthat uses asyncio or uvicorn in a PyInstaller bundle on Windows.\n\nWithout this fix, the asyncio event loop will fail to bind network ports, causing\nsilent failures where uvicorn reports \"Application startup complete\" but the\nservice is actually inaccessible.\n\"\"\"\n\nimport sys\n\n\ndef setup_windows_event_loop():\n    \"\"\"\n    Configure Windows event loop policy for PyInstaller bundles.\n\n    This MUST be called BEFORE any asyncio or uvicorn initialization.\n\n    Context:\n    - PyInstaller bundles on Windows have a broken default event loop policy\n    - The default policy (ProactorEventLoop) silently fails to bind ports\n    - WindowsSelectorEventLoopPolicy is the only policy that works reliably\n\n    This function is idempotent and safe to call multiple times.\n    \"\"\"\n    if sys.platform == 'win32' and getattr(sys, 'frozen', False):\n        import asyncio\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n        print('[BOOT] \u2713 Applied WindowsSelectorEventLoopPolicy for PyInstaller',\n              file=sys.stderr)\n    else:\n        # Not Windows or not frozen - no action needed\n        pass\n",
    "web_service/frontend/app/components/LiveModeToggle.tsx": "// web_platform/frontend/src/components/LiveModeToggle.tsx\n'use client';\n\nimport React from 'react';\n\ninterface LiveModeToggleProps {\n  isLive: boolean;\n  onToggle: (isLive: boolean) => void;\n  isDisabled: boolean;\n}\n\nexport const LiveModeToggle: React.FC<LiveModeToggleProps> = ({ isLive, onToggle, isDisabled }) => {\n  const handleToggle = () => {\n    if (!isDisabled) {\n      onToggle(!isLive);\n    }\n  };\n\n  return (\n    <button\n      onClick={handleToggle}\n      disabled={isDisabled}\n      className={`relative inline-flex items-center h-8 rounded-full w-32 transition-colors duration-300 ease-in-out focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-offset-slate-800 focus:ring-blue-500 ${\n        isDisabled ? 'cursor-not-allowed bg-slate-700' : 'cursor-pointer'\n      } ${isLive ? 'bg-green-600' : 'bg-slate-600'}`}\n    >\n      <span className=\"sr-only\">Toggle Live Mode</span>\n      <span\n        className={`absolute left-1 top-1 inline-block w-6 h-6 rounded-full bg-white transform transition-transform duration-300 ease-in-out ${\n          isLive ? 'translate-x-[104px]' : 'translate-x-0'\n        }`}\n      />\n      <span\n        className={`absolute left-8 transition-opacity duration-200 ease-in-out ${\n          !isLive && !isDisabled ? 'opacity-100' : 'opacity-50'\n        }`}\n      >\n        Poll\n      </span>\n      <span\n        className={`absolute right-4 transition-opacity duration-200 ease-in-out ${\n          isLive && !isDisabled ? 'opacity-100' : 'opacity-50'\n        }`}\n      >\n        \u26a1 Live\n      </span>\n    </button>\n  );\n};\n",
    "web_service/frontend/app/components/RaceFilters.tsx": "// web_platform/frontend/src/components/RaceFilters.tsx\n'use client';\n\nimport { useState, useCallback } from 'react';\nimport { Settings, RotateCcw } from 'lucide-react';\n\ninterface FilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nexport interface RaceFiltersProps {\n  onParamsChange: (params: FilterParams) => void;\n  isLoading: boolean;\n  refetch: () => void;\n}\n\nconst DEFAULT_PARAMS: FilterParams = {\n  maxFieldSize: 10,\n  minFavoriteOdds: 2.5,\n  minSecondFavoriteOdds: 4.0,\n};\n\nexport function RaceFilters({ onParamsChange, isLoading, refetch }: RaceFiltersProps) {\n  const [params, setParams] = useState<FilterParams>(DEFAULT_PARAMS);\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  // Handle individual parameter changes\n  const handleChange = useCallback((key: keyof FilterParams, value: number) => {\n    setParams(prev => {\n      const updated = { ...prev, [key]: value };\n      onParamsChange(updated);\n      return updated;\n    });\n    // Debounce the refetch call\n    const timer = setTimeout(() => {\n      refetch();\n    }, 500);\n    return () => clearTimeout(timer);\n  }, [onParamsChange, refetch]);\n\n  // Reset to defaults\n  const handleReset = useCallback(() => {\n    setParams(DEFAULT_PARAMS);\n    onParamsChange(DEFAULT_PARAMS);\n    refetch();\n  }, [onParamsChange, refetch]);\n\n  return (\n    <div className=\"bg-gradient-to-r from-slate-800 to-slate-900 rounded-lg p-4 mb-6 border border-slate-700\">\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-2\">\n          <Settings className=\"w-5 h-5 text-amber-500\" />\n          <h3 className=\"text-lg font-semibold text-white\">Race Filters</h3>\n        </div>\n        <button\n          onClick={() => setIsExpanded(!isExpanded)}\n          className=\"text-sm text-slate-400 hover:text-slate-200 transition\"\n        >\n          {isExpanded ? 'Hide' : 'Show'}\n        </button>\n      </div>\n\n      {isExpanded && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 pt-4 border-t border-slate-700\">\n          {/* Max Field Size */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Max Field Size\n              <span className=\"text-amber-500 ml-2\">{params.maxFieldSize}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2\"\n              max=\"20\"\n              value={params.maxFieldSize}\n              onChange={(e) => handleChange('maxFieldSize', parseInt(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Filters races with larger fields</p>\n          </div>\n\n          {/* Min Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"1.5\"\n              max=\"5\"\n              step=\"0.1\"\n              value={params.minFavoriteOdds}\n              onChange={(e) => handleChange('minFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = pickier favorites</p>\n          </div>\n\n          {/* Min Second Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min 2nd Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minSecondFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2.0\"\n              max=\"8\"\n              step=\"0.1\"\n              value={params.minSecondFavoriteOdds}\n              onChange={(e) => handleChange('minSecondFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = better odds separation</p>\n          </div>\n\n          {/* Reset Button */}\n          <div className=\"md:col-span-3 flex justify-end pt-4 border-t border-slate-700\">\n            <button\n              onClick={handleReset}\n              disabled={isLoading}\n              className=\"inline-flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 text-slate-200 rounded text-sm font-medium transition disabled:opacity-50\"\n            >\n              <RotateCcw className=\"w-4 h-4\" />\n              Reset to Defaults\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n}\n",
    "web_service/frontend/app/components/TrifectaFactors.tsx": "// TrifectaFactors.tsx - FINAL, DYNAMIC VERSION\n'use client';\nimport React from 'react';\n\ninterface TrifectaFactorsProps {\n  factorsJson: string | null;\n}\n\nexport function TrifectaFactors({ factorsJson }: TrifectaFactorsProps) {\n  if (!factorsJson) {\n    return <div className=\"text-sm text-gray-500\">No analysis factors available.</div>;\n  }\n\n  try {\n    const factors = JSON.parse(factorsJson);\n    const positiveFactors = Object.entries(factors).filter(([key, value]: [string, any]) => value.ok);\n\n    if (positiveFactors.length === 0) {\n      return <div className=\"text-sm text-gray-500\">No positive factors identified.</div>;\n    }\n\n    return (\n      <div className=\"mt-2 text-xs\">\n        <h4 className=\"font-semibold mb-1\">Key Factors:</h4>\n        <ul className=\"list-disc list-inside space-y-1\">\n          {positiveFactors.map(([key, value]: [string, any]) => (\n            <li key={key} className=\"text-gray-700\">\n              <span className=\"font-medium text-green-600\">\u2713</span> {value.reason} ({value.points > 0 ? `+${value.points}` : value.points} pts)\n            </li>\n          ))}\n        </ul>\n      </div>\n    );\n  } catch (error) {\n    console.error(\"Failed to parse trifecta factors:\", error);\n    return <div className=\"text-sm text-red-500\">Error displaying analysis factors.</div>;\n  }\n}",
    "web_service/frontend/app/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n",
    "web_service/frontend/next-env.d.ts": "/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.\n",
    "wix/WixUI_CustomInstallDir.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\"\n     xmlns:WixUI=\"http://schemas.microsoft.com/wix/WixUIExtension\">\n  <Fragment>\n    <UI Id=\"WixUI_CustomInstallDir\">\n        <DialogRef Id=\"BrowseDlg\" />\n        <DialogRef Id=\"DiskCostDlg\" />\n        <DialogRef Id=\"ErrorDlg\" />\n        <DialogRef Id=\"FatalError\" />\n        <DialogRef Id=\"FilesInUse\" />\n        <DialogRef Id=\"MsiRMFilesInUse\" />\n        <DialogRef Id=\"PrepareDlg\" />\n        <DialogRef Id=\"UserExit\" />\n        <DialogRef Id=\"WelcomeDlg\" />\n        <DialogRef Id=\"InstallDirDlg\" />\n        <DialogRef Id=\"VerifyReadyDlg\" />\n\n        <!-- Use our custom progress dialog instead of the default -->\n        <DialogRef Id=\"InstallProgressDlg\" />\n\n        <Publish Dialog=\"WelcomeDlg\" Control=\"Next\" Event=\"NewDialog\" Value=\"InstallDirDlg\">1</Publish>\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"WelcomeDlg\">1</Publish>\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Next\" Event=\"SetTargetPath\" Value=\"[WIXUI_INSTALLDIR]\" Order=\"1\" />\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Next\" Event=\"NewDialog\" Value=\"VerifyReadyDlg\" Order=\"2\">1</Publish>\n        <Publish Dialog=\"VerifyReadyDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"InstallDirDlg\" Order=\"1\">NOT Installed</Publish>\n        <Publish Dialog=\"VerifyReadyDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"MaintenanceTypeDlg\" Order=\"2\">Installed</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}