{
    ".github/dependabot.yml": "# System Timestamp: 2025-11-29 13:19:26.933797\n# To get started with Dependabot version updates, you'll need to specify which\n# package ecosystems to update and where the package manifests are located.\n# Please see the documentation for all configuration options:\n# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates\n\nversion: 2\nupdates:\n  - package-ecosystem: \"pip\" # See documentation for possible values\n    directory: \"/\" # Location of package manifests\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/web_service/frontend\"\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/electron\"\n    schedule:\n      interval: \"daily\"\n",
    ".github/workflows/build-monolith-tinyfield.yml.deactivated": "name: Build Monolith (TinyField)\n\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  build:\n    name: 'Build Fortuna Monolith (TinyField)'\n    runs-on: windows-latest\n\n    steps:\n      - name: \ud83d\udce5 Checkout\n        uses: actions/checkout@v4\n\n      # ========== FRONTEND ==========\n      - name: \ud83c\udfa8 Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - name: \ud83e\uddf9 Clean Previous Builds\n        working-directory: ./web_service/frontend\n        shell: pwsh\n        run: |\n          Remove-Item -Path \"public\" -Recurse -Force -ErrorAction SilentlyContinue\n          Remove-Item -Path \".next\" -Recurse -Force -ErrorAction SilentlyContinue\n          Write-Host \"\u2705 Cleaned\"\n\n      - name: \ud83d\udccb Check Next.js Config\n        working-directory: ./web_service/frontend\n        shell: pwsh\n        run: |\n          Write-Host \"Checking package.json...\"\n          if (Test-Path \"package.json\") {\n            $pkg = Get-Content package.json | ConvertFrom-Json\n            Write-Host \"  name: $($pkg.name)\"\n            Write-Host \"  build: $($pkg.scripts.build)\"\n          } else {\n            Write-Error \"package.json not found!\"\n            exit 1\n          }\n\n      - name: \ud83c\udfd7\ufe0f Build Frontend\n        working-directory: ./web_service/frontend\n        shell: pwsh\n        run: |\n          Write-Host \"=== BUILDING FRONTEND ===\" -ForegroundColor Cyan\n\n          # Create/verify next.config.js\n          $configLines = @(\n            \"/** @type {import('next').NextConfig} */\",\n            \"const nextConfig = {\",\n            \"  output: 'export',\",\n            \"  distDir: 'build',\",\n            \"  images: { unoptimized: true },\",\n            \"  trailingSlash: true,\",\n            \"}\",\n            \"module.exports = nextConfig\"\n          )\n          $configContent = $configLines -join [System.Environment]::NewLine\n          Set-Content -Path \"next.config.js\" -Value $configContent -Encoding UTF8\n          Write-Host \"\u2705 next.config.js set for 'build' directory output\"\n\n          Write-Host \"Installing dependencies...\"\n          npm install --legacy-peer-deps\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"npm install failed\"\n            exit 1\n          }\n\n          Write-Host \"Building...\"\n          npm run build\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"npm run build failed\"\n            exit 1\n          }\n\n          Write-Host \"Moving build artifacts to 'public'...\"\n          New-Item -ItemType Directory -Force -Path \"public\" | Out-Null\n          Move-Item -Path \"build/*\" -Destination \"public\" -Force\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"Failed to move build artifacts\"\n            exit 1\n          }\n          Write-Host \"\u2705 Artifacts moved.\"\n\n          # Verify output\n          Write-Host \"`nVerifying output...\"\n          if (-not (Test-Path \"public\")) {\n            Write-Host \"\u274c 'public' directory not found!\" -ForegroundColor Red\n            Write-Host \"Contents of current dir:\"\n            Get-ChildItem | Format-Table Name\n            exit 1\n          }\n\n          if (-not (Test-Path \"public/index.html\")) {\n            Write-Host \"\u274c public/index.html not found!\" -ForegroundColor Red\n            Write-Host \"Contents of 'public':\"\n            Get-ChildItem -Path \"public\" -Recurse | Format-Table Name\n            exit 1\n          }\n\n          $count = (Get-ChildItem -Path \"public\" -Recurse -File).Count\n          Write-Host \"\u2705 Frontend built: $count files\" -ForegroundColor Green\n\n      # ========== BACKEND ==========\n      - name: \ud83d\udc0d Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10.11'\n\n      - name: \ud83d\udce6 Install Python Dependencies\n        shell: pwsh\n        run: |\n          pip install --upgrade pip wheel\n          pip install pyinstaller==6.6.0\n          pip install pywin32 # CRITICAL: Provides the 'win32timezone' hidden import\n          pip install -r web_service/backend/requirements.txt\n\n      - name: \ud83d\udcc2 Create Data Directories\n        shell: pwsh\n        run: |\n          New-Item -ItemType Directory -Force -Path \"web_service/backend/data\" | Out-Null\n          New-Item -ItemType Directory -Force -Path \"web_service/backend/json\" | Out-Null\n          New-Item -ItemType Directory -Force -Path \"web_service/backend/logs\" | Out-Null\n\n      # ========== BUILD ==========\n      - name: \ud83d\udd28 Build with PyInstaller\n        shell: pwsh\n        run: |\n          Write-Host \"=== BUILDING MONOLITH ===\" -ForegroundColor Cyan\n\n          # Final verification\n          if (-not (Test-Path \"web_service/frontend/public/index.html\")) {\n            Write-Error \"\u274c Frontend not ready!\"\n            Write-Host \"Frontend path check:\"\n            Test-Path \"web_service/frontend/public\"\n            Test-Path \"web_service/frontend/public/index.html\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Frontend verified\"\n          Write-Host \"Running PyInstaller...\"\n\n          pyinstaller --noconfirm --clean fortuna-desktop-tinyfield.spec 2>&1 | Tee-Object -FilePath \"build.log\"\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Host \"Last 100 lines of build.log:\" -ForegroundColor Red\n            Get-Content \"build.log\" -Tail 100\n            exit 1\n          }\n\n          $exe = \"dist/Fortuna-Desktop-TinyField/Fortuna-Desktop-TinyField.exe\"\n          if (Test-Path $exe) {\n            $mb = (Get-Item $exe).Length / 1MB\n            Write-Host \"\u2705 BUILD SUCCESS: $([math]::Round($mb, 2)) MB\" -ForegroundColor Green\n          } else {\n            Write-Error \"EXE not created!\"\n            exit 1\n          }\n\n      # ========== PACKAGE & UPLOAD ==========\n      - name: \ud83d\udce6 Package Artifact for Distribution\n        shell: pwsh\n        run: |\n          $distDir = \"dist/Fortuna-Desktop-TinyField\"\n          Write-Host \"=== PACKAGING ARTIFACT ===\" -ForegroundColor Cyan\n\n          # Create README.txt\n          $readmeLines = @(\n            'Fortuna Desktop (TinyField) - User Guide',\n            '',\n            '**HOW TO RUN**',\n            '1. Double-click \"Fortuna-Desktop-TinyField.exe\".',\n            '2. The application window will open and the dashboard will load.',\n            '',\n            '**WHAT TO EXPECT**',\n            '- To stop the application, simply close the window.'\n          )\n          $readmeLines | Out-File -FilePath \"$distDir/README.txt\" -Encoding utf8\n          Write-Host \"\u2705 Created README.txt\"\n\n          # Create ZIP archive\n          $zipFileName = \"Fortuna-Desktop-TinyField-Windows-${{ github.run_number }}.zip\"\n          Compress-Archive -Path \"$distDir/*\" -DestinationPath $zipFileName -Force\n          Write-Host \"\u2705 Created $zipFileName\" -ForegroundColor Green\n\n      - name: \ud83d\udce4 Upload Packaged Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: Fortuna-Desktop-TinyField-Windows-${{ github.run_number }}\n          path: Fortuna-Desktop-TinyField-Windows-${{ github.run_number }}.zip\n          if-no-files-found: error\n\n      # ========== TEST ==========\n      - name: Install VC++ Redistributable\n        shell: pwsh\n        run: |\n          # Download and install VC++ Runtime\n          $url = \"https://aka.ms/vs/17/release/vc_redist.x64.exe\"\n          Invoke-WebRequest -Uri $url -OutFile vc_redist.x64.exe\n          .\\vc_redist.x64.exe /install /quiet /norestart\n\n      - name: \ud83e\uddea Smoke Test\n        continue-on-error: true\n        shell: pwsh\n        timeout-minutes: 5\n        run: |\n          Write-Host \"=== SMOKE TEST ===\" -ForegroundColor Cyan\n          $distDir = \"dist/Fortuna-Desktop-TinyField\"\n          $exe = \"$distDir/Fortuna-Desktop-TinyField.exe\"\n          $outputLog = \"$distDir/smoke-test-output.log\"\n          $internalLog = \"$distDir/fortuna-desktop-tinyfield.log\" # Assumes log is named after exe\n\n          Write-Host \"--- 1. Directory Manifest ---\" -ForegroundColor Yellow\n          Get-ChildItem -Path $distDir -Recurse | Format-Table Name, Length\n          if (-not (Test-Path $exe)) {\n            Write-Error \"FATAL: Executable not found at $exe\"\n            exit 1\n          }\n\n          Write-Host \"--- 2. Network State Dump ---\" -ForegroundColor Yellow\n          netstat -ano\n          Write-Host \"--- End of Network State ---\"\n\n          Write-Host \"--- 3. Starting Executable ---\" -ForegroundColor Yellow\n          Write-Host \"Starting: $exe\"\n          cmd /c \"start /b `\"$exe`\" > `\"$outputLog`\" 2>&1\"\n          Start-Sleep -Seconds 10 # Increased wait for GUI app\n\n          $proc = Get-Process -Name \"Fortuna-Desktop-TinyField\" -ErrorAction SilentlyContinue\n          if (-not $proc) {\n              Write-Host \"\u274c Process failed to start!\" -ForegroundColor Red\n              # Failure analysis will run at the end\n              $success = $false\n          } else {\n              Write-Host \"PID: $($proc.Id). Waiting up to 45 seconds for health check...\"\n              $success = $false\n              for ($i = 1; $i -le 45; $i++) {\n                  if ($proc.HasExited) {\n                      Write-Host \"\u274c Process exited prematurely (code: $($proc.ExitCode))\" -ForegroundColor Red\n                      break\n                  }\n                  try {\n                      $response = Invoke-WebRequest -Uri \"http://127.0.0.1:8000/api/health\" -UseBasicParsing -TimeoutSec 1 -ErrorAction Stop\n                      if ($response.StatusCode -eq 200) {\n                          Write-Host \"\u2705 SMOKE TEST PASSED!\" -ForegroundColor Green\n                          $success = $true\n                          break\n                      }\n                  } catch {\n                      Write-Host \"Health check attempt $i failed. Retrying... Error: $($_.Exception.Message)\"\n                      Start-Sleep -Seconds 1\n                  }\n              }\n          }\n\n          # --- Cleanup ---\n          if ($proc -and -not $proc.HasExited) {\n              Stop-Process -Id $proc.Id -Force -ErrorAction SilentlyContinue\n          }\n\n          # --- Final Verdict & Diagnostics ---\n          if ($success) {\n            Write-Host \"Smoke test SUCCEEDED.\"\n            exit 0\n          } else {\n            Write-Error \"SMOKE TEST FAILED.\"\n\n            Write-Host \"--- 4. Failure Analysis ---\" -ForegroundColor Yellow\n            if (Test-Path $outputLog) {\n                Write-Host \"--- Displaying startup output log (smoke-test-output.log) ---\"\n                Get-Content $outputLog -ErrorAction SilentlyContinue\n            } else {\n                Write-Host \"--- WARNING: Startup output log not found! ($outputLog) ---\"\n            }\n\n            if (Test-Path $internalLog) {\n                Write-Host \"--- Displaying internal application log (fortuna-monolith.log) ---\"\n                Get-Content $internalLog -ErrorAction SilentlyContinue\n            } else {\n                Write-Host \"--- WARNING: Internal application log not found! ($internalLog) ---\"\n                Write-Host \"This usually means the application crashed before it could initialize its logging.\"\n            }\n            exit 1\n          }\n\n      - name: \ud83d\udccb Upload Build Log\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: build-log-${{ github.run_number }}\n          path: build.log\n          if-no-files-found: ignore\n",
    "JSON_BACKUP_MANIFEST.md": "# Checkmate Ultimate Solo: JSON Backup Manifest (Total Recall Edition)\n\n**Purpose:** To provide a single, complete, and verified list of direct links to the JSON backups of all CORE and Operational files. This is the definitive entry point for external AI code review.\n\n---\n\n## 1.0 CORE Architecture (JSON Backups)\n\n### Python Backend\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/api.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/engine.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/models.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/__init__.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/base.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/utils.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/betfair_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/pointsbet_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/racing_and_sports_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/tvg_adapter.py.json\n\n### TypeScript Frontend\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/package.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/package-lock.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/next.config.mjs.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/tailwind.config.ts.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/tsconfig.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/page.tsx.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/layout.tsx.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/globals.css.json\n\n---\n\n## 2.0 Operational & Tooling (JSON Backups)\n\n### Project Tooling\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/.gitignore.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/convert_to_json.py.json\n\n### Environment & Setup\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/setup_windows.bat.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/.env.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/requirements.txt.json\n\n### Strategic Blueprints\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/README.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/ARCHITECTURAL_MANDATE.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/HISTORY.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/STATUS.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/WISDOM.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/PROJECT_MANIFEST.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/ROADMAP_APPENDICES.md.json",
    "README_WINDOWS.md": "# \ud83d\udc34 Fortuna Faucet - User Guide for Windows\n\nWelcome to Fortuna Faucet! This guide provides simple, step-by-step instructions to get you up and running.\n\n## Installation\n\nInstalling the application is a straightforward process using our official installer.\n\n1.  **Download the Installer:**\n    *   Go to the [**Latest Release Page**](https://github.com/masonj0/fortuna/releases/latest) on GitHub.\n    *   Download the file ending in `.msi` (e.g., `JBMason's 1st App-X.X.X.msi`).\n\n2.  **Run the Installer:**\n    *   Double-click the downloaded `.msi` file to launch the setup wizard.\n    *   Follow the on-screen instructions to complete the installation.\n\n## What to Expect After Installation\n\nOnce the setup is complete, you will find a new folder in your Start Menu named **\"JBMason's 1st App\"**.\n\n*   **Launching the App:** Inside this folder, click on the **\"JBMason's 1st App\"** shortcut to start the application.\n*   **How it Works:** The shortcut launches the main application window (the dashboard). The backend data engine starts automatically in the background and will close when you exit the application.\n\nThat's it! All previous installation methods are now obsolete. Enjoy using the application!\n",
    "START_DEV_ENVIRONMENT.bat": "@echo off\nREM This script provides a user-friendly, double-clickable way to start the\nREM development environment by running the fortuna-quick-start.ps1 script.\nREM It bypasses the system's PowerShell execution policy for this script only.\n\necho Starting Fortuna Faucet Development Environment...\necho This will open two new terminal windows for the backend and frontend.\n\npowershell.exe -ExecutionPolicy Bypass -File \"%~dp0scripts\\fortuna-quick-start.ps1\"\n\necho.\necho Script execution finished. The development servers are running in new windows.\npause\n",
    "USER_GUIDE.MD": "# Fortuna Faucet - User Guide\n\nThis document provides instructions for users of the Fortuna Faucet application.\n\n## Installation\n\n1.  Download the `Fortuna-Faucet-Installer.msi` file from the latest release.\n2.  Run the installer and follow the on-screen instructions.\n3.  The application will be installed to `C:\\Program Files\\Fortuna Faucet` by default.\n\n## Usage\n\n1.  Launch the application from the Start Menu or desktop shortcut.\n2.  The main window will display the live race dashboard.\n3.  Use the settings page to configure your preferences.\n",
    "e2e/jules-smoke-test.py": "import asyncio\nfrom playwright.async_api import async_playwright, expect\n\nasync def main():\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        try:\n            await page.goto(\"http://127.0.0.1:8000\")\n            await expect(page.get_by_test_id(\"main-heading\")).to_be_visible()\n        finally:\n            await page.screenshot(path=\"playwright-screenshot.png\")\n            await browser.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "electron/assets/.gitkeep": "# This directory is for application icons (e.g., icon.ico, tray-icon.png)",
    "fortuna-backend-hooks/hook-tenacity.py": "\"\"\"\nPyInstaller hook for tenacity.\n\nTenacity uses dynamic imports for async support and retry strategies that\nPyInstaller cannot automatically detect. This hook ensures all tenacity\nsubmodules are collected into the bundle.\n\nThis is especially critical for tenacity 8.2.3+ which includes async retry support.\n\"\"\"\n\nfrom PyInstaller.utils.hooks import collect_submodules\n\n# Collect all tenacity submodules recursively\nhiddenimports = collect_submodules('tenacity')\n\n# Explicitly add critical submodules that might be missed\n# These are the modules tenacity dynamically imports for retry strategies and async support\ncritical_submodules = [\n    'tenacity.retry',\n    'tenacity.stop',\n    'tenacity.wait',\n    'tenacity.retry_if_result',\n    'tenacity.retry_if_exception',\n    'tenacity.before_sleep',\n    'tenacity.after',\n    'tenacity.before',\n    'tenacity.retry_error',\n    'tenacity.compat',\n    'tenacity.future',\n    'tenacity.asyncio',  # Critical for async retry support\n]\n\n# Merge and deduplicate\nhiddenimports = list(set(hiddenimports + critical_submodules))\n",
    "fortuna-desktop-tinyfield.spec": "# fortuna-desktop.spec\n# This spec file is for creating a windowed, GUI-based application\n# using pywebview. It is based on fortuna-monolith.spec.\n\nfrom PyInstaller.utils.hooks import collect_submodules\nfrom pathlib import Path\nimport sys\nimport os\n\nblock_cipher = None\n\n# ===== GET PROJECT ROOT =====\nspec_path = Path(SPECPATH) if 'SPECPATH' in dir() else Path(os.path.dirname(os.path.abspath(__file__)))\nproject_root = spec_path.parent if spec_path.name == 'fortuna-desktop-tinyfield.spec' else spec_path\n\n# ===== FRONTEND VALIDATION =====\nfrontend_out = project_root / 'web_service' / 'frontend' / 'public'\nif not frontend_out.exists() or not (frontend_out / 'index.html').exists():\n    print(\"[ERROR] FATAL: Frontend 'public' directory with index.html not found!\")\n    sys.exit(1)\n\n# ===== BACKEND VALIDATION =====\nbackend_root = project_root / 'web_service' / 'backend'\nmain_script = project_root / 'run_desktop_app.py'\nif not main_script.exists():\n    print(f\"[ERROR] FATAL: Main script not found at {main_script}!\")\n    sys.exit(1)\n\n# ===== DATA FILES =====\ndatas = [\n    (str(frontend_out), 'public')\n]\n\n# ===== HIDDEN IMPORTS =====\nhiddenimports = list(set(\n    collect_submodules('web_service.backend') +\n    [\n        'uvicorn', 'uvicorn.logging', 'uvicorn.loops', 'uvicorn.loops.auto',\n        'uvicorn.protocols', 'uvicorn.protocols.http', 'uvicorn.protocols.http.auto',\n        'uvicorn.protocols.http.h11_impl', 'uvicorn.lifespan', 'uvicorn.lifespan.on',\n        'fastapi', 'starlette', 'pydantic', 'anyio', 'structlog', 'tenacity',\n        'sqlalchemy', 'greenlet', 'win32timezone'\n    ]\n))\n\n# ===== ANALYSIS =====\na = Analysis(\n    [str(main_script)],\n    pathex=[str(project_root), str(backend_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    cipher=block_cipher,\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\n# ===== BUILD EXECUTABLE (WINDOWED) =====\nexe = EXE(\n    pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n    name='Fortuna-Desktop-TinyField',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    runtime_tmpdir=None,\n    console=False,  # This creates a windowed application\n    disable_windowed_traceback=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon=None # Consider adding an icon here later\n)\n\ncoll = COLLECT(\n    exe, a.binaries, a.zipfiles, a.datas,\n    strip=False,\n    upx=True,\n    name='Fortuna-Desktop-TinyField'\n)\n",
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "pre-build-check.sh": "#!/bin/bash\n\necho -e \"\\e[36m=== FORTUNA PRE-BUILD VERIFICATION ===\\e[0m\"\n\n# 1. Check all required files exist\necho -e \"\\n\\e[1m[1] Checking required files...\\e[0m\"\nrequired_files=(\n    \"web_service/backend/main.py\"\n    \"web_service/backend/api.py\"\n    \"web_service/backend/config.py\"\n    \"web_service/backend/port_check.py\"\n    \"web_service/backend/requirements.txt\"\n    \"web_service/frontend/package.json\"\n    \"web_service/frontend/next.config.js\"\n    \"fortuna-monolith.spec\"\n)\n\nmissing_files=()\nall_found=true\nfor file in \"${required_files[@]}\"; do\n    if [ -f \"$file\" ]; then\n        echo -e \"  \\e[32m\u2705 $file\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c $file\\e[0m\"\n        missing_files+=(\"$file\")\n        all_found=false\n    fi\ndone\n\nif [ \"$all_found\" = false ]; then\n    echo -e \"\\n\\e[31m\u274c FATAL: Missing files:\\e[0m\"\n    for file in \"${missing_files[@]}\"; do\n        echo \"  - $file\"\n    done\n    exit 1\nfi\n\n# 2. Test Python imports\necho -e \"\\n\\e[1m[2] Testing Python imports...\\e[0m\"\ncat > test_imports.py << EOL\nimport sys\nsys.path.insert(0, '.')\n\ntry:\n    from web_service.backend.api import app\n    print('\u2705 api.app imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import api.app: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.config import get_settings\n    settings = get_settings()\n    print(f'\u2705 config.get_settings imported (host={settings.UVICORN_HOST}, port={settings.FORTUNA_PORT})')\nexcept ImportError as e:\n    print(f'\u274c Failed to import config: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    print('\u2705 port_check.check_port_and_exit_if_in_use imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import port_check: {e}')\n    sys.exit(1)\n\nprint('\u2705 All imports successful')\nEOL\n\npython test_imports.py\nif [ $? -ne 0 ]; then\n    echo -e \"\\e[31m\u274c Import test FAILED\\e[0m\"\n    rm test_imports.py\n    exit 1\nfi\nrm test_imports.py\n\n# 3. Check frontend\necho -e \"\\n\\e[1m[3] Checking frontend...\\e[0m\"\nif [ -f \"web_service/frontend/next.config.js\" ]; then\n    if grep -q \"output: 'export'\" \"web_service/frontend/next.config.js\"; then\n        echo -e \"  \\e[32m\u2705 next.config.js has output: 'export'\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c next.config.js missing output: 'export'\\e[0m\"\n        exit 1\n    fi\nelse\n    echo -e \"  \\e[33m\u26a0\ufe0f  next.config.js will be created during build\\e[0m\"\nfi\n\n# 4. Check spec file\necho -e \"\\n\\e[1m[4] Checking fortuna-monolith.spec...\\e[0m\"\nif [ -f \"fortuna-monolith.spec\" ]; then\n    if grep -q \"SPECPATH\" \"fortuna-monolith.spec\"; then\n        echo -e \"  \\e[32m\u2705 spec uses SPECPATH\\e[0m\"\n    else\n        echo -e \"  \\e[33m\u26a0\ufe0f  spec doesn't use SPECPATH (may have path issues)\\e[0m\"\n    fi\nelse\n    echo -e \"  \\e[31m\u274c fortuna-monolith.spec not found\\e[0m\"\n    exit 1\nfi\n\necho -e \"\\n\\e[32m\u2705 ALL CHECKS PASSED - Safe to build!\\e[0m\"\n",
    "pyproject.toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"paddock-parser-ng\"\nversion = \"0.1.0\"\ndescription = \"A toolkit to identify the best racecards for betting.\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\n\n[project.scripts]\npaddock_parser_ui = \"paddock_parser.entry_points:run_terminal_ui\"\npaddock_parser_dashboard = \"paddock_parser.entry_points:run_dashboard\"\npaddock_parser_predict = \"paddock_parser.entry_points:run_prediction_engine\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n# Configuration for the Ruff linter\n[tool.ruff]\n# Allow lines to be up to 120 characters long.\nline-length = 120\n\n[tool.ruff.lint]\n# Enable Pyflakes (F), pycodestyle (E, W), and isort (I) rules.\nselect = [\"E\", \"F\", \"W\", \"I\"]\nignore = []\n\n[tool.ruff.lint.isort]\n# Sort imports within their sections alphabetically.\nforce-single-line = true\n",
    "run_desktop_app.py": "import sys\nimport os\nimport threading\nimport logging\nimport traceback\nfrom pathlib import Path\n\n# --- 1. SETUP LOGGING (CRITICAL FOR DEBUGGING) ---\ndef setup_logging():\n    # Log to %TEMP% so we can retrieve it if the app crashes\n    log_dir = Path(os.environ.get(\"TEMP\", \".\"))\n    log_file = log_dir / \"fortuna-desktop.log\"\n\n    logging.basicConfig(\n        filename=log_file,\n        level=logging.DEBUG,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        filemode='w'\n    )\n\n    # Also print to console for dev mode\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    logging.getLogger('').addHandler(console)\n    return logging.getLogger(__name__)\n\nlogger = setup_logging()\n\n# --- 2. ROBUST IMPORTS ---\ntry:\n    import uvicorn\n    import webview\n    from fastapi import FastAPI\n    from fastapi.staticfiles import StaticFiles\n    from fastapi.middleware.cors import CORSMiddleware\nexcept Exception as e:\n    logger.critical(f\"Failed to import core dependencies: {e}\\n{traceback.format_exc()}\")\n    sys.exit(1)\n\n# --- 3. DEFINE APP ---\ndef create_app():\n    app = FastAPI()\n    app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n\n    # Health Check (Used by CI Smoke Test)\n    @app.get(\"/api/health\")\n    def health():\n        return {\"status\": \"ok\", \"mode\": \"desktop\"}\n\n    # Try to load real backend\n    try:\n        # Lazy import to prevent crash if backend deps are missing\n        from web_service.backend.api import router as api_router\n        app.include_router(api_router, prefix=\"/api\")\n        logger.info(\"Loaded Backend API\")\n    except Exception as e:\n        logger.warning(f\"Could not load Backend API: {e}\")\n        @app.get(\"/api/error\")\n        def api_error(): return {\"error\": str(e)}\n\n    return app\n\n# --- 4. ASSET PATHS ---\ndef get_asset_path():\n    # PyInstaller unpacks data to _MEIPASS\n    if hasattr(sys, '_MEIPASS'):\n        return Path(sys._MEIPASS) / 'public'\n    # Dev mode path\n    return Path(__file__).parent / 'web_service' / 'frontend' / 'public'\n\n# --- 5. SERVER THREAD ---\ndef start_server(app, port):\n    try:\n        # Mount Frontend\n        static_dir = get_asset_path()\n        if static_dir.exists():\n            app.mount(\"/\", StaticFiles(directory=str(static_dir), html=True), name=\"static\")\n            logger.info(f\"Serving frontend from {static_dir}\")\n        else:\n            logger.error(f\"Frontend not found at {static_dir}\")\n\n        logger.info(f\"Starting Uvicorn on port {port}\")\n        uvicorn.run(app, host=\"127.0.0.1\", port=port, log_level=\"info\")\n    except Exception as e:\n        logger.critical(f\"Server crashed: {e}\\n{traceback.format_exc()}\")\n\nif __name__ == '__main__':\n    try:\n        if sys.platform == 'win32':\n            import multiprocessing\n            multiprocessing.freeze_support()\n\n        port = 8000\n        app = create_app()\n\n        # Start Backend Thread\n        t = threading.Thread(target=start_server, args=(app, port), daemon=True)\n        t.start()\n\n        # Start GUI\n        logger.info(\"Launching WebView...\")\n        webview.create_window(\"Fortuna Faucet\", f\"http://127.0.0.1:{port}\", width=1280, height=800)\n        webview.start()\n        logger.info(\"App closed normally.\")\n\n    except Exception as e:\n        logger.critical(f\"Fatal Crash: {e}\\n{traceback.format_exc()}\")\n        sys.exit(1)",
    "scripts/canary_check.py": "#!/usr/bin/env python3\n\"\"\"Lightweight canary check for data sources.\"\"\"\n\nimport asyncio\nimport json\nimport sys\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\nfrom dataclasses import dataclass, asdict\nimport httpx\n\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\n\n@dataclass\nclass CanaryResult:\n    source: str\n    success: bool\n    latency_ms: float\n    message: str\n    timestamp: str = \"\"\n\n    def __post_init__(self):\n        if not self.timestamp:\n            self.timestamp = datetime.utcnow().isoformat()\n\n\nasync def check_browser_basic() -> CanaryResult:\n    \"\"\"Basic browser connectivity check.\"\"\"\n    start = time.perf_counter()\n\n    try:\n        from scrapling.fetchers import AsyncStealthySession\n\n        async with AsyncStealthySession(headless=True) as session:\n            response = await session.fetch('https://httpbin.org/status/200')\n\n        latency = (time.perf_counter() - start) * 1000\n\n        return CanaryResult(\n            source=\"httpbin\",\n            success=response.status == 200,\n            latency_ms=latency,\n            message=\"OK\" if response.status == 200 else f\"Status: {response.status}\"\n        )\n    except Exception as e:\n        return CanaryResult(\n            source=\"httpbin\",\n            success=False,\n            latency_ms=(time.perf_counter() - start) * 1000,\n            message=str(e)\n        )\n\n\nasync def check_twinspires() -> CanaryResult:\n    \"\"\"Check TwinSpires accessibility.\"\"\"\n    start = time.perf_counter()\n\n    try:\n        from scrapling.fetchers import AsyncStealthySession\n\n        async with AsyncStealthySession(headless=True) as session:\n            response = await session.fetch(\n                'https://www.twinspires.com/bet/todays-races/time',\n                timeout=30000\n            )\n\n        latency = (time.perf_counter() - start) * 1000\n\n        # Check if we got blocked\n        text = response.text.lower()\n        if 'captcha' in text or 'access denied' in text:\n            return CanaryResult(\n                source=\"TwinSpires\",\n                success=False,\n                latency_ms=latency,\n                message=\"Blocked by anti-bot\"\n            )\n\n        # Check for race content\n        has_races = 'race' in text or 'track' in text\n\n        return CanaryResult(\n            source=\"TwinSpires\",\n            success=response.status == 200 and has_races,\n            latency_ms=latency,\n            message=\"OK\" if has_races else f\"No race content found (Status: {response.status})\"\n        )\n\n    except Exception as e:\n        return CanaryResult(\n            source=\"TwinSpires\",\n            success=False,\n            latency_ms=(time.perf_counter() - start) * 1000,\n            message=str(e)\n        )\n\n\nasync def check_http_source(name: str, url: str, expected_keywords: list[str]) -> CanaryResult:\n    \"\"\"Check a source via simple HTTP.\"\"\"\n    start = time.perf_counter()\n    try:\n        async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:\n            response = await client.get(\n                url,\n                headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n            )\n\n        latency = (time.perf_counter() - start) * 1000\n        text = response.text.lower()\n\n        if response.status_code != 200:\n            return CanaryResult(name, False, latency, f\"HTTP {response.status_code}\")\n\n        found = [kw for kw in expected_keywords if kw.lower() in text]\n        success = len(found) > 0\n\n        return CanaryResult(\n            source=name,\n            success=success,\n            latency_ms=latency,\n            message=\"OK\" if success else f\"Content check failed (Found keywords: {found})\"\n        )\n    except Exception as e:\n        return CanaryResult(name, False, (time.perf_counter() - start) * 1000, str(e))\n\n\nasync def main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Canary health check\")\n    parser.add_argument(\"--mode\", choices=[\"full\", \"quick\"], default=\"full\", help=\"Check mode\")\n    args = parser.parse_args()\n\n    print(\"=\" * 60)\n    print(\"CANARY HEALTH CHECK\")\n    print(f\"Time: {datetime.utcnow().isoformat()}\")\n    print(f\"Mode: {args.mode.upper()}\")\n    print(\"=\" * 60)\n\n    results = []\n\n    # Run checks\n    if args.mode == \"full\":\n        print(\"\\n\u2192 Running Browser-based checks...\")\n        results.append(await check_browser_basic())\n        results.append(await check_twinspires())\n    else:\n        print(\"\\n\u2192 Skipping Browser-based checks (Quick Mode)\")\n\n    print(\"\\n\u2192 Running HTTP-based checks...\")\n    http_checks = [\n        (\"Racing.com\", \"https://www.racing.com/\", [\"racing\", \"horse\"]),\n        (\"TAB.com.au\", \"https://www.tab.com.au/racing\", [\"racing\", \"odds\"]),\n        (\"Racenet\", \"https://www.racenet.com.au/\", [\"racing\", \"form\"]),\n        (\"AtTheRaces\", \"https://www.attheraces.com/\", [\"racecards\", \"at the races\"]),\n        (\"SportingLife\", \"https://www.sportinglife.com/horse-racing\", [\"racecards\", \"sporting life\"])\n    ]\n\n    for name, url, kws in http_checks:\n        results.append(await check_http_source(name, url, kws))\n\n    for result in results:\n        status = \"\u2705\" if result.success else \"\u274c\"\n        print(f\"  {status} {result.source}: {result.message} ({result.latency_ms:.0f}ms)\")\n\n    # Calculate summary\n    total = len(results)\n    passed = sum(1 for r in results if r.success)\n    success_rate = passed / total if total > 0 else 0\n\n    if success_rate >= 0.8:\n        status = \"healthy\"\n    elif success_rate >= 0.5:\n        status = \"degraded\"\n    else:\n        status = \"unhealthy\"\n\n    summary = {\n        \"status\": status,\n        \"success_rate\": f\"{success_rate:.0%}\",\n        \"total_checks\": total,\n        \"passed\": passed,\n        \"failed\": total - passed,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"results\": [asdict(r) for r in results]\n    }\n\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"Status: {status.upper()}\")\n    print(f\"Success Rate: {success_rate:.0%} ({passed}/{total})\")\n    print(\"=\" * 60)\n\n    with open(\"canary_result.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n\n    return 0 if status != \"unhealthy\" else 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(asyncio.run(main()))\n",
    "scripts/generate_manifests.py": "# scripts/generate_manifests.py\nimport json\nimport os\nfrom pathlib import Path\n\n# --- Configuration ---\nROOT_DIR = Path(\".\")\nOUTPUT_DIR = Path(\".\")\nNUM_MANIFESTS = 5 # We will create 5 balanced manifests\n\n# --- Inclusion/Exclusion Rules ---\n# This script is now comprehensive. Instead of a narrow include list,\n# it scans everything and uses a more precise exclusion list.\nINCLUDE_ONLY_DIRS = None # Deactivated: We now scan all directories by default\n\nEXCLUDE_DIRS = {\n    # Standard git/ide/v-env exclusions\n    \".git\", \".idea\", \".vscode\", \"node_modules\", \".next\", \".venv\",\n    # Build artifacts and caches\n    \"dist\", \"build\", \"__pycache__\", \".pytest_cache\", \"out\", \"build_wix\",\n    # Agent-specific/Volatile directories\n    \"attic\", \"installer\", \"ReviewableJSON\", \"jules-scratch\",\n    # Legacy code not relevant to the current monolith\n    \"PREV_src\", \"python_service\",\n}\n\nEXCLUDE_FILES_BY_EXTENSION = {\n    # Archives and logs\n    \".zip\", \".json\", \".log\", \".db\", \".sqlite3\",\n    # Binary/Image formats not useful for LLM context\n    \".png\", \".ico\", \".bmp\", \".exe\", \".dll\", \".pyd\", \".pdf\",\n    # Deactivated workflows (keep them for history, but not for active context)\n    \".ymlx\"\n}\n\n\ndef get_all_project_files():\n    \"\"\"\n    Walks the entire project directory to find all relevant files for archiving,\n    respecting a detailed set of exclusion rules.\n    \"\"\"\n    all_files_with_size = []\n    print(\"\\n--- Starting Comprehensive File Audit ---\")\n    scanned_count = 0\n    included_count = 0\n\n    for root, dirs, files in os.walk(ROOT_DIR, topdown=True):\n        current_path = Path(root)\n\n        # 1. Directory Exclusion: Prune entire directory subtrees\n        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS and not d.endswith('.egg-info')]\n\n        for name in files:\n            scanned_count += 1\n            file_path = current_path / name\n\n            # 2. Filename/Extension Exclusion\n            if name.startswith(('MANIFEST_PART', 'FORTUNA_ALL_PART', '.env')):\n                continue\n            if file_path.suffix in EXCLUDE_FILES_BY_EXTENSION:\n                continue\n\n            # Special case: allow '.spec' files which are critical configs\n            if file_path.suffix == '.spec' and name not in ['api.spec']:\n                 pass # keep it\n            elif file_path.suffix in ['.spec']:\n                 continue # exclude other .spec files\n\n            try:\n                posix_path = str(file_path.as_posix())\n                size = os.path.getsize(file_path)\n                all_files_with_size.append((posix_path, size))\n                included_count += 1\n            except FileNotFoundError:\n                print(f\"[WARNING] File not found during scan: {file_path}\")\n                continue\n\n    print(f\"Scanned {scanned_count} files, included {included_count} for manifest.\")\n    print(\"--- File Audit Complete ---\\n\")\n    return all_files_with_size\n\n\ndef balance_files_by_size(files_with_size, num_bins):\n    \"\"\"\n    Distributes files into a specified number of bins, balancing by size and count.\n    Uses a hybrid greedy and round-robin approach for better distribution.\n    \"\"\"\n    # Define categories for more granular balancing\n    categories = {\n        'large': [], 'medium': [], 'small': [], 'config': [], 'docs': [],\n        'workflows': [], 'scripts': [], 'source': []\n    }\n\n    # Categorize files based on extension and size\n    for path, size in files_with_size:\n        ext = Path(path).suffix.lower()\n        if 'github/workflows' in path:\n            categories['workflows'].append((path, size))\n        elif ext in ['.md', '.txt']:\n            categories['docs'].append((path, size))\n        elif ext in ['.json', '.toml', '.ini', '.spec', '.lock']:\n            categories['config'].append((path, size))\n        elif ext == '.py' and 'scripts' in path:\n            categories['scripts'].append((path, size))\n        elif ext in ['.py', '.js', '.ts', '.tsx', '.css', '.html', '.wxs']:\n            if size > 50 * 1024:  # Over 50KB\n                categories['large'].append((path, size))\n            elif size > 10 * 1024: # Over 10KB\n                categories['medium'].append((path, size))\n            else:\n                categories['small'].append((path, size))\n        else:\n            categories['source'].append((path, size))\n\n    bins = [[] for _ in range(num_bins)]\n    bin_sizes = [0] * num_bins\n\n    # Distribute large files first using greedy approach\n    for category in ['large', 'medium']:\n        # Sort descending to place largest files first\n        categories[category].sort(key=lambda x: x[1], reverse=True)\n        for path, size in categories[category]:\n            min_bin_index = bin_sizes.index(min(bin_sizes))\n            bins[min_bin_index].append(path)\n            bin_sizes[min_bin_index] += size\n\n    # Distribute remaining files using round-robin to balance file count\n    current_bin = 0\n    for category in ['small', 'config', 'docs', 'workflows', 'scripts', 'source']:\n        # Sort alphabetically for consistent distribution\n        categories[category].sort(key=lambda x: x[0])\n        for path, size in categories[category]:\n            bins[current_bin].append(path)\n            bin_sizes[current_bin] += size\n            current_bin = (current_bin + 1) % num_bins\n\n    # Print the balancing results for verification\n    print(\"--- Manifest Balancing Results (Enhanced) ---\")\n    for i, (file_list, total_size) in enumerate(zip(bins, bin_sizes)):\n        print(\n            f\" Manifest {i+1}: {len(file_list):>4} files, \"\n            f\"Total size: {total_size / 1024 / 1024:>6.2f} MB\"\n        )\n    print(\"------------------------------------------\")\n\n    return bins\n\n\ndef main():\n    \"\"\"Generate balanced manifest files based on file size.\"\"\"\n    print(\"--- Starting Manifest Generation (Size-Balanced) ---\")\n    all_files = get_all_project_files()\n    print(f\"Found {len(all_files)} total project files to consider.\")\n\n    balanced_manifests = balance_files_by_size(all_files, NUM_MANIFESTS)\n\n    # Write the updated manifest files\n    for i, file_list in enumerate(balanced_manifests):\n        manifest_name = f\"MANIFEST_PART{i+1}.json\"\n        output_path = OUTPUT_DIR / manifest_name\n        sorted_files = sorted(file_list) # Sort alphabetically for consistency\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(sorted_files, f, indent=4)\n        print(f\"\u2705 Wrote {len(sorted_files)} entries to {output_path}\")\n\n    print(\"\\n[SUCCESS] All manifest files have been generated and balanced.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/import_test.py": "\nimport sys\nsys.path.insert(0, '.')\ntry:\n    from web_service.backend.api import app\n    print(f'\u2705 app object loaded: {type(app).__name__}')\nexcept Exception as e:\n    print(f'\u274c CRITICAL IMPORT ERROR: {e}')\n    import traceback\n    traceback.print_exc()\n    sys.exit(1)\n",
    "scripts/install_fortuna_gui.bat": "@echo off\nREM Interactive MSI installation with standard Windows UI\n\ntitle Fortuna Faucet Installation Wizard\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Administrator privileges required\n    echo Please right-click this file and select \"Run as Administrator\"\n    pause\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" /L*v \"%TEMP%\\fortuna_install.log\"\n\nif %errorlevel% equ 0 (\n    echo Installation completed successfully!\n    echo Access dashboard at: http://localhost:3000\n) else (\n    echo Installation failed. Log: %TEMP%\\fortuna_install.log\n)\npause",
    "scripts/retry_with_backoff.sh": "#!/bin/bash\n# scripts/retry_with_backoff.sh\n# Retries a command with exponential backoff.\n# Usage: ./retry_with_backoff.sh <command> <args>\n\nMAX_ATTEMPTS=3\nATTEMPT=1\nDELAY=2\n\nwhile [ $ATTEMPT -le $MAX_ATTEMPTS ]; do\n  echo \"\ud83d\ude80 Attempt $ATTEMPT/$MAX_ATTEMPTS: $@\"\n  \"$@\"\n  EXIT_CODE=$?\n\n  if [ $EXIT_CODE -eq 0 ]; then\n    echo \"\u2705 Command succeeded on attempt $ATTEMPT\"\n    exit 0\n  fi\n\n  if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then\n    echo \"\u26a0\ufe0f Command failed with exit code $EXIT_CODE. Retrying in ${DELAY}s...\"\n    sleep $DELAY\n    DELAY=$((DELAY * 2))\n  fi\n\n  ATTEMPT=$((ATTEMPT + 1))\ndone\n\necho \"\u274c Command failed after $MAX_ATTEMPTS attempts.\"\nexit $EXIT_CODE\n",
    "scripts/verify_integration.py": "#!/usr/bin/env python3\n\"\"\"\nQuick verification script to check SmartFetcher integration status\nRun this in your project root to verify everything is wired correctly\n\"\"\"\n\nimport sys\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nclass IntegrationChecker:\n    \"\"\"Check if SmartFetcher is properly integrated\"\"\"\n    \n    def __init__(self, project_root: str = \".\"):\n        self.project_root = Path(project_root)\n        self.results = {\n            \"smart_fetcher_exists\": False,\n            \"base_adapter_integrated\": False,\n            \"adapters_using_strategy\": [],\n            \"adapters_need_update\": [],\n            \"tests_exist\": False,\n            \"issues\": []\n        }\n    \n    def check_smart_fetcher(self) -> bool:\n        \"\"\"Check if smart_fetcher.py exists\"\"\"\n        smart_fetcher_path = self.project_root / \"python_service\" / \"core\" / \"smart_fetcher.py\"\n        \n        if smart_fetcher_path.exists():\n            print(\"\u2705 SmartFetcher exists at:\", smart_fetcher_path)\n            \n            # Check key classes\n            content = smart_fetcher_path.read_text()\n            if \"class SmartFetcher\" in content:\n                print(\"  \u2713 SmartFetcher class found\")\n            if \"class BrowserEngine\" in content:\n                print(\"  \u2713 BrowserEngine enum found\")\n            if \"class FetchStrategy\" in content:\n                print(\"  \u2713 FetchStrategy dataclass found\")\n            \n            return True\n        else:\n            print(\"\u274c SmartFetcher NOT found at:\", smart_fetcher_path)\n            self.results[\"issues\"].append(\"SmartFetcher file missing\")\n            return False\n    \n    def check_base_adapter(self) -> bool:\n        \"\"\"Check if BaseAdapterV3 uses SmartFetcher\"\"\"\n        base_path = self.project_root / \"python_service\" / \"adapters\" / \"base_adapter_v3.py\"\n        \n        if not base_path.exists():\n            print(\"\u274c BaseAdapterV3 NOT found at:\", base_path)\n            self.results[\"issues\"].append(\"BaseAdapterV3 missing\")\n            return False\n        \n        print(\"\\n\u2705 BaseAdapterV3 found at:\", base_path)\n        content = base_path.read_text()\n        \n        # Check for SmartFetcher import\n        has_import = False\n        if \"from python_service.core.smart_fetcher import\" in content or \\\n           \"from ..core.smart_fetcher import\" in content:\n            print(\"  \u2713 SmartFetcher import found\")\n            has_import = True\n        else:\n            print(\"  \u26a0\ufe0f  SmartFetcher import NOT found\")\n            self.results[\"issues\"].append(\"BaseAdapterV3 doesn't import SmartFetcher\")\n        \n        # Check for smart_fetcher initialization\n        has_init = False\n        if \"self.smart_fetcher = SmartFetcher\" in content:\n            print(\"  \u2713 SmartFetcher initialization found\")\n            has_init = True\n        else:\n            print(\"  \u26a0\ufe0f  SmartFetcher initialization NOT found\")\n            self.results[\"issues\"].append(\"BaseAdapterV3 doesn't initialize SmartFetcher\")\n        \n        # Check for make_request using smart_fetcher\n        has_usage = False\n        if \"self.smart_fetcher.fetch\" in content:\n            print(\"  \u2713 make_request uses smart_fetcher.fetch()\")\n            has_usage = True\n        else:\n            print(\"  \u26a0\ufe0f  make_request doesn't use smart_fetcher\")\n            self.results[\"issues\"].append(\"make_request doesn't use SmartFetcher\")\n        \n        return has_import and has_init and has_usage\n    \n    def check_adapters(self) -> Tuple[List[str], List[str]]:\n        \"\"\"Check which adapters implement _configure_fetch_strategy\"\"\"\n        adapters_dir = self.project_root / \"python_service\" / \"adapters\"\n        \n        if not adapters_dir.exists():\n            print(\"\\n\u274c Adapters directory not found\")\n            return [], []\n        \n        print(f\"\\n\ud83d\udcc2 Scanning adapters in: {adapters_dir}\")\n        \n        using_strategy = []\n        need_update = []\n        \n        for adapter_file in adapters_dir.glob(\"*_adapter.py\"):\n            content = adapter_file.read_text()\n            \n            # Skip base adapter\n            if \"base_adapter\" in adapter_file.name:\n                continue\n            \n            adapter_name = adapter_file.stem.replace(\"_adapter\", \"\").title()\n            \n            if \"_configure_fetch_strategy\" in content:\n                using_strategy.append(adapter_name)\n                print(f\"  \u2713 {adapter_name}: Has _configure_fetch_strategy\")\n            else:\n                need_update.append(adapter_name)\n                print(f\"  \u26a0\ufe0f  {adapter_name}: Missing _configure_fetch_strategy\")\n        \n        return using_strategy, need_update\n    \n    def check_tests(self) -> bool:\n        \"\"\"Check if tests exist for SmartFetcher\"\"\"\n        test_paths = [\n            self.project_root / \"tests\" / \"test_smart_fetcher.py\",\n            self.project_root / \"tests\" / \"core\" / \"test_smart_fetcher.py\",\n        ]\n        \n        print(\"\\n\ud83e\uddea Checking for tests...\")\n        \n        for test_path in test_paths:\n            if test_path.exists():\n                print(f\"\u2705 Tests found at: {test_path}\")\n                return True\n        \n        print(\"\u26a0\ufe0f  No SmartFetcher tests found\")\n        self.results[\"issues\"].append(\"SmartFetcher tests missing\")\n        return False\n    \n    def check_workflow(self) -> bool:\n        \"\"\"Check if GitHub workflow is configured correctly\"\"\"\n        workflow_path = self.project_root / \".github\" / \"workflows\" / \"unified-race-report.yml\"\n        \n        if not workflow_path.exists():\n            print(\"\\n\u26a0\ufe0f  GitHub workflow not found\")\n            return False\n        \n        print(\"\\n\ud83d\udd27 Checking GitHub workflow...\")\n        content = workflow_path.read_text()\n        \n        checks = {\n            \"CAMOUFOX_AVAILABLE\": \"Camoufox availability flag\",\n            \"CHROMIUM_AVAILABLE\": \"Chromium availability flag\",\n            \"verify_browsers\": \"Browser verification script\",\n            \"debug-output\": \"Debug output directory\"\n        }\n        \n        all_good = True\n        for key, description in checks.items():\n            if key in content:\n                print(f\"  \u2713 {description}\")\n            else:\n                print(f\"  \u26a0\ufe0f  Missing: {description}\")\n                all_good = False\n        \n        return all_good\n    \n    def generate_report(self) -> str:\n        \"\"\"Generate markdown report\"\"\"\n        report = [\"# \ud83d\udd0d SmartFetcher Integration Status Report\\n\\n\"]\n        report.append(f\"**Project Root**: {self.project_root.absolute()}\\n\\n\")\n        \n        # Summary\n        report.append(\"## \ud83d\udcca Summary\\n\\n\")\n        report.append(f\"- SmartFetcher exists: {'\u2705' if self.results['smart_fetcher_exists'] else '\u274c'}\\n\")\n        report.append(f\"- BaseAdapter integrated: {'\u2705' if self.results['base_adapter_integrated'] else '\u274c'}\\n\")\n        report.append(f\"- Adapters using strategy: {len(self.results['adapters_using_strategy'])}\\n\")\n        report.append(f\"- Adapters need update: {len(self.results['adapters_need_update'])}\\n\")\n        report.append(f\"- Tests exist: {'\u2705' if self.results['tests_exist'] else '\u26a0\ufe0f'}\\n\\n\")\n        \n        # Issues\n        if self.results[\"issues\"]:\n            report.append(\"## \u26a0\ufe0f Issues Found\\n\\n\")\n            for issue in self.results[\"issues\"]:\n                report.append(f\"- {issue}\\n\")\n            report.append(\"\\n\")\n        \n        # Adapters using strategy\n        if self.results[\"adapters_using_strategy\"]:\n            report.append(\"## \u2705 Adapters with FetchStrategy\\n\\n\")\n            for adapter in sorted(self.results[\"adapters_using_strategy\"]):\n                report.append(f\"- {adapter}\\n\")\n            report.append(\"\\n\")\n        \n        # Next steps\n        report.append(\"## \ud83d\ude80 Next Steps\\n\\n\")\n        \n        if not self.results[\"base_adapter_integrated\"]:\n            report.append(\"1. **CRITICAL**: Integrate SmartFetcher into BaseAdapterV3\\n\")\n        \n        if self.results[\"adapters_need_update\"]:\n            report.append(f\"2. Update {len(self.results['adapters_need_update'])} adapters to use FetchStrategy\\n\")\n        \n        return \"\".join(report)\n    \n    def run(self):\n        \"\"\"Run all checks\"\"\"\n        print(\"=\" * 60)\n        print(\"SMARTFETCHER INTEGRATION VERIFICATION\")\n        print(\"=\" * 60)\n        \n        self.results[\"smart_fetcher_exists\"] = self.check_smart_fetcher()\n        self.results[\"base_adapter_integrated\"] = self.check_base_adapter()\n        \n        using, needing = self.check_adapters()\n        self.results[\"adapters_using_strategy\"] = using\n        self.results[\"adapters_need_update\"] = needing\n        \n        self.results[\"tests_exist\"] = self.check_tests()\n        self.check_workflow()\n        \n        report = self.generate_report()\n        report_path = Path(\"integration_check_report.md\")\n        report_path.write_text(report)\n        print(f\"\\n\ud83d\udcc4 Report saved to: {report_path.absolute()}\")\n        \n        if self.results[\"base_adapter_integrated\"] and len(self.results[\"adapters_using_strategy\"]) > 0:\n            print(\"\u2705 SmartFetcher is INTEGRATED and WORKING!\")\n            return 0\n        else:\n            print(\"\u274c SmartFetcher integration INCOMPLETE\")\n            return 1\n\nif __name__ == \"__main__\":\n    checker = IntegrationChecker(\".\")\n    sys.exit(checker.run())\n",
    "tests/adapters/test_greyhound_adapter.py": "from datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.greyhound_adapter import GreyhoundAdapter\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef test_settings():\n    \"\"\"Provides a valid Settings object for testing.\"\"\"\n    return get_test_settings()\n\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n\n    mock_api_response = {\n        \"cards\": [\n            {\n                \"track_name\": \"Test Track\",\n                \"races\": [\n                    {\n                        \"race_id\": \"test_race_123\",\n                        \"race_number\": 1,\n                        \"start_time\": int(datetime.now().timestamp()),\n                        \"runners\": [\n                            {\n                                \"dog_name\": \"Rapid Rover\",\n                                \"trap_number\": 1,\n                                \"odds\": {\"win\": \"2.5\"},\n                            },\n                            {\n                                \"dog_name\": \"Swift Sprint\",\n                                \"trap_number\": 2,\n                                \"scratched\": True,\n                            },\n                            {\n                                \"dog_name\": \"Lazy Larry\",\n                                \"trap_number\": 3,\n                                \"odds\": {\"win\": \"10.0\"},\n                            },\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = await adapter.get_races(today)\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == \"greyhound_test_race_123\"\n    assert race.venue == \"Test Track\"\n    assert len(race.runners) == 2  # One was scratched\n\n    runner1 = race.runners[0]\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.number == 1\n    assert runner1.odds[\"Greyhound Racing\"].win == 2.5\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter handles an empty API response gracefully.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value={\"cards\": []})\n\n    # ACT\n    races = await adapter.get_races(today)\n\n    # ASSERT\n    assert races == []\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(test_settings):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = await adapter.get_races(today)\n\n    # ASSERT\n    assert races == []\n",
    "tests/analyzers/test_trifecta_analyzer.py": "# Dedicated test suite for the TrifectaAnalyzer, resurrected and expanded.\nfrom datetime import datetime\n\nimport pytest\n\nfrom python_service.analyzer import TrifectaAnalyzer\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\n@pytest.fixture\ndef analyzer():\n    return TrifectaAnalyzer()\n\n\n@pytest.fixture\ndef runners():\n    return []\n\n\n@pytest.fixture\ndef create_race(runners):\n    return Race(\n        id=\"test-race\",\n        venue=\"TEST\",\n        race_number=1,\n        start_time=datetime.now(),\n        runners=runners,\n        source=\"test\",\n    )\n\n\ndef test_analyzer_name(analyzer):\n    assert analyzer.name == \"trifecta_analyzer\"\n\n\n# Test cases resurrected from legacy scorer and logic tests\ndef test_qualifies_with_exactly_three_runners(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds3 = {\"TestOdds\": OddsData(win=Decimal(\"5.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n        Runner(number=3, name=\"C\", odds=odds3, scratched=False),\n    ]\n    assert analyzer.is_race_qualified(create_race) is True\n\n\ndef test_qualifies_with_more_than_three_runners(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds3 = {\"TestOdds\": OddsData(win=Decimal(\"5.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds4 = {\"TestOdds\": OddsData(win=Decimal(\"6.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n        Runner(number=3, name=\"C\", odds=odds3, scratched=False),\n        Runner(number=4, name=\"D\", odds=odds4, scratched=False),\n    ]\n    assert analyzer.is_race_qualified(create_race) is True\n\n\n# New test cases for edge-case hardening\ndef test_rejects_with_fewer_than_three_runners(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n    ]\n    assert analyzer.is_race_qualified(create_race) is False\n\n\ndef test_rejects_if_scratched_runners_reduce_field_below_three(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds3 = {\"TestOdds\": OddsData(win=Decimal(\"5.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n        Runner(number=3, name=\"C\", odds=odds3, scratched=True),  # Scratched\n    ]\n    assert analyzer.is_race_qualified(create_race) is False\n\n\ndef test_handles_empty_runner_list(analyzer, create_race):\n    race = create_race\n    race.runners = []\n    assert analyzer.is_race_qualified(race) is False\n\n\ndef test_handles_none_race_object(analyzer):\n    assert analyzer.is_race_qualified(None) is False\n",
    "tests/fixtures/twinspires_sample.html": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <title>Race Results - Twinspires</title>\n</head>\n<body>\n    <div id=\"race-card\">\n        <h1>Race 5 - Churchill Downs - 2025-10-26</h1>\n        <div class=\"race-details\">\n            <span class=\"post-time\">Post Time: 04:30 PM</span>\n            <span class=\"distance\">1 Mile</span>\n            <span class=\"surface\">Dirt</span>\n        </div>\n        <ul class=\"runners-list\">\n            <li class=\"runner\">\n                <span class=\"runner-number\">1</span>\n                <span class=\"runner-name\">Braveheart</span>\n                <span class=\"runner-odds\">5/2</span>\n            </li>\n            <li class=\"runner\">\n                <span class=\"runner-number\">2</span>\n                <span class=\"runner-name\">Speedster</span>\n                <span class=\"runner-odds\">10/1</span>\n            </li>\n            <li class=\"runner scratched\">\n                <span class=\"runner-number\">3</span>\n                <span class=\"runner-name\">Steady Eddy</span>\n                <span class=\"runner-odds\">SCR</span>\n            </li>\n             <li class=\"runner\">\n                <span class=\"runner-number\">4</span>\n                <span class=\"runner-name\">Gallant Gus</span>\n                <span class=\"runner-odds\">3/1</span>\n            </li>\n        </ul>\n    </div>\n</body>\n</html>\n",
    "tests/test_engine.py": "import pytest\nfrom unittest.mock import AsyncMock, MagicMock, patch\nfrom datetime import datetime, date\nfrom decimal import Decimal\nfrom tests.conftest import create_mock_race, get_test_settings\n\n# Import your actual classes\nfrom python_service.engine import OddsEngine\nfrom python_service.adapters.base_adapter_v3 import BaseAdapterV3\nfrom python_service.models import Race\n\n@pytest.mark.asyncio\nasync def test_engine_initialization():\n    \"\"\"Test that the engine loads config correctly.\"\"\"\n    engine = OddsEngine(config=get_test_settings())\n    assert engine.config.API_KEY == \"test-override-key-123\"\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine._time_adapter_fetch\")\nasync def test_fetch_all_odds_success(mock_fetch, clear_cache):\n    \"\"\"Test happy path: fetching odds from a single adapter.\"\"\"\n    # ARRANGE\n    settings = get_test_settings()\n    settings.CACHE_ENABLED = False\n    engine = OddsEngine(config=settings)\n\n    today = datetime.now()\n    mock_race = Race(**create_mock_race(\n        \"MockSource\", \"Churchill Downs\", 1, today,\n        [{\"number\": 1, \"name\": \"Secretariat\", \"odds\": \"1.5\"}]\n    ))\n\n    # This is the new way to mock the data\n    mock_fetch.return_value = (\"MockSource\", {\"races\": [mock_race], \"source_info\": {\"name\": \"MockSource\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n\n    # We still need to give the engine an adapter to iterate over\n    mock_adapter = MagicMock(spec=BaseAdapterV3)\n    mock_adapter.source_name = \"MockSource\"\n    engine.adapters = {\"MockSource\": mock_adapter}\n    # Update health monitor status for this mock adapter\n    from python_service.adapter_manager import AdapterStatus, AdapterHealth\n    engine.health_monitor.statuses[\"MockSource\"] = AdapterStatus(\n        name=\"MockSource\",\n        health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0,\n        last_success=None,\n        consecutive_failures=0,\n        avg_response_time_ms=0,\n        last_error=None\n    )\n\n    # ACT\n    result = await engine.fetch_all_odds(date.today().strftime(\"%Y-%m-%d\"))\n\n    # ASSERT\n    assert len(result[\"races\"]) == 1\n    assert result[\"races\"][0][\"venue\"] == \"Churchill Downs\"\n    assert result[\"races\"][0][\"runners\"][0][\"name\"] == \"Secretariat\"\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine._time_adapter_fetch\")\nasync def test_fetch_all_odds_resilience(mock_fetch, clear_cache):\n    \"\"\"Test that one failing adapter does not crash the whole engine.\"\"\"\n    # ARRANGE\n    settings = get_test_settings()\n    settings.CACHE_ENABLED = False\n    engine = OddsEngine(config=settings)\n\n    # Mock successful adapter data\n    good_race = Race(**create_mock_race(\"GoodSource\", \"Track A\", 1, datetime.now(), []))\n    good_payload = (\"GoodSource\", {\"races\": [good_race], \"source_info\": {\"name\": \"GoodSource\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n\n    # Mock failed adapter data\n    bad_payload = (\"BadSource\", {\"races\": [], \"source_info\": {\"name\": \"BadSource\", \"status\": \"FAILED\", \"error_message\": \"API Down\", \"races_fetched\": 0, \"fetch_duration\": 0.1}}, 0.1)\n\n    mock_fetch.side_effect = [good_payload, bad_payload]\n\n    # We still need to give the engine adapters to iterate over\n    good_adapter = MagicMock(spec=BaseAdapterV3); good_adapter.source_name = \"GoodSource\"\n    bad_adapter = MagicMock(spec=BaseAdapterV3); bad_adapter.source_name = \"BadSource\"\n    engine.adapters = {\"GoodSource\": good_adapter, \"BadSource\": bad_adapter}\n    # Update health monitor status\n    from python_service.adapter_manager import AdapterStatus, AdapterHealth\n    engine.health_monitor.statuses[\"GoodSource\"] = AdapterStatus(\n        name=\"GoodSource\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n    engine.health_monitor.statuses[\"BadSource\"] = AdapterStatus(\n        name=\"BadSource\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n\n    # ACT\n    result = await engine.fetch_all_odds(\"2025-12-08\")\n\n    # ASSERT\n    assert len(result[\"races\"]) == 1, \"Should return data from the good adapter\"\n    assert result[\"races\"][0][\"source\"] == \"GoodSource\"\n\n    # Verify error logging in sourceInfo\n    bad_info = next((s for s in result[\"sourceInfo\"] if s[\"name\"] == \"BadSource\"), None)\n    assert bad_info is not None\n    assert bad_info[\"status\"] == \"FAILED\"\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine._time_adapter_fetch\")\nasync def test_race_aggregation_and_deduplication(mock_fetch, clear_cache):\n    \"\"\"Test merging identical races from different sources.\"\"\"\n    settings = get_test_settings()\n    settings.CACHE_ENABLED = False\n    engine = OddsEngine(config=settings)\n    now = datetime.now()\n\n    # Same race, different sources, slightly different odds\n    race_a = Race(**create_mock_race(\"SourceA\", \"Ascot\", 1, now, [{\"number\": 1, \"name\": \"Horse X\", \"odds\": \"2.0\"}]))\n    race_b = Race(**create_mock_race(\"SourceB\", \"Ascot\", 1, now, [{\"number\": 1, \"name\": \"Horse X\", \"odds\": \"2.2\"}]))\n\n    payload_a = (\"SourceA\", {\"races\": [race_a], \"source_info\": {\"name\": \"SourceA\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n    payload_b = (\"SourceB\", {\"races\": [race_b], \"source_info\": {\"name\": \"SourceB\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n    mock_fetch.side_effect = [payload_a, payload_b]\n\n    adapter_a = MagicMock(spec=BaseAdapterV3); adapter_a.source_name = \"SourceA\"\n    adapter_b = MagicMock(spec=BaseAdapterV3); adapter_b.source_name = \"SourceB\"\n    engine.adapters = {\"SourceA\": adapter_a, \"SourceB\": adapter_b}\n    # Update health monitor status\n    from python_service.adapter_manager import AdapterStatus, AdapterHealth\n    engine.health_monitor.statuses[\"SourceA\"] = AdapterStatus(\n        name=\"SourceA\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n    engine.health_monitor.statuses[\"SourceB\"] = AdapterStatus(\n        name=\"SourceB\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n\n\n    # ACT\n    result = await engine.fetch_all_odds(\"2025-12-08\")\n\n    # ASSERT\n    assert len(result[\"races\"]) == 1, \"Should deduplicate into a single race object\"\n    merged_race = result[\"races\"][0]\n    runner = merged_race[\"runners\"][0]\n\n    # Check that odds from both sources are present\n    # Note: This assertion depends on how your engine merges odds.\n    # If it merges into a dict, this passes. If it overwrites, adjust accordingly.\n    odds_keys = runner[\"odds\"].keys()\n    assert \"SourceA\" in odds_keys\n    assert \"SourceB\" in odds_keys",
    "tests/test_silent_deployment.ps1": "Write-Host \"Testing silent deployment...\" -ForegroundColor Cyan\n\n& msiexec.exe /i \"Fortuna-Faucet-2.1.0-x64.msi\" `\n    /qn /l*v \"silent_test.log\" `\n    ALLUSERS=1 INSTALLSCOPE=perMachine\n\nif ($LASTEXITCODE -eq 0) {\n    Write-Host \"\u2713 Silent deployment successful\"\n} else {\n    Write-Host \"\u2717 Silent deployment failed\"\n    Write-Host \"Log: silent_test.log\"\n    exit 1\n}",
    "tests/utils.py": "# tests/utils.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Dict\nfrom typing import List\n\nfrom python_service.models import OddsData\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\ndef create_mock_race(\n    source: str,\n    track_name: str,\n    race_number: int,\n    start_time: datetime,\n    runners_data: List[Dict],\n) -> dict:\n    \"\"\"\n    Creates a dictionary representing a race, suitable for Pydantic model validation.\n    This is a test utility to generate consistent race data.\n    \"\"\"\n    runners = []\n    for i, runner_info in enumerate(runners_data):\n        odds_data = {}\n        if \"odds\" in runner_info:\n            odds_value = Decimal(str(runner_info[\"odds\"]))\n            odds_data[source] = OddsData(win=odds_value, source=source, last_updated=datetime.now())\n\n        runners.append(\n            Runner(\n                number=runner_info.get(\"number\", i + 1),\n                name=runner_info.get(\"name\", f\"Runner {i + 1}\"),\n                odds=odds_data,\n                scratched=runner_info.get(\"scratched\", False),\n            ).model_dump()\n        )\n\n    # Use Pydantic model to create and then dump the data to ensure it's valid\n    race = Race(\n        id=f\"test_{track_name}_{race_number}\",\n        venue=track_name,\n        race_number=race_number,\n        start_time=start_time,\n        runners=runners,\n        source=source,\n    )\n    return race.model_dump()\n",
    "web_service/backend/__init__.py": "# This file makes this directory a package.\n",
    "web_service/backend/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairExchange\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for a given date.\"\"\"\n        if not await self._authenticate(self.http_client):\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "web_service/backend/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\n\n\nclass EquibaseAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            block_resources=True,\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.equibase.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        # Try different possible index URLs for Equibase\n        index_urls = [\n            f\"/entries/{date}\",\n            f\"/static/entry/index.html\",\n            f\"/static/entry/{date}/index.html\",\n        ]\n        \n        index_response = None\n        for url in index_urls:\n            try:\n                self.logger.info(f\"Trying Equibase index: {url}\")\n                index_response = await self.make_request(\"GET\", url, headers=self._get_headers())\n                if index_response and index_response.text and len(index_response.text) > 1000:\n                    break\n            except Exception:\n                continue\n\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Equibase index page\")\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"equibase_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # More robust race link detection\n        race_links = []\n        for a in parser.css(\"a\"):\n            href = a.attributes.get(\"href\", \"\")\n            if \"/static/entry/\" in href or \"entry-race-level\" in a.attributes.get(\"class\", \"\"):\n                 race_links.append(href)\n\n        race_links = list(set(race_links))\n\n        semaphore = asyncio.Semaphore(5)\n\n        async def fetch_single_html(race_url: str):\n            async with semaphore:\n                try:\n                    response = await self.make_request(\"GET\", race_url, headers=self._get_headers())\n                    return response.text if response else \"\"\n                except Exception as e:\n                    self.logger.warning(\"Failed to fetch race page\", url=race_url, error=str(e))\n                    return \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": [p for p in html_pages if p], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first(\"div.track-information strong\")\n                if not venue_node:\n                    continue\n                venue = clean_text(venue_node.text())\n\n                race_number_node = parser.css_first(\"div.race-information strong\")\n                if not race_number_node:\n                    continue\n                race_number_text = race_number_node.text().replace(\"Race\", \"\").strip()\n                if not race_number_text.isdigit():\n                    continue\n                race_number = int(race_number_text)\n\n                post_time_node = parser.css_first(\"p.post-time span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    if runner := self._parse_runner(node):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first(\"td:nth-child(1)\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            name_node = node.css_first(\"td:nth-child(3)\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            odds_node = node.css_first(\"td:nth-child(10)\")\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if odds_data := create_odds_data(self.source_name, win_odds):\n                    odds[self.source_name] = odds_data\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError, IndexError):\n            self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n            return None\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        try:\n            # Handle formats like \"12:30 PM ET\" or just \"12:30 PM\"\n            parts = time_str.replace(\"Post Time:\", \"\").strip().split(\" \")\n            if len(parts) >= 2:\n                time_part = f\"{parts[0]} {parts[1]}\"\n                dt_str = f\"{date_str} {time_part}\"\n                return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        except Exception:\n            self.logger.warning(f\"Failed to parse post time: {time_str}\")\n        \n        # Fallback to a safe default\n        return datetime.now()\n",
    "web_service/backend/adapters/mixins/__init__.py": "# python_service/adapters/mixins/__init__.py\nfrom .debug_mixin import DebugMixin\nfrom .headers_mixin import BrowserHeadersMixin\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n__all__ = [\"DebugMixin\", \"BrowserHeadersMixin\", \"BetfairAuthMixin\"]\n",
    "web_service/backend/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for PointsBet Greyhound API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/sports/greyhound-racing/events/by-date/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw events data from the PointsBet API.\"\"\"\n        response = await self.make_request(\"GET\", date)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw events data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        all_races = []\n        for event in raw_data:\n            try:\n                if race := self._parse_race(event):\n                    all_races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.error(\n                    \"Error parsing PointsBet greyhound event\",\n                    event_id=event.get(\"eventId\"),\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race(self, event: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single event object from the API response.\"\"\"\n        event_id = event.get(\"eventId\")\n        venue = event.get(\"venueName\")\n        race_number = event.get(\"raceNumber\")\n        start_time_str = event.get(\"startsAt\")\n\n        if not all([event_id, venue, race_number, start_time_str]):\n            return None\n\n        runners = []\n        for runner_data in event.get(\"runners\", []):\n            name = runner_data.get(\"name\")\n            number = runner_data.get(\"saddleNumber\")\n            if not all([name, number]):\n                continue\n\n            runners.append(\n                Runner(\n                    name=name,\n                    number=number,\n                    scratched=runner_data.get(\"isScratched\", False),\n                    odds={},\n                )\n            )\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            start_time = datetime.now()\n\n        return Race(\n            id=f\"pbg_{event_id}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/stubs/__init__.py": "# python_service/adapters/stubs/__init__.py\n\"\"\"Non-functional stub adapters.\"\"\"\n\nfrom .horseracingnation_adapter import HorseRacingNationAdapter\nfrom .nyrabets_adapter import NYRABetsAdapter\nfrom .punters_adapter import PuntersAdapter\nfrom .racingtv_adapter import RacingTVAdapter\nfrom .tab_adapter import TabAdapter\nfrom .template_adapter import TemplateAdapter\n\n__all__ = [\n    \"HorseRacingNationAdapter\",\n    \"NYRABetsAdapter\",\n    \"PuntersAdapter\",\n    \"RacingTVAdapter\",\n    \"TabAdapter\",\n    \"TemplateAdapter\",\n]\n",
    "web_service/backend/adapters/stubs/tab_adapter.py": "# python_service/adapters/stubs/tab_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass TabAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for tab.com.au.\"\"\"\n\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n",
    "web_service/backend/adapters/twinspires_adapter.py": "\"\"\"\nTwinSpires Racing Adapter - Production Implementation\n\nUses Scrapling's AsyncStealthySession for anti-bot bypass with:\n- Persistent session pooling\n- Exponential backoff retry logic\n- Comprehensive selector strategies\n- Detailed diagnostics for debugging\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport re\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom scrapling.parser import Selector\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .constants import MAX_VALID_ODDS\nfrom .mixins import DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\nlogger = logging.getLogger(__name__)\n\n\nclass TwinSpiresAdapter(DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Production adapter for TwinSpires racing data.\n\n    Features:\n    - StealthySession with automatic Playwright fallback\n    - Exponential backoff retry logic\n    - Comprehensive selector strategies\n    - Debug HTML capture for failure analysis\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    # Selector strategies - ordered by reliability\n    RACE_CONTAINER_SELECTORS = [\n        'div[class*=\"RaceCard\"]',\n        'div[class*=\"race-card\"]',\n        'div[data-testid*=\"race\"]',\n        'div[data-race-id]',\n        'section[class*=\"race\"]',\n        'article[class*=\"race\"]',\n        '.race-container',\n        '[data-race]',\n        # Broader fallbacks\n        'div[class*=\"card\"][class*=\"race\" i]',\n        'div[class*=\"event\"]',\n    ]\n\n    TRACK_NAME_SELECTORS = [\n        '[class*=\"track-name\"]',\n        '[class*=\"trackName\"]',\n        '[data-track-name]',\n        'h2[class*=\"track\"]',\n        'h3[class*=\"track\"]',\n        '.track-title',\n        '[class*=\"venue\"]',\n    ]\n\n    RACE_NUMBER_SELECTORS = [\n        '[class*=\"race-number\"]',\n        '[class*=\"raceNumber\"]',\n        '[class*=\"race-num\"]',\n        '[data-race-number]',\n        'span[class*=\"number\"]',\n    ]\n\n    POST_TIME_SELECTORS = [\n        'time[datetime]',\n        '[class*=\"post-time\"]',\n        '[class*=\"postTime\"]',\n        '[class*=\"mtp\"]',  # Minutes to post\n        '[data-post-time]',\n        '[class*=\"race-time\"]',\n    ]\n\n    RUNNER_ROW_SELECTORS = [\n        'tr[class*=\"runner\"]',\n        'div[class*=\"runner\"]',\n        'li[class*=\"runner\"]',\n        '[data-runner-id]',\n        'div[class*=\"horse-row\"]',\n        'tr[class*=\"horse\"]',\n        'div[class*=\"entry\"]',\n        '.runner-row',\n        '.horse-entry',\n    ]\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config,\n            enable_cache=True,\n            cache_ttl=180.0,\n            rate_limit=1.5  # Slightly more conservative\n        )\n        self.attempted_url: Optional[str] = None\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        TwinSpires has strong anti-bot protections.\n        Using CAMOUFLAGE stealth mode and blocking non-essential resources.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CAMOUFOX,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            block_resources=True,\n            max_retries=3,\n            timeout=45,\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetch race data from TwinSpires for given date.\n\n        Args:\n            date: Date string in YYYY-MM-DD format\n\n        Returns:\n            Dictionary with races data or None on failure\n        \"\"\"\n        self.logger.info(f\"Fetching TwinSpires races for {date}\")\n\n        # Try multiple URL patterns\n        url_patterns = [\n            f\"{self.BASE_URL}/bet/todays-races/time\",\n            f\"{self.BASE_URL}/racing/entries/{date}\",\n            f\"{self.BASE_URL}/races/today\",\n        ]\n\n        for url in url_patterns:\n            self.attempted_url = url\n            self.logger.info(f\"Trying URL pattern: {url}\")\n\n            try:\n                response = await self.make_request(\n                    \"GET\",\n                    url,\n                    network_idle=True,\n                    wait_selector='div[class*=\"race\"], [class*=\"RaceCard\"], [class*=\"track\"]',\n                )\n            except Exception as e:\n                self.logger.warning(f\"Failed to fetch {url}: {e}\")\n                continue\n\n            if response and response.status == 200:\n                # Save debug HTML\n                self._save_debug_html(response.text, f'twinspires_{date}')\n\n                # Extract races\n                races_data = self._extract_races_from_page(response, date)\n\n                if races_data:\n                    self.logger.info(f\"Successfully extracted {len(races_data)} races from {url}\")\n                    return {\n                        \"races\": races_data,\n                        \"date\": date,\n                        \"source\": \"twinspires_live\",\n                        \"url\": url,\n                    }\n                else:\n                    self.logger.warning(f\"No races extracted from {url}, trying next pattern\")\n\n        self.logger.error(\"All URL patterns failed\")\n        return None\n\n    def _extract_races_from_page(self, response, date: str) -> List[dict]:\n        \"\"\"\n        Extract race information from page response.\n\n        Uses multiple selector strategies with fallback.\n        \"\"\"\n        races_data = []\n        page = Selector(response.text)\n\n        # Try each selector pattern\n        race_elements = []\n        selector_used = None\n\n        for selector in self.RACE_CONTAINER_SELECTORS:\n            try:\n                elements = page.css(selector)\n                if elements and len(elements) > 0:\n                    # Verify these look like race containers\n                    sample = elements[0]\n                    sample_text = str(sample.html) if hasattr(sample, 'html') else str(sample)\n\n                    # Quick sanity check - should have some race-like content\n                    if any(kw in sample_text.lower() for kw in ['race', 'post', 'horse', 'runner', 'odds']):\n                        race_elements = elements\n                        selector_used = selector\n                        break\n            except Exception as e:\n                self.logger.debug(f\"Selector '{selector}' failed: {e}\")\n                continue\n\n        if race_elements:\n            self.logger.info(f\"Found {len(race_elements)} race containers using: '{selector_used}'\")\n        else:\n            self.logger.warning(\"No race containers found with any selector\")\n            # Return full page for further analysis\n            return [{\n                \"html\": response.text,\n                \"track\": \"Unknown\",\n                \"race_number\": 0,\n                \"date\": date,\n                \"full_page\": True,\n            }]\n\n        # Extract data from each race element\n        for i, race_elem in enumerate(race_elements, 1):\n            try:\n                race_data = self._extract_single_race_data(race_elem, i, date)\n                if race_data:\n                    races_data.append(race_data)\n            except Exception as e:\n                self.logger.warning(f\"Failed to extract race {i}: {e}\")\n                continue\n\n        return races_data\n\n    def _extract_single_race_data(self, race_elem, default_num: int, date: str) -> Optional[dict]:\n        \"\"\"Extract data from a single race element.\"\"\"\n        try:\n            # Get HTML string\n            html = str(race_elem.html) if hasattr(race_elem, 'html') else str(race_elem)\n\n            # Extract track name\n            track_name = self._find_with_selectors(race_elem, self.TRACK_NAME_SELECTORS)\n            if not track_name:\n                track_name = f\"Track {default_num}\"\n\n            # Extract race number\n            race_num_text = self._find_with_selectors(race_elem, self.RACE_NUMBER_SELECTORS)\n            race_number = default_num\n            if race_num_text:\n                digits = ''.join(filter(str.isdigit, race_num_text))\n                if digits:\n                    race_number = int(digits)\n\n            # Extract post time\n            post_time_text = self._find_with_selectors(race_elem, self.POST_TIME_SELECTORS)\n\n            return {\n                \"html\": html,\n                \"track\": track_name.strip(),\n                \"race_number\": race_number,\n                \"post_time_text\": post_time_text,\n                \"date\": date,\n                \"full_page\": False,\n            }\n\n        except Exception as e:\n            self.logger.debug(f\"Extract single race error: {e}\")\n            return None\n\n    def _find_with_selectors(self, element, selectors: List[str]) -> Optional[str]:\n        \"\"\"Try multiple selectors and return first matching text.\"\"\"\n        for selector in selectors:\n            try:\n                found = element.css_first(selector)\n                if found:\n                    text = found.text.strip() if hasattr(found, 'text') else str(found).strip()\n                    if text:\n                        return text\n            except Exception:\n                continue\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parse extracted race data into Race objects.\"\"\"\n        if not raw_data or \"races\" not in raw_data:\n            self.logger.warning(\"No races data to parse\")\n            return []\n\n        races_list = raw_data[\"races\"]\n        date_str = raw_data.get(\"date\", datetime.now().strftime(\"%Y-%m-%d\"))\n\n        self.logger.info(f\"Parsing {len(races_list)} races\")\n\n        parsed_races = []\n\n        for race_data in races_list:\n            try:\n                race = self._parse_single_race(race_data, date_str)\n                if race and race.runners:\n                    parsed_races.append(race)\n                    self.logger.debug(\n                        f\"Parsed race\",\n                        track=race.venue,\n                        race=race.race_number,\n                        runners=len(race.runners)\n                    )\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to parse race\",\n                    track=race_data.get(\"track\"),\n                    error=str(e),\n                    exc_info=True\n                )\n                continue\n\n        self.logger.info(f\"Successfully parsed {len(parsed_races)} races with runners\")\n        return parsed_races\n\n    def _parse_single_race(self, race_data: dict, date_str: str) -> Optional[Race]:\n        \"\"\"Parse a single race from extracted data.\"\"\"\n        html = race_data.get(\"html\", \"\")\n        if not html:\n            return None\n\n        page = Selector(html)\n\n        track_name = race_data.get(\"track\", \"Unknown\")\n        race_number = race_data.get(\"race_number\", 1)\n\n        # Parse start time\n        start_time = self._parse_post_time(\n            race_data.get(\"post_time_text\"),\n            page,\n            date_str\n        )\n\n        # Parse runners\n        runners = self._parse_runners(page)\n\n        # Generate race ID\n        track_id = re.sub(r'[^a-z0-9]', '', track_name.lower())\n        date_compact = date_str.replace('-', '')\n        race_id = f\"ts_{track_id}_{date_compact}_R{race_number}\"\n\n        # Determine discipline\n        discipline = self._detect_discipline(page, html)\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            discipline=discipline,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_post_time(\n        self,\n        time_text: Optional[str],\n        page,\n        date_str: str\n    ) -> Optional[datetime]:\n        \"\"\"Parse post time from text or page elements.\"\"\"\n        base_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n        # Try provided time text first\n        if time_text:\n            parsed = self._parse_time_string(time_text, base_date)\n            if parsed:\n                return parsed\n\n        # Try finding time in page\n        for selector in self.POST_TIME_SELECTORS:\n            elem = page.css_first(selector)\n            if not elem:\n                continue\n\n            # Check datetime attribute\n            dt_attr = elem.attrib.get('datetime') if hasattr(elem, 'attrib') else None\n            if dt_attr:\n                try:\n                    return datetime.fromisoformat(dt_attr.replace('Z', '+00:00'))\n                except ValueError:\n                    pass\n\n            # Try text content\n            text = elem.text.strip() if hasattr(elem, 'text') else str(elem).strip()\n            parsed = self._parse_time_string(text, base_date)\n            if parsed:\n                return parsed\n\n        # Default to now + 1 hour if nothing found\n        self.logger.debug(\"Could not determine post time, using default\")\n        return datetime.combine(base_date, datetime.now().time()) + timedelta(hours=1)\n\n    def _parse_time_string(self, time_str: str, base_date) -> Optional[datetime]:\n        \"\"\"Parse various time string formats.\"\"\"\n        if not time_str:\n            return None\n\n        # Clean up string\n        time_clean = re.sub(r'\\s+(EST|EDT|CST|CDT|MST|MDT|PST|PDT|ET|PT|CT|MT)$', '', time_str, flags=re.I)\n        time_clean = time_clean.strip()\n\n        # Handle \"MTP\" (minutes to post) format\n        mtp_match = re.search(r'(\\d+)\\s*(?:min|mtp)', time_clean, re.I)\n        if mtp_match:\n            minutes = int(mtp_match.group(1))\n            return datetime.now() + timedelta(minutes=minutes)\n\n        # Try various time formats\n        formats = [\n            '%I:%M %p',      # 3:45 PM\n            '%I:%M%p',       # 3:45PM\n            '%H:%M',         # 15:45\n            '%I:%M:%S %p',   # 3:45:00 PM\n        ]\n\n        for fmt in formats:\n            try:\n                time_obj = datetime.strptime(time_clean, fmt).time()\n                return datetime.combine(base_date, time_obj)\n            except ValueError:\n                continue\n\n        return None\n\n    def _parse_runners(self, page) -> List[Runner]:\n        \"\"\"Parse runner information from race HTML.\"\"\"\n        runners = []\n\n        # Find runner elements\n        runner_elements = []\n        for selector in self.RUNNER_ROW_SELECTORS:\n            try:\n                elements = page.css(selector)\n                if elements and len(elements) > 0:\n                    runner_elements = elements\n                    self.logger.debug(f\"Found {len(elements)} runners with: {selector}\")\n                    break\n            except Exception:\n                continue\n\n        if not runner_elements:\n            self.logger.debug(\"No runner elements found\")\n            return runners\n\n        for i, elem in enumerate(runner_elements):\n            try:\n                runner = self._parse_single_runner(elem, i + 1)\n                if runner:\n                    runners.append(runner)\n            except Exception as e:\n                self.logger.debug(f\"Failed to parse runner {i + 1}: {e}\")\n                continue\n\n        return runners\n\n    def _parse_single_runner(self, elem, default_number: int) -> Optional[Runner]:\n        \"\"\"Parse a single runner element.\"\"\"\n        # Get element content\n        elem_str = str(elem.html) if hasattr(elem, 'html') else str(elem)\n        elem_lower = elem_str.lower()\n\n        # Check if scratched\n        scratched = any(s in elem_lower for s in ['scratched', 'scr', 'scratch'])\n\n        # Extract program number\n        number_selectors = [\n            '[class*=\"program\"]',\n            '[class*=\"saddle\"]',\n            '[class*=\"post\"]',\n            '[class*=\"number\"]',\n            '[data-program-number]',\n            'td:first-child',\n        ]\n\n        number = None\n        for selector in number_selectors:\n            try:\n                num_elem = elem.css_first(selector)\n                if num_elem:\n                    num_text = num_elem.text.strip() if hasattr(num_elem, 'text') else str(num_elem)\n                    digits = ''.join(filter(str.isdigit, num_text))\n                    if digits:\n                        number = int(digits)\n                        break\n            except Exception:\n                continue\n\n        if number is None:\n            number = default_number\n\n        # Extract horse name\n        name_selectors = [\n            '[class*=\"horse-name\"]',\n            '[class*=\"horseName\"]',\n            '[class*=\"runner-name\"]',\n            'a[class*=\"name\"]',\n            '[data-horse-name]',\n            'td:nth-child(2)',\n        ]\n\n        name = None\n        for selector in name_selectors:\n            try:\n                name_elem = elem.css_first(selector)\n                if name_elem:\n                    name_text = name_elem.text.strip() if hasattr(name_elem, 'text') else None\n                    if name_text and len(name_text) > 1:\n                        # Clean up name\n                        name = re.sub(r'\\([^)]*\\)', '', name_text).strip()\n                        break\n            except Exception:\n                continue\n\n        if not name:\n            return None\n\n        # Extract odds\n        odds = {}\n        if not scratched:\n            odds_selectors = [\n                '[class*=\"odds\"]',\n                '[class*=\"ml\"]',  # Morning line\n                '[class*=\"morning-line\"]',\n                '[data-odds]',\n            ]\n\n            for selector in odds_selectors:\n                try:\n                    odds_elem = elem.css_first(selector)\n                    if odds_elem:\n                        odds_text = odds_elem.text.strip() if hasattr(odds_elem, 'text') else None\n                        if odds_text and odds_text.upper() not in ['SCR', 'SCRATCHED', '--', 'N/A']:\n                            win_odds = parse_odds_to_decimal(odds_text)\n                            if odds_data := create_odds_data(self.source_name, win_odds):\n                                odds[self.source_name] = odds_data\n                                break\n                except Exception:\n                    continue\n\n        return Runner(\n            number=number,\n            name=name,\n            scratched=scratched,\n            odds=odds,\n        )\n\n    def _detect_discipline(self, page, html: str) -> str:\n        \"\"\"Detect race discipline (Thoroughbred, Harness, etc).\"\"\"\n        html_lower = html.lower()\n\n        if any(kw in html_lower for kw in ['harness', 'trotter', 'pacer', 'standardbred']):\n            return \"Harness\"\n        elif any(kw in html_lower for kw in ['quarter horse', 'quarterhorse']):\n            return \"Quarter Horse\"\n        elif any(kw in html_lower for kw in ['greyhound', 'dog']):\n            return \"Greyhound\"\n\n        # Try finding breed element\n        breed_selectors = ['[class*=\"breed\"]', '[class*=\"type\"]', '[data-breed]']\n        for selector in breed_selectors:\n            try:\n                elem = page.css_first(selector)\n                if elem:\n                    text = elem.text.strip().lower() if hasattr(elem, 'text') else ''\n                    if 'harness' in text:\n                        return \"Harness\"\n                    elif 'quarter' in text:\n                        return \"Quarter Horse\"\n            except Exception:\n                continue\n\n        return \"Thoroughbred\"\n\n    async def cleanup(self):\n        \"\"\"Cleanup resources.\"\"\"\n        await self.close()\n        self.logger.info(\"TwinSpires adapter cleaned up\")\n",
    "web_service/backend/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any, List\n\nfrom selectolax.parser import HTMLParser\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    Standardized on selectolax for performance.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(\"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        parser = HTMLParser(response.text)\n        # Assuming the first step is a simple CSS selector for track links\n        track_links = [self.base_url + a.attributes[\"href\"] for a in parser.css(self.definition[\"steps\"][0][\"selector\"]) if a.attributes.get(\"href\")]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/api.py": "# web_service/backend/api.py\n# Reconstructed by Jules to merge features from python_service with web_service structure.\n\nimport asyncio\nimport os\nimport sys\nfrom pathlib import Path\nfrom contextlib import asynccontextmanager\nfrom datetime import date\nfrom typing import List, Optional\n\nimport structlog\nfrom fastapi import APIRouter, Depends, FastAPI, HTTPException, Query, Request, WebSocket\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import FileResponse\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom starlette.websockets import WebSocketDisconnect\n\n# Corrected imports for web_service.backend\nfrom .config import get_settings\nfrom .engine import OddsEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .middleware.error_handler import UserFriendlyException, user_friendly_exception_handler, validation_exception_handler\nfrom .models import AggregatedResponse, QualifiedRacesResponse, Race\nfrom .security import verify_api_key\n\nlog = structlog.get_logger()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Manages application startup and shutdown events.\"\"\"\n    configure_logging()\n    log.info(\"Lifespan: Startup sequence initiated.\")\n\n    settings = get_settings()\n    engine = OddsEngine(config=settings)\n    app.state.engine = engine\n\n    log.info(\"Lifespan: Engine initialized successfully. Startup complete.\")\n    yield\n    log.info(\"Lifespan: Shutdown sequence initiated.\")\n    if hasattr(app.state, \"engine\") and app.state.engine:\n        try:\n            # CRITICAL: Close all browser sessions\n            await app.state.engine.shutdown()\n            log.info(\"Engine shutdown complete\")\n        except Exception as e:\n            log.error(f\"Error during shutdown: {e}\", exc_info=True)\n    log.info(\"Lifespan: Shutdown sequence complete.\")\n\n# --- FastAPI App Initialization ---\nrouter = APIRouter()\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI(\n    title=\"Fortuna Faucet Web Service API\",\n    version=\"3.0\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\",\n)\n\n# Conditionally apply rate limiting middleware, disable in CI\n# The check is now more robust, looking for any truthy value.\nis_ci = os.environ.get(\"CI\", \"false\").lower() in (\"true\", \"1\", \"yes\")\nif not is_ci:\n    app.state.limiter = limiter\n    app.add_middleware(SlowAPIMiddleware)\n    app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\nelse:\n    # In CI, we don't want rate limiting, so we provide a no-op limiter.\n    # The limiter instance is still required by endpoints even if not used.\n    log.info(\"CI environment detected. Rate limiting is disabled.\")\n    app.state.limiter = Limiter(key_func=get_remote_address, enabled=False)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\napp.add_exception_handler(UserFriendlyException, user_friendly_exception_handler)\nrouter.include_router(health_router)\n\n# Add CORS middleware for frontend development\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=get_settings().ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# --- Dependency Injection ---\ndef get_engine(request: Request) -> OddsEngine:\n    if not hasattr(request.app.state, \"engine\") or request.app.state.engine is None:\n        raise HTTPException(status_code=503, detail=\"The OddsEngine is not available.\")\n    return request.app.state.engine\n\n# --- API Endpoints (Restored and Adapted) ---\n\nfrom fastapi.responses import JSONResponse\n\n@router.get(\"/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[str] = Query(None, description=\"Date in YYYY-MM-DD format.\"),\n    source: Optional[str] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    \"\"\"Fetches all race data for a given date from all or a specific source.\"\"\"\n    try:\n        if race_date is None:\n            race_date = date.today().strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(race_date, source)\n    except Exception as e:\n        log.error(\"Unhandled exception in get_races endpoint\", exc_info=True, error=str(e))\n        return JSONResponse(\n            status_code=500,\n            content={\n                \"races\": [],\n                \"errors\": [{\n                    \"adapter_name\": \"FortunaEngine\",\n                    \"error_message\": f\"An unexpected server error occurred: {str(e)}\",\n                    \"attempted_url\": None\n                }],\n                \"source_info\": [],\n                \"metadata\": {\n                    \"fetch_time\": datetime.now().isoformat(),\n                    \"sources_queried\": 0,\n                    \"sources_successful\": 0,\n                    \"total_races\": 0,\n                    \"total_errors\": 1,\n                    \"data_freshness\": \"error\"\n                }\n            }\n        )\n\n@router.get(\"/races/qualified/tiny_field_trifecta\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_tiny_field_trifecta_races(\n    request: Request,\n    race_date: Optional[str] = Query(None, description=\"Date in YYYY-MM-DD format.\"),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    \"\"\"Fetches all race data and runs the tiny_field_trifecta analyzer to find qualified races.\"\"\"\n    if race_date is None:\n        race_date = date.today().strftime(\"%Y-%m-%d\")\n    response = await engine.fetch_all_odds(race_date)\n    races = [Race(**r) for r in response.get(\"races\", [])]\n\n    analyzer = engine.analyzer_engine.get_analyzer(\"tiny_field_trifecta\")\n    result = analyzer.qualify_races(races)\n\n    return QualifiedRacesResponse(qualified_races=result.get(\"races\", []), analysis_metadata=result.get(\"criteria\", {}))\n\n@router.get(\"/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[str] = Query(None, description=\"Date in YYYY-MM-DD format.\"),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    # Example query parameters for an analyzer\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_odds: float = Query(2.0, ge=1.0),\n):\n    \"\"\"Fetches all race data and runs a specific analyzer to find qualified races.\"\"\"\n    response = await engine.fetch_all_odds(race_date)\n    races = [Race(**r) for r in response.get(\"races\", [])]\n\n    try:\n        analyzer = engine.analyzer_engine.get_analyzer(analyzer_name, max_field_size=max_field_size, min_odds=min_odds)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(qualified_races=result.get(\"races\", []), analysis_metadata=result.get(\"criteria\", {}))\n    except ValueError:\n        raise HTTPException(status_code=404, detail=f\"Analyzer '{analyzer_name}' not found.\")\n\n\n# Add other endpoints as needed, following the pattern above.\n\napp.include_router(router, prefix=\"/api\")\n\n# Mount static files (frontend)\n# This logic ensures that the frontend is served both in development and in the frozen executable.\nstatic_dir = None\nif getattr(sys, 'frozen', False):\n    # Running in a PyInstaller bundle\n    static_dir = Path(sys.executable).parent / \"public\"\nelse:\n    # Running in a normal Python environment\n    static_dir = Path(__file__).parent.parent.joinpath(\"frontend\", \"public\")\n\nif static_dir and static_dir.exists():\n    app.mount(\"/\", StaticFiles(directory=static_dir, html=True), name=\"static\")\n\n    @app.middleware(\"http\")\n    async def spa_middleware(request: Request, call_next):\n        \"\"\"\n        Middleware to handle SPA routing. If a request is not for an API endpoint\n        and the file is not found, it serves index.html. This is crucial for\n        letting the frontend handle routing.\n        \"\"\"\n        response = await call_next(request)\n        # If a 404 is returned for a non-API, non-file path, serve the SPA index.\n        if response.status_code == 404 and not request.url.path.startswith(\"/api/\"):\n            # A simple check to avoid redirecting file requests (e.g. for .css, .js)\n            if \".\" not in request.url.path.split(\"/\")[-1]:\n                return FileResponse(static_dir / \"index.html\")\n        return response\nelse:\n    log.warning(f\"Static frontend directory not found at '{static_dir}'. The frontend will not be served.\")\n\n\n# --- Adapter Management Endpoints (v3.0.0) ---\n\nfrom typing import Dict, Any\n\nadapter_router = APIRouter()\n\n@adapter_router.get(\"/adapters/status\", response_model=List[Dict[str, Any]])\nasync def get_adapter_status_v3(\n    request: Request,\n    engine: OddsEngine = Depends(get_engine),\n):\n    \"\"\"\n    Get status of all adapters, including whether they require API keys.\n    This version is designed to be called by the adapter-aware script.\n    \"\"\"\n    try:\n        statuses = []\n        for name, adapter in engine.adapters.items():\n            statuses.append({\n                \"name\": name,\n                \"adapter_name\": name,\n                \"status\": \"active\",\n                \"enabled\": True,\n                \"requires_api_key\": _adapter_requires_key(adapter),\n                \"api_key_required\": _adapter_requires_key(adapter),\n            })\n        return statuses\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error fetching adapter status: {str(e)}\")\n\n@adapter_router.post(\"/adapters/disable\", response_model=Dict[str, Any])\nasync def disable_adapter(\n    payload: Dict[str, str],\n    request: Request,\n    engine: OddsEngine = Depends(get_engine),\n):\n    \"\"\"\n    Disable a specific adapter at runtime.\n    \"\"\"\n    adapter_name = payload.get(\"adapter_name\")\n    if not adapter_name:\n        raise HTTPException(status_code=400, detail=\"adapter_name is required\")\n\n    if adapter_name not in engine.adapters:\n        raise HTTPException(status_code=404, detail=f\"Adapter '{adapter_name}' not found\")\n\n    if not hasattr(engine, 'disabled_adapters'):\n        engine.disabled_adapters = set()\n    engine.disabled_adapters.add(adapter_name)\n\n    return {\"success\": True, \"message\": f\"Adapter '{adapter_name}' disabled\"}\n\ndef _adapter_requires_key(adapter) -> bool:\n    \"\"\"\n    Helper to determine if an adapter requires an API key.\n    Checks for common attributes and class name patterns.\n    \"\"\"\n    if not adapter or not hasattr(adapter, '__class__'):\n        return False\n\n    for attr in ['api_key_required', 'requires_api_key', 'requires_key']:\n        if hasattr(adapter, attr) and getattr(adapter, attr):\n            return True\n\n    key_indicators = ['betfair', 'tvg', 'equibase']\n    adapter_class_name = adapter.__class__.__name__.lower()\n    if any(indicator in adapter_class_name for indicator in key_indicators):\n        return True\n\n    return False\n\n# Include the new adapter router\nrouter.include_router(adapter_router)\n\n# Export app for Uvicorn\n__all__ = [\"app\"]\n",
    "web_service/backend/cache_manager.py": "# python_service/cache_manager.py\nimport asyncio\nimport hashlib\nimport json\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Optional\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self):\n        self.redis_client = None\n        self.memory_cache = {}\n        self.is_configured = False\n        log.info(\"CacheManager initialized (not connected).\")\n\n    async def connect(self, redis_url: str):\n        if self.is_configured or not REDIS_AVAILABLE or not redis_url:\n            return\n\n        try:\n            log.info(\"Attempting to connect to Redis...\", url=redis_url)\n            # Use the async version of the client\n            self.redis_client = redis.asyncio.from_url(redis_url, decode_responses=True)\n            await self.redis_client.ping()  # Verify connection asynchronously\n            self.is_configured = True\n            log.info(\"Redis cache connected successfully.\")\n        except (redis.exceptions.ConnectionError, asyncio.TimeoutError) as e:\n            log.warning(\n                \"Failed to connect to Redis. Falling back to in-memory cache.\",\n                error=str(e),\n            )\n            self.redis_client = None\n            self.is_configured = False\n\n    async def disconnect(self):\n        if self.redis_client:\n            await self.redis_client.close()\n            log.info(\"Redis connection closed.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    async def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = await self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis GET failed, falling back to memory cache.\", error=e)\n\n        entry = self.memory_cache.get(key)\n        if entry and entry.get(\"expires_at\", datetime.min) > datetime.now():\n            return entry.get(\"value\")\n        return None\n\n    async def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        try:\n            serialized = json.dumps(value, default=str)\n        except (TypeError, ValueError) as e:\n            log.error(\"Failed to serialize value for caching.\", value=value, error=str(e))\n            return\n\n        if self.redis_client:\n            try:\n                await self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis SET failed, falling back to memory cache.\", error=e)\n\n        self.memory_cache[key] = {\n            \"value\": value,\n            \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds),\n        }\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager()\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = await cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n\n            try:\n                await cache_manager.set(cache_key, result, ttl_seconds)\n            except Exception as e:\n                log.error(\"Failed to store result in cache.\", error=str(e), key=cache_key)\n\n            return result\n\n        return wrapper\n\n    return decorator\n\n\nclass StaleDataCache:\n    \"\"\"In-memory cache for storing the last known good data for a given date.\"\"\"\n\n    def __init__(self, max_age_hours: int = 24):\n        self._cache: dict[str, dict] = {}\n        self.max_age = timedelta(hours=max_age_hours)\n        self.logger = structlog.get_logger(cache_type=\"StaleDataCache\")\n\n    async def get(self, date_key: str) -> Optional[dict]:\n        \"\"\"\n        Retrieves stale data if it exists and is within the max_age.\n        Returns a dictionary with the data and its age, or None.\n        \"\"\"\n        entry = self._cache.get(date_key)\n        if not entry:\n            self.logger.debug(\"Cache miss\", key=date_key)\n            return None\n\n        now = datetime.utcnow()\n        timestamp = entry[\"timestamp\"]\n        age = now - timestamp\n\n        if age > self.max_age:\n            self.logger.warning(\"Cache entry expired\", key=date_key, age_hours=age.total_seconds() / 3600)\n            del self._cache[date_key]\n            return None\n\n        age_hours = age.total_seconds() / 3600\n        self.logger.info(\"Cache hit\", key=date_key, age_hours=round(age_hours, 2))\n        return {\"data\": entry[\"data\"], \"age_hours\": age_hours}\n\n    async def set(self, date_key: str, data: Any):\n        \"\"\"\n        Stores the latest successful data fetch for a given date.\n        \"\"\"\n        self.logger.info(\"Updating stale cache\", key=date_key)\n        self._cache[date_key] = {\n            \"timestamp\": datetime.utcnow(),\n            \"data\": data,\n        }\n",
    "web_service/backend/credentials_manager.py": "# python_service/credentials_manager.py\ntry:\n    import keyring\n\n    # This check is crucial for cross-platform compatibility\n    import keyring.backends.windows\n\n    IS_WINDOWS = True\nexcept ImportError:\n    keyring = None\n    IS_WINDOWS = False\n\n\nclass SecureCredentialsManager:\n    \"\"\"Manages secrets in the system's native credential store.\"\"\"\n\n    SERVICE_NAME = \"Fortuna\"\n\n    @staticmethod\n    def save_credential(account: str, secret: str) -> bool:\n        \"\"\"Saves a secret for a given account (e.g., 'api_key', 'betfair_username').\"\"\"\n        if not IS_WINDOWS:\n            print(\"Credential storage is only supported on Windows.\")\n            return False\n        try:\n            keyring.set_password(SecureCredentialsManager.SERVICE_NAME, account, secret)\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credential for {account}: {e}\")\n            return False\n\n    @staticmethod\n    def get_credential(account: str) -> str:\n        \"\"\"Retrieves a secret for a given account.\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            return keyring.get_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credential for {account}: {e}\")\n            return None\n\n    @staticmethod\n    def get_betfair_credentials() -> tuple[str, str]:\n        \"\"\"Convenience method to retrieve both Betfair username and password.\"\"\"\n        username = SecureCredentialsManager.get_credential(\"betfair_username\")\n        password = SecureCredentialsManager.get_credential(\"betfair_password\")\n        return username, password\n\n    @staticmethod\n    def delete_credential(account: str):\n        \"\"\"Deletes a specific credential.\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception:\n            pass\n",
    "web_service/backend/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters import (\n    AtTheRacesAdapter,\n    BetfairAdapter,\n    BetfairGreyhoundAdapter,\n    BrisnetAdapter,\n    EquibaseAdapter,\n    FanDuelAdapter,\n    GbgbApiAdapter,\n    GreyhoundAdapter,\n    HarnessAdapter,\n    HorseRacingNationAdapter,\n    NYRABetsAdapter,\n    OddscheckerAdapter,\n    PointsBetGreyhoundAdapter,\n    PuntersAdapter,\n    RacingAndSportsAdapter,\n    RacingAndSportsGreyhoundAdapter,\n    RacingPostAdapter,\n    RacingTVAdapter,\n    SportingLifeAdapter,\n    TabAdapter,\n    TheRacingApiAdapter,\n    TimeformAdapter,\n    TVGAdapter,\n    TwinSpiresAdapter,\n    UniversalAdapter,\n    XpressbetAdapter,\n)\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .core.exceptions import AuthenticationError\nfrom .adapter_manager import AdapterHealthMonitor, AdapterHealth, AdapterStatus\nfrom .cache_manager import StaleDataCache\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n        exclude_adapters: Optional[List[str]] = None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n        self.health_monitor = AdapterHealthMonitor()\n        self.stale_data_cache = StaleDataCache(max_age_hours=24)\n        self.exclude_adapters = set(exclude_adapters) if exclude_adapters else set()\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: Dict[str, BaseAdapterV3] = {}\n\n            # NOTE: Many adapters require API keys (e.g., TVG, Betfair, TheRacingAPI).\n            # If the required API key is not found in the environment configuration,\n            # the adapter will fail to initialize and be skipped. This is expected\n            # behavior in environments where secrets are not configured.\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                UniversalAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                adapter_name = adapter_cls.__name__\n                if adapter_name in self.exclude_adapters:\n                    self.logger.info(f\"Intentionally skipping adapter: {adapter_name}\")\n                    continue\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_name}\")\n                    if adapter_name == \"UniversalAdapter\":\n                        # UniversalAdapter requires a definition_path, which we don't have here.\n                        # For now, we'll skip it unless it's explicitly configured.\n                        self.logger.info(\"UniversalAdapter PoC requires a definition_path. Skipping.\")\n                        continue\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_name}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters[adapter_instance.source_name] = adapter_instance\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_name,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_name}\",\n                        exc_info=True,\n                    )\n\n            for adapter_instance in self.adapters.values():\n                self.health_monitor.statuses[adapter_instance.source_name] = AdapterStatus(\n                    name=adapter_instance.source_name,\n                    health=AdapterHealth.HEALTHY,\n                    success_rate_24h=1.0,\n                    last_success=None,\n                    consecutive_failures=0,\n                    avg_response_time_ms=0,\n                    last_error=None,\n                )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters.values():\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    async def shutdown(self):\n        \"\"\"Gracefully shuts down all adapters that require cleanup.\"\"\"\n        self.logger.info(\"Shutting down adapters with cleanup methods...\")\n        for adapter_name, adapter in self.adapters.items():\n            if hasattr(adapter, 'cleanup'):\n                try:\n                    self.logger.info(f\"Cleaning up {adapter_name}...\")\n                    await adapter.cleanup()\n                except Exception as e:\n                    self.logger.error(f\"Error cleaning up {adapter_name}\", error=str(e), exc_info=True)\n        await self.close()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters.values()]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            processed_races = []\n            for race_data in race_data_list:\n                if isinstance(race_data, Race):\n                    processed_races.append(race_data)\n                else:\n                    processed_races.append(Race(**race_data))\n            races = processed_races\n            if races:\n                is_success = True\n            else:\n                is_success = False\n                error_message = \"Adapter ran successfully but fetched zero races.\"\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except AuthenticationError as e:\n            self.logger.warning(\n                \"Authentication failed for adapter, skipping.\",\n                adapter=adapter.source_name,\n                error=str(e),\n            )\n            error_message = str(e)\n            is_success = False\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        # Update health monitor\n        await self.health_monitor.update_adapter_status(\n            adapter_name=adapter.source_name,\n            success=is_success,\n            latency_ms=duration * 1000,\n            error=error_message,\n        )\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": adapter.attempted_url or attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n\n                # Maintain source as string\n                sources = set(existing_race.source.split(\", \"))\n                sources.add(race.source)\n                existing_race.source = \", \".join(sorted(list(sources)))\n\n        return list(race_map.values())\n\n    def _calculate_coverage(self, results: List[Dict[str, Any]]) -> float:\n        # Stub implementation\n        return 0.0\n\n    def _merge_adapter_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for adapter_result in results:\n            source_info = adapter_result.get(\"source_info\", {})\n            source_infos.append(source_info)\n            if source_info.get(\"status\") == \"SUCCESS\":\n                all_races.extend(adapter_result.get(\"races\", []))\n            else:\n                errors.append({\n                    \"adapter_name\": source_info.get(\"name\"),\n                    \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                    \"attempted_url\": source_info.get(\"attempted_url\")\n                })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        return {\n            \"races\": deduped_races,\n            \"errors\": errors,\n            \"source_info\": source_infos\n        }\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None, min_required_adapters: int = 2) -> Dict[str, Any]:\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        # Re-introduce live caching\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        all_payloads = []\n        attempted_adapters = []\n        all_adapter_names = list(self.adapters.keys())\n        if source_filter:\n            all_adapter_names = [name for name in all_adapter_names if name.lower() == source_filter.lower()]\n\n        ordered_adapter_names = self.health_monitor.get_ordered_adapters(all_adapter_names)\n\n        # Tier 1: Healthy\n        healthy_names = [name for name in ordered_adapter_names if self.health_monitor.statuses[name].health == AdapterHealth.HEALTHY]\n        if healthy_names:\n            tasks = [self._fetch_with_semaphore(self.adapters[name], date) for name in healthy_names]\n            attempted_adapters.extend(healthy_names)\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            for res in results:\n                if not isinstance(res, Exception):\n                    _adapter_name, payload, _duration = res\n                    all_payloads.append(payload)\n\n        successful_count = len([p for p in all_payloads if p['source_info']['status'] == 'SUCCESS'])\n\n        # Tier 2: Degraded\n        if successful_count < min_required_adapters:\n            degraded_names = [name for name in ordered_adapter_names if self.health_monitor.statuses[name].health == AdapterHealth.DEGRADED]\n            if degraded_names:\n                tasks = [self._fetch_with_semaphore(self.adapters[name], date) for name in degraded_names]\n                attempted_adapters.extend(degraded_names)\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n                for res in results:\n                    if not isinstance(res, Exception):\n                        _adapter_name, payload, _duration = res\n                        all_payloads.append(payload)\n\n        # Tier 3: Stale cache fallback\n        if not any(p['source_info']['status'] == 'SUCCESS' for p in all_payloads):\n            log.warning(\"All live adapters failed, attempting to use stale cache.\")\n            stale_data = await self.stale_data_cache.get(date)\n            if stale_data:\n                log.info(\"Using stale data from cache.\", cache_age_hours=stale_data['age_hours'])\n                stale_results = stale_data['data']\n                if 'metadata' in stale_results:\n                    stale_results['metadata']['data_freshness'] = 'stale'\n                stale_results['warnings'] = [\"Using cached data from a previous run as all live sources failed.\"]\n                return stale_results\n\n        if not all_payloads:\n            log.error(\"All adapter fetches failed and no stale data available.\")\n            return {\"races\": [], \"errors\": [{\"adapter_name\": \"all\", \"error_message\": \"All adapters failed and no stale data available.\"}], \"source_info\": [], \"metadata\": {}}\n\n        merged_results = self._merge_adapter_results(all_payloads)\n        successful_count = len([s for s in merged_results[\"source_info\"] if s[\"status\"] == \"SUCCESS\"])\n\n        try:\n            parsed_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except (ValueError, TypeError):\n            parsed_date = datetime.now().date()\n\n        response_obj = AggregatedResponse(\n            date=parsed_date,\n            races=merged_results[\"races\"],\n            errors=merged_results[\"errors\"],\n            source_info=merged_results[\"source_info\"],\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": attempted_adapters,\n                \"sources_successful\": successful_count,\n                \"total_races\": len(merged_results[\"races\"]),\n                \"total_errors\": len(merged_results[\"errors\"]),\n                'coverage': self._calculate_coverage(all_payloads),\n                'data_freshness': 'live'\n            },\n        )\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Cache successful live results\n        if successful_count > 0:\n            await self.stale_data_cache.set(date, response_data)\n            await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "web_service/backend/fortuna_windows_service.py": "# fortuna_windows_service.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Ensure the script's directory is at the front of the path\nscript_dir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, script_dir)\n\ntry:\n    from fortuna_service import FortunaBackgroundService\nexcept ImportError as e:\n    # Log a detailed error to the Windows Event Log if the import fails\n    servicemanager.LogErrorMsg(f\"FATAL: Could not import FortunaBackgroundService. Error: {e}\")\n    sys.exit(1)  # Exit with an error code\n\n\nclass FortunaWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaV8Service\"\n    _svc_display_name_ = \"Fortuna V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.fortuna_service = FortunaBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(name)s - %(levelname)s - %(message)s\",\n            handlers=[servicemanager.LogHandler()],\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.fortuna_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        self.fortuna_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaWindowsService)\n",
    "web_service/backend/main.py": "import sys\nfrom pathlib import Path\nimport uvicorn\nimport logging\nimport traceback\n\n# CRITICAL: Set up paths and logging for PyInstaller\nif getattr(sys, 'frozen', False):\n    # Running as compiled executable\n    PROJECT_ROOT = Path(sys.executable).parent\n    LOG_FILE = PROJECT_ROOT / 'fortuna-monolith.log'\n    # Redirect stdout and stderr to the log file to capture all output\n    sys.stdout = open(LOG_FILE, 'w', encoding='utf-8')\n    sys.stderr = sys.stdout\nelse:\n    # Running as script\n    PROJECT_ROOT = Path(__file__).parent.parent.parent\n    LOG_FILE = PROJECT_ROOT / 'fortuna-monolith-dev.log'\n\n# Configure logging to write to the log file and original stdout\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(LOG_FILE),\n        logging.StreamHandler(sys.__stdout__)\n    ]\n)\nlog = logging.getLogger(__name__)\n\n# Add project root to path\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\nlog.info(f\"PROJECT_ROOT added to sys.path: {PROJECT_ROOT}\")\nlog.info(f\"Full sys.path: {sys.path}\")\n\n# Now that the path is set, we can import our application modules\nfrom web_service.backend.api import app\nfrom web_service.backend.config import get_settings\nfrom web_service.backend.port_check import check_port_and_exit_if_in_use\n\n\ndef main():\n    \"\"\"Main entry point for Fortuna Monolith.\"\"\"\n    try:\n        log.info(\"=\"*70)\n        log.info(\"\ud83d\ude80 Fortuna Monolith Initializing...\")\n        log.info(f\"Python Executable: {sys.executable}\")\n        log.info(f\"Working Directory: {Path.cwd()}\")\n        log.info(\"=\"*70)\n\n        settings = get_settings()\n\n        log.info(f\"Checking port {settings.FORTUNA_PORT} on host {settings.UVICORN_HOST}\")\n        check_port_and_exit_if_in_use(settings.FORTUNA_PORT, settings.UVICORN_HOST)\n        log.info(f\"Port {settings.FORTUNA_PORT} is available.\")\n\n        log.info(f\"Host: {settings.UVICORN_HOST}\")\n        log.info(f\"Port: {settings.FORTUNA_PORT}\")\n        log.info(f\"Mode: {'Frozen (Executable)' if getattr(sys, 'frozen', False) else 'Development'}\")\n        log.info(f\"Log file: {LOG_FILE}\")\n\n        log.info(\"Starting Uvicorn server...\")\n        uvicorn.run(\n            app,\n            host=settings.UVICORN_HOST,\n            port=settings.FORTUNA_PORT,\n            log_level=\"info\",\n            access_log=True,\n        )\n        log.info(\"Uvicorn server stopped gracefully.\")\n\n    except Exception as e:\n        log.critical(\"--- !!! A FATAL ERROR OCCURRED DURING STARTUP !!! ---\")\n        log.critical(traceback.format_exc())\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    # Multiprocessing support for PyInstaller\n    from multiprocessing import freeze_support\n    freeze_support()\n    main()\n",
    "web_service/backend/models_v3.py": "# python_service/models_v3.py\n# Defines the data structures for the V3 adapter architecture.\n\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import List\n\n\n@dataclass\nclass NormalizedRunner:\n    runner_id: str\n    name: str\n    saddle_cloth: str\n    odds_decimal: float\n\n\n@dataclass\nclass NormalizedRace:\n    race_key: str\n    track_key: str\n    start_time_iso: str\n    race_name: str\n    runners: List[NormalizedRunner] = field(default_factory=list)\n    source_ids: List[str] = field(default_factory=list)\n",
    "web_service/backend/requirements-x86-constraints.txt": "# x86 Build Constraints - MANDATORY for successful x86 builds\n# These packages MUST be installed from pre-built wheels only\n#\n# USAGE:\n#   pip install --platform win32 --only-binary=:all: -r requirements-x86-constraints.txt\n#\n# The --only-binary=:all: flag ensures all packages come from wheels, not source.\n# If a wheel is not available, the installation will FAIL LOUDLY instead of\n# attempting to compile from source (which will fail silently on x86).\n\n# Database and ORM\nsqlalchemy==2.0.28\ngreenlet==3.0.3\n\n# Data Science Stack (use older versions with guaranteed x86 wheel availability)\npandas==1.5.3\nnumpy==1.23.5\nscipy==1.10.1\n\n# Note: These versions are older than the main requirements.txt to ensure\n# pre-built x86 wheels exist. This is a necessary trade-off for x86 support.\n",
    "web_service/backend/tests/__init__.py": "",
    "web_service/backend/utils/text.py": "# python_service/utils/text.py\n# Centralized text and name normalization utilities\nimport re\nfrom typing import Optional\n\n\ndef clean_text(text: Optional[str]) -> Optional[str]:\n    \"\"\"Strips leading/trailing whitespace and collapses internal whitespace.\"\"\"\n    if not text:\n        return None\n    return \" \".join(text.strip().split())\n\n\ndef normalize_venue_name(name: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Normalizes a UK or Irish racecourse name to a standard format.\n    Handles common abbreviations and variations.\n    \"\"\"\n    if not name:\n        return None\n\n    # Use a temporary variable for matching, but return the properly cased name\n    cleaned_name_upper = clean_text(name).upper()\n\n    VENUE_MAP = {\n        \"ASCOT\": \"Ascot\",\n        \"AYR\": \"Ayr\",\n        \"BANGOR-ON-DEE\": \"Bangor-on-Dee\",\n        \"CATTERICK BRIDGE\": \"Catterick\",\n        \"CHELMSFORD CITY\": \"Chelmsford\",\n        \"EPSOM DOWNS\": \"Epsom\",\n        \"FONTWELL\": \"Fontwell Park\",\n        \"HAYDOCK\": \"Haydock Park\",\n        \"KEMPTON\": \"Kempton Park\",\n        \"LINGFIELD\": \"Lingfield Park\",\n        \"NEWMARKET (ROWLEY)\": \"Newmarket\",\n        \"NEWMARKET (JULY)\": \"Newmarket\",\n        \"SANDOWN\": \"Sandown Park\",\n        \"STRATFORD\": \"Stratford-on-Avon\",\n        \"YARMOUTH\": \"Great Yarmouth\",\n        \"CURRAGH\": \"Curragh\",\n        \"DOWN ROYAL\": \"Down Royal\",\n    }\n\n    # Check primary map first\n    if cleaned_name_upper in VENUE_MAP:\n        return VENUE_MAP[cleaned_name_upper]\n\n    # Handle cases where the key is the desired output but needs to be mapped from a variation\n    # e.g. CHELMSFORD maps to Chelmsford\n    # Title case the cleaned name for a sensible default\n    title_cased_name = clean_text(name).title()\n    if title_cased_name in VENUE_MAP.values():\n        return title_cased_name\n\n    # Return the title-cased cleaned name as a fallback\n    return title_cased_name\n\n\ndef normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n",
    "web_service/frontend/app/components/AdapterStatusPanel.tsx": "// web_platform/frontend/src/components/AdapterStatusPanel.tsx\n'use client';\n\nimport React from 'react';\nimport { SourceInfo } from '../types/racing';\n\ninterface AdapterStatusPanelProps {\n  adapter: SourceInfo;\n  onFetchRaces: (sourceName: string) => void;\n}\n\nexport const AdapterStatusPanel: React.FC<AdapterStatusPanelProps> = ({ adapter, onFetchRaces }) => {\n  const isConfigured = adapter.status !== 'CONFIG_ERROR';\n\n  return (\n    <div className={`p-4 rounded-lg border ${isConfigured ? 'bg-slate-800 border-slate-700' : 'bg-yellow-900/20 border-yellow-700/50'}`}>\n      <div className=\"flex justify-between items-center\">\n        <h3 className=\"font-bold text-lg text-white\">{adapter.name}</h3>\n        <span className={`px-2 py-0.5 rounded-full text-xs font-medium ${isConfigured ? 'bg-green-500/20 text-green-300' : 'bg-yellow-500/20 text-yellow-300'}`}>\n          {isConfigured ? 'Ready' : 'Not Configured'}\n        </span>\n      </div>\n      <div className=\"mt-4 flex gap-2\">\n        <button\n          onClick={() => onFetchRaces(adapter.name)}\n          disabled={!isConfigured}\n          className=\"flex-1 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 disabled:bg-slate-700 disabled:text-slate-400 disabled:cursor-not-allowed\"\n        >\n          Automatic Load\n        </button>\n        <button\n          disabled\n          className=\"flex-1 px-4 py-2 bg-slate-700 text-slate-400 rounded cursor-not-allowed\"\n        >\n          Manual Entry (Coming Soon)\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/ManualOverridePanel.tsx": "// web_platform/frontend/src/components/ManualOverridePanel.tsx\nimport React, { useState } from 'react';\nimport { Race } from '../types/racing';\n\ninterface ManualOverridePanelProps {\n  adapterName: string;\n  attemptedUrl: string;\n  apiKey: string | null;\n  onParseSuccess: (adapterName: string, parsedRaces: Race[]) => void;\n}\n\nconst ManualOverridePanel: React.FC<ManualOverridePanelProps> = ({\n  adapterName,\n  attemptedUrl,\n  apiKey,\n  onParseSuccess,\n}) => {\n  const [showPanel, setShowPanel] = useState(true);\n  const [htmlContent, setHtmlContent] = useState('');\n  const [isSubmitting, setIsSubmitting] = useState(false);\n  const [error, setError] = useState<string | null>(null);\n\n  const handleSubmit = async () => {\n    if (!htmlContent.trim()) {\n      setError('HTML content cannot be empty.');\n      return;\n    }\n    if (!apiKey) {\n      setError('API key is not available. Cannot submit.');\n      return;\n    }\n\n    setIsSubmitting(true);\n    setError(null);\n\n    try {\n      const response = await fetch('/api/races/parse-manual', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'X-API-Key': apiKey,\n        },\n        body: JSON.stringify({\n          adapter_name: adapterName,\n          html_content: htmlContent,\n        }),\n      });\n\n      if (!response.ok) {\n        const errorData = await response.json();\n        throw new Error(errorData.detail || 'Failed to parse HTML.');\n      }\n\n      const parsedRaces: Race[] = await response.json();\n      onParseSuccess(adapterName, parsedRaces);\n      setShowPanel(false); // Hide panel on success\n\n    } catch (err) {\n      const errorMessage = err instanceof Error ? err.message : 'An unknown error occurred.';\n      setError(errorMessage);\n      console.error('Manual parse submission failed:', err);\n    } finally {\n      setIsSubmitting(false);\n    }\n  };\n\n\n  if (!showPanel) {\n    return null;\n  }\n\n  return (\n    <div className=\"bg-red-900 bg-opacity-50 border border-red-700 p-4 rounded-lg shadow-lg mb-4\">\n      <div className=\"flex justify-between items-center\">\n        <div>\n          <h3 className=\"font-bold text-red-300\">Data Fetch Failed: {adapterName}</h3>\n          <p className=\"text-sm text-red-400\">\n            The application failed to automatically retrieve data from:{' '}\n            <a href={attemptedUrl} target=\"_blank\" rel=\"noopener noreferrer\" className=\"underline hover:text-red-200\">\n              {attemptedUrl}\n            </a>\n          </p>\n        </div>\n        <button onClick={() => setShowPanel(false)} className=\"text-red-400 hover:text-red-200 text-2xl\">&times;</button>\n      </div>\n      <div className=\"mt-4\">\n        <p className=\"text-sm text-red-300 mb-2\">\n          <strong>To resolve this:</strong>\n          <ol className=\"list-decimal list-inside pl-4\">\n            <li>Click the link above to open the page in a new tab.</li>\n            <li>Right-click on the page and select \"View Page Source\".</li>\n            <li>Copy the entire HTML source code.</li>\n            <li>Paste the code into the text area below and click \"Submit Manual Data\".</li>\n          </ol>\n        </p>\n        <textarea\n          className=\"w-full h-24 p-2 bg-gray-900 border border-gray-700 rounded text-gray-300 font-mono text-xs\"\n          placeholder={`Paste HTML source for ${adapterName} here...`}\n          value={htmlContent}\n          onChange={(e) => setHtmlContent(e.target.value)}\n          disabled={isSubmitting}\n        />\n        {error && <p className=\"text-red-400 text-sm mt-2\">{error}</p>}\n        <div className=\"mt-2 flex gap-2\">\n          <button\n            onClick={handleSubmit}\n            className=\"px-3 py-1.5 bg-blue-600 text-white rounded hover:bg-blue-700 text-sm disabled:bg-blue-800 disabled:cursor-not-allowed\"\n            disabled={isSubmitting}\n          >\n            {isSubmitting ? 'Submitting...' : 'Submit Manual Data'}\n          </button>\n          <button\n            onClick={() => setShowPanel(false)}\n            className=\"px-3 py-1.5 bg-gray-700 text-white rounded hover:bg-gray-600 text-sm\"\n          >\n            Skip for Now\n          </button>\n        </div>\n      </div>\n    </div>\n  );\n};\n\nexport default ManualOverridePanel;\n",
    "web_service/frontend/app/components/SettingsPage.tsx": "// src/components/SettingsPage.tsx\n'use client';\n\nimport React, { useState, useEffect } from 'react';\n\nexport function SettingsPage() {\n  const [apiKey, setApiKey] = useState('');\n  const [betfairAppKey, setBetfairAppKey] = useState('');\n  const [betfairUsername, setBetfairUsername] = useState('');\n  const [betfairPassword, setBetfairPassword] = useState('');\n\n  useEffect(() => {\n    // Fetch the current API key when the component mounts\n    const fetchApiKey = async () => {\n      if (window.electronAPI?.getApiKey) {\n        const key = await window.electronAPI.getApiKey();\n        if (key) {\n          setApiKey(key);\n        }\n      }\n    };\n    fetchApiKey();\n  }, []);\n\n  const handleGenerateApiKey = async () => {\n    if (window.electronAPI?.generateApiKey) {\n      const newKey = await window.electronAPI.generateApiKey();\n      setApiKey(newKey);\n    }\n  };\n\n  const handleSaveSettings = async () => {\n    if (window.electronAPI?.saveApiKey && window.electronAPI?.saveBetfairCredentials) {\n      await window.electronAPI.saveApiKey(apiKey);\n      await window.electronAPI.saveBetfairCredentials({\n        appKey: betfairAppKey,\n        username: betfairUsername,\n        password: betfairPassword,\n      });\n      alert('Settings saved successfully!');\n    }\n  };\n\n  return (\n    <div className=\"bg-slate-800 p-8 rounded-lg border border-slate-700 text-white max-w-2xl mx-auto\">\n      <h2 className=\"text-3xl font-bold text-white mb-6\">Application Settings</h2>\n\n      <div className=\"space-y-8\">\n        <div>\n          <h3 className=\"text-xl font-semibold text-slate-300 mb-2\">API Key</h3>\n          <p className=\"text-sm text-slate-400 mb-3\">This key is required for the dashboard to communicate with the backend service.</p>\n          <div className=\"flex items-center space-x-2\">\n            <input\n              type=\"text\"\n              readOnly\n              value={apiKey}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 font-mono text-sm\"\n            />\n            <button\n              onClick={handleGenerateApiKey}\n              className=\"px-4 py-2 bg-blue-600 hover:bg-blue-700 rounded transition-colors font-semibold\"\n            >\n              Generate New Key\n            </button>\n          </div>\n        </div>\n\n        <div>\n          <h3 className=\"text-xl font-semibold text-slate-300 mb-2\">Betfair Credentials (Optional)</h3>\n           <p className=\"text-sm text-slate-400 mb-3\">Required for adapters that use the Betfair Exchange API.</p>\n          <div className=\"space-y-3\">\n            <input\n              type=\"password\"\n              placeholder=\"App Key\"\n              value={betfairAppKey}\n              onChange={(e) => setBetfairAppKey(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n            <input\n              type=\"text\"\n              placeholder=\"Username\"\n              value={betfairUsername}\n              onChange={(e) => setBetfairUsername(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n            <input\n              type=\"password\"\n              placeholder=\"Password\"\n              value={betfairPassword}\n              onChange={(e) => setBetfairPassword(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n          </div>\n        </div>\n\n        <div className=\"flex justify-end pt-6 border-t border-slate-700\">\n          <button\n            onClick={handleSaveSettings}\n            className=\"px-8 py-3 bg-green-600 hover:bg-green-700 rounded font-bold text-lg transition-colors\"\n          >\n            Save All Settings\n          </button>\n        </div>\n      </div>\n    </div>\n  );\n}\n",
    "web_service/frontend/app/hooks/useRealTimeRaces.ts": "import { useState, useEffect } from 'react';\nimport { io, Socket } from 'socket.io-client';\nimport { Race } from '../types/racing';\n\nconst API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8080';\n\nexport function useRealTimeRaces() {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [isConnected, setIsConnected] = useState(false);\n\n  useEffect(() => {\n    const socket: Socket = io(API_URL);\n\n    socket.on('connect', () => setIsConnected(true));\n    socket.on('disconnect', () => setIsConnected(false));\n\n    socket.on('races_update', (data: { races: Race[] }) => {\n      if (data && Array.isArray(data.races)) {\n        setRaces(data.races);\n      }\n    });\n\n    // Cleanup on component unmount\n    return () => {\n      socket.disconnect();\n    };\n  }, []);\n\n  return { races, isConnected };\n}",
    "web_service/frontend/app/types/electron.d.ts": "// web_platform/frontend/src/types/electron.d.ts\n\n/**\n * This declaration file extends the global Window interface to include the\n * 'electronAPI' object exposed by the preload script. This provides\n * TypeScript with type information for the functions we're using for IPC.\n */\nexport {};\n\ndeclare global {\n  interface Window {\n    electronAPI?: {\n      /**\n       * Asynchronously fetches the secure API key from the main process.\n       * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n       */\n      getApiKey: () => Promise<string | null>;\n      /**\n       * Registers a callback for backend status updates from the main process.\n       * @param callback The function to execute. Receives an object with state and logs.\n       * @returns A function to unsubscribe the listener.\n       */\n      onBackendStatusUpdate: (callback: (status: { state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }) => void) => () => void;\n\n      /**\n       * Sends a command to the main process to restart the backend executable.\n       */\n      restartBackend: () => void;\n\n      /**\n       * Asynchronously fetches the current backend status from the main process.\n       * @returns {Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>}\n       */\n      getBackendStatus: () => Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>;\n      generateApiKey: () => Promise<string>;\n      saveApiKey: (apiKey: string) => Promise<{ success: boolean }>;\n      saveBetfairCredentials: (credentials: { appKey: string; username: string; password: string }) => Promise<{ success: boolean }>;\n      getApiPort: () => Promise<number | null>;\n    };\n  }\n}\n",
    "web_service/frontend/postcss.config.js": "module.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n};",
    "wix/product.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\"\n     xmlns:util=\"http://schemas.microsoft.com/wix/UtilExtension\">\n\n  <Product\n    Id=\"*\"\n    Name=\"Fortuna Faucet - Racing Analysis Engine\"\n    Language=\"1033\"\n    Version=\"$(var.Version)\"\n    Manufacturer=\"Mason J0 Studios\"\n    UpgradeCode=\"12345678-1234-1234-1234-123456789012\">\n\n    <Package\n      InstallerVersion=\"200\"\n      Compressed=\"yes\"\n      InstallScope=\"perMachine\"\n      Platform=\"x64\"\n      Description=\"Horse racing analysis platform\"\n      Comments=\"Professional-grade installer\"/>\n\n    <Media Id=\"1\" Cabinet=\"fortuna.cab\" EmbedCab=\"yes\"/>\n\n    <!-- Directory Structure -->\n    <Directory Id=\"TARGETDIR\" Name=\"SourceDir\">\n      <Directory Id=\"ProgramFiles64Folder\">\n        <Directory Id=\"INSTALLFOLDER\" Name=\"Fortuna Faucet\">\n        </Directory>\n      </Directory>\n      <Directory Id=\"ProgramMenuFolder\">\n        <Directory Id=\"ApplicationProgramsFolder\" Name=\"Fortuna Faucet\"/>\n      </Directory>\n    </Directory>\n\n    <!-- Environment Variable -->\n    <ComponentGroup Id=\"EnvironmentComponentGroup\" Directory=\"INSTALLFOLDER\">\n      <Component Id=\"FortunaPortEnvironmentVar\" Guid=\"*\">\n        <Environment Id=\"FortunaPort\" Name=\"FORTUNA_PORT\" Value=\"8000\" Permanent=\"no\" Action=\"set\" System=\"yes\" />\n        <RegistryValue Root=\"HKLM\" Key=\"Software\\Fortuna Faucet\" Name=\"Path\" Type=\"string\" Value=\"[INSTALLFOLDER]\" KeyPath=\"yes\" />\n      </Component>\n    </ComponentGroup>\n\n    <!-- Features -->\n    <!-- Service Installation -->\n    <ComponentGroup Id=\"ServiceComponentGroup\" Directory=\"INSTALLFOLDER\">\n        <Component Id=\"FortunaBackendService\" Guid=\"*\">\n            <File Id=\"fortuna-backend.exe\" Source=\"$(var.BackendPath)\" KeyPath=\"yes\" />\n            <ServiceInstall\n                Id=\"FortunaServiceInstall\"\n                Type=\"ownProcess\"\n                Name=\"Fortuna\"\n                DisplayName=\"Fortuna Faucet Backend\"\n                Description=\"Data aggregation and analysis engine for horse racing.\"\n                Start=\"auto\"\n                Account=\"LocalSystem\"\n                ErrorControl=\"normal\" />\n            <ServiceControl Id=\"StartFortunaService\" Start=\"install\" Stop=\"uninstall\" Remove=\"uninstall\" Name=\"Fortuna\" Wait=\"no\" />\n        </Component>\n    </ComponentGroup>\n\n    <Feature Id=\"ProductFeature\" Title=\"Fortuna Faucet\" Level=\"1\">\n        <ComponentGroupRef Id=\"BackendFileGroup\"/>\n        <ComponentGroupRef Id=\"FrontendFileGroup\"/>\n        <ComponentGroupRef Id=\"ShortcutsComponentGroup\"/>\n        <ComponentGroupRef Id=\"EnvironmentComponentGroup\"/>\n        <ComponentGroupRef Id=\"ServiceComponentGroup\"/>\n    </Feature>\n\n    <!-- Shortcuts -->\n    <ComponentGroup Id=\"ShortcutsComponentGroup\" Directory=\"ApplicationProgramsFolder\">\n      <Component Id=\"ApplicationShortcuts\" Guid=\"*\">\n          <util:InternetShortcut Id=\"DashboardShortcut\" Name=\"Fortuna Faucet Dashboard\" Target=\"http://localhost:3000\"/>\n          <Shortcut Id=\"UninstallShortcut\" Name=\"Uninstall Fortuna Faucet\" Description=\"Remove this application\" Target=\"[SystemFolder]msiexec.exe\" Arguments=\"/x [ProductCode]\" Advertise=\"no\"/>\n          <RemoveFolder Id=\"ApplicationProgramsFolder\" On=\"uninstall\"/>\n          <RegistryValue Root=\"HKCU\" Key=\"Software\\Fortuna Faucet\" Name=\"Installed\" Type=\"integer\" Value=\"1\" KeyPath=\"yes\"/>\n      </Component>\n    </ComponentGroup>\n\n    <!-- UI -->\n    <UI>\n      <UIRef Id=\"WixUI_CustomInstallDir\" />\n    </UI>\n    <UIRef Id=\"WixUI_Common\" />\n    <Property Id=\"WIXUI_INSTALLDIR\" Value=\"INSTALLFOLDER\" />\n    <WixVariable Id=\"WixUILicenseRtf\" Value=\"electron\\assets\\license.rtf\"/>\n    <WixVariable Id=\"WixUIBannerBmp\" Value=\"electron\\assets\\banner.bmp\"/>\n    <WixVariable Id=\"WixUIDialogBmp\" Value=\"electron\\assets\\dialog.bmp\"/>\n\n    <Condition Message=\"Windows 7 or later (64-bit) is required\">\n      <![CDATA[Installed OR (VersionNT64 >= 601)]]>\n    </Condition>\n\n  </Product>\n</Wix>\n"
}