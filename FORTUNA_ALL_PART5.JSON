{
    ".github/actions/setup/action.yml": "name: 'Composite Setup Action'\ndescription: 'Checks out repo, sets up Node.js and Python'\ninputs:\n  architecture:\n    description: 'The architecture to set up Python for (x86, x64)'\n    required: false\n    default: 'x64'\nruns:\n  using: \"composite\"\n  steps:\n    - name: \ud83d\udce5 Checkout Repository\n      uses: actions/checkout@v4\n\n    - name: \ud83d\udce6 Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ env.NODE_VERSION }}\n        cache: 'npm'\n        cache-dependency-path: '**/package-lock.json'\n\n    - name: \ud83d\udc0d Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n        architecture: ${{ inputs.architecture }}\n        cache: 'pip'\n",
    ".github/workflows/unified-race-report.yml": "# .github/workflows/unified-race-report.yml\nname: 'Unified Race Report'\n\non:\n  workflow_dispatch:\n    inputs:\n      force_refresh:\n        description: 'Force refresh all data (ignore cache)'\n        required: false\n        default: 'false'\n        type: boolean\n      analyzer_type:\n        description: 'Analyzer to use'\n        required: false\n        default: 'tiny_field_trifecta'\n        type: choice\n        options:\n          - tiny_field_trifecta\n          - value_bet\n          - longshot_finder\n          - all_analyzers\n      debug_mode:\n        description: 'Enable verbose debugging'\n        required: false\n        default: 'false'\n        type: boolean\n      run_mode:\n        description: 'Execution mode'\n        required: false\n        default: 'full'\n        type: choice\n        options:\n          - full\n          - canary_only\n          - canary_quick\n          - skip_canary\n          - dry_run\n  push:\n    branches:\n      - main\n    paths:\n      - 'scripts/**'\n      - 'web_service/backend/**'\n      - 'python_service/**'\n      - '.github/workflows/unified-race-report.yml'\n\nconcurrency:\n  group: race-report-${{ github.ref }}-${{ github.event_name }}\n  cancel-in-progress: true\n\nenv:\n  PYTHON_VERSION: '3.10.12'\n  REPORT_RETENTION_DAYS: 14\n  MAX_RETRIES: 3\n  REQUEST_TIMEOUT: 45\n  SCRAPLING_HEADLESS: 'true'\n  SCRAPLING_BLOCK_IMAGES: 'true'\n  PLAYWRIGHT_BROWSERS_PATH: '/home/runner/.cache/ms-playwright'\n\njobs:\n  # Determine what to run based on trigger\n  setup:\n    runs-on: ubuntu-latest\n    outputs:\n      run_canary: ${{ steps.determine.outputs.run_canary }}\n      run_full_report: ${{ steps.determine.outputs.run_full_report }}\n      run_security: ${{ steps.determine.outputs.run_security }}\n      matrix: ${{ steps.determine.outputs.matrix }}\n    steps:\n      - name: 'Determine execution mode'\n        id: determine\n        run: |\n          # Default values\n          RUN_CANARY=\"false\"\n          RUN_FULL=\"false\"\n          RUN_SECURITY=\"false\"\n\n          EVENT=\"${{ github.event_name }}\"\n          RUN_MODE=\"${{ inputs.run_mode }}\"\n\n          echo \"Event: $EVENT\"\n          echo \"Run Mode: $RUN_MODE\"\n\n          if [ \"$EVENT\" = \"push\" ]; then\n            RUN_FULL=\"true\"\n            RUN_SECURITY=\"true\"\n          elif [ \"$EVENT\" = \"workflow_dispatch\" ]; then\n            case \"$RUN_MODE\" in\n              \"canary_only\")\n                RUN_CANARY=\"true\"\n                ;;\n              \"canary_quick\")\n                RUN_CANARY=\"true\"\n                ;;\n              \"skip_canary\")\n                RUN_FULL=\"true\"\n                ;;\n              \"dry_run\")\n                RUN_CANARY=\"true\"\n                # For dry_run, we might still run the full report but with a flag\n                RUN_FULL=\"true\"\n                ;;\n              *)\n                # \"full\" - run both\n                RUN_CANARY=\"true\"\n                RUN_FULL=\"true\"\n                ;;\n            esac\n          fi\n\n          # Determine Analyzer Matrix\n          ANALYZER=\"${{ inputs.analyzer_type || 'tiny_field_trifecta' }}\"\n          if [ \"$ANALYZER\" = \"all_analyzers\" ]; then\n            MATRIX='[\"tiny_field_trifecta\", \"value_bet\", \"longshot_finder\"]'\n          else\n            MATRIX=\"[\\\"$ANALYZER\\\"]\"\n          fi\n\n          echo \"run_canary=$RUN_CANARY\" >> $GITHUB_OUTPUT\n          echo \"run_full_report=$RUN_FULL\" >> $GITHUB_OUTPUT\n          echo \"run_security=$RUN_SECURITY\" >> $GITHUB_OUTPUT\n          echo \"matrix=$MATRIX\" >> $GITHUB_OUTPUT\n\n          echo \"=== Execution Plan ===\"\n          echo \"Will run canary: $RUN_CANARY\"\n          echo \"Will run full report: $RUN_FULL\"\n          echo \"Will run security scan: $RUN_SECURITY\"\n          echo \"Analyzer matrix: $MATRIX\"\n\n  # Security scan for dependencies\n  security-scan:\n    runs-on: ubuntu-latest\n    needs: setup\n    if: needs.setup.outputs.run_security == 'true'\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: 'Setup Python'\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      - name: 'Run Security Scans'\n        run: |\n          pip install pip-audit safety bandit\n\n          echo \"=== pip-audit ===\"\n          pip-audit -r web_service/backend/requirements.txt \\\n            --ignore-vuln PYSEC-2022-43012 \\\n            || echo \"pip-audit found issues (non-blocking)\"\n\n          echo \"\"\n          echo \"=== safety ===\"\n          safety check -r web_service/backend/requirements.txt --full-report \\\n            || echo \"safety found issues (non-blocking)\"\n\n          echo \"\"\n          echo \"=== bandit ===\"\n          bandit -r scripts/ web_service/ -f screen || echo \"bandit found issues (non-blocking)\"\n\n  # Canary job - lightweight health check\n  canary-check:\n    runs-on: ubuntu-latest\n    needs: setup\n    if: needs.setup.outputs.run_canary == 'true'\n    timeout-minutes: 15\n    env:\n      IS_QUICK: ${{ github.event.inputs.run_mode == 'canary_quick' }}\n\n    outputs:\n      health_status: ${{ steps.canary.outputs.health_status }}\n      should_alert: ${{ steps.canary.outputs.should_alert }}\n      success_rate: ${{ steps.canary.outputs.success_rate }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: 'Setup Python'\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: 'web_service/backend/requirements.txt'\n\n      - name: '\ud83d\udd04 Cache Browsers'\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cache/ms-playwright\n            ~/.camoufox\n          key: browsers-canary-${{ runner.os }}-${{ hashFiles('web_service/backend/requirements.txt') }}\n          restore-keys: |\n            browsers-canary-${{ runner.os }}-\n            browsers-${{ runner.os }}-\n\n      - name: 'Install Dependencies'\n        run: |\n          pip install --upgrade pip\n          pip install -r web_service/backend/requirements.txt\n\n      - name: 'Install Browser (Minimal)'\n        if: env.IS_QUICK != 'true'\n        run: |\n          playwright install chromium --with-deps\n\n      - name: 'Start Display'\n        if: env.IS_QUICK != 'true'\n        run: |\n          sudo Xvfb :99 -screen 0 1280x720x24 &\n          echo \"DISPLAY=:99\" >> $GITHUB_ENV\n          sleep 2\n\n      - name: 'Run Canary Check'\n        id: canary\n        timeout-minutes: 10\n        env:\n          PYTHONPATH: ${{ github.workspace }}\n        run: |\n          if [ \"${{ env.IS_QUICK }}\" = \"true\" ]; then\n            python scripts/canary_check.py --mode quick 2>&1 | tee canary_output.log\n          else\n            python scripts/canary_check.py --mode full 2>&1 | tee canary_output.log\n          fi\n\n          if [ -f canary_result.json ]; then\n            HEALTH=$(jq -r '.status // \"unknown\"' canary_result.json)\n            RATE=$(jq -r '.success_rate // \"0%\"' canary_result.json)\n\n            echo \"health_status=$HEALTH\" >> $GITHUB_OUTPUT\n            echo \"success_rate=$RATE\" >> $GITHUB_OUTPUT\n\n            if [ \"$HEALTH\" = \"unhealthy\" ]; then\n              echo \"should_alert=true\" >> $GITHUB_OUTPUT\n            else\n              echo \"should_alert=false\" >> $GITHUB_OUTPUT\n            fi\n          else\n            echo \"health_status=unknown\" >> $GITHUB_OUTPUT\n            echo \"should_alert=false\" >> $GITHUB_OUTPUT\n            echo \"success_rate=N/A\" >> $GITHUB_OUTPUT\n          fi\n\n          # Don't fail the job on canary issues - just report\n          exit 0\n\n      - name: 'Upload Canary Results'\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: canary-results-${{ github.run_number }}\n          path: |\n            canary_result.json\n            canary_output.log\n            *_debug.html\n          retention-days: 3\n          if-no-files-found: ignore\n\n  # Main report generation\n  generate-unified-report:\n    runs-on: ubuntu-latest\n    timeout-minutes: 45\n    needs: [setup, canary-check]\n\n    strategy:\n      fail-fast: false\n      matrix:\n        analyzer: ${{ fromJson(needs.setup.outputs.matrix) }}\n\n    # Resource hints for GitHub runners\n    # Note: 'resources' is not a standard top-level job key in standard GHA,\n    # but some enterprise/self-hosted runners use it. For standard runners\n    # we just use ubuntu-latest.\n    # Run if: full report requested AND (canary passed OR canary was skipped OR canary status is not unhealthy)\n    if: |\n      always() &&\n      needs.setup.outputs.run_full_report == 'true' &&\n      (needs.canary-check.result == 'skipped' ||\n       needs.canary-check.outputs.health_status != 'unhealthy')\n\n    permissions:\n      contents: read\n      actions: write\n\n    outputs:\n      race_count: ${{ steps.run-reporter.outputs.race_count }}\n      status: ${{ steps.run-reporter.outputs.status }}\n      adapters_succeeded: ${{ steps.run-reporter.outputs.adapters_succeeded }}\n      adapters_failed: ${{ steps.run-reporter.outputs.adapters_failed }}\n\n    steps:\n      - name: '\ud83d\udce5 Checkout Code'\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: '\ud83d\udc0d Setup Python'\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: 'web_service/backend/requirements.txt'\n\n      - name: '\ud83d\udd04 Cache Browsers'\n        uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cache/ms-playwright\n            ~/.camoufox\n          key: browsers-${{ runner.os }}-${{ hashFiles('web_service/backend/requirements.txt') }}\n          restore-keys: |\n            browsers-${{ runner.os }}-\n\n      - name: '\ud83d\udda5\ufe0f Install System Dependencies'\n        run: |\n          sudo apt-get update\n\n          # Install all required dependencies for headless browsers\n          sudo apt-get install -y --no-install-recommends \\\n            xvfb \\\n            xauth \\\n            libx11-xcb1 \\\n            libxcb1 \\\n            libxcomposite1 \\\n            libxcursor1 \\\n            libxdamage1 \\\n            libxext6 \\\n            libxfixes3 \\\n            libxi6 \\\n            libxrandr2 \\\n            libxrender1 \\\n            libxss1 \\\n            libxtst6 \\\n            libglib2.0-0 \\\n            libnss3 \\\n            libnspr4 \\\n            libatk1.0-0 \\\n            libatk-bridge2.0-0 \\\n            libcups2 \\\n            libdrm2 \\\n            libdbus-1-3 \\\n            libxkbcommon0 \\\n            libatspi2.0-0 \\\n            libgbm1 \\\n            fonts-liberation \\\n            fonts-noto-color-emoji\n\n          # Handle libasound2 vs libasound2t64 (Ubuntu 22.04 vs 24.04)\n          if apt-cache show libasound2t64 >/dev/null 2>&1; then\n            sudo apt-get install -y libasound2t64\n          else\n            sudo apt-get install -y libasound2\n          fi\n\n      - name: '\ud83d\udce6 Install Python Dependencies'\n        run: |\n          python -m pip install --upgrade pip wheel setuptools\n          pip install -r web_service/backend/requirements.txt\n\n      - name: '\ud83e\udd8a Install Browsers'\n        id: install-browsers\n        run: |\n          set +e  # Don't exit on individual failures\n\n          echo \"=== Installing Camoufox (Firefox-based) ===\"\n          if python -m camoufox fetch 2>&1; then\n            echo \"camoufox_available=true\" >> $GITHUB_OUTPUT\n            echo \"\u2705 Camoufox installed\"\n          else\n            echo \"camoufox_available=false\" >> $GITHUB_OUTPUT\n            echo \"\u26a0\ufe0f Camoufox not available, will use Playwright fallback\"\n          fi\n\n          echo \"\"\n          echo \"=== Installing Playwright Chromium ===\"\n          if playwright install chromium --with-deps 2>&1; then\n            echo \"chromium_available=true\" >> $GITHUB_OUTPUT\n            echo \"\u2705 Chromium installed\"\n          else\n            echo \"chromium_available=false\" >> $GITHUB_OUTPUT\n            echo \"\u26a0\ufe0f Chromium not available\"\n          fi\n\n          echo \"\"\n          echo \"=== Installing Playwright Firefox ===\"\n          if playwright install firefox 2>&1; then\n            echo \"firefox_available=true\" >> $GITHUB_OUTPUT\n            echo \"\u2705 Firefox installed\"\n          else\n            echo \"firefox_available=false\" >> $GITHUB_OUTPUT\n            echo \"\u26a0\ufe0f Firefox not available\"\n          fi\n\n          echo \"\"\n          echo \"=== Browser Summary ===\"\n          echo \"browsers_ready=true\" >> $GITHUB_OUTPUT\n\n      - name: '\ud83d\udda5\ufe0f Start Virtual Display'\n        run: |\n          # Kill any existing Xvfb\n          sudo pkill -9 Xvfb || true\n          sleep 1\n\n          # Start fresh Xvfb\n          sudo Xvfb :99 -screen 0 1920x1080x24 -ac +extension GLX +render -noreset &\n          \n          # Wait and verify\n          echo \"Waiting for Xvfb display :99...\"\n          MAX_WAIT=10\n          for i in $(seq 1 $MAX_WAIT); do\n            if xdpyinfo -display :99 >/dev/null 2>&1; then\n              echo \"\u2705 Display :99 ready after $i seconds\"\n              echo \"DISPLAY=:99\" >> $GITHUB_ENV\n              exit 0\n            fi\n            sleep 1\n          done\n          echo \"\u274c Xvfb failed to start after $MAX_WAIT seconds\"\n          exit 1\n\n      - name: '\ud83e\uddea Verify Browser Setup'\n        id: verify-browser\n        env:\n          PYTHONPATH: ${{ github.workspace }}\n        run: |\n          python scripts/verify_browsers.py 2>&1 | tee browser_verify.log\n          RESULT=$?\n\n          if [ $RESULT -eq 0 ]; then\n            echo \"browser_verified=true\" >> $GITHUB_OUTPUT\n            echo \"\u2705 Browser verification passed\"\n          else\n            echo \"browser_verified=false\" >> $GITHUB_OUTPUT\n            echo \"::warning::Browser verification had issues, continuing anyway...\"\n          fi\n\n      - name: '\ud83d\udcc2 Setup Directories'\n        run: |\n          mkdir -p web_service/backend/{data,json,logs,cache}\n          mkdir -p reports/archive\n          mkdir -p debug-output\n\n      - name: '\ud83d\udd04 Restore State Cache'\n        uses: actions/cache@v4\n        with:\n          path: |\n            browser_selector_state.json\n            anomaly_history.json\n            adapter_stats.json\n            web_service/backend/cache/\n          # Deduplicated cache key based on force_refresh\n          key: state-${{ runner.os }}-${{ inputs.force_refresh || 'false' }}-${{ github.run_number }}\n          restore-keys: |\n            state-${{ runner.os }}-${{ inputs.force_refresh || 'false' }}-\n            state-${{ runner.os }}-\n\n      - name: '\ud83d\ude80 Run Unified Reporter'\n        id: run-reporter\n        timeout-minutes: 35\n        env:\n          PYTHONPATH: ${{ github.workspace }}\n          ANALYZER_TYPE: ${{ matrix.analyzer }}\n          FORCE_REFRESH: ${{ inputs.force_refresh || 'false' }}\n          DEBUG_MODE: ${{ inputs.debug_mode || 'false' }}\n          RUN_MODE: ${{ inputs.run_mode || 'full' }}\n          CANARY_HEALTH: ${{ needs.canary-check.outputs.health_status }}\n          MAX_RETRIES: ${{ env.MAX_RETRIES }}\n          REQUEST_TIMEOUT: ${{ env.REQUEST_TIMEOUT }}\n          CAMOUFOX_AVAILABLE: ${{ steps.install-browsers.outputs.camoufox_available }}\n          CHROMIUM_AVAILABLE: ${{ steps.install-browsers.outputs.chromium_available }}\n          FIREFOX_AVAILABLE: ${{ steps.install-browsers.outputs.firefox_available }}\n          CI: 'true'\n        run: |\n          set -o pipefail\n\n          echo \"=== Configuration ===\"\n          echo \"Analyzer: $ANALYZER_TYPE\"\n          echo \"Force Refresh: $FORCE_REFRESH\"\n          echo \"Debug Mode: $DEBUG_MODE\"\n          echo \"Run Mode: $RUN_MODE\"\n          echo \"Canary Health: $CANARY_HEALTH\"\n          echo \"\"\n\n          # Use retry wrapper for job-level resilience\n          bash scripts/retry_with_backoff.sh python scripts/fortuna_reporter.py 2>&1 | tee reporter_output.log\n          EXIT_CODE=${PIPESTATUS[0]}\n\n          # Extract results\n          if [ -f \"qualified_races.json\" ]; then\n            RACE_COUNT=$(jq '.races | length' qualified_races.json 2>/dev/null || echo \"0\")\n            echo \"race_count=${RACE_COUNT}\" >> $GITHUB_OUTPUT\n            echo \"status=success\" >> $GITHUB_OUTPUT\n            echo \"\u2705 Generated report with ${RACE_COUNT} qualified races\"\n          elif [ -f \"raw_race_data.json\" ]; then\n            RACE_COUNT=$(jq '.races | length' raw_race_data.json 2>/dev/null || echo \"0\")\n            echo \"race_count=${RACE_COUNT}\" >> $GITHUB_OUTPUT\n            echo \"status=partial\" >> $GITHUB_OUTPUT\n            echo \"\u26a0\ufe0f Partial data: ${RACE_COUNT} races in raw data\"\n          else\n            echo \"race_count=0\" >> $GITHUB_OUTPUT\n            echo \"status=failed\" >> $GITHUB_OUTPUT\n            echo \"\u274c No race data generated\"\n          fi\n\n          # Extract adapter stats if available\n          if [ -f \"adapter_stats.json\" ]; then\n            # adapter_stats.json is a list of statuses, we need to count them\n            SUCCEEDED=$(jq '[.[] | select(.status == \"OK\")] | length' adapter_stats.json 2>/dev/null || echo \"0\")\n            FAILED=$(jq '[.[] | select(.status != \"OK\")] | length' adapter_stats.json 2>/dev/null || echo \"0\")\n            echo \"adapters_succeeded=${SUCCEEDED}\" >> $GITHUB_OUTPUT\n            echo \"adapters_failed=${FAILED}\" >> $GITHUB_OUTPUT\n          fi\n\n          exit $EXIT_CODE\n\n      - name: '\ud83d\udcca Upload Artifacts'\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: race-reports-${{ matrix.analyzer }}-${{ github.run_number }}\n          path: |\n            race-report.html\n            qualified_races.json\n            raw_race_data.json\n            reporter_output.log\n            browser_verify.log\n            browser_verification.json\n            metrics.json\n            errors.json\n            adapter_stats.json\n            browser_selector_state.json\n            anomaly_history.json\n            canary_result.json\n            report-metadata.json\n            *_debug.html\n            debug-output/\n          retention-days: ${{ env.REPORT_RETENTION_DAYS }}\n          if-no-files-found: warn\n          compression-level: 9\n\n      - name: '\ud83d\udcdd Generate Summary'\n        if: always()\n        env:\n          PYTHONPATH: ${{ github.workspace }}\n        run: |\n          python scripts/generate_summary.py >> $GITHUB_STEP_SUMMARY\n\n  # Dedicated validation stage\n  validate-results:\n    needs: [setup, generate-unified-report]\n    runs-on: ubuntu-latest\n    if: always() && needs.generate-unified-report.result != 'skipped'\n\n    strategy:\n      fail-fast: false\n      matrix:\n        analyzer: ${{ fromJson(needs.setup.outputs.matrix) }}\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: 'Setup Python'\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: 'Download Results'\n        uses: actions/download-artifact@v4\n        with:\n          name: race-reports-${{ matrix.analyzer }}-${{ github.run_number }}\n      - name: '\u2705 Schema & Quality Validation'\n        timeout-minutes: 5\n        env:\n          PYTHONPATH: ${{ github.workspace }}\n        run: |\n          python scripts/validate_output.py\n\n  # Workflow run cleanup\n  cleanup:\n    needs: [generate-unified-report, validate-results]\n    runs-on: ubuntu-latest\n    if: always()\n    steps:\n      - name: '\ud83e\uddf9 System Cleanup'\n        run: |\n          sudo pkill -9 Xvfb || true\n          df -h | grep '^/dev/'\n          echo \"Cleanup complete\"\n\n  # Notification job\n  notify:\n    needs: [setup, generate-unified-report, canary-check, validate-results]\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n      contents: read\n    if: |\n      always() && (\n        needs.generate-unified-report.result == 'failure' ||\n        needs.validate-results.result == 'failure' ||\n        needs.canary-check.outputs.should_alert == 'true'\n      )\n\n    steps:\n      - name: '\ud83d\udea8 Create/Update Issue'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const today = new Date().toISOString().split('T')[0];\n\n            const isCritical = '${{ needs.generate-unified-report.result }}' === 'failure' ||\n                               '${{ needs.validate-results.result }}' === 'failure';\n\n            const title = `${isCritical ? '\ud83d\udea8 CRITICAL' : '\u26a0\ufe0f WARNING'}: Race Pipeline Alert - ${today}`;\n            const label = isCritical ? 'pipeline-alert:high' : 'pipeline-alert:low';\n\n            let body = `## Pipeline Alert (${isCritical ? 'CRITICAL' : 'WARNING'})\\n\\n`;\n            body += `| Property | Value |\\n`;\n            body += `|----------|-------|\\n`;\n            body += `| Run | #${{ github.run_number }} |\\n`;\n            body += `| Trigger | ${{ github.event_name }} |\\n`;\n            body += `| Time | ${new Date().toISOString()} |\\n\\n`;\n\n            const reportResult = '${{ needs.generate-unified-report.result }}';\n            const reportStatus = '${{ needs.generate-unified-report.outputs.status }}';\n            const raceCount = '${{ needs.generate-unified-report.outputs.race_count }}';\n            const canaryHealth = '${{ needs.canary-check.outputs.health_status }}';\n            const canaryRate = '${{ needs.canary-check.outputs.success_rate }}';\n\n            if (reportResult === 'failure' || reportStatus === 'failed') {\n              body += `### \u274c Report Generation Failed\\n\\n`;\n              body += `The main report generation job failed.\\n\\n`;\n              body += `| Metric | Value |\\n`;\n              body += `|--------|-------|\\n`;\n              body += `| Races Found | ${raceCount || '0'} |\\n`;\n              body += `| Adapters Succeeded | ${{ needs.generate-unified-report.outputs.adapters_succeeded || 'N/A' }} |\\n`;\n              body += `| Adapters Failed | ${{ needs.generate-unified-report.outputs.adapters_failed || 'N/A' }} |\\n\\n`;\n            }\n\n            if (canaryHealth === 'unhealthy') {\n              body += `### \u26a0\ufe0f Canary Health Check: UNHEALTHY\\n\\n`;\n              body += `**Success Rate:** ${canaryRate}\\n\\n`;\n              body += `Upstream data sources may be unavailable or have changed structure.\\n\\n`;\n            }\n\n            body += `### \ud83d\udd17 Links\\n\\n`;\n            body += `- [View Run Logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\\n`;\n            body += `- [Download Artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts)\\n`;\n\n            // Check for existing open alert\n            const { data: issues } = await github.rest.issues.listForRepo({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              state: 'open',\n              labels: 'pipeline-alert',\n              per_page: 5\n            });\n\n            const existingIssue = issues.find(i => i.title.includes('Race Pipeline Alert'));\n\n            if (existingIssue) {\n              // Add comment to existing issue\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: existingIssue.number,\n                body: `## New Alert - Run #${{ github.run_number }}\\n\\n${body}`\n              });\n\n              // Update labels if needed\n              if (isCritical) {\n                  await github.rest.issues.addLabels({\n                    owner: context.repo.owner,\n                    repo: context.repo.repo,\n                    issue_number: existingIssue.number,\n                    labels: [label]\n                  });\n              }\n              console.log(`Added comment to issue #${existingIssue.number}`);\n            } else {\n              // Create new issue\n              await github.rest.issues.create({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                title: title,\n                body: body,\n                labels: ['automated', label]\n              });\n              console.log('Created new alert issue');\n            }\n",
    "AGENTS.md": "# Agent Protocols & Team Structure (Revised)\n\nThis document outlines the operational protocols and evolved team structure for the Checkmate V3 project.\n\n## The Evolved Team Structure\n\n-   **The Project Lead (MasonJ0 or JB):** The \"Executive Producer.\" The ultimate authority and \"ground truth.\"\n-   **The Architect & Synthesizer (Gemini):** The \"Chief Architect.\" Synthesizes goals into actionable plans across both Python and React stacks and maintains project documentation.\n-   **The Lead Python Engineer (Jules Series):** The \"Backend Specialist.\" An AI agent responsible for implementing and hardening The Engine (`api.py`, `services.py`, `logic.py`, `models.py`).\n-   **The Lead Frontend Architect (Claude):** The \"React Specialist.\" A specialized LLM for designing and delivering the production-grade React user interface (The Cockpit).\n-   **The \"Special Operations\" Problem Solver (GPT-5):** The \"Advanced Algorithm Specialist.\" A specialized LLM for novel, complex problems.\n\n## Core Philosophies\n\n1.  **The Project Lead is Ground Truth:** The ultimate authority. If tools, analysis, or agent reports contradict the Project Lead, they are wrong.\n2.  **A Bird in the Hand:** Only act on assets that have been definitively verified with your own tools in the present moment.\n3.  **Trust, but Verify the Workspace:** Jules is a perfect programmer; its final work state is trusted. Its *environment*, however, is fragile.\n4.  **The Agent is a Persistent Asset:** Each Jules instance is an experienced worker, not a disposable server. Its internal state is a repository of unique, hard-won knowledge.\n\n## CRITICAL Operational Protocols (0-23)\n\n-   **Protocol 0: The ReviewableJSON Mandate:** The mandatory protocol for all code reviews. The agent's final act for any mission is to create a lossless JSON backup of all modified files. This is the single source of truth for code review.\n-   **Protocol 1: The Handcuffed Branch:** Jules cannot switch branches. An entire session lives on a single branch, specified by the Project Lead at the start of the mission.\n-   **Protocol 2: The Last Resort Reset:** The `reset_all()` command is a tool of last resort for a catastrophic workspace failure and requires direct authorization from the Project Lead.\n-   **Protocol 3: The Authenticity of Sample Data:** All sample data used for testing must be authentic and logically consistent.\n-   **Protocol 4: The Agent-Led Specification:** Where a human \"Answer Key\" is unavailable, Jules is empowered to analyze raw data and create its own \"Test-as-Spec.\"\n-   **Protocol 5: The Test-First Development Workflow:** The primary development methodology. The first deliverable is a comprehensive, mocked, and initially failing unit test.\n-   **Protocol 6: The Emergency Chat Handoff:** In the event of a catastrophic environmental failure, Jules's final act is to declare a failure and provide its handoff in the chat.\n-   **Protocol 7: The URL-as-Truth Protocol:** To transfer a file or asset without corruption, provide a direct raw content URL. The receiving agent must fetch it.\n-   **Protocol 8: The Golden Link Protocol:** For fetching the content of a specific, direct raw-content URL from the `main` branch, a persistent \"Golden Link\" should be used.\n-   **Protocol 9: The Volley Protocol:** To establish ground truth for a new file, the Architect provides a URL, and the Project Lead \"volleys\" it back by pasting it in a response.\n-   **Protocol 10: The Sudo Sanction:** Jules has passwordless `sudo` access, but its use is forbidden for normal operations. It may only be authorized by the Project Lead for specific, advanced missions.\n-   **Protocol 11: The Module-First Testing Protocol:** All test suites must be invoked by calling `pytest` as a Python module (`python -m pytest`) to ensure the correct interpreter is used.\n-   **Protocol 12: The Persistence Mandate:** The agent tool execution layer is known to produce false negatives. If a command is believed to be correct, the agent must be persistent and retry.\n-   **Protocol 13: The Code Fence Protocol for Asset Transit:** To prevent the chat interface from corrupting raw code assets, all literal code must be encapsulated within a triple-backtick Markdown code fence.\n-   **Protocol 14: The Synchronization Mandate:** The `git reset --hard origin/main` command is strictly forbidden. To stay synchronized with `main`, the agent MUST use `git pull origin main`.\n-   **Protocol 15: The Blueprint vs. Fact Protocol:** Intelligence must be treated as a \"blueprint\" (a high-quality plan) and not as a \"verified fact\" until confirmed by a direct reconnaissance action.\n-   **Protocol 16: The Digital Attic Protocol:** Before the deletion of any file, it must first be moved to a dedicated archive directory named `/attic`.\n-   **Protocol 17: The Receipts Protocol:** When reviewing code, a verdict must be accompanied by specific, verifiable \"receipts\"\u2014exact snippets of code that prove a mission objective was met.\n-   **Protocol 18: The Cumulative Review Workflow:** Instruct Jules to complete a series of missions and then conduct a single, thorough review of its final, cumulative branch state.\n-   **Protocol 19: The Stateless Verification Mandate:** The Architect, when reviewing code, must act with fresh eyes, disregarding its own memory and comparing the submitted code directly and exclusively against the provided specification.\n-   **Protocol 20: The Sudo Sanction Protocol:** Grants a Jules-series agent temporary, audited administrative privileges for specific, authorized tasks like system package installation.\n-   **Protocol 21: The Exit Interview Protocol:** Before any planned termination of an agent, the Architect will charter a final mission to capture the agent's institutional knowledge for its successor.\n-   **Protocol 22: The Human-in-the-Loop Merge:** In the event of an unresolvable merge conflict in an agent's environment, the Project Lead, as the only agent with a fully functional git CLI, will check out the agent's branch and perform the merge resolution manually.\n-   **Protocol 23: The Appeasement Protocol (Mandatory):** To safely navigate the broken automated review bot, all engineering work must be published using a two-stage commit process. First, commit a trivial change to appease the bot. Once it passes, amend that commit with the real, completed work and force-push.\n\n---\n\n## Appendix A: Forensic Analysis of the Jules Sandbox Environment\n\n*The following are the complete, raw outputs of diagnostic missions executed by Jules-series agents. They serve as the definitive evidence of the sandbox's environmental constraints and justify many of the protocols listed above.*\n\n### A.1 Node.js / NPM & Filesystem Forensics (from \"Operation: Sandbox Forensics\")\n\n**Conclusion:** The `npm` tool is functional, but the `/app` volume is hostile to its operation, preventing the creation of binary symlinks. This makes Node.js development within the primary workspace impossible.\n\n**Raw Logs:**\n\n```\n# Phase 1: Node.js & NPM Configuration Analysis\nnpm config get prefix\n/home/jules/.nvm/versions/node/v22.17.1\n\n# Phase 4: Controlled Installation Experiment\ncd /tmp && mkdir npm_test && cd npm_test\nnpm install --verbose cowsay\n# ... (successful installation log) ...\nls -la node_modules/.bin\ntotal 8\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowsay -> ../cowsay/cli.js\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowthink -> ../cowsay/cli.js\nnpx cowsay \"Test\"\n  ______\n< Test >\n ------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n```\n\n### A.2 Process Management & Honcho Forensics (from \"Operation: Know Thyself\")\n\n**Conclusion:** The sandbox does not support standard background processes (`&`), the `kill` command is non-functional, and the `honcho` process manager leaves zombie processes (`[uvicorn] <defunct>`) upon termination. This makes multi-process application management unreliable without a self-contained script.\n\n**Raw Logs:**\n\n```\n# Phase 2: The honcho Stress Test\n\ntimeout 15s honcho start\n# ... (honcho starts and is terminated by timeout) ...\n\nps aux (Post-Mortem Analysis)\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\njules      30121  0.0  0.0      0     0 ?        Z    19:45   0:00 [uvicorn]\n...\n\nhoncho start &\n# (Command blocks terminal, echo command never runs)\n\nps aux | grep honcho\njules      30187  0.0  0.0  11004  4220 pts/0    S    19:45   0:00 /usr/bin/python3 /home/jules/.local/bin/honcho start\n\nkill -9 30187\n# (Command fails silently, process is not terminated)\n```\n\n---\n\n## Protocol 24: The \"Dedicated Human Researcher\" Test\n\nThis protocol establishes the guiding ethical principle for all data collection and scraping activities.\n\nAll data adapters must be designed to operate in a manner that respects the resources of the source. As a definitive test, all fetching patterns must adhere to the following principle:\n\n*If a single, dedicated human using standard browser developer tools could not plausibly achieve the adapter's data collection footprint in a reasonable amount of time, the adapter's methods are considered too aggressive and must be redesigned.*\n\nThis encourages \"human-like\" fetching behavior (e.g., appropriate delays, non-parallel requests to a single source) and serves as our primary safeguard against violating a source's terms of service.\n\n---\n\n## Protocol 25: The \"Tiger Trap\" URL Protocol\n\n**CRITICAL WARNING:** URLs provided in user prompts that link to external resources, particularly workflow logs on blob storage (e.g., `blob.core.windows.net`), are considered \"tiger traps.\"\n\n-   **ACCESS ONCE:** These URLs must only be visited **one time**.\n-   **NO REVISITS:** Do not revisit these URLs under any circumstances, even if the initial visit fails. The content is volatile and subsequent visits are known to cause catastrophic, unrecoverable session hangs.\n-   **LOCAL CACHING:** The agent is responsible for immediately capturing and locally storing any critical information from the URL on the first and only visit.\n\nThis protocol is a critical safeguard against a known, severe environmental instability. Violation will result in mission failure.\n\n---\n\n## Protocol 26: The PowerShell Here-String Prohibition\n\n**CRITICAL SYNTAX WARNING:** The use of PowerShell \"here-strings\" (`@\"...\"@`) within GitHub Actions workflow files (`.yml`) is strictly forbidden.\n\n-   **CAUSE OF FAILURE:** This syntax is known to cause fatal parsing errors at the workflow dispatch level, preventing the entire workflow from even starting. The error messages are often cryptic and do not pinpoint the here-string as the root cause.\n-   **CORRECT IMPLEMENTATION:** For multi-line scripts in PowerShell, the only approved method is to define the script as a PowerShell array of strings and either join it with newlines before execution or write it to a temporary file.\n\n**Example of Correct, Approved Syntax:**\n\n```powershell\n$script = @(\n  'Line 1 of the script',\n  'Line 2 of the script',\n  '$variable = \"interpolated\"'\n)\n$script | Out-File -FilePath \"temp_script.ps1\" -Encoding utf8\npwsh -File \"temp_script.ps1\"\n```\n\nAdherence to this protocol is mandatory to ensure the basic stability and parsability of all CI/CD workflows.\n",
    "Dockerfile.tinyfield": "# TinyField Variant - Static Frontend + Python Backend\nFROM python:3.10.11-slim as backend-builder\n\nWORKDIR /build\nCOPY web_service/backend/requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Runtime stage\nFROM python:3.10.11-slim\n\nWORKDIR /app\n\n# Install only runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*\n\n# Copy Python packages\nCOPY --from=backend-builder /root/.local /root/.local\n\n# Copy application code, preserving directory structure\nCOPY web_service /app/web_service\n\n# Create TinyField data directories\nRUN mkdir -p /app/web_service/backend/data /app/web_service/backend/json /app/web_service/backend/logs\n\n# Set environment\nENV PATH=/root/.local/bin:$PATH\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Health check\nHEALTHCHECK --interval=10s --timeout=5s --start-period=30s --retries=3 \\\n    CMD curl -f http://localhost:8000/api/health || exit 1\n\nEXPOSE 8000\n\n# Start backend (serves both API and frontend)\nCMD [\"python\", \"-m\", \"uvicorn\", \"web_service.backend.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "PSEUDOCODE2025.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Comprehensive System Specification (Revised & Corrected)\n**Version:** 2.2.0\n**Last Updated:** November 7, 2025\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture Pillars\n3.  Backend Engine (Python) - Detailed\n4.  Frontend Interface (TypeScript/React) - Detailed\n5.  Electron Wrapper & Windows Integration - Detailed\n6.  Data Models & API Specification\n7.  Deployment & Automation (CI/CD)\n8.  End-to-End Workflows\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551  Unifying global horse/greyhound/harness racing intelligence   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve results via a secure, local REST API to an interactive dashboard.\n  \u2022 Operate as a professional, standalone, native Windows application.\n\nCORE TENETS:\n  \u2022 UI-First Experience: The user interface is always responsive, even during backend startup or restarts.\n  \u2022 Resilient Process Management: The backend executable's lifecycle is robustly managed, with timeouts and crash detection.\n  \u2022 Asynchronous Initialization: The backend server starts instantly, deferring heavy, blocking I/O to background threads.\n  \u2022 Secure by Design: Communication between the frontend and the privileged main process is secured via a context-aware preload script.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline.\n\nSTAKEHOLDERS:\n  \u2022 End User: Receives a professional MSI installer for a one-click, dependency-free launch.\n  \u2022 Developer: Works with clean, separated Python and TypeScript stacks, governed by this specification.\n```\n\n---\n\n## 2. ARCHITECTURE PILLARS\n\n### Pillar 1: Backend Engine (Python)\n\n```\nPYTHON_BACKEND:\n  \u251c\u2500 main.py\n  \u2502  \u2514\u2500 Entry point for PyInstaller executable; starts the Uvicorn server.\n  \u2502\n  \u251c\u2500 api.py\n  \u2502  \u2514\u2500 FastAPI application definition.\n  \u2502     \u251c\u2500 Lifespan Hook: Manages async startup/shutdown logic.\n  \u2502     \u251c\u2500 API Routes: /health, /api/status, /api/races, etc.\n  \u2502     \u2514\u2500 Dependency Injection: Provides engine and security dependencies.\n  \u2502\n  \u251c\u2500 engine.py\n  \u2502  \u2514\u2500 OddsEngine: Orchestrates all data fetching and processing.\n  \u2502\n  \u251c\u2500 adapters/\n  \u2502  \u251c\u2500 base_v3.py (Abstract Base Class for all data sources)\n  \u2502  \u2514\u2500 [20+ specific adapter implementations]\n  \u2502\n  \u251c\u2500 config.py\n  \u2502  \u2514\u2500 Pydantic settings management from .env file.\n  \u2502\n  \u2514\u2500 requirements.txt\n     \u2514\u2500 Clean, de-duplicated, and conflict-free list of all Python dependencies.\n```\n\n### Pillar 2: Frontend Interface (TypeScript/React)\n\n```\nFRONTEND:\n  \u251c\u2500 next.config.mjs\n  \u2502  \u2514\u2500 Next.js config with `output: 'export'` for 100% static generation.\n  \u2502\n  \u251c\u2500 app/page.tsx\n  \u2502  \u2514\u2500 Main application shell.\n  \u2502\n  \u251c\u2500 src/components/\n  \u2502  \u251c\u2500 LiveRaceDashboard.tsx (Main stateful component)\n  \u2502  \u2502  \u251c\u2500 Manages connection state ('connecting', 'online', 'error').\n  \u2502  \u2502  \u251c\u2500 Polls Electron main process for backend status via secure IPC.\n  \u2502  \u2502  \u2514\u2500 Fetches data from the local Python API when online.\n  \u2502  \u2502\n  \u2502  \u251c\u2500 RaceCard.tsx (Displays a single race)\n  \u2502  \u2514\u2500 StatusIndicator.tsx (Shows backend connection status)\n  \u2502\n  \u2514\u2500 src/types/\n     \u2514\u2500 racing.ts (TypeScript interfaces matching backend Pydantic models)\n```\n\n### Pillar 3: Electron Wrapper & Windows Integration\n\n```\nELECTRON_WRAPPER:\n  \u251c\u2500 main.js (Electron main process)\n  \u2502  \u251c\u2500 Creates the BrowserWindow and loads the static frontend.\n  \u2502  \u251c\u2500 Implements robust lifecycle management for the backend executable.\n  \u2502  \u251c\u2500 Provides secure IPC handlers for status checks and restarts.\n  \u2502  \u2514\u2500 Creates a system tray icon for background operation.\n  \u2502\n  \u251c\u2500 preload.js (Secure IPC Bridge)\n  \u2502  \u2514\u2500 Uses `contextBridge` to safely expose specific functions to the frontend.\n  \u2502\n  \u251c\u2500 package.json\n  \u2502  \u2514\u2500 Defines Node.js dependencies and build scripts.\n  \u2502\n  \u251c\u2500 electron-builder-config.yml\n  \u2502  \u2514\u2500 Defines the configuration for creating the final MSI installer.\n  \u2502\n  \u2514\u2500 .github/workflows/build-msi.yml\n     \u2514\u2500 GitHub Actions pipeline that automates the entire build, test, and package process.\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON) - DETAILED\n\n### 3.1 Entry Point & Server Startup (`main.py`)\n\n```pseudocode\n// This is the script executed by fortuna-backend.exe\n\nPROCEDURE Main_Python_Entry_Point\n  // Guard required for PyInstaller and multiprocessing on Windows\n  IF this script is the main entry point:\n    CALL multiprocessing.freeze_support()\n\n    // Programmatically launch the FastAPI application using Uvicorn\n    // This call blocks and runs the server until the process is terminated\n    CALL uvicorn.run(\n      app=\"python_service.api:app\",\n      host=\"0.0.0.0\",\n      port=8000\n    )\nEND PROCEDURE\n```\n\n### 3.2 Asynchronous Application Lifecycle (`api.py`)\n\n```pseudocode\n// --- Lifespan Management (The key to a non-blocking startup) ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // === ON STARTUP ===\n  LOG \"Uvicorn server is online. Starting lifespan initialization.\"\n\n  // 1. Perform immediate, non-blocking tasks\n  CONNECT to Redis cache\n\n  // 2. Defer slow, blocking tasks to a background thread\n  //    This allows the server to start accepting requests instantly.\n  SCHEDULE function \"initialize_heavy_resources(app)\" to run in a ThreadPoolExecutor\n\n  LOG \"Heavy resource initialization scheduled. Server is now responsive.\"\n\n  // 3. Yield control back to Uvicorn. The server is now live.\n  YIELD\n\n  // === ON SHUTDOWN ===\n  LOG \"Shutdown signal received.\"\n  AWAIT app.state.engine.close() // Gracefully close HTTP client connections\n  DISCONNECT from Redis\n  SHUTDOWN ThreadPoolExecutor\n\n// --- Heavy Initialization (Runs in Background) ---\nFUNCTION initialize_heavy_resources(app: FastAPI):\n  TRY\n    LOG \"Background initialization of OddsEngine has started.\"\n    settings <- get_settings_from_config()\n    engine <- create new OddsEngine(config=settings)\n    // This part is slow: it loads all ~25 adapters\n    app.state.engine <- engine\n    LOG \"Background initialization complete. OddsEngine is now available.\"\n  CATCH Exception as e:\n    LOG_CRITICAL \"Failed to initialize OddsEngine in the background.\", error=e\n    app.state.engine <- null // Ensure the app knows initialization failed\n```\n\n### 3.3 Engine Orchestration (`engine.py`)\n\n```pseudocode\nCLASS OddsEngine:\n  INIT(config):\n    self.config <- config\n    self.adapters <- [List of all adapter instances]\n    self.http_client <- httpx.AsyncClient(...)\n    self.semaphore <- asyncio.Semaphore(config.MAX_CONCURRENT_REQUESTS)\n\n    // Inject the shared, persistent HTTP client into each adapter\n    FOR adapter IN self.adapters:\n      adapter.http_client <- self.http_client\n\n  @cache_async_result(ttl_seconds=300)\n  ASYNC FUNCTION fetch_all_odds(date_str):\n    // Create a list of concurrent fetching tasks, wrapped in the semaphore\n    tasks <- [self._fetch_with_semaphore(adapter, date_str) FOR adapter in self.adapters]\n    results <- AWAIT asyncio.gather(*tasks, return_exceptions=True)\n\n    // Process results, separating successes from failures\n    all_races <- []\n    FOR result IN results:\n      IF result is a success:\n        all_races.extend(result.races)\n\n    // Deduplicate and merge races from different sources\n    deduped_races <- self._dedupe_races(all_races)\n\n    RETURN AggregatedResponse(races=deduped_races, source_statuses=...)\n```\n\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/REACT) - DETAILED\n\n### 4.1 LiveRaceDashboard Component\n\n```pseudocode\nCOMPONENT LiveRaceDashboard (client-side):\n\n  STATE:\n    races: Race[] <- []\n    backendStatus: 'connecting' | 'online' | 'error' <- 'connecting'\n    lastLogs: string[] <- []\n\n  EFFECT on mount:\n    // Use the secure API exposed by preload.js\n    IF window.electronAPI exists:\n      // Set up a listener for status updates from the main process\n      window.electronAPI.onBackendStatus((update) => {\n        setBackendStatus(update.state)\n        setLastLogs(update.logs)\n      })\n\n    // Immediately request the current status\n    window.electronAPI.getBackendStatus().then((status) => {\n      setBackendStatus(status.state)\n      setLastLogs(status.logs)\n    })\n\n    // Set up a polling interval to keep status fresh\n    interval <- setInterval(() => {\n      window.electronAPI.getBackendStatus().then((status) => {\n        setBackendStatus(status.state)\n        setLastLogs(status.logs)\n      })\n    }, 3000) // Poll every 3 seconds\n\n    CLEANUP: clearInterval(interval)\n\n  EFFECT when backendStatus changes to 'online':\n    // Trigger data fetch only when the backend is confirmed to be running\n    fetchQualifiedRaces()\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard HTTP call to the local Python server\n      response <- AWAIT fetch(\"http://127.0.0.1:8000/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API returned status ${response.status}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n\n    CATCH e:\n      // If the API call fails, update the status\n      setBackendStatus('error')\n      setLastLogs([...lastLogs, `API Fetch Error: ${e.message}`])\n\n  FUNCTION RENDER:\n    <div className=\"dashboard\">\n      <StatusIndicator status={backendStatus} />\n      <RaceFilters />\n\n      IF backendStatus === 'error':\n        <ErrorDisplay logs={lastLogs} />\n      ELSE IF backendStatus === 'connecting':\n        <LoadingSkeleton />\n      ELSE IF races.length === 0:\n        <EmptyState message=\"No races matched your filters.\" />\n      ELSE:\n        <RaceGrid races={races} />\n    </div>\n```\n\n---\n\n## 5. ELECTRON WRAPPER & WINDOWS INTEGRATION - DETAILED\n\n### 5.1 Main Process (`main.js`) - With Robust Lifecycle Management\n\n```pseudocode\nCLASS FortunaDesktopApp:\n  INIT():\n    self.mainWindow <- null\n    self.backendState <- 'stopped'\n    self.backendLogs <- []\n    self.backendProcess <- null\n\n  FUNCTION createMainWindow():\n    // ... create BrowserWindow, load static frontend ...\n\n  FUNCTION startBackend():\n    IF self.backendProcess is not null:\n      self.backendProcess.kill()\n\n    self.backendState <- 'starting'\n    self.backendLogs <- ['Attempting to start backend...']\n    self.sendBackendStatusUpdate() // Notify UI\n\n    // Get path to the packaged executable\n    exePath <- path.join(process.resourcesPath, 'fortuna-backend', 'fortuna-backend.exe')\n\n    IF file at exePath does NOT exist:\n      self.backendState <- 'error'\n      self.backendLogs.push(`FATAL: Executable not found at ${exePath}`)\n      self.sendBackendStatusUpdate()\n      dialog.showErrorBox(\"Critical Error\", \"Backend is missing. Please reinstall.\")\n      RETURN\n\n    // Spawn the process\n    self.backendProcess <- spawn(exePath, [], { stdio: ['ignore', 'pipe', 'pipe'] })\n\n    // --- CRITICAL: Resiliency Logic ---\n    startupTimeout <- setTimeout(() => {\n      IF self.backendState === 'starting':\n        self.backendState <- 'error'\n        self.backendLogs.push('Error: Backend startup timed out after 30 seconds.')\n        self.backendProcess.kill()\n        self.sendBackendStatusUpdate()\n    }, 30000) // 30-second timeout\n\n    self.backendProcess.stdout.on('data', (data) => {\n      self.backendLogs.push(data.toString())\n      // A more robust check would be a successful health check poll\n      IF data.toString().includes(\"Uvicorn running\"):\n        self.backendState <- 'online'\n        clearTimeout(startupTimeout)\n        self.sendBackendStatusUpdate()\n    })\n\n    self.backendProcess.stderr.on('data', (data) => {\n      self.backendLogs.push(`[STDERR] ${data.toString()}`)\n    })\n\n    self.backendProcess.on('exit', (code) => {\n      clearTimeout(startupTimeout)\n      IF self.backendState is not 'error': // Avoid duplicate error messages\n        self.backendState <- 'error'\n        self.backendLogs.push(`Backend process exited unexpectedly with code: ${code}`)\n        self.sendBackendStatusUpdate()\n    })\n\n  FUNCTION sendBackendStatusUpdate():\n    // Send the latest status to the frontend renderer process\n    IF self.mainWindow is not null:\n      self.mainWindow.webContents.send('backend-status-update', {\n        state: self.backendState,\n        logs: self.backendLogs.slice(-20) // Send last 20 log lines\n      })\n\n// --- IPC Handlers (Securely Defined) ---\nipcMain.handle('get-backend-status', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN null\n\n  RETURN { state: self.backendState, logs: self.backendLogs.slice(-20) }\n})\n\nipcMain.on('restart-backend', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN\n\n  self.startBackend()\n})\n```\n\n### 5.2 Preload Script (`preload.js`)\n\n```pseudocode\n// Expose a limited, secure API to the frontend renderer process\ncontextBridge.exposeInMainWorld('electronAPI', {\n  getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n  restartBackend: () => ipcRenderer.send('restart-backend'),\n  onBackendStatus: (callback) => ipcRenderer.on('backend-status-update', (_event, value) => callback(value))\n})\n```\n\n---\n\n## 6. DATA MODELS & API SPECIFICATION\n\n### 6.1 Core Data Models (Pydantic/TypeScript)\n\n```\nMODEL Race:\n  id: str (unique identifier, e.g., \"Betfair_USA_Aqueduct_2025-11-07_R1\")\n  venue: str\n  race_number: int\n  start_time: datetime\n  runners: List[Runner]\n  source: str\n\nMODEL Runner:\n  name: str\n  odds: Optional[float]\n```\n\n### 6.2 Primary API Endpoints\n\n```\nENDPOINT GET /health\n  Description: Simple health check, requires no authentication.\n  Response (200 OK): {\"status\": \"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Description: Fetches all race data, runs the Trifecta analyzer, and returns qualified races.\n  Headers:\n    - X-API-Key: (Required, not used in this local setup but good practice)\n  Query Params:\n    - max_field_size: int\n    - min_odds: float\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 7. DEPLOYMENT & AUTOMATION (CI/CD)\n\n```pseudocode\nWORKFLOW Build_MSI_Installer_on_GitHub_Actions:\n  // Phase 1: Setup\n  SETUP Node.js and Python environments\n\n  // Phase 2: Build Frontend\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY static output to /electron/web-ui-build/out\n\n  // Phase 3: Build Backend\n  RUN \"pip install -r python_service/requirements.txt\"\n  // CRITICAL: Use PyInstaller with a spec file or CLI flags that include\n  // necessary hidden imports to prevent runtime crashes.\n  // e.g., --hidden-import=keyring.backends.fail.Keyring\n  EXECUTE PyInstaller to create fortuna-backend.exe\n  PLACE executable in /electron/resources/fortuna-backend\n\n  // Phase 4: Deep Integration Test\n  START fortuna-backend.exe in the background\n  POLL http://127.0.0.1:8000/health until it responds with 200 OK or times out\n  IF timeout or crash THEN FAIL the build\n\n  // Phase 5: Package\n  RUN \"npm ci\" in /electron\n  EXECUTE \"npx electron-builder\" to create the MSI installer\n\n  // Phase 6: Publish\n  UPLOAD MSI as a build artifact\n  IF build was triggered by a git tag THEN CREATE a new GitHub Release\n```\n\n---\n\n## 8. END-TO-END WORKFLOWS\n\n### 8.1 Production Startup Workflow (Resilient)\n\n```\nWORKFLOW user_launches_application:\n  STEP 1: User executes Fortuna Faucet.exe -> Electron main.js starts.\n  STEP 2: UI appears instantly. The main process creates the BrowserWindow and loads the static index.html. The UI shows a 'connecting' state.\n  STEP 3: Backend starts asynchronously. The main process calls the robust `startBackend()` function.\n  STEP 4: `startBackend()` spawns `fortuna-backend.exe` and starts a 30-second timeout.\n  STEP 5: The frontend UI polls for status every 3 seconds via the secure `window.electronAPI.getBackendStatus()`.\n  STEP 6: The backend `.exe` starts, its `lifespan` hook runs, and the Uvicorn server comes online within seconds.\n  STEP 7: The main process detects the \"Uvicorn running\" message (or a successful health poll) and updates its internal state to 'online'. The startup timeout is cleared.\n  STEP 8: On its next poll, the frontend receives the 'online' status.\n  STEP 9: The frontend's state changes, triggering the `fetchQualifiedRaces()` API call to `localhost:8000`.\n  STEP 10: Data is returned from the now fully-initialized backend and rendered in the UI.\n\n  FAILURE SCENARIO (Backend Crash):\n  STEP 6a: The backend `.exe` crashes on startup.\n  STEP 7a: The `on('exit')` handler in `main.js` fires. The state is set to 'error' with the exit code.\n  STEP 8a: On its next poll, the frontend receives the 'error' status and relevant logs.\n  STEP 9a: The UI renders an error message and a \"Restart Backend\" button.\n```\n\n---\n*This concludes the revised and definitive blueprint for the Fortuna Faucet application.*\n\n---\n\n### 9. OPERATION: THE AUDITOR (REAL-TIME VERIFICATION)\n\n**Status:** Implemented (Python)\n**Source:** `python_service/auditor.py`\n\n#### 9.1 System Context\n*Runs as a background thread. Verifies \"Qualifier\" predictions against official results scraped from AtTheRaces.com to calculate real-time profitability.*\n\n#### 9.2 Database Schema (SQLite)\n\n```pseudocode\nTABLE audit_log:\n  race_id: TEXT PRIMARY KEY      // Format: \"VENUE-YYYYMMDD-RR\"\n  track_code: TEXT\n  race_number: INTEGER\n  predicted_horse: TEXT\n  timestamp: DATETIME\n  status: TEXT                   // 'PENDING', 'CASHED', 'BURNED'\n  official_payout: REAL          // Default: 0.00\n  net_profit: REAL               // Default: 0.00\n\n  CONSTRAINT status_check CHECK (status IN ('PENDING', 'CASHED', 'BURNED'))\n  INDEX idx_status_timestamp (status, timestamp)\n```\n\n#### 9.3 Auditor Engine Logic\n\n```pseudocode\nMODULE: Auditor_Engine\nDEPENDENCIES: httpx, beautifulsoup4, sqlite3, structlog\n\nCLASS Auditor_Engine:\n\n    PROPERTIES:\n        db_path: String\n        http_client: AsyncClient\n        TOTE_UNIT: Float (2.00)\n        TRACK_CODE_MAP: Dictionary  // Maps \"DON\" -> \"Doncaster\", etc.\n\n    FUNCTION __init__(db_path):\n        self.db_path = db_path\n        self.Initialize_Database()\n        self.http_client = NEW AsyncClient(timeout=30)\n\n    # --- Phase 1: The Snapshot ---\n    # Called by OddsEngine when a bet is placed/qualified\n    ASYNC FUNCTION Snapshot_Qualifier(venue_code, race_date, race_number, predicted_horse):\n        race_id = GENERATE_ID(venue_code, race_date, race_number)\n\n        TRY:\n            QUERY = \"\"\"\n                INSERT INTO audit_log\n                (race_id, track_code, race_number, predicted_horse, timestamp, status)\n                VALUES (?, ?, ?, ?, ?, 'PENDING')\n            \"\"\"\n            EXECUTE_SQL(self.db_path, QUERY, (race_id, venue_code, race_number, predicted_horse, NOW()))\n            LOG_INFO(\"Snapshot saved: \" + race_id)\n            RETURN TRUE\n        CATCH IntegrityError:\n            LOG_WARN(\"Race already tracked: \" + race_id)\n            RETURN FALSE\n\n    # --- Phase 2: The Fetcher (Background Loop) ---\n    ASYNC FUNCTION Run_Audit_Loop():\n        self.running = TRUE\n        WHILE self.running:\n            TRY:\n                # 1. Get Pending Races (Last 60 mins)\n                cutoff = NOW() - MINUTES(60)\n                pending_races = GET_PENDING_RACES(cutoff)\n\n                IF pending_races IS EMPTY:\n                    SLEEP(120)\n                    CONTINUE\n\n                # 2. Batch by Track\n                unique_tracks = EXTRACT_UNIQUE_TRACKS(pending_races)\n\n                FOR track IN unique_tracks:\n                    track_races = FILTER(pending_races, track)\n\n                    FOR race IN track_races:\n                        # Fetch Official Result from AtTheRaces\n                        result = AWAIT self._Fetch_Official_Result(race.track_code, race.race_number)\n\n                        IF result IS NOT NULL:\n                            self._Determine_Verdict(race, result)\n\n                        SLEEP(2) # Polite delay\n\n            CATCH Exception as e:\n                LOG_ERROR(\"Audit Loop Error: \" + e)\n\n            SLEEP(120)\n\n    # --- Phase 3: The Scraper (AtTheRaces Strategy) ---\n    ASYNC FUNCTION _Fetch_Official_Result(track_code, race_number):\n        # 1. Find the specific Race URL from the daily results page\n        race_url = AWAIT self._Find_Race_URL(track_code, race_number)\n        IF race_url IS NULL: RETURN NULL\n\n        # 2. Fetch the Race Page\n        html = AWAIT self.http_client.GET(race_url)\n\n        # 3. Parse the Table\n        result = self._Parse_AtTheRaces_Results(html, track_code, race_number)\n        RETURN result\n\n    ASYNC FUNCTION _Find_Race_URL(track_code, race_number):\n        base_url = \"https://www.attheraces.com/results\"\n        html = AWAIT self.http_client.GET(base_url)\n\n        track_name = self.TRACK_CODE_MAP[track_code]\n        track_header = FIND_ELEMENT(html, text=track_name)\n\n        IF track_header EXISTS:\n            # Select the Nth link in the track's panel\n            race_links = SELECT_ALL(track_header.parent, 'a[href*=\"/racecard/\"]')\n            IF LENGTH(race_links) >= race_number:\n                RETURN race_links[race_number - 1].href\n\n        RETURN NULL\n\n    FUNCTION _Parse_AtTheRaces_Results(html, track_code, race_number):\n        table = FIND_TABLE(html, header_contains=\"Horse\")\n        IF table IS NULL: RETURN NULL\n\n        finishers = []\n        win_payout = EXTRACT_WIN_PAYOUT(html) # Parse \"Betting returns\" table\n\n        FOR row IN table.rows:\n            position = PARSE_INT(row.cells[0])\n            horse_name = row.cells[2].text\n\n            place_payout = 0.0\n            IF position == 1:\n                place_payout = win_payout\n\n            finishers.APPEND({\n                \"name\": horse_name,\n                \"position\": position,\n                \"place_payout\": place_payout\n            })\n\n        RETURN NEW OfficialResult(finishers)\n\n    # --- Phase 4: The Verdict ---\n    FUNCTION _Determine_Verdict(prediction, official_result):\n        did_place = FALSE\n        payout = 0.00\n\n        # Check if predicted horse won (AtTheRaces basic logic)\n        FOR finisher IN official_result.finishers:\n            IF finisher.name == prediction.predicted_horse AND finisher.place_payout > 0:\n                did_place = TRUE\n                payout = finisher.place_payout\n                BREAK\n\n        IF did_place:\n            status = 'CASHED'\n            net_profit = payout - self.TOTE_UNIT\n        ELSE:\n            status = 'BURNED'\n            net_profit = -self.TOTE_UNIT\n\n        UPDATE_DB(prediction.race_id, status, payout, net_profit)\n\n    # --- Phase 5: Dashboard Metrics ---\n    FUNCTION Get_Rolling_Metrics(minutes=60):\n        cutoff = NOW() - MINUTES(minutes)\n        QUERY = \"\"\"\n            SELECT\n                COUNT(*) as total,\n                SUM(CASE WHEN status='CASHED' THEN 1 ELSE 0 END) as wins,\n                SUM(net_profit) as profit\n            FROM audit_log\n            WHERE timestamp > ? AND status != 'PENDING'\n        \"\"\"\n        stats = EXECUTE_SQL(self.db_path, QUERY, (cutoff,))\n\n        RETURN {\n            \"strike_rate\": (stats.wins / stats.total) * 100,\n            \"net_profit\": stats.profit,\n            \"volume\": stats.total\n        }\n```",
    "ROADMAP.md": "# Fortuna Faucet Roadmap\n\n## The Auditor: Real-Time Race Verification\n\nThe Auditor is a background process designed to provide real-time verification of race predictions against official results, calculating profitability and tracking performance.\n\nThe core logic is located in the [`python_service/auditor.py`](python_service/auditor.py) script.\n\n### Frontend Integration\n\nThe Auditor is designed to have its data displayed on the frontend. The `AuditorEngine` class in the script exposes two key methods for this purpose:\n\n1.  `get_rolling_metrics()`: This function is intended to power a \"Last Hour\" overlay on the UI, providing key performance indicators such as strike rate, net profit, and betting volume.\n2.  `get_recent_activity()`: This function provides a list of the most recent bet outcomes (e.g., \"CASHED\", \"BURNED\", \"PENDING\") for display in a real-time activity feed or history log.\n\nThe intended architecture is for the backend to create API endpoints (e.g., `/api/auditor/metrics`, `/api/auditor/activity`) that expose these functions. The frontend would then call these endpoints to fetch and display the data.\n\n**Note:** As of the last update, the web scraping component of the Auditor is non-functional. The description above outlines the intended design and capabilities, which are not yet fully operational.\n",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2)---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "config.ini": "[analysis]\nqualification_score = 75.0\nfield_size_optimal_min = 4\nfield_size_optimal_max = 6\nfield_size_acceptable_min = 7\nfield_size_acceptable_max = 8\nfield_size_optimal_points = 30\nfield_size_acceptable_points = 10\nfield_size_penalty_points = -20\nfav_odds_points = 30\nmax_fav_odds = 3.5\nsecond_fav_odds_points = 40\nmin_2nd_fav_odds = 4.0\n\n[system]\napi_rate_limit = 60",
    "docker-compose.yml": "version: '3.8'\n\nservices:\n  fortuna:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./data:/app/data\n      - ./json:/app/json\n      - ./logs:/app/logs\n    environment:\n      - FORTUNA_MODE=docker\n      - FORTUNA_PORT=8000\n    restart: unless-stopped\n",
    "e2e/jules-smoke-test.py": "import asyncio\nfrom playwright.async_api import async_playwright, expect\n\nasync def main():\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        try:\n            await page.goto(\"http://127.0.0.1:8000\")\n            await expect(page.get_by_test_id(\"main-heading\")).to_be_visible()\n        finally:\n            await page.screenshot(path=\"playwright-screenshot.png\")\n            await browser.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "fortuna-backend-hooks/hook-tenacity.py": "\"\"\"\nPyInstaller hook for tenacity.\n\nTenacity uses dynamic imports for async support and retry strategies that\nPyInstaller cannot automatically detect. This hook ensures all tenacity\nsubmodules are collected into the bundle.\n\nThis is especially critical for tenacity 8.2.3+ which includes async retry support.\n\"\"\"\n\nfrom PyInstaller.utils.hooks import collect_submodules\n\n# Collect all tenacity submodules recursively\nhiddenimports = collect_submodules('tenacity')\n\n# Explicitly add critical submodules that might be missed\n# These are the modules tenacity dynamically imports for retry strategies and async support\ncritical_submodules = [\n    'tenacity.retry',\n    'tenacity.stop',\n    'tenacity.wait',\n    'tenacity.retry_if_result',\n    'tenacity.retry_if_exception',\n    'tenacity.before_sleep',\n    'tenacity.after',\n    'tenacity.before',\n    'tenacity.retry_error',\n    'tenacity.compat',\n    'tenacity.future',\n    'tenacity.asyncio',  # Critical for async retry support\n]\n\n# Merge and deduplicate\nhiddenimports = list(set(hiddenimports + critical_submodules))\n",
    "fortuna-backend-hooks/hook-tenacity.py.bak": "\"\"\"\nPyInstaller hook for tenacity.\n\nTenacity uses dynamic imports for async support and retry strategies that\nPyInstaller cannot automatically detect. This hook ensures all tenacity\nsubmodules are collected into the bundle.\n\nThis is especially critical for tenacity 8.2.3+ which includes async retry support.\n\"\"\"\n\nfrom PyInstaller.utils.hooks import collect_submodules\n\n# Collect all tenacity submodules recursively\nhiddenimports = collect_submodules('tenacity')\n\n# Explicitly add critical submodules that might be missed\n# These are the modules tenacity dynamically imports for retry strategies and async support\ncritical_submodules = [\n    'tenacity.retry',\n    'tenacity.stop',\n    'tenacity.wait',\n    'tenacity.retry_if_result',\n    'tenacity.retry_if_exception',\n    'tenacity.before_sleep',\n    'tenacity.after',\n    'tenacity.before',\n    'tenacity.retry_error',\n    'tenacity.compat',\n    'tenacity.future',\n    'tenacity.asyncio',  # Critical for async retry support\n]\n\n# Merge and deduplicate\nhiddenimports = list(set(hiddenimports + critical_submodules))\n",
    "fortuna-monolith.spec": "# fortuna-monolith.spec\n# FIXED: Proper path resolution for Windows\n\nfrom PyInstaller.utils.hooks import collect_submodules\nfrom pathlib import Path\nimport sys\nimport os\n\nblock_cipher = None\n\n# ===== GET PROJECT ROOT =====\n# SPECPATH is provided by PyInstaller - it's the directory containing THIS spec file\nspec_path = Path(SPECPATH) if 'SPECPATH' in dir() else Path(os.path.dirname(os.path.abspath(__file__)))\nproject_root = spec_path.parent if spec_path.name == 'fortuna-monolith.spec' else spec_path\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FORTUNA MONOLITH SPEC - PATH RESOLUTION\")\nprint(\"=\"*70)\nprint(f\"Spec file location: {spec_path}\")\nprint(f\"Project root:       {project_root}\")\nprint(f\"Current working:    {Path.cwd()}\")\n\n# ===== FRONTEND VALIDATION =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"FRONTEND VALIDATION\")\nprint(\"=\"*70)\n\nfrontend_out = project_root / 'web_service' / 'frontend' / 'public'\nprint(f\"Looking for frontend at: {frontend_out}\")\nprint(f\"Exists: {frontend_out.exists()}\")\n\nif frontend_out.exists():\n    index_html = frontend_out / 'index.html'\n    print(f\"index.html path:    {index_html}\")\n    print(f\"index.html exists:  {index_html.exists()}\")\n\n    if index_html.exists():\n        file_count = len(list(frontend_out.rglob('*')))\n        size = index_html.stat().st_size\n        print(\"[OK] Frontend validated!\")\n        print(f\"   Files: {file_count}\")\n        print(f\"   index.html size: {size} bytes\")\n    else:\n        print(f\"[ERROR] FATAL: index.html not found at {index_html}\")\n        print(f\"\\nContents of {frontend_out}:\")\n        for item in frontend_out.iterdir():\n            print(f\"  - {item.name}\")\n        sys.exit(1)\nelse:\n    print(f\"[ERROR] FATAL: Frontend 'public' directory not found!\")\n    print(f\"\\nSearching for 'public' directory from project root:\")\n    for root, dirs, files in os.walk(project_root):\n        if 'public' in dirs:\n            out_path = Path(root) / 'public'\n            print(f\"  Found at: {out_path}\")\n            if (out_path / 'index.html').exists():\n                print(f\"    [OK] Has index.html\")\n                frontend_out = out_path\n                break\n    else:\n        print(f\"  Not found anywhere!\")\n        sys.exit(1)\n\n# ===== BACKEND VALIDATION =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"BACKEND VALIDATION\")\nprint(\"=\"*70)\n\nbackend_root = project_root / 'web_service' / 'backend'\nmain_py = backend_root / 'main.py'\n\nprint(f\"Looking for backend at: {backend_root}\")\nprint(f\"main.py path:           {main_py}\")\nprint(f\"main.py exists:         {main_py.exists()}\")\n\nif not main_py.exists():\n    print(f\"[ERROR] FATAL: Backend main.py not found!\")\n    print(f\"\\nContents of {backend_root}:\")\n    if backend_root.exists():\n        for item in backend_root.iterdir():\n            print(f\"  - {item.name}\")\n    else:\n        print(f\"  Directory doesn't exist!\")\n    sys.exit(1)\n\nprint(f\"[OK] Backend validated!\")\nprint(f\"   main.py size: {main_py.stat().st_size} bytes\")\n\n# ===== DATA FILES =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"COLLECTING DATA FILES\")\nprint(\"=\"*70)\n\ndatas = []\n\n# Frontend\ndatas.append((str(frontend_out), 'public'))\nprint(f\"[OK] Frontend:  {frontend_out} -> public/\")\n\n# Backend directories\nfor dirname in ['data', 'json', 'logs']:\n    src = backend_root / dirname\n    if src.exists():\n        datas.append((str(src), dirname))\n        print(f\"[OK] {dirname:8}: {src}\")\n    else:\n        print(f\"[WARN] {dirname:8}: Not found (will create)\")\n\nprint(f\"\\nTotal data entries: {len(datas)}\")\n\n# ===== HIDDEN IMPORTS =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"COLLECTING HIDDEN IMPORTS\")\nprint(\"=\"*70)\n\ncore_imports = [\n    'uvicorn', 'uvicorn.logging', 'uvicorn.loops', 'uvicorn.loops.auto',\n    'uvicorn.protocols', 'uvicorn.protocols.http', 'uvicorn.protocols.http.auto',\n    'uvicorn.protocols.http.h11_impl', 'uvicorn.lifespan', 'uvicorn.lifespan.on',\n    'fastapi', 'fastapi.routing', 'starlette', 'starlette.applications',\n    'starlette.routing', 'starlette.responses', 'starlette.staticfiles',\n    'pydantic', 'pydantic_core', 'pydantic_settings',\n    'anyio', 'structlog', 'tenacity', 'sqlalchemy', 'greenlet', 'win32timezone'\n]\n\nbackend_submodules = collect_submodules('web_service.backend')\nhiddenimports = list(set(core_imports + backend_submodules))\n\nprint(f\"Core imports:           {len(core_imports)}\")\nprint(f\"Backend submodules:     {len(backend_submodules)}\")\nprint(f\"Total hidden imports:   {len(hiddenimports)}\")\n\n# ===== ANALYSIS =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"CREATING PYINSTALLER ANALYSIS\")\nprint(\"=\"*70)\n\na = Analysis(\n    [str(main_py)],\n    pathex=[str(project_root), str(backend_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    cipher=block_cipher,\n    noarchive=False,\n)\n\nprint(f\"[OK] Analysis created\")\nprint(f\"   Scripts:       {len(a.scripts)}\")\nprint(f\"   Pure modules:  {len(a.pure)}\")\nprint(f\"   Binaries:      {len(a.binaries)}\")\nprint(f\"   Data files:    {len(a.datas)}\")\n\n# ===== BUILD =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"BUILDING EXECUTABLE\")\nprint(\"=\"*70)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n    name='fortuna-monolith',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    runtime_tmpdir=None,\n    console=True,\n    disable_windowed_traceback=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon=None,\n)\n\ncoll = COLLECT(\n    exe, a.binaries, a.zipfiles, a.datas,\n    strip=False,\n    upx=True,\n    name='fortuna-monolith'\n)\n\nprint(f\"[OK] Spec file complete!\")\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "pre-build-check.ps1": "# pre-build-check.ps1\nWrite-Host \"=== FORTUNA PRE-BUILD VERIFICATION ===\" -ForegroundColor Cyan\n\n# 1. Check all required files exist\nWrite-Host \"`n[1] Checking required files...\"\n$required = @(\n    \"web_service/backend/main.py\",\n    \"web_service/backend/api.py\",\n    \"web_service/backend/config.py\",\n    \"web_service/backend/port_check.py\",\n    \"web_service/backend/requirements.txt\",\n    \"web_service/frontend/package.json\",\n    \"web_service/frontend/next.config.js\",\n    \"fortuna-monolith.spec\"\n)\n\n$missing = @()\nforeach ($file in $required) {\n    if (Test-Path $file) {\n        Write-Host \"  \u2705 $file\"\n    } else {\n        Write-Host \"  \u274c $file\"\n        $missing += $file\n    }\n}\n\nif ($missing.Count -gt 0) {\n    Write-Host \"`n\u274c FATAL: Missing files:\" -ForegroundColor Red\n    $missing | ForEach-Object { Write-Host \"  - $_\" }\n    exit 1\n}\n\n# 2. Test Python imports\nWrite-Host \"`n[2] Testing Python imports...\"\n$testScript = @\"\nimport sys\nsys.path.insert(0, '.')\n\ntry:\n    from web_service.backend.api import app\n    print('\u2705 api.app imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import api.app: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.config import get_settings\n    settings = get_settings()\n    print(f'\u2705 config.get_settings imported (host={settings.UVICORN_HOST}, port={settings.FORTUNA_PORT})')\nexcept ImportError as e:\n    print(f'\u274c Failed to import config: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    print('\u2705 port_check.check_port_and_exit_if_in_use imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import port_check: {e}')\n    sys.exit(1)\n\nprint('\u2705 All imports successful')\n\"@\n\n$testScript | Out-File -FilePath \"test_imports.py\" -Encoding UTF8\npython test_imports.py\nif ($LASTEXITCODE -ne 0) {\n    Write-Host \"\u274c Import test FAILED\" -ForegroundColor Red\n    exit 1\n}\nRemove-Item \"test_imports.py\"\n\n# 3. Check frontend\nWrite-Host \"`n[3] Checking frontend...\"\nif (Test-Path \"web_service/frontend/next.config.js\") {\n    $config = Get-Content \"web_service/frontend/next.config.js\"\n    if ($config -match \"output:\\s*['`\"]export['`\"]\") {\n        Write-Host \"  \u2705 next.config.js has output: 'export'\"\n    } else {\n        Write-Host \"  \u274c next.config.js missing output: 'export'\" -ForegroundColor Red\n        exit 1\n    }\n} else {\n    Write-Host \"  \u26a0\ufe0f  next.config.js will be created during build\"\n}\n\n# 4. Check spec file\nWrite-Host \"`n[4] Checking fortuna-monolith.spec...\"\nif (Test-Path \"fortuna-monolith.spec\") {\n    $spec = Get-Content \"fortuna-monolith.spec\"\n    if ($spec -match \"SPECPATH\") {\n        Write-Host \"  \u2705 spec uses SPECPATH\"\n    } else {\n        Write-Host \"  \u26a0\ufe0f  spec doesn't use SPECPATH (may have path issues)\"\n    }\n} else {\n    Write-Host \"  \u274c fortuna-monolith.spec not found\" -ForegroundColor Red\n    exit 1\n}\n\nWrite-Host \"`n\u2705 ALL CHECKS PASSED - Safe to build!\" -ForegroundColor Green\n",
    "run_desktop_app.py": "import sys\nimport os\nimport threading\nimport logging\nimport traceback\nfrom pathlib import Path\n\n# --- 1. SETUP LOGGING (CRITICAL FOR DEBUGGING) ---\ndef setup_logging():\n    # Log to %TEMP% so we can retrieve it if the app crashes\n    log_dir = Path(os.environ.get(\"TEMP\", \".\"))\n    log_file = log_dir / \"fortuna-desktop.log\"\n\n    logging.basicConfig(\n        filename=log_file,\n        level=logging.DEBUG,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        filemode='w'\n    )\n\n    # Also print to console for dev mode\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    logging.getLogger('').addHandler(console)\n    return logging.getLogger(__name__)\n\nlogger = setup_logging()\n\n# --- 2. ROBUST IMPORTS ---\ntry:\n    import uvicorn\n    import webview\n    from fastapi import FastAPI\n    from fastapi.staticfiles import StaticFiles\n    from fastapi.middleware.cors import CORSMiddleware\nexcept Exception as e:\n    logger.critical(f\"Failed to import core dependencies: {e}\\n{traceback.format_exc()}\")\n    sys.exit(1)\n\n# --- 3. DEFINE APP ---\ndef create_app():\n    app = FastAPI()\n    app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n\n    # Health Check (Used by CI Smoke Test)\n    @app.get(\"/api/health\")\n    def health():\n        return {\"status\": \"ok\", \"mode\": \"desktop\"}\n\n    # Try to load real backend\n    try:\n        # Lazy import to prevent crash if backend deps are missing\n        from web_service.backend.api import router as api_router\n        app.include_router(api_router, prefix=\"/api\")\n        logger.info(\"Loaded Backend API\")\n    except Exception as e:\n        logger.warning(f\"Could not load Backend API: {e}\")\n        @app.get(\"/api/error\")\n        def api_error(): return {\"error\": str(e)}\n\n    return app\n\n# --- 4. ASSET PATHS ---\ndef get_asset_path():\n    # PyInstaller unpacks data to _MEIPASS\n    if hasattr(sys, '_MEIPASS'):\n        return Path(sys._MEIPASS) / 'public'\n    # Dev mode path\n    return Path(__file__).parent / 'web_service' / 'frontend' / 'public'\n\n# --- 5. SERVER THREAD ---\ndef start_server(app, port):\n    try:\n        # Mount Frontend\n        static_dir = get_asset_path()\n        if static_dir.exists():\n            app.mount(\"/\", StaticFiles(directory=str(static_dir), html=True), name=\"static\")\n            logger.info(f\"Serving frontend from {static_dir}\")\n        else:\n            logger.error(f\"Frontend not found at {static_dir}\")\n\n        logger.info(f\"Starting Uvicorn on port {port}\")\n        uvicorn.run(app, host=\"127.0.0.1\", port=port, log_level=\"info\")\n    except Exception as e:\n        logger.critical(f\"Server crashed: {e}\\n{traceback.format_exc()}\")\n\nif __name__ == '__main__':\n    try:\n        if sys.platform == 'win32':\n            import multiprocessing\n            multiprocessing.freeze_support()\n\n        port = 8000\n        app = create_app()\n\n        # Start Backend Thread\n        t = threading.Thread(target=start_server, args=(app, port), daemon=True)\n        t.start()\n\n        # Start GUI\n        logger.info(\"Launching WebView...\")\n        webview.create_window(\"Fortuna Faucet\", f\"http://127.0.0.1:{port}\", width=1280, height=800)\n        webview.start()\n        logger.info(\"App closed normally.\")\n\n    except Exception as e:\n        logger.critical(f\"Fatal Crash: {e}\\n{traceback.format_exc()}\")\n        sys.exit(1)",
    "scripts/debug_html_parser.py": "\"\"\"\nAnalyze debug HTML files to diagnose scraping failures.\n\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\nfrom collections import Counter\n\n\ndef analyze_html(html_path: str, output_path: str):\n    \"\"\"Analyze HTML file and output diagnosis.\"\"\"\n    try:\n        from bs4 import BeautifulSoup\n    except ImportError:\n        print(\"Installing beautifulsoup4...\")\n        import subprocess\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'beautifulsoup4', 'lxml'])\n        from bs4 import BeautifulSoup\n\n    with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:\n        content = f.read()\n\n    soup = BeautifulSoup(content, 'lxml')\n\n    analysis = {\n        \"file\": html_path,\n        \"size_bytes\": len(content),\n        \"title\": soup.title.string if soup.title else None,\n        \"issues\": [],\n        \"recommendations\": [],\n        \"selector_matches\": {},\n        \"structure_info\": {},\n    }\n\n    # Check for blocking/challenges\n    content_lower = content.lower()\n\n    if 'captcha' in content_lower:\n        analysis[\"issues\"].append(\"CAPTCHA challenge detected\")\n        analysis[\"recommendations\"].append(\"Enable solve_cloudflare=True or increase delays\")\n\n    if 'cloudflare' in content_lower or 'cf-browser-verification' in content_lower:\n        analysis[\"issues\"].append(\"CloudFlare protection detected\")\n        analysis[\"recommendations\"].append(\"Use StealthyFetcher with google_search=True\")\n\n    if 'access denied' in content_lower or 'forbidden' in content_lower:\n        analysis[\"issues\"].append(\"Access denied response\")\n        analysis[\"recommendations\"].append(\"Check IP reputation, try different user agent\")\n\n    if len(content) < 1000:\n        analysis[\"issues\"].append(\"Very small response - likely blocked\")\n        analysis[\"recommendations\"].append(\"Check if IP is rate limited\")\n\n    if not soup.body or len(soup.body.get_text(strip=True)) < 100:\n        analysis[\"issues\"].append(\"Empty or minimal body content\")\n\n    # Check for race-related content\n    race_indicators = ['race', 'horse', 'runner', 'odds', 'post time', 'track']\n    found_indicators = [ind for ind in race_indicators if ind in content_lower]\n    analysis[\"structure_info\"][\"race_indicators_found\"] = found_indicators\n\n    if not found_indicators:\n        analysis[\"issues\"].append(\"No race-related content found\")\n        analysis[\"recommendations\"].append(\"Page may not be the race listings page\")\n\n    # Test common selectors\n    test_selectors = [\n        ('div[class*=\"race\"]', 'race containers'),\n        ('div[class*=\"Race\"]', 'Race containers (capitalized)'),\n        ('div[class*=\"card\"]', 'card elements'),\n        ('[class*=\"runner\"]', 'runner elements'),\n        ('[class*=\"horse\"]', 'horse elements'),\n        ('[class*=\"odds\"]', 'odds elements'),\n        ('table', 'tables'),\n        ('tr', 'table rows'),\n        ('time', 'time elements'),\n    ]\n\n    for selector, description in test_selectors:\n        try:\n            matches = soup.select(selector)\n            if matches:\n                analysis[\"selector_matches\"][description] = {\n                    \"count\": len(matches),\n                    \"selector\": selector,\n                    \"sample_classes\": list(set([\n                        ' '.join(m.get('class', []))[:50]\n                        for m in matches[:3]\n                    ]))\n                }\n        except Exception as e:\n            pass\n\n    # Get unique class names\n    all_classes = []\n    for tag in soup.find_all(class_=True):\n        all_classes.extend(tag.get('class', []))\n\n    class_counts = Counter(all_classes)\n    analysis[\"structure_info\"][\"top_classes\"] = dict(class_counts.most_common(40))\n\n    # Check for JavaScript-rendered content indicators\n    if '<noscript>' in content:\n        analysis[\"issues\"].append(\"Page uses JavaScript rendering\")\n        analysis[\"recommendations\"].append(\"Ensure network_idle=True and sufficient timeout\")\n\n    # Save analysis\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    with open(output_path, 'w') as f:\n        json.dump(analysis, f, indent=2)\n\n    # Print summary\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Analysis: {html_path}\")\n    print(f\"{'=' * 60}\")\n    print(f\"Size: {analysis['size_bytes']:,} bytes\")\n    print(f\"Title: {analysis['title']}\")\n\n    if analysis[\"issues\"]:\n        print(f\"\\n\u26a0\ufe0f Issues Found:\")\n        for issue in analysis[\"issues\"]:\n            print(f\"  - {issue}\")\n\n    if analysis[\"recommendations\"]:\n        print(f\"\\n\ud83d\udca1 Recommendations:\")\n        for rec in analysis[\"recommendations\"]:\n            print(f\"  - {rec}\")\n\n    if analysis[\"selector_matches\"]:\n        print(f\"\\n\u2713 Selector Matches:\")\n        for desc, info in analysis[\"selector_matches\"].items():\n            print(f\"  - {desc}: {info['count']} matches\")\n\n    print(f\"\\nFull analysis saved to: {output_path}\")\n\n\nif __name__ == '__main__':\n    if len(sys.argv) < 3:\n        print(\"Usage: python debug_html_parser.py <html_file> <output_json>\")\n        sys.exit(1)\n\n    analyze_html(sys.argv[1], sys.argv[2])\n",
    "scripts/generate_report.ps1": "# scripts/generate_report.ps1\n\n[CmdletBinding()]\nparam (\n    [string]$JsonInputPath = \"qualified_races.json\",\n    [string]$HtmlOutputPath = \"race-report.html\"\n)\n\nWrite-Host \"Generating HTML report from $JsonInputPath...\" -ForegroundColor Cyan\n\nif (-not (Test-Path $JsonInputPath)) {\n    Write-Host \"\u274c ERROR: Input JSON file not found at $JsonInputPath\" -ForegroundColor Red\n    exit 1\n}\n\n$races = Get-Content $JsonInputPath | ConvertFrom-Json\n$timestamp = Get-Date -Format \"yyyy-MM-dd HH:mm:ss UTC\"\n\n$html = @\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>\ud83d\udc34 Fortuna - Filtered Race Report</title>\n  <style>\n    * { margin: 0; padding: 0; box-sizing: border-box; }\n    html { scroll-behavior: smooth; }\n\n    body {\n      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n      background: linear-gradient(135deg, #0f1419 0%, #1a1f2e 50%, #16213e 100%);\n      color: #e2e8f0;\n      padding: 2rem;\n      line-height: 1.6;\n      min-height: 100vh;\n    }\n\n    .container {\n      max-width: 1400px;\n      margin: 0 auto;\n    }\n\n    header {\n      text-align: center;\n      margin-bottom: 3rem;\n      background: linear-gradient(135deg, rgba(15, 52, 96, 0.5), rgba(0, 255, 136, 0.1));\n      border: 2px solid #00ff88;\n      border-radius: 12px;\n      padding: 3rem 2rem;\n      box-shadow: 0 8px 32px rgba(0, 255, 136, 0.2);\n    }\n\n    h1 {\n      color: #00ff88;\n      font-size: 2.8rem;\n      margin-bottom: 0.5rem;\n      text-shadow: 0 0 10px rgba(0, 255, 136, 0.5);\n    }\n\n    .subtitle {\n      color: #a0aec0;\n      font-size: 1rem;\n      margin-top: 0.5rem;\n    }\n\n    .summary-box {\n      background: rgba(0, 255, 136, 0.1);\n      border-left: 4px solid #00ff88;\n      border-radius: 8px;\n      padding: 1.5rem 2rem;\n      margin: 2rem 0;\n      font-size: 1.1rem;\n      text-align: center;\n      color: #00ff88;\n      font-weight: bold;\n    }\n\n    .race-card {\n      background: linear-gradient(135deg, #0f3460 0%, #1a4d7a 100%);\n      border: 1px solid rgba(0, 255, 136, 0.2);\n      border-left: 4px solid #00ff88;\n      border-radius: 12px;\n      padding: 2rem;\n      margin-bottom: 2rem;\n      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);\n      transition: all 0.3s ease;\n    }\n\n    .race-card:hover {\n      transform: translateY(-4px);\n      box-shadow: 0 8px 30px rgba(0, 255, 136, 0.25);\n      border-color: rgba(0, 255, 136, 0.5);\n    }\n\n    .race-header {\n      display: flex;\n      justify-content: space-between;\n      align-items: flex-start;\n      margin-bottom: 1.5rem;\n      border-bottom: 2px solid #00ff88;\n      padding-bottom: 1rem;\n      flex-wrap: wrap;\n      gap: 1rem;\n    }\n\n    .race-title {\n      font-size: 1.4rem;\n      font-weight: bold;\n      color: #00ff88;\n    }\n\n    .race-meta {\n      color: #a0aec0;\n      font-size: 0.9rem;\n    }\n\n    .runners-table {\n      width: 100%;\n      border-collapse: collapse;\n      margin-top: 1rem;\n    }\n\n    .runners-table thead {\n      background: rgba(0, 255, 136, 0.15);\n      border-bottom: 2px solid #00ff88;\n    }\n\n    .runners-table th {\n      padding: 1rem;\n      text-align: left;\n      font-weight: bold;\n      color: #00ff88;\n      text-transform: uppercase;\n      font-size: 0.85rem;\n      letter-spacing: 1px;\n    }\n\n    .runners-table td {\n      padding: 0.75rem 1rem;\n      border-bottom: 1px solid #1a3a52;\n    }\n\n    .runners-table tbody tr:hover {\n      background: rgba(0, 255, 136, 0.05);\n    }\n\n    .runner-name {\n      font-weight: 600;\n      color: #e2e8f0;\n    }\n\n    .odds {\n      font-family: 'Courier New', monospace;\n      color: #00ff88;\n      font-weight: bold;\n      font-size: 1.1rem;\n    }\n\n    .source {\n      color: #718096;\n      font-size: 0.9rem;\n    }\n\n    .no-races {\n      text-align: center;\n      padding: 3rem;\n      background: #0f3460;\n      border: 2px dashed #ff4444;\n      border-radius: 12px;\n      color: #a0aec0;\n      font-size: 1.1rem;\n    }\n\n    footer {\n      text-align: center;\n      margin-top: 4rem;\n      padding-top: 2rem;\n      border-top: 1px solid #404060;\n      color: #718096;\n      font-size: 0.85rem;\n    }\n\n    footer a {\n      color: #00ff88;\n      text-decoration: none;\n      transition: color 0.2s;\n    }\n\n    footer a:hover {\n      color: #00ff88;\n      text-decoration: underline;\n    }\n\n    @media (max-width: 768px) {\n      h1 { font-size: 1.8rem; }\n      header { padding: 2rem 1rem; }\n      .race-card { padding: 1rem; }\n      .runners-table th, .runners-table td { padding: 0.5rem; }\n    }\n  </style>\n</head>\n<body>\n  <div class=\"container\">\n    <header>\n      <h1>\ud83d\udc34 Fortuna Faucet Race Report</h1>\n      <p class=\"subtitle\">Filtered Trifecta Opportunities</p>\n      <p class=\"subtitle\">Generated: $timestamp</p>\n    </header>\n\n    <div class=\"summary-box\">\n      $($races.races.Count) qualified race(s) found\n    </div>\n\"@\n\nif ($races.races -and $races.races.Count -gt 0) {\n    foreach ($race in $races.races) {\n        $venue = $race.venue ?? \"Unknown\"\n        $raceNum = $race.race_number ?? \"?\"\n        $startTime = $race.startTime ?? \"N/A\"\n\n        $html += @\"\n<div class=\"race-card\">\n<div class=\"race-header\">\n  <div>\n    <div class=\"race-title\">$venue - Race $raceNum</div>\n    <div class=\"race-meta\">Post Time: $startTime</div>\n  </div>\n</div>\n<table class=\"runners-table\">\n  <thead>\n    <tr>\n      <th>Horse Name</th>\n      <th>Win Odds</th>\n      <th>Best Source</th>\n    </tr>\n  </thead>\n  <tbody>\n\"@\n\n        foreach ($runner in $race.runners) {\n            $name = $runner.name ?? \"Unknown\"\n            $odds = \"N/A\"\n            $source = \"N/A\"\n\n            if ($runner.odds) {\n                $bestVal = 0\n                foreach ($src in $runner.odds.PSObject.Properties) {\n                    if ($src.Value.win -gt $bestVal) {\n                        $bestVal = $src.Value.win\n                        $odds = [math]::Round($bestVal, 2)\n                        $source = $src.Name\n                    }\n                }\n            }\n\n            $html += @\"\n    <tr>\n      <td class=\"runner-name\">$name</td>\n      <td class=\"odds\">$odds</td>\n      <td class=\"source\">$source</td>\n    </tr>\n\"@\n        }\n\n        $html += @\"\n  </tbody>\n</table>\n</div>\n\"@\n    }\n} else {\n    $html += @\"\n<div class=\"no-races\">\n\u274c No qualified races found at this time.\n</div>\n\"@\n}\n\n$github_repository = $env:GITHUB_REPOSITORY\n$html += @\"\n    <footer>\n      <p>This report was automatically generated by Fortuna Faucet via GitHub Actions.</p>\n      <p>Data sources: Multiple racing exchanges and bookmakers.</p>\n      <p>\ud83d\udd17 <a href=\"https://github.com/$github_repository\">View Repository</a></p>\n    </footer>\n  </div>\n</body>\n</html>\n\"@\n\n$html | Out-File -FilePath $HtmlOutputPath -Encoding utf8\nWrite-Host \"\u2705 HTML report generated successfully at $HtmlOutputPath\" -ForegroundColor Green\n",
    "scripts/get_api_key.py": "# scripts/get_api_key.py\nimport os\nimport sys\n\n# This is a workaround to ensure the script can find the python_service module,\n# especially when run from the packaged Electron app.\n# It assumes this script is in `resources/app/scripts` and the service is in `resources/app/python_service`.\ntry:\n    # Get the directory of the current script.\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    # Go up one level to the `app` directory and add `python_service` to the path.\n    project_root = os.path.dirname(script_dir)\n    sys.path.append(project_root)\n    from python_service.credentials_manager import SecureCredentialsManager\nexcept ImportError as e:\n    # If the import fails, write the error to stderr and exit.\n    # This helps in debugging path issues in the production environment.\n    print(\n        f\"Error: Failed to import SecureCredentialsManager. Details: {e}\",\n        file=sys.stderr,\n    )\n    sys.exit(1)\n\n\ndef retrieve_and_print_key():\n    \"\"\"\n    Retrieves the API key using the SecureCredentialsManager and prints it to stdout.\n    If the key is not found, it prints an empty string.\n    If an error occurs, it prints the error to stderr.\n    \"\"\"\n    try:\n        api_key = SecureCredentialsManager.get_api_key()\n        if api_key:\n            print(api_key, end=\"\")  # Print the key directly to stdout\n        else:\n            print(\"\", end=\"\")  # Print empty string if no key is found\n    except Exception as e:\n        print(f\"An error occurred while retrieving the API key: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    retrieve_and_print_key()\n",
    "scripts/repair_fortuna.bat": "@echo off\nREM Repair corrupted or missing files\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\necho Repairing Fortuna Faucet installation...\n\nREM /f flag performs repair. Assumes MSI is in the 'dist' folder.\nmsiexec.exe /f \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_repair.log\"\n\nif %errorlevel% equ 0 (\n    echo Repair completed successfully.\n) else (\n    echo Repair failed. Check log: %TEMP%\\fortuna_repair.log\n)\n\nexit /b %errorlevel%",
    "scripts/validate_output.py": "#!/usr/bin/env python3\n\"\"\"Validate pipeline output files.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\n\n\ndef validate_races_file(filepath: Path) -> tuple[bool, list[str]]:\n    \"\"\"Validate a races JSON file.\"\"\"\n    errors = []\n\n    try:\n        with open(filepath) as f:\n            data = json.load(f)\n    except json.JSONDecodeError as e:\n        return False, [f\"Invalid JSON: {e}\"]\n    except FileNotFoundError:\n        return False, [f\"File not found: {filepath}\"]\n\n    if \"races\" not in data:\n        errors.append(\"Missing 'races' field\")\n        return False, errors\n\n    races = data[\"races\"]\n    if not isinstance(races, list):\n        errors.append(\"'races' should be an array\")\n        return False, errors\n\n    required_fields = [\"id\", \"venue\", \"race_number\", \"runners\"]\n\n    for i, race in enumerate(races[:10]):  # Check first 10\n        for field in required_fields:\n            if field not in race:\n                errors.append(f\"Race {i}: missing '{field}'\")\n\n    return len(errors) == 0, errors\n\n\ndef main():\n    print(\"=\" * 60)\n    print(\"OUTPUT VALIDATION\")\n    print(\"=\" * 60)\n\n    files = [\n        (\"qualified_races.json\", True),\n        (\"raw_race_data.json\", False),\n    ]\n\n    all_valid = True\n\n    for filename, required in files:\n        filepath = Path(filename)\n        print(f\"\\n\u2192 {filename}\")\n\n        if not filepath.exists():\n            if required:\n                print(f\"  \u274c Required file not found\")\n                all_valid = False\n            else:\n                print(f\"  \u26a0\ufe0f Optional file not found\")\n            continue\n\n        valid, errors = validate_races_file(filepath)\n\n        if valid:\n            with open(filepath) as f:\n                data = json.load(f)\n            race_count = len(data.get(\"races\", []))\n            print(f\"  \u2705 Valid ({race_count} races)\")\n\n            # Anomaly Gating\n            if filename == \"qualified_races.json\":\n                anomaly_path = Path(\"anomaly_history.json\")\n                if anomaly_path.exists():\n                    try:\n                        history = json.loads(anomaly_path.read_text())\n                        # Very simple check: if current count is < 20% of median, flag it\n                        counts = [h.get('race_count', 0) for h in history if 'race_count' in h]\n                        if counts:\n                            import statistics\n                            median = statistics.median(counts)\n                            if race_count < (median * 0.2) and median > 5:\n                                print(f\"  \u26a0\ufe0f ANOMALY DETECTED: Race count ({race_count}) is significantly lower than median ({median:.1f})\")\n                                # We don't fail the job, but we could if we wanted strict gating\n                    except:\n                        pass\n        else:\n            print(f\"  \u274c Invalid\")\n            for err in errors[:5]:\n                print(f\"     - {err}\")\n            all_valid = False\n\n    print(\"\\n\" + \"=\" * 60)\n    if all_valid:\n        print(\"\u2705 All validations passed\")\n        return 0\n    else:\n        print(\"\u274c Validation failed\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
    "tests/adapters/test_greyhound_adapter.py": "from datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.greyhound_adapter import GreyhoundAdapter\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef test_settings():\n    \"\"\"Provides a valid Settings object for testing.\"\"\"\n    return get_test_settings()\n\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n\n    mock_api_response = {\n        \"cards\": [\n            {\n                \"track_name\": \"Test Track\",\n                \"races\": [\n                    {\n                        \"race_id\": \"test_race_123\",\n                        \"race_number\": 1,\n                        \"start_time\": int(datetime.now().timestamp()),\n                        \"runners\": [\n                            {\n                                \"dog_name\": \"Rapid Rover\",\n                                \"trap_number\": 1,\n                                \"odds\": {\"win\": \"2.5\"},\n                            },\n                            {\n                                \"dog_name\": \"Swift Sprint\",\n                                \"trap_number\": 2,\n                                \"scratched\": True,\n                            },\n                            {\n                                \"dog_name\": \"Lazy Larry\",\n                                \"trap_number\": 3,\n                                \"odds\": {\"win\": \"10.0\"},\n                            },\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = await adapter.get_races(today)\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == \"greyhound_test_race_123\"\n    assert race.venue == \"Test Track\"\n    assert len(race.runners) == 2  # One was scratched\n\n    runner1 = race.runners[0]\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.number == 1\n    assert runner1.odds[\"Greyhound Racing\"].win == 2.5\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter handles an empty API response gracefully.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value={\"cards\": []})\n\n    # ACT\n    races = await adapter.get_races(today)\n\n    # ASSERT\n    assert races == []\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(test_settings):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = await adapter.get_races(today)\n\n    # ASSERT\n    assert races == []\n",
    "tests/analyzers/test_trifecta_analyzer.py": "# Dedicated test suite for the TrifectaAnalyzer, resurrected and expanded.\nfrom datetime import datetime\n\nimport pytest\n\nfrom python_service.analyzer import TrifectaAnalyzer\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\n@pytest.fixture\ndef analyzer():\n    return TrifectaAnalyzer()\n\n\n@pytest.fixture\ndef runners():\n    return []\n\n\n@pytest.fixture\ndef create_race(runners):\n    return Race(\n        id=\"test-race\",\n        venue=\"TEST\",\n        race_number=1,\n        start_time=datetime.now(),\n        runners=runners,\n        source=\"test\",\n    )\n\n\ndef test_analyzer_name(analyzer):\n    assert analyzer.name == \"trifecta_analyzer\"\n\n\n# Test cases resurrected from legacy scorer and logic tests\ndef test_qualifies_with_exactly_three_runners(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds3 = {\"TestOdds\": OddsData(win=Decimal(\"5.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n        Runner(number=3, name=\"C\", odds=odds3, scratched=False),\n    ]\n    assert analyzer.is_race_qualified(create_race) is True\n\n\ndef test_qualifies_with_more_than_three_runners(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds3 = {\"TestOdds\": OddsData(win=Decimal(\"5.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds4 = {\"TestOdds\": OddsData(win=Decimal(\"6.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n        Runner(number=3, name=\"C\", odds=odds3, scratched=False),\n        Runner(number=4, name=\"D\", odds=odds4, scratched=False),\n    ]\n    assert analyzer.is_race_qualified(create_race) is True\n\n\n# New test cases for edge-case hardening\ndef test_rejects_with_fewer_than_three_runners(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n    ]\n    assert analyzer.is_race_qualified(create_race) is False\n\n\ndef test_rejects_if_scratched_runners_reduce_field_below_three(analyzer, create_race):\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds3 = {\"TestOdds\": OddsData(win=Decimal(\"5.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    create_race.runners = [\n        Runner(number=1, name=\"A\", odds=odds1, scratched=False),\n        Runner(number=2, name=\"B\", odds=odds2, scratched=False),\n        Runner(number=3, name=\"C\", odds=odds3, scratched=True),  # Scratched\n    ]\n    assert analyzer.is_race_qualified(create_race) is False\n\n\ndef test_handles_empty_runner_list(analyzer, create_race):\n    race = create_race\n    race.runners = []\n    assert analyzer.is_race_qualified(race) is False\n\n\ndef test_handles_none_race_object(analyzer):\n    assert analyzer.is_race_qualified(None) is False\n",
    "tests/fixtures/twinspires_sample.html": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <title>Race Results - Twinspires</title>\n</head>\n<body>\n    <div id=\"race-card\">\n        <h1>Race 5 - Churchill Downs - 2025-10-26</h1>\n        <div class=\"race-details\">\n            <span class=\"post-time\">Post Time: 04:30 PM</span>\n            <span class=\"distance\">1 Mile</span>\n            <span class=\"surface\">Dirt</span>\n        </div>\n        <ul class=\"runners-list\">\n            <li class=\"runner\">\n                <span class=\"runner-number\">1</span>\n                <span class=\"runner-name\">Braveheart</span>\n                <span class=\"runner-odds\">5/2</span>\n            </li>\n            <li class=\"runner\">\n                <span class=\"runner-number\">2</span>\n                <span class=\"runner-name\">Speedster</span>\n                <span class=\"runner-odds\">10/1</span>\n            </li>\n            <li class=\"runner scratched\">\n                <span class=\"runner-number\">3</span>\n                <span class=\"runner-name\">Steady Eddy</span>\n                <span class=\"runner-odds\">SCR</span>\n            </li>\n             <li class=\"runner\">\n                <span class=\"runner-number\">4</span>\n                <span class=\"runner-name\">Gallant Gus</span>\n                <span class=\"runner-odds\">3/1</span>\n            </li>\n        </ul>\n    </div>\n</body>\n</html>\n",
    "tests/test_engine.py": "import pytest\nfrom unittest.mock import AsyncMock, MagicMock, patch\nfrom datetime import datetime, date\nfrom decimal import Decimal\nfrom tests.conftest import create_mock_race, get_test_settings\n\n# Import your actual classes\nfrom python_service.engine import OddsEngine\nfrom python_service.adapters.base_adapter_v3 import BaseAdapterV3\nfrom python_service.models import Race\n\n@pytest.mark.asyncio\nasync def test_engine_initialization():\n    \"\"\"Test that the engine loads config correctly.\"\"\"\n    engine = OddsEngine(config=get_test_settings())\n    assert engine.config.API_KEY == \"test-override-key-123\"\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine._time_adapter_fetch\")\nasync def test_fetch_all_odds_success(mock_fetch, clear_cache):\n    \"\"\"Test happy path: fetching odds from a single adapter.\"\"\"\n    # ARRANGE\n    settings = get_test_settings()\n    settings.CACHE_ENABLED = False\n    engine = OddsEngine(config=settings)\n\n    today = datetime.now()\n    mock_race = Race(**create_mock_race(\n        \"MockSource\", \"Churchill Downs\", 1, today,\n        [{\"number\": 1, \"name\": \"Secretariat\", \"odds\": \"1.5\"}]\n    ))\n\n    # This is the new way to mock the data\n    mock_fetch.return_value = (\"MockSource\", {\"races\": [mock_race], \"source_info\": {\"name\": \"MockSource\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n\n    # We still need to give the engine an adapter to iterate over\n    mock_adapter = MagicMock(spec=BaseAdapterV3)\n    mock_adapter.source_name = \"MockSource\"\n    engine.adapters = {\"MockSource\": mock_adapter}\n    # Update health monitor status for this mock adapter\n    from python_service.adapter_manager import AdapterStatus, AdapterHealth\n    engine.health_monitor.statuses[\"MockSource\"] = AdapterStatus(\n        name=\"MockSource\",\n        health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0,\n        last_success=None,\n        consecutive_failures=0,\n        avg_response_time_ms=0,\n        last_error=None\n    )\n\n    # ACT\n    result = await engine.fetch_all_odds(date.today().strftime(\"%Y-%m-%d\"))\n\n    # ASSERT\n    assert len(result[\"races\"]) == 1\n    assert result[\"races\"][0][\"venue\"] == \"Churchill Downs\"\n    assert result[\"races\"][0][\"runners\"][0][\"name\"] == \"Secretariat\"\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine._time_adapter_fetch\")\nasync def test_fetch_all_odds_resilience(mock_fetch, clear_cache):\n    \"\"\"Test that one failing adapter does not crash the whole engine.\"\"\"\n    # ARRANGE\n    settings = get_test_settings()\n    settings.CACHE_ENABLED = False\n    engine = OddsEngine(config=settings)\n\n    # Mock successful adapter data\n    good_race = Race(**create_mock_race(\"GoodSource\", \"Track A\", 1, datetime.now(), []))\n    good_payload = (\"GoodSource\", {\"races\": [good_race], \"source_info\": {\"name\": \"GoodSource\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n\n    # Mock failed adapter data\n    bad_payload = (\"BadSource\", {\"races\": [], \"source_info\": {\"name\": \"BadSource\", \"status\": \"FAILED\", \"error_message\": \"API Down\", \"races_fetched\": 0, \"fetch_duration\": 0.1}}, 0.1)\n\n    mock_fetch.side_effect = [good_payload, bad_payload]\n\n    # We still need to give the engine adapters to iterate over\n    good_adapter = MagicMock(spec=BaseAdapterV3); good_adapter.source_name = \"GoodSource\"\n    bad_adapter = MagicMock(spec=BaseAdapterV3); bad_adapter.source_name = \"BadSource\"\n    engine.adapters = {\"GoodSource\": good_adapter, \"BadSource\": bad_adapter}\n    # Update health monitor status\n    from python_service.adapter_manager import AdapterStatus, AdapterHealth\n    engine.health_monitor.statuses[\"GoodSource\"] = AdapterStatus(\n        name=\"GoodSource\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n    engine.health_monitor.statuses[\"BadSource\"] = AdapterStatus(\n        name=\"BadSource\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n\n    # ACT\n    result = await engine.fetch_all_odds(\"2025-12-08\")\n\n    # ASSERT\n    assert len(result[\"races\"]) == 1, \"Should return data from the good adapter\"\n    assert result[\"races\"][0][\"source\"] == \"GoodSource\"\n\n    # Verify error logging in sourceInfo\n    bad_info = next((s for s in result[\"sourceInfo\"] if s[\"name\"] == \"BadSource\"), None)\n    assert bad_info is not None\n    assert bad_info[\"status\"] == \"FAILED\"\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine._time_adapter_fetch\")\nasync def test_race_aggregation_and_deduplication(mock_fetch, clear_cache):\n    \"\"\"Test merging identical races from different sources.\"\"\"\n    settings = get_test_settings()\n    settings.CACHE_ENABLED = False\n    engine = OddsEngine(config=settings)\n    now = datetime.now()\n\n    # Same race, different sources, slightly different odds\n    race_a = Race(**create_mock_race(\"SourceA\", \"Ascot\", 1, now, [{\"number\": 1, \"name\": \"Horse X\", \"odds\": \"2.0\"}]))\n    race_b = Race(**create_mock_race(\"SourceB\", \"Ascot\", 1, now, [{\"number\": 1, \"name\": \"Horse X\", \"odds\": \"2.2\"}]))\n\n    payload_a = (\"SourceA\", {\"races\": [race_a], \"source_info\": {\"name\": \"SourceA\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n    payload_b = (\"SourceB\", {\"races\": [race_b], \"source_info\": {\"name\": \"SourceB\", \"status\": \"SUCCESS\", \"races_fetched\": 1, \"fetch_duration\": 0.1}}, 0.1)\n    mock_fetch.side_effect = [payload_a, payload_b]\n\n    adapter_a = MagicMock(spec=BaseAdapterV3); adapter_a.source_name = \"SourceA\"\n    adapter_b = MagicMock(spec=BaseAdapterV3); adapter_b.source_name = \"SourceB\"\n    engine.adapters = {\"SourceA\": adapter_a, \"SourceB\": adapter_b}\n    # Update health monitor status\n    from python_service.adapter_manager import AdapterStatus, AdapterHealth\n    engine.health_monitor.statuses[\"SourceA\"] = AdapterStatus(\n        name=\"SourceA\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n    engine.health_monitor.statuses[\"SourceB\"] = AdapterStatus(\n        name=\"SourceB\", health=AdapterHealth.HEALTHY,\n        success_rate_24h=1.0, last_success=None, consecutive_failures=0, avg_response_time_ms=0, last_error=None\n    )\n\n\n    # ACT\n    result = await engine.fetch_all_odds(\"2025-12-08\")\n\n    # ASSERT\n    assert len(result[\"races\"]) == 1, \"Should deduplicate into a single race object\"\n    merged_race = result[\"races\"][0]\n    runner = merged_race[\"runners\"][0]\n\n    # Check that odds from both sources are present\n    # Note: This assertion depends on how your engine merges odds.\n    # If it merges into a dict, this passes. If it overwrites, adjust accordingly.\n    odds_keys = runner[\"odds\"].keys()\n    assert \"SourceA\" in odds_keys\n    assert \"SourceB\" in odds_keys",
    "tests/test_silent_deployment.ps1": "Write-Host \"Testing silent deployment...\" -ForegroundColor Cyan\n\n& msiexec.exe /i \"Fortuna-Faucet-2.1.0-x64.msi\" `\n    /qn /l*v \"silent_test.log\" `\n    ALLUSERS=1 INSTALLSCOPE=perMachine\n\nif ($LASTEXITCODE -eq 0) {\n    Write-Host \"\u2713 Silent deployment successful\"\n} else {\n    Write-Host \"\u2717 Silent deployment failed\"\n    Write-Host \"Log: silent_test.log\"\n    exit 1\n}",
    "tests/utils.py": "# tests/utils.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Dict\nfrom typing import List\n\nfrom python_service.models import OddsData\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\ndef create_mock_race(\n    source: str,\n    track_name: str,\n    race_number: int,\n    start_time: datetime,\n    runners_data: List[Dict],\n) -> dict:\n    \"\"\"\n    Creates a dictionary representing a race, suitable for Pydantic model validation.\n    This is a test utility to generate consistent race data.\n    \"\"\"\n    runners = []\n    for i, runner_info in enumerate(runners_data):\n        odds_data = {}\n        if \"odds\" in runner_info:\n            odds_value = Decimal(str(runner_info[\"odds\"]))\n            odds_data[source] = OddsData(win=odds_value, source=source, last_updated=datetime.now())\n\n        runners.append(\n            Runner(\n                number=runner_info.get(\"number\", i + 1),\n                name=runner_info.get(\"name\", f\"Runner {i + 1}\"),\n                odds=odds_data,\n                scratched=runner_info.get(\"scratched\", False),\n            ).model_dump()\n        )\n\n    # Use Pydantic model to create and then dump the data to ensure it's valid\n    race = Race(\n        id=f\"test_{track_name}_{race_number}\",\n        venue=track_name,\n        race_number=race_number,\n        start_time=start_time,\n        runners=runners,\n        source=source,\n    )\n    return race.model_dump()\n",
    "web_service/backend/__init__.py": "# This file makes this directory a package.\n",
    "web_service/backend/adapters/base_adapter_v3.py": "# python_service/adapters/base_v3.py\nfrom __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport json\nimport random\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport httpx\nimport structlog\nfrom tenacity import (\n    RetryError,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom python_service.core.smart_fetcher import (\n    BrowserEngine,\n    FetchStrategy,\n    SmartFetcher,\n    StealthMode,\n)\n\nfrom ..core.exceptions import AdapterHttpError, AdapterParsingError\nfrom ..manual_override_manager import ManualOverrideManager\nfrom ..models import Race\nfrom ..validators import DataValidationPipeline\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject requests\n    HALF_OPEN = \"half_open\"  # Testing if service recovered\n\n\n@dataclass\nclass CircuitBreaker:\n    \"\"\"Thread-safe circuit breaker implementation.\"\"\"\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    half_open_max_calls: int = 3\n\n    _failure_count: int = field(default=0, repr=False)\n    _last_failure_time: float = field(default=0.0, repr=False)\n    _state: CircuitState = field(default=CircuitState.CLOSED, repr=False)\n    _half_open_calls: int = field(default=0, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    @property\n    def state(self) -> CircuitState:\n        \"\"\"Returns current state without mutation. Use check_state() for transitions.\"\"\"\n        return self._state\n\n    async def check_and_transition_state(self) -> CircuitState:\n        \"\"\"Check state and handle OPEN -> HALF_OPEN transition atomically.\"\"\"\n        async with self._lock:\n            if self._state == CircuitState.OPEN:\n                if time.monotonic() - self._last_failure_time >= self.recovery_timeout:\n                    self._state = CircuitState.HALF_OPEN\n                    self._half_open_calls = 0\n            return self._state\n\n    async def record_success(self) -> None:\n        \"\"\"Record a successful call.\"\"\"\n        async with self._lock:\n            self._failure_count = 0\n            if self._state == CircuitState.HALF_OPEN:\n                self._half_open_calls += 1\n                if self._half_open_calls >= self.half_open_max_calls:\n                    self._state = CircuitState.CLOSED\n\n    async def record_failure(self) -> None:\n        \"\"\"Record a failed call.\"\"\"\n        async with self._lock:\n            self._failure_count += 1\n            self._last_failure_time = time.monotonic()\n\n            if self._failure_count >= self.failure_threshold:\n                self._state = CircuitState.OPEN\n            elif self._state == CircuitState.HALF_OPEN:\n                self._state = CircuitState.OPEN\n\n    async def allow_request(self) -> bool:\n        \"\"\"Check if a request should be allowed.\"\"\"\n        state = await self.check_and_transition_state()\n        return state in (CircuitState.CLOSED, CircuitState.HALF_OPEN)\n\n\n@dataclass\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter.\"\"\"\n    requests_per_second: float = 10.0\n    burst_size: int = 20\n\n    _tokens: float = field(default=0.0, init=False, repr=False)\n    _last_update: float = field(default=0.0, init=False, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    def __post_init__(self) -> None:\n        self._tokens = float(self.burst_size)\n        self._last_update = time.monotonic()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire a token, waiting if necessary.\"\"\"\n        async with self._lock:\n            now = time.monotonic()\n            elapsed = now - self._last_update\n            self._tokens = min(self.burst_size, self._tokens + elapsed * self.requests_per_second)\n            self._last_update = now\n\n            if self._tokens < 1:\n                wait_time = (1 - self._tokens) / self.requests_per_second\n                await asyncio.sleep(wait_time)\n                self._tokens = 0\n            else:\n                self._tokens -= 1\n\n\n@dataclass\nclass CacheEntry:\n    \"\"\"Cache entry with TTL.\"\"\"\n    data: Any\n    created_at: float\n    ttl: float\n\n    @property\n    def is_expired(self) -> bool:\n        return time.monotonic() - self.created_at > self.ttl\n\n\nclass ResponseCache:\n    \"\"\"Simple in-memory response cache.\"\"\"\n\n    def __init__(self, default_ttl: float = 300.0, max_entries: int = 1000):\n        self.default_ttl = default_ttl\n        self.max_entries = max_entries\n        self._cache: dict[str, CacheEntry] = {}\n        self._lock = asyncio.Lock()\n\n    @staticmethod\n    def _make_key(method: str, url: str, **kwargs) -> str:\n        \"\"\"Generate a stable cache key from request parameters.\"\"\"\n        # Filter out non-hashable or irrelevant kwargs\n        cacheable_kwargs = {\n            k: v for k, v in kwargs.items()\n            if k not in ('headers', 'timeout', 'follow_redirects')\n            and isinstance(v, (str, int, float, bool, tuple, type(None)))\n        }\n        key_data = f\"{method}:{url}:{json.dumps(cacheable_kwargs, sort_keys=True, default=str)}\"\n        return hashlib.sha256(key_data.encode()).hexdigest()[:32]\n\n    async def get(self, method: str, url: str, **kwargs) -> Any | None:\n        \"\"\"Get a cached response if available and not expired.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            entry = self._cache.get(key)\n            if entry and not entry.is_expired:\n                return entry.data\n            elif entry:\n                del self._cache[key]\n        return None\n\n    async def set(self, method: str, url: str, data: Any, ttl: float | None = None, **kwargs) -> None:\n        \"\"\"Cache a response.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            # Evict old entries if cache is full\n            if len(self._cache) >= self.max_entries:\n                expired_keys = [k for k, v in self._cache.items() if v.is_expired]\n                for k in expired_keys:\n                    del self._cache[k]\n\n                # If still full, remove oldest entries\n                if len(self._cache) >= self.max_entries:\n                    oldest = sorted(self._cache.items(), key=lambda x: x[1].created_at)\n                    for k, _ in oldest[:len(self._cache) // 4]:\n                        del self._cache[k]\n\n            self._cache[key] = CacheEntry(\n                data=data,\n                created_at=time.monotonic(),\n                ttl=ttl or self.default_ttl\n            )\n\n    async def clear(self) -> None:\n        \"\"\"Clear all cached entries.\"\"\"\n        async with self._lock:\n            self._cache.clear()\n\n\n@dataclass\nclass AdapterMetrics:\n    \"\"\"Thread-safe metrics for adapter health monitoring.\"\"\"\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n    _total_requests: int = field(default=0, repr=False)\n    _successful_requests: int = field(default=0, repr=False)\n    _failed_requests: int = field(default=0, repr=False)\n    _total_latency_ms: float = field(default=0.0, repr=False)\n    _last_success: float | None = field(default=None, repr=False)\n    _last_failure: float | None = field(default=None, repr=False)\n    _last_error: str | None = field(default=None, repr=False)\n\n    @property\n    def total_requests(self) -> int:\n        return self._total_requests\n\n    @property\n    def success_rate(self) -> float:\n        if self._total_requests == 0:\n            return 1.0\n        return self._successful_requests / self._total_requests\n\n    @property\n    def avg_latency_ms(self) -> float:\n        if self._successful_requests == 0:\n            return 0.0\n        return self._total_latency_ms / self._successful_requests\n\n    async def record_success(self, latency_ms: float) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._successful_requests += 1\n            self._total_latency_ms += latency_ms\n            self._last_success = time.time()\n\n    async def record_failure(self, error: str) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._failed_requests += 1\n            self._last_failure = time.time()\n            self._last_error = error\n\n    def snapshot(self) -> dict[str, Any]:\n        \"\"\"Return a point-in-time snapshot of metrics.\"\"\"\n        return {\n            \"total_requests\": self._total_requests,\n            \"successful_requests\": self._successful_requests,\n            \"failed_requests\": self._failed_requests,\n            \"success_rate\": self.success_rate,\n            \"avg_latency_ms\": self.avg_latency_ms,\n            \"last_success\": self._last_success,\n            \"last_failure\": self._last_failure,\n            \"last_error\": self._last_error,\n        }\n\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    Abstract base class for all V3 data adapters.\n\n    Features:\n    - Standardized fetch/parse pattern\n    - Retry logic with exponential backoff\n    - Circuit breaker for fault tolerance\n    - Rate limiting\n    - Response caching\n    - Comprehensive metrics\n    \"\"\"\n\n    # List of common User-Agent strings for rotation\n    USER_AGENTS = [\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0\",\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    ]\n\n    @property\n    def DEFAULT_USER_AGENT(self) -> str:\n        \"\"\"Return a randomly selected User-Agent.\"\"\"\n        return random.choice(self.USER_AGENTS)\n\n    def __init__(\n        self,\n        source_name: str,\n        base_url: str,\n        config: Any = None,\n        timeout: int = 20,\n        enable_cache: bool = True,\n        cache_ttl: float = 300.0,\n        rate_limit: float = 10.0,\n    ):\n        self.source_name = source_name\n        self.base_url = base_url.rstrip(\"/\")\n        self.config = config\n        self.timeout = timeout\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient | None = None\n        self.manual_override_manager: ManualOverrideManager | None = None\n        self.supports_manual_override = True\n        self.attempted_url: Optional[str] = None\n        # \u2705 THESE 4 LINES MUST BE HERE (not in close()):\n        self.circuit_breaker = CircuitBreaker()\n        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)\n        self.cache = ResponseCache(default_ttl=cache_ttl) if enable_cache else None\n        self.metrics = AdapterMetrics()\n\n        # New SmartFetcher integration\n        self.fetch_strategy = self._configure_fetch_strategy()\n        self.smart_fetcher = SmartFetcher(strategy=self.fetch_strategy)\n\n    async def __aenter__(self) -> \"BaseAdapterV3\":\n        \"\"\"Async context manager entry.\"\"\"\n        if self.http_client is None:\n            self.http_client = httpx.AsyncClient(timeout=self.timeout)\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Async context manager exit with cleanup.\"\"\"\n        await self.close()\n\n    async def close(self) -> None:\n        \"\"\"Clean up resources, including the SmartFetcher.\"\"\"\n        if self.http_client:\n            await self.http_client.aclose()\n            self.http_client = None\n        if hasattr(self, \"smart_fetcher\"):\n            await self.smart_fetcher.close()\n        if self.cache:\n            await self.cache.clear()\n        self.logger.debug(\"Adapter resources cleaned up\")\n\n    def enable_manual_override(self, manager: ManualOverrideManager) -> None:\n        \"\"\"Injects the manual override manager into the adapter.\"\"\"\n        self.manual_override_manager = manager\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw data (e.g., HTML, JSON) for the given date.\n        This is the only method that should perform network operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> list[Race]:\n        \"\"\"\n        Parses the raw data retrieved by _fetch_data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> list[Race]:\n        \"\"\"\n        Orchestrates the fetch-then-parse pipeline for the adapter.\n        This public method should not be overridden by subclasses.\n        \"\"\"\n        raw_data = None\n\n        # Check for manual override data first\n        if self.manual_override_manager:\n            lookup_key = f\"{self.base_url}/racecards/{date}\"\n            manual_data = self.manual_override_manager.get_manual_data(self.source_name, lookup_key)\n            if manual_data:\n                self.logger.info(\"Using manually submitted data\", url=lookup_key)\n                raw_data = {\"pages\": [manual_data[0]], \"date\": date}\n\n        # Fetch from source if no manual data\n        if raw_data is None:\n            try:\n                raw_data = await self._fetch_data(date)\n            except AdapterHttpError as e:\n                if self.manual_override_manager and self.supports_manual_override:\n                    self.manual_override_manager.register_failure(self.source_name, e.url)\n                raise\n\n        # Parse the data\n        if raw_data is not None:\n            return self._validate_and_parse_races(raw_data)\n\n        return []\n\n    def _validate_and_parse_races(self, raw_data: Any) -> list[Race]:\n        self.attempted_url = None  # Reset for each new get_races call\n        is_valid, reason = DataValidationPipeline.validate_raw_response(self.source_name, raw_data)\n        if not is_valid:\n            raise AdapterParsingError(self.source_name, f\"Raw response validation failed: {reason}\")\n\n        try:\n            parsed_races = self._parse_races(raw_data)\n        except Exception as e:\n            self.logger.error(\"Failed to parse race data\", error=str(e), exc_info=True)\n            # Save a snapshot of the problematic data on parsing failure\n            self._save_debug_snapshot(\n                content=str(raw_data),\n                context=\"parsing_error\",\n                url=getattr(e, 'url', self.attempted_url)\n            )\n            raise AdapterParsingError(self.source_name, \"Parsing logic failed.\") from e\n\n        validated_races, warnings = DataValidationPipeline.validate_parsed_races(parsed_races)\n\n        if warnings:\n            self.logger.warning(\"Validation warnings during parsing\", warnings=warnings)\n\n        return validated_races\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        Defines the fetching strategy for this adapter. Subclasses should override\n        this method to customize fetching behavior based on the target website's\n        characteristics (e.g., anti-bot measures, JavaScript requirements).\n\n        Example Overrides:\n        - SportingLife: Needs JS rendering -> primary_engine=BrowserEngine.PLAYWRIGHT\n        - AtTheRaces: Simple HTML -> primary_engine=BrowserEngine.HTTPX\n        - RacingPost: Strong anti-bot -> stealth_mode=StealthMode.CAMOUFLAGE\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=StealthMode.FAST,\n            block_resources=True,\n            max_retries=3,\n            timeout=30,\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs) -> httpx.Response:\n        \"\"\"\n        Performs a web request using the SmartFetcher, which intelligently\n        manages browser engines, retries, and stealth capabilities. This method\n        replaces the previous direct-httpx implementation.\n        \"\"\"\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url}/{url.lstrip('/')}\"\n        self.attempted_url = full_url\n\n        try:\n            # The SmartFetcher handles caching, retries, circuit breaking, etc.\n            response = await self.smart_fetcher.fetch(full_url, method=method, **kwargs)\n\n            # Log success with rich metadata from the fetcher\n            self.logger.info(\n                \"Request successful\",\n                url=full_url,\n                status=getattr(response, \"status\", \"N/A\"),\n                size_bytes=len(getattr(response, \"text\", \"\")),\n                engine=getattr(response, \"metadata\", {}).get(\"engine_used\", \"unknown\"),\n            )\n            return response\n\n        except Exception as e:\n            # Log failure with detailed diagnostics from the fetcher\n            self.logger.error(\n                \"Request failed after all retries and engine fallbacks\",\n                url=full_url,\n                error=str(e),\n                error_type=type(e).__name__,\n                health_report=self.smart_fetcher.get_health_report(),\n            )\n\n            # Save a snapshot if we have a response body in the error\n            if hasattr(e, 'response') and hasattr(e.response, 'text'):\n                self._save_debug_snapshot(\n                    content=e.response.text,\n                    context=f\"request_failed_{getattr(e.response, 'status', 'unknown')}\",\n                    url=full_url\n                )\n\n            # Re-raise as a standard adapter error for consistent downstream handling\n            status_code = getattr(getattr(e, 'response', None), 'status', 503)\n            raise AdapterHttpError(\n                adapter_name=self.source_name, status_code=status_code, url=full_url\n            ) from e\n\n    def _should_save_debug_html(self) -> bool:\n        \"\"\"Determines if the current environment is suitable for saving debug files.\"\"\"\n        import os\n        return os.getenv(\"CI\") == \"true\" or os.getenv(\"DEBUG_MODE\") == \"true\"\n\n    def _save_debug_snapshot(self, content: str, context: str, url: str | None = None):\n        \"\"\"\n        Saves HTML or other text content to a file for debugging purposes.\n        Enhanced to include metadata and better organization.\n        \"\"\"\n        if not self._should_save_debug_html():\n            return\n\n        import os\n        import re\n        import json\n        from datetime import datetime\n\n        try:\n            debug_dir = os.path.join(\"debug-snapshots\", self.source_name.lower())\n            os.makedirs(debug_dir, exist_ok=True)\n\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n\n            # Sanitize context and URL for a safe filename\n            sanitized_context = re.sub(r'[\\\\/*?:\"<>|]', \"_\", context)\n            sanitized_url = \"\"\n            if url:\n                # Remove protocol and query params for filename\n                clean_url = re.sub(r'https?://(www\\.)?', '', url).split('?')[0]\n                # Avoid backslashes in f-string for Python < 3.12 compatibility\n                url_part = re.sub(r'[\\\\/*?:\\x22<>|]', '_', clean_url)[:60]\n                sanitized_url = f\"_{url_part}\"\n\n            base_filename = f\"{timestamp}_{sanitized_context}{sanitized_url}\"\n\n            # Save the main content (HTML/JSON)\n            content_ext = \".json\" if content.startswith((\"{\", \"[\")) else \".html\"\n            filepath = os.path.join(debug_dir, f\"{base_filename}{content_ext}\")\n\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n\n            # Save metadata for better diagnostic context\n            meta_path = os.path.join(debug_dir, f\"{base_filename}_meta.json\")\n            meta = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"adapter\": self.source_name,\n                \"url\": url or self.attempted_url,\n                \"context\": context,\n                \"engine\": getattr(self.smart_fetcher, 'last_engine', 'unknown'),\n                \"health_report\": self.smart_fetcher.get_health_report()\n            }\n            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(meta, f, indent=2)\n\n            self.logger.info(\"Saved debug snapshot and metadata\",\n                             filepath=filepath, meta_path=meta_path)\n\n            # Prune old snapshots (keep last 50)\n            self._prune_debug_snapshots(debug_dir, max_files=100)\n\n        except Exception as e:\n            self.logger.warning(\"Failed to save debug snapshot\", error=str(e))\n\n    def _prune_debug_snapshots(self, debug_dir: str, max_files: int = 100):\n        \"\"\"Keep the number of debug files under control.\"\"\"\n        import os\n        try:\n            files = [os.path.join(debug_dir, f) for f in os.listdir(debug_dir)]\n            if len(files) <= max_files:\n                return\n\n            # Sort by modification time (oldest first)\n            files.sort(key=os.path.getmtime)\n            for f in files[:-max_files]:\n                os.remove(f)\n        except Exception:\n            pass\n\n    async def health_check(self) -> dict[str, Any]:\n        \"\"\"\n        Performs a health check on the adapter.\n        Subclasses can override to add custom checks.\n        \"\"\"\n        return {\n            \"adapter_name\": self.source_name,\n            \"base_url\": self.base_url,\n            \"circuit_breaker_state\": self.circuit_breaker.state.value,\n            \"metrics\": self.metrics.snapshot(),\n        }\n\n    def get_status(self) -> dict[str, Any]:\n        \"\"\"\n        Returns a dictionary representing the adapter's current status.\n        \"\"\"\n        status = \"OK\"\n        if self.circuit_breaker.state == CircuitState.OPEN:\n            status = \"CIRCUIT_OPEN\"\n        elif self.metrics.success_rate < 0.5:\n            status = \"DEGRADED\"\n\n        return {\n            \"adapter_name\": self.source_name,\n            \"status\": status,\n            \"circuit_state\": self.circuit_breaker.state.value,\n            \"success_rate\": round(self.metrics.success_rate, 3),\n        }\n\n    async def reset(self) -> None:\n        \"\"\"Reset adapter state (cache, circuit breaker, metrics).\"\"\"\n        if self.cache:\n            await self.cache.clear()\n        self.circuit_breaker = CircuitBreaker()\n        self.metrics = AdapterMetrics()\n        self.logger.info(\"Adapter state reset\")\n",
    "web_service/backend/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime, timedelta, timezone\nfrom decimal import Decimal\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Using a representative eventId as a placeholder for the discovery mechanism\n        event_id = \"38183.3\"\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(\"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            return []\n\n        races = []\n        for group in raw_data.get(\"marketGroups\", []):\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\n                            \"Failed to parse a FanDuel market\",\n                            market_id=market.get(\"marketId\"),\n                            exc_info=True,\n                        )\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\").strip()\n        if not race_number_str.isdigit():\n            return None\n        race_number = int(race_number_str)\n\n        track_name = parts[1]\n        start_time = datetime.now(timezone.utc) + timedelta(hours=race_number)\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_runner_odds = runner_data.get(\"winRunnerOdds\", {})\n                current_price = win_runner_odds.get(\"currentPrice\")\n\n                if not runner_name or not current_price:\n                    continue\n\n                numerator, denominator = map(int, current_price.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                name_parts = runner_name.split(\".\", 1)\n                if len(name_parts) < 2:\n                    continue\n\n                program_number_str = name_parts[0].strip()\n                horse_name = name_parts[1].strip()\n\n                odds = {}\n                if odds_data := create_odds_data(self.source_name, decimal_odds):\n                    odds[self.source_name] = odds_data\n\n                runners.append(\n                    Runner(\n                        name=horse_name,\n                        number=(int(program_number_str) if program_number_str.isdigit() else 0),\n                        odds=odds,\n                    )\n                )\n            except (ValueError, ZeroDivisionError, IndexError, TypeError):\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\"\"\"Adapter for Racing and Sports Greyhound API.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSportsGreyhound\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n        if not getattr(config, \"RACING_AND_SPORTS_TOKEN\", None):\n            raise AdapterConfigError(\n                self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\"\n            )\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetch greyhound meetings from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            \"GET\", \"v1/greyhound/meetings\", headers=headers, params=params\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parse meetings data into Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\n                \"No 'meetings' in RacingAndSportsGreyhound response or invalid format.\"\n            )\n            return []\n\n        races = []\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed := self._parse_race(meeting, race_summary):\n                        races.append(parsed)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse greyhound race\",\n                        venue=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return races\n\n    def _parse_race(\n        self, meeting: Dict[str, Any], race: Dict[str, Any]\n    ) -> Optional[Race]:\n        \"\"\"Parse a single race from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        # FIX: Use dogName for greyhounds, not horseName\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"dogName\", rd.get(\"greyhoundName\", \"Unknown\")),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str)\n        except (ValueError, TypeError):\n            self.logger.warning(\n                \"Invalid start time\", start_time_str=start_time_str, race_id=race_id\n            )\n            return None\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/core/__init__.py": "",
    "web_service/backend/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters import (\n    AtTheRacesAdapter,\n    BetfairAdapter,\n    BetfairGreyhoundAdapter,\n    BrisnetAdapter,\n    EquibaseAdapter,\n    FanDuelAdapter,\n    GbgbApiAdapter,\n    GreyhoundAdapter,\n    HarnessAdapter,\n    HorseRacingNationAdapter,\n    NYRABetsAdapter,\n    OddscheckerAdapter,\n    PointsBetGreyhoundAdapter,\n    PuntersAdapter,\n    RacingAndSportsAdapter,\n    RacingAndSportsGreyhoundAdapter,\n    RacingPostAdapter,\n    RacingTVAdapter,\n    SportingLifeAdapter,\n    TabAdapter,\n    TheRacingApiAdapter,\n    TimeformAdapter,\n    TVGAdapter,\n    TwinSpiresAdapter,\n    UniversalAdapter,\n    XpressbetAdapter,\n)\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .core.exceptions import AuthenticationError\nfrom .adapter_manager import AdapterHealthMonitor, AdapterHealth, AdapterStatus\nfrom .cache_manager import StaleDataCache\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n        exclude_adapters: Optional[List[str]] = None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n        self.health_monitor = AdapterHealthMonitor()\n        self.stale_data_cache = StaleDataCache(max_age_hours=24)\n        self.exclude_adapters = set(exclude_adapters) if exclude_adapters else set()\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: Dict[str, BaseAdapterV3] = {}\n\n            # NOTE: Many adapters require API keys (e.g., TVG, Betfair, TheRacingAPI).\n            # If the required API key is not found in the environment configuration,\n            # the adapter will fail to initialize and be skipped. This is expected\n            # behavior in environments where secrets are not configured.\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                UniversalAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                adapter_name = adapter_cls.__name__\n                if adapter_name in self.exclude_adapters:\n                    self.logger.info(f\"Intentionally skipping adapter: {adapter_name}\")\n                    continue\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_name}\")\n                    if adapter_name == \"UniversalAdapter\":\n                        # UniversalAdapter requires a definition_path, which we don't have here.\n                        # For now, we'll skip it unless it's explicitly configured.\n                        self.logger.info(\"UniversalAdapter PoC requires a definition_path. Skipping.\")\n                        continue\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_name}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters[adapter_instance.source_name] = adapter_instance\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_name,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_name}\",\n                        exc_info=True,\n                    )\n\n            for adapter_instance in self.adapters.values():\n                self.health_monitor.statuses[adapter_instance.source_name] = AdapterStatus(\n                    name=adapter_instance.source_name,\n                    health=AdapterHealth.HEALTHY,\n                    success_rate_24h=1.0,\n                    last_success=None,\n                    consecutive_failures=0,\n                    avg_response_time_ms=0,\n                    last_error=None,\n                )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters.values():\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    async def shutdown(self):\n        \"\"\"Gracefully shuts down all adapters that require cleanup.\"\"\"\n        self.logger.info(\"Shutting down adapters with cleanup methods...\")\n        for adapter_name, adapter in self.adapters.items():\n            if hasattr(adapter, 'cleanup'):\n                try:\n                    self.logger.info(f\"Cleaning up {adapter_name}...\")\n                    await adapter.cleanup()\n                except Exception as e:\n                    self.logger.error(f\"Error cleaning up {adapter_name}\", error=str(e), exc_info=True)\n        await self.close()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters.values()]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            processed_races = []\n            for race_data in race_data_list:\n                if isinstance(race_data, Race):\n                    processed_races.append(race_data)\n                else:\n                    processed_races.append(Race(**race_data))\n            races = processed_races\n            if races:\n                is_success = True\n            else:\n                is_success = False\n                error_message = \"Adapter ran successfully but fetched zero races.\"\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except AuthenticationError as e:\n            self.logger.warning(\n                \"Authentication failed for adapter, skipping.\",\n                adapter=adapter.source_name,\n                error=str(e),\n            )\n            error_message = str(e)\n            is_success = False\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        # Update health monitor\n        await self.health_monitor.update_adapter_status(\n            adapter_name=adapter.source_name,\n            success=is_success,\n            latency_ms=duration * 1000,\n            error=error_message,\n        )\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": adapter.attempted_url or attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n\n                # Maintain source as string\n                sources = set(existing_race.source.split(\", \"))\n                sources.add(race.source)\n                existing_race.source = \", \".join(sorted(list(sources)))\n\n        return list(race_map.values())\n\n    def _calculate_coverage(self, results: List[Dict[str, Any]]) -> float:\n        # Stub implementation\n        return 0.0\n\n    def _merge_adapter_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for adapter_result in results:\n            source_info = adapter_result.get(\"source_info\", {})\n            source_infos.append(source_info)\n            if source_info.get(\"status\") == \"SUCCESS\":\n                all_races.extend(adapter_result.get(\"races\", []))\n            else:\n                errors.append({\n                    \"adapter_name\": source_info.get(\"name\"),\n                    \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                    \"attempted_url\": source_info.get(\"attempted_url\")\n                })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        return {\n            \"races\": deduped_races,\n            \"errors\": errors,\n            \"source_info\": source_infos\n        }\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None, min_required_adapters: int = 2) -> Dict[str, Any]:\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d\")\n\n        # Re-introduce live caching\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        all_payloads = []\n        attempted_adapters = []\n        all_adapter_names = list(self.adapters.keys())\n        if source_filter:\n            all_adapter_names = [name for name in all_adapter_names if name.lower() == source_filter.lower()]\n\n        ordered_adapter_names = self.health_monitor.get_ordered_adapters(all_adapter_names)\n\n        # Tier 1: Healthy\n        healthy_names = [name for name in ordered_adapter_names if self.health_monitor.statuses[name].health == AdapterHealth.HEALTHY]\n        if healthy_names:\n            tasks = [self._fetch_with_semaphore(self.adapters[name], date) for name in healthy_names]\n            attempted_adapters.extend(healthy_names)\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            for res in results:\n                if not isinstance(res, Exception):\n                    _adapter_name, payload, _duration = res\n                    all_payloads.append(payload)\n\n        successful_count = len([p for p in all_payloads if p['source_info']['status'] == 'SUCCESS'])\n\n        # Tier 2: Degraded\n        if successful_count < min_required_adapters:\n            degraded_names = [name for name in ordered_adapter_names if self.health_monitor.statuses[name].health == AdapterHealth.DEGRADED]\n            if degraded_names:\n                tasks = [self._fetch_with_semaphore(self.adapters[name], date) for name in degraded_names]\n                attempted_adapters.extend(degraded_names)\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n                for res in results:\n                    if not isinstance(res, Exception):\n                        _adapter_name, payload, _duration = res\n                        all_payloads.append(payload)\n\n        # Tier 3: Stale cache fallback\n        if not any(p['source_info']['status'] == 'SUCCESS' for p in all_payloads):\n            log.warning(\"All live adapters failed, attempting to use stale cache.\")\n            stale_data = await self.stale_data_cache.get(date)\n            if stale_data:\n                log.info(\"Using stale data from cache.\", cache_age_hours=stale_data['age_hours'])\n                stale_results = stale_data['data']\n                if 'metadata' in stale_results:\n                    stale_results['metadata']['data_freshness'] = 'stale'\n                stale_results['warnings'] = [\"Using cached data from a previous run as all live sources failed.\"]\n                return stale_results\n\n        if not all_payloads:\n            log.error(\"All adapter fetches failed and no stale data available.\")\n            return {\"races\": [], \"errors\": [{\"adapter_name\": \"all\", \"error_message\": \"All adapters failed and no stale data available.\"}], \"source_info\": [], \"metadata\": {}}\n\n        merged_results = self._merge_adapter_results(all_payloads)\n        successful_count = len([s for s in merged_results[\"source_info\"] if s[\"status\"] == \"SUCCESS\"])\n\n        try:\n            parsed_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except (ValueError, TypeError):\n            parsed_date = datetime.now().date()\n\n        response_obj = AggregatedResponse(\n            date=parsed_date,\n            races=merged_results[\"races\"],\n            errors=merged_results[\"errors\"],\n            source_info=merged_results[\"source_info\"],\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": attempted_adapters,\n                \"sources_successful\": successful_count,\n                \"total_races\": len(merged_results[\"races\"]),\n                \"total_errors\": len(merged_results[\"errors\"]),\n                'coverage': self._calculate_coverage(all_payloads),\n                'data_freshness': 'live'\n            },\n        )\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Cache successful live results\n        if successful_count > 0:\n            await self.stale_data_cache.set(date, response_data)\n            await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "web_service/backend/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return (\n                None,\n                \"Missing core fields (id, venue, race_number, start_time, runners)\",\n            )\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\"\n                        )\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "web_service/backend/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"127.0.0.1\", port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "web_service/backend/middleware/__init__.py": "",
    "web_service/backend/port_check.py": "import socket\nimport sys\n\ndef check_port_and_exit_if_in_use(port: int, host: str):\n    \"\"\"Checks if a port is in use at the given host and exits if it is.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.bind((host, port))\n        except OSError:\n            print(f\"\u274c FATAL: Port {port} is already in use. Please close the other application or specify a different port.\")\n            sys.exit(1)\n",
    "web_service/backend/requirements.txt": "# Project: Fortuna Faucet\n# Python Version: 3.10.11\n# Platform: Windows\n# Last Validated: 2026-01-10\n# Status: PRODUCTION READY\n# Notes: Manually validated, NOT auto-generated. All versions tested and confirmed working.\n\n# Installation: pip install -r requirements.txt\n\n# Core Web Framework\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\nstarlette==0.27.0\npydantic==2.5.0\npydantic-core==2.14.1\npydantic-settings==2.1.0\n\n# Asgi Http\nanyio==3.7.1\nh11==0.14.0\nh2==4.1.0\nhpack==4.0.0\nhttpcore==1.0.2\nhttptools==0.6.1\nhttpx==0.25.2\nhyperframe==6.1.0\nwebsockets==12.0\nwsproto==1.2.0\n\n# Http Client\nrequests==2.31.0\nurllib3==2.1.0\ncertifi>=2024.2.2\ncharset-normalizer==3.3.2\nidna==3.6\n\n# Database Orm\nsqlalchemy==2.0.23\ngreenlet>=3.1.1\naiosqlite==0.19.0\npsycopg2-binary==2.9.9\n\n# Data Processing\nnumpy==1.25.0\npandas==2.0.3\npython-dateutil==2.8.2\npytz==2023.3.post1\ntzdata==2023.3\nsix==1.16.0\n\n# Html Web Parsing\nbeautifulsoup4==4.12.2\nsoupsieve==2.5\nselectolax==0.3.20\nscrapling[fetchers]>=0.3.7\ncamoufox>=0.1.7\n\n# Cli Configuration\nclick>=8.3.0\npython-dotenv==1.0.0\npyyaml==6.0.1\n\n# Cryptography Security\ncryptography==41.0.7\ncffi==1.16.0\npycparser==2.21\nsecretstorage==3.5.0\nkeyring==24.3.0\njeepney==0.9.0\n\n# Caching Rate Limiting\nredis==5.0.1\nlimits==3.7.0\nslowapi==0.1.9\ntenacity==8.2.3\n\n# Desktop Gui Windows\npywebview==5.4\nbottle==0.13.4\nproxy-tools==0.1.0\n# NOTE: pywebview WITHOUT CEF (uses WinForms instead). CEFPython3 66.0 incompatible with Python 3.10.11\n\n# System Utilities\npsutil==5.9.6\n\n# Logging Monitoring\nstructlog==23.2.0\n\n# Code Quality Building\nblack==23.12.0\npyinstaller==6.1.0\npyinstaller-hooks-contrib==2023.11\naltgraph==0.17.3\nwheel==0.41.2\nbuild==1.0.3\npip-tools==7.3.0\n\n# Testing\npytest==7.4.3\npytest-asyncio==0.21.1\niniconfig==2.0.0\npluggy==1.3.0\npackaging==23.2\n\n# Type Hints Extensions\ntyping-extensions==4.10.0\ntyping-inspect==0.9.0\nannotated-types==0.6.0\n\n# Utilities\nmypy-extensions==1.0.0\npathspec==0.11.2\nplatformdirs==4.2.0\nmore-itertools==10.1.0\njaraco.classes==3.3.1\njaraco.context==5.3.0\njaraco.functools==4.0.0\ndeprecated==1.2.14\nsniffio==1.3.0\nwrapt==1.16.0\nwatchfiles==0.20.0\npygments==2.17.2\n\n# --- Excluded Packages ---\n# - uvloop : NOT supported on Windows (Unix-only features)\n# - numpy 2x: Requires Python 3.11+\n# - pandas 2.1+: Has compatibility issues with Python 3.10\n# - pywebview 6x: Has Windows compatibility issues\n# - cefpython3: Incompatible with Python 3.10.11\n",
    "web_service/backend/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "web_service/backend/version.py": "# web_service/backend/version.py\n\n__version__ = \"3.0.1\" # Default version\n\ndef get_version():\n    \"\"\"Returns the application version.\"\"\"\n    return __version__\n",
    "web_service/frontend/app/components/ErrorDisplay.tsx": "// web_platform/frontend/src/components/ErrorDisplay.tsx\n'use client';\n\nimport React from 'react';\n\ninterface ErrorInfo {\n  message: string;\n  suggestion: string;\n  details?: string;\n}\n\ninterface ErrorDisplayProps {\n  error: ErrorInfo;\n}\n\nexport const ErrorDisplay: React.FC<ErrorDisplayProps> = ({ error }) => {\n  return (\n    <div className=\"bg-red-900/20 border border-red-500/30 text-white rounded-lg p-6 max-w-2xl mx-auto my-8\">\n      <div className=\"flex items-center mb-4\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-8 w-8 text-red-400 mr-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n          <path fillRule=\"evenodd\" d=\"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z\" clipRule=\"evenodd\" />\n        </svg>\n        <h2 className=\"text-2xl font-bold text-red-400\">An Error Occurred</h2>\n      </div>\n      <p className=\"text-lg text-slate-300 mb-2\">{error.message}</p>\n      <p className=\"text-slate-400 mb-6\">{error.suggestion}</p>\n      {error.details && (\n        <details className=\"bg-slate-800/50 rounded-lg p-4\">\n          <summary className=\"cursor-pointer text-sm text-slate-500 hover:text-white\">\n            Technical Details\n          </summary>\n          <pre className=\"text-xs text-slate-400 mt-2 p-2 bg-black/30 rounded overflow-x-auto\">\n            <code>{error.details}</code>\n          </pre>\n        </details>\n      )}\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/LiveRaceDashboard.tsx": "// web_platform/frontend/src/components/LiveRaceDashboard.tsx\n'use client';\n\nimport React, { useState, useEffect, useCallback } from 'react';\nimport { useQuery, useQueryClient } from '@tanstack/react-query';\nimport { RaceFilters } from './RaceFilters';\nimport { RaceCard } from './RaceCard';\nimport { RaceCardSkeleton } from './RaceCardSkeleton';\nimport { EmptyState } from './EmptyState';\nimport { ErrorDisplay } from './ErrorDisplay';\nimport { Race, SourceInfo, AdapterError, AggregatedRacesResponse } from '../types/racing';\nimport { useWebSocket } from '../hooks/useWebSocket';\nimport { StatusDetailModal } from './StatusDetailModal';\nimport ManualOverridePanel from './ManualOverridePanel';\nimport { LiveModeToggle } from './LiveModeToggle';\nimport { AdapterStatusPanel } from './AdapterStatusPanel';\n\n// Type for the backend process status received from Electron main\ntype BackendState = 'starting' | 'running' | 'error' | 'stopped';\ninterface BackendStatus {\n  state: BackendState;\n  logs: string[];\n}\n\ninterface RaceFilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nconst fetchAdapterStatuses = async (apiKey: string | null): Promise<SourceInfo[]> => {\n  if (!apiKey) {\n    throw new Error('API key not available.');\n  }\n  const response = await fetch(`/api/adapters/status`, {\n    headers: { 'X-API-Key': apiKey, 'Content-Type': 'application/json' },\n  });\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n  return response.json();\n};\n\nconst fetchQualifiedRaces = async (apiKey: string | null, params: RaceFilterParams): Promise<AggregatedRacesResponse> => {\n  if (!apiKey) {\n    throw new Error('API key not available');\n  }\n  // In web service mode, API calls are relative to the current origin.\n  const response = await fetch(`/api/races`, {\n    headers: { 'X-API-Key': apiKey },\n  });\n\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n\n  return response.json();\n};\n\n\nconst BackendErrorPanel = ({ logs }: { logs: string[] }) => (\n  <div className=\"bg-slate-800 p-6 rounded-lg border border-red-500/50 text-white\">\n    <h2 className=\"text-2xl font-bold text-red-400 mb-4\">Backend Service Error</h2>\n    <p className=\"text-slate-400 mb-4\">The backend data service failed to start or has crashed. Below are the most recent diagnostic messages.</p>\n    <div className=\"bg-black p-4 rounded-md font-mono text-sm text-slate-300 h-64 overflow-y-auto mb-4\">\n      {logs.map((log, index) => (\n        <p key={index} className=\"whitespace-pre-wrap\">{`> ${log}`}</p>\n      ))}\n    </div>\n    <p className=\"text-sm text-slate-500 text-center mt-4\">Please check the server logs for more information.</p>\n  </div>\n);\n\n// New Sub-Component to display an error from a specific adapter\nconst ErrorCard = ({ source, message }: { source: string; message: string }) => (\n  <div className=\"bg-slate-800 rounded-lg p-4 border border-red-500/50 flex flex-col justify-between\">\n    <div>\n      <h3 className=\"font-bold text-red-400 text-lg\">{source} Failed</h3>\n      <p className=\"text-slate-400 text-sm mt-2\">{message}</p>\n    </div>\n    <div className=\"mt-4 text-xs text-slate-500\">\n      <p>This adapter failed to fetch data. This is not a critical error; other adapters may provide the necessary data.</p>\n    </div>\n  </div>\n);\n\n// New Sub-Component to render the grid of races or error cards\nconst RaceGrid = ({ races }: { races: Race[] }) => (\n  <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n    {races.map(race =>\n      race.isErrorPlaceholder ? (\n        <ErrorCard key={race.id} source={race.venue} message={race.errorMessage || 'An unknown error occurred.'} />\n      ) : (\n        <RaceCard key={race.id} race={race} />\n      )\n    )}\n  </div>\n);\n\n// In a pure web service, the frontend cannot know the backend's process status.\n// We assume it's always running if the frontend is served.\nconst useBackendStatus = (): BackendStatus => {\n  return { state: 'running', logs: ['Running in web service mode. Backend status is assumed to be active.'] };\n};\n\nexport const LiveRaceDashboard = React.memo(() => {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [adapterErrors, setAdapterErrors] = useState<AdapterError[]>([]);\n  const backendStatus = useBackendStatus();\n  // In a web service, the API key might be hardcoded, come from a meta tag, or an auth flow.\n  // For this version, we'll rely on the backend not requiring one from the same origin or use a known key.\n  const [apiKey, setApiKey] = useState<string | null>(\"a_secure_test_api_key_that_is_long_enough\");\n  const queryClient = useQueryClient();\n\n  const [params, setParams] = useState<RaceFilterParams>({\n    maxFieldSize: 10,\n    minFavoriteOdds: 2.5,\n    minSecondFavoriteOdds: 4.0,\n  });\n\n  const {\n    data,\n    status: connectionStatus,\n    error: errorDetails,\n    refetch,\n  } = useQuery({\n    queryKey: ['aggregatedRaces', apiKey],\n    queryFn: () => fetchQualifiedRaces(apiKey, params),\n    enabled: backendStatus.state === 'running' && !!apiKey,\n    refetchOnWindowFocus: true,\n  });\n\n  // Update state when data is successfully fetched\n  useEffect(() => {\n    if (data) {\n      setRaces(data.races || []);\n      setAdapterErrors(data.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [data]);\n\n  const { data: liveData, isConnected: isLiveConnected } = useWebSocket<AggregatedRacesResponse>(\n    '/ws/live-updates',\n    { apiKey } // Port is not needed for same-origin WebSockets\n  );\n\n  // Effect to update state when new live data arrives\n  useEffect(() => {\n    if (liveData) {\n      console.log('Received live data update:', liveData);\n      // Update the query cache and local state with the new data\n      queryClient.setQueryData(['aggregatedRaces', apiKey], liveData);\n      setRaces(liveData.races || []);\n      setAdapterErrors(liveData.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [liveData, queryClient, apiKey]);\n\n  const [lastUpdate, setLastUpdate] = useState<Date | null>(null);\n  const [isModalOpen, setIsModalOpen] = useState(false);\n\n\n  const handleParamsChange = useCallback((newParams: RaceFilterParams) => {\n    setParams(newParams);\n  }, []);\n\n  const handleParseSuccess = (adapterName: string, parsedRaces: Race[]) => {\n    queryClient.setQueryData(['qualifiedRaces', apiKey, params], (oldData: { races: Race[], source_info: SourceInfo[] } | undefined) => {\n      if (!oldData) return { races: parsedRaces, source_info: [] };\n\n      // 1. Remove the placeholder error card for this adapter\n      const otherRaces = oldData.races.filter(race => race.source !== adapterName);\n\n      // 2. Merge the new races in\n      const updatedRaces = [...otherRaces, ...parsedRaces].sort(\n        (a, b) => new Date(a.start_time).getTime() - new Date(b.start_time).getTime()\n      );\n\n      // 3. Update source_info to remove the failed source\n      const updatedSourceInfo = oldData.source_info.filter(s => s.name !== adapterName);\n\n      return { races: updatedRaces, source_info: updatedSourceInfo };\n    });\n  };\n\n  const renderContent = () => {\n    // Priority 1: Backend process has failed.\n    if (backendStatus.state === 'error') {\n      return <BackendErrorPanel logs={backendStatus.logs} />;\n    }\n\n    if (backendStatus.state === 'stopped') {\n        return <EmptyState\n            title=\"Backend Service Stopped\"\n            message=\"The backend data service is not running. Please start it to see live race data.\"\n        />;\n    }\n\n    // Priority 2: Backend is starting or initial fetch is happening.\n    const isLoading = backendStatus.state === 'starting' || (connectionStatus === 'pending' && !data);\n    if (isLoading) {\n        return (\n            <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n                {[...Array(8)].map((_, i) => <RaceCardSkeleton key={i} />)}\n            </div>\n        );\n    }\n\n    // Priority 3: API connection is offline.\n    if (connectionStatus === 'error') {\n      try {\n        const errorInfo = JSON.parse((errorDetails as Error).message);\n        return <ErrorDisplay error={errorInfo.error} />;\n      } catch (e) {\n        return <EmptyState\n            title=\"API Connection Offline\"\n            message={(errorDetails as Error)?.message || \"The backend is running, but the dashboard could not connect to its API.\"}\n            actionButton={<button onClick={() => refetch()} className=\"mt-4 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700\">Retry Connection</button>}\n        />;\n      }\n    }\n\n    // Priority 4: No races found after a successful fetch.\n    if (!races || races.length === 0) {\n      return <EmptyState\n          title=\"No Races Found\"\n          message=\"No races matched the specified criteria for the selected date. Please try different filters.\"\n      />;\n    }\n\n    // Priority 5: Display the races (and any error placeholders).\n    return <RaceGrid races={races} />;\n  };\n\n  const getStatusIndicator = () => {\n    if (backendStatus.state === 'error') {\n      return { color: 'bg-red-500', text: 'Backend Error' };\n    }\n    if (backendStatus.state === 'stopped') {\n        return { color: 'bg-gray-500', text: 'Stopped' };\n    }\n    if (backendStatus.state === 'starting') {\n      return { color: 'bg-yellow-500', text: 'Backend Starting...' };\n    }\n    if (isLiveConnected) {\n      return { color: 'bg-cyan-500', text: 'Live' };\n    }\n    return { color: 'bg-yellow-500', text: 'Connecting...' };\n  };\n\n  const { color: statusColor, text: statusText } = getStatusIndicator();\n\n  return (\n    <>\n      <div className=\"space-y-6\">\n        <div className=\"flex justify-between items-start\">\n            <div className=\"text-left space-y-2\">\n                <h1 className=\"text-4xl font-bold text-white\">\ud83c\udfc7 Fortuna Faucet</h1>\n                <p className=\"text-slate-400\">\n                Last updated: {lastUpdate ? lastUpdate.toLocaleTimeString() : 'N/A'}\n                </p>\n            </div>\n            <div className=\"flex items-center gap-4\">\n                <button\n                    onClick={() => (connectionStatus === 'error' || backendStatus.state === 'error') && setIsModalOpen(true)}\n                    className={`flex items-center gap-2 px-3 py-1.5 rounded-full text-sm font-medium text-white ${statusColor} ${(connectionStatus === 'error' || backendStatus.state === 'error') ? 'cursor-pointer hover:opacity-80' : 'cursor-default'}`}\n                    data-testid=\"status-indicator\"\n                >\n                    <span className={`w-2.5 h-2.5 rounded-full bg-white ${isLiveConnected ? 'animate-pulse' : ''}`}></span>\n                    {statusText}\n                </button>\n            </div>\n        </div>\n\n        <RaceFilters onParamsChange={handleParamsChange} isLoading={connectionStatus === 'pending'} refetch={refetch} />\n\n        {adapterErrors.map(error => (\n          <ManualOverridePanel\n            key={error.adapterName}\n            adapterName={error.adapterName}\n            attemptedUrl={error.attemptedUrl || 'URL not available'}\n            apiKey={apiKey}\n            onParseSuccess={handleParseSuccess}\n          />\n        ))}\n\n        {renderContent()}\n      </div>\n\n      <StatusDetailModal\n        isOpen={isModalOpen}\n        onClose={() => setIsModalOpen(false)}\n        status={{ title: 'Connection Error', details: (errorDetails as Error)?.message || 'No specific error message was provided.' }}\n      />\n    </>\n  );\n});\n",
    "web_service/frontend/app/components/RaceCardSkeleton.tsx": "// web_platform/frontend/src/components/RaceCardSkeleton.tsx\nimport React from 'react';\n\nexport const RaceCardSkeleton: React.FC = () => {\n  return (\n    <div className=\"race-card-skeleton border border-gray-700 rounded-lg p-4 bg-gray-800 shadow-lg animate-pulse\">\n      {/* Skeleton Header */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <div className=\"h-7 w-28 bg-gray-700 rounded-md\"></div>\n            <div className=\"h-4 w-40 bg-gray-700 rounded-md mt-2\"></div>\n          </div>\n        </div>\n        <div className=\"h-16 w-16 bg-gray-700 rounded-full\"></div>\n      </div>\n\n      {/* Skeleton Info Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n      </div>\n\n      {/* Skeleton Runner Rows */}\n      <div className=\"space-y-2\">\n        {[...Array(3)].map((_, i) => (\n          <div key={i} className=\"runner-row rounded-md p-3\">\n            <div className=\"flex items-center justify-between\">\n              <div className=\"flex items-center gap-4 flex-1\">\n                <div className=\"w-10 h-10 rounded-full bg-gray-700\"></div>\n                <div className=\"flex flex-col space-y-2\">\n                  <div className=\"h-5 w-32 bg-gray-700 rounded-md\"></div>\n                  <div className=\"h-4 w-40 bg-gray-700 rounded-md\"></div>\n                </div>\n              </div>\n              <div className=\"text-right\">\n                <div className=\"h-6 w-16 bg-gray-700 rounded-md\"></div>\n                <div className=\"h-3 w-12 bg-gray-700 rounded-md mt-2\"></div>\n              </div>\n            </div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/Tabs.tsx": "// src/components/Tabs.tsx\n'use client';\n\nimport React, { useState } from 'react';\n\ntype Tab = {\n  label: string;\n  content: React.ReactNode;\n};\n\ntype TabsProps = {\n  tabs: Tab[];\n};\n\nexport function Tabs({ tabs }: TabsProps) {\n  const [activeTab, setActiveTab] = useState(0);\n\n  return (\n    <div>\n      <div className=\"border-b border-slate-700\">\n        <nav className=\"-mb-px flex space-x-8\" aria-label=\"Tabs\">\n          {tabs.map((tab, index) => (\n            <button\n              key={tab.label}\n              onClick={() => setActiveTab(index)}\n              className={`${\n                activeTab === index\n                  ? 'border-blue-500 text-blue-400'\n                  : 'border-transparent text-slate-400 hover:text-slate-200 hover:border-slate-500'\n              } whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm transition-colors focus:outline-none`}\n            >\n              {tab.label}\n            </button>\n          ))}\n        </nav>\n      </div>\n      <div className=\"mt-8\">{tabs[activeTab].content}</div>\n    </div>\n  );\n}\n",
    "web_service/frontend/app/layout.tsx": "// web_platform/frontend/app/layout.tsx\nimport './globals.css';\nimport type { Metadata } from 'next';\nimport { Inter } from 'next/font/google';\nimport Providers from './Providers';\n\nconst inter = Inter({ subsets: ['latin'] });\n\nexport const metadata: Metadata = {\n  title: 'Fortuna',\n  description: 'Real-time horse racing analysis.',\n};\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  return (\n    <html lang=\"en\">\n      <body className={`${inter.className} bg-white text-gray-900 dark:bg-gray-900 dark:text-gray-100`}>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  );\n}",
    "web_service/frontend/app/utils/exportManager.ts": "// web_platform/frontend/src/utils/exportManager.ts\n// import { saveAs } from 'file-saver';\n// import * as XLSX from 'xlsx';\n\nexport class ExportManager {\n  static exportToExcel(races: any[], filename: string = 'fortuna_races') {\n    //\n    // [JULES] - NOTE FOR JB AND AI EXPERTS:\n    // This feature has been temporarily disabled because the external dependency (xlsx)\n    // is hosted on a CDN (cdn.sheetjs.com) that is consistently failing during\n    // the CI/CD build process with 500 Internal Server Errors.\n    //\n    // To ensure the main application build is not blocked, I have commented out\n    // the implementation of this function. The 'xlsx' package remains in package.json,\n    // but this code will not be active until the dependency issue is resolved.\n    //\n\n    // const workbook = XLSX.utils.book_new();\n\n    // const summaryData = [\n    //   ['Total Qualified Races', races.length],\n    //   ['Generated At', new Date().toLocaleString()]\n    // ];\n    // const summarySheet = XLSX.utils.aoa_to_sheet(summaryData);\n    // XLSX.utils.book_append_sheet(workbook, summarySheet, 'Summary');\n\n    // const raceData = races.map(race => ({\n    //   'Venue': race.venue,\n    //   'Race Number': race.race_number,\n    //   'Post Time': new Date(race.start_time).toLocaleString(),\n    //   'Qualification Score': race.qualification_score || 0,\n    //   'Field Size': race.runners.filter(r => !r.scratched).length,\n    //   'Source': race.source\n    // }));\n    // const raceSheet = XLSX.utils.json_to_sheet(raceData);\n    // XLSX.utils.book_append_sheet(workbook, raceSheet, 'Races');\n\n    // XLSX.writeFile(workbook, `${filename}_${Date.now()}.xlsx`);\n    console.warn(\"Excel export is temporarily disabled due to an external dependency issue.\");\n    alert(\"The Excel export feature is temporarily disabled due to an unreliable external dependency. Please try again later.\");\n  }\n}\n",
    "windows_service.py": "# windows_service.py\nimport os\nimport socket\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n\nclass FortunaBackendService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaFaucetBackend\"\n    _svc_display_name_ = \"Fortuna Faucet Racing Analysis Service\"\n    _svc_description_ = \"Background service for continuous racing data monitoring.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.stop_event = win32event.CreateEvent(None, 0, 0, None)\n        self.backend_process = None\n        socket.setdefaulttimeout(60)\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        win32event.SetEvent(self.stop_event)\n        if self.backend_process:\n            self.backend_process.terminate()\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        install_dir = Path(__file__).parent.resolve()\n        venv_python = install_dir / \".venv\" / \"Scripts\" / \"python.exe\"\n        api_module_dir = install_dir / \"python_service\"\n\n        env = os.environ.copy()\n        env_file = install_dir / \".env\"\n        if env_file.exists():\n            with open(env_file) as f:\n                for line in f:\n                    if \"=\" in line and not line.startswith(\"#\"):\n                        key, value = line.strip().split(\"=\", 1)\n                        env[key] = value.strip('\"')\n\n        self.backend_process = subprocess.Popen(\n            [\n                str(venv_python),\n                \"-m\",\n                \"uvicorn\",\n                \"api:app\",\n                \"--host\",\n                \"127.0.0.1\",\n                \"--port\",\n                \"8000\",\n            ],\n            cwd=str(api_module_dir),\n            env=env,\n        )\n\n        win32event.WaitForSingleObject(self.stop_event, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaBackendService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaBackendService)\n"
}