{
    ".github/actions/run-asgi-diagnostics/action.yml": "name: 'Run ASGI Import Killer Diagnostics'\ndescription: 'Runs a multi-phase diagnostic to verify Python ASGI application imports.'\n\ninputs:\n  python-version:\n    description: 'The version of Python to use.'\n    required: true\n  backend-dir:\n    description: 'The directory of the backend service to test.'\n    required: true\n  backend-module-path:\n    description: 'The Python module path for the backend service (e.g., web_service.backend).'\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n      - name: \u2699\ufe0f Setup Python (EXACT VERSION)\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ inputs.python-version }}\n\n      - name: \ud83d\udccb Capture Python Info\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          Write-Host \"Python executable: $(which python)\" -ForegroundColor Cyan\n          python --version\n          python -m site\n          python -c \"import sys; print('Prefix:', sys.prefix); print('Base prefix:', sys.base_prefix)\"\n\n      - name: \ud83d\udce5 Install Requirements (Exactly as Backend Build)\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel --quiet\n\n          Write-Host \"Installing requirements.txt...\" -ForegroundColor Cyan\n          pip install -r (Join-Path \"${{ inputs.backend-dir }}\" \"requirements.txt\") -v 2>&1 | Tee-Object \"install-requirements.log\"\n\n          if (Test-Path (Join-Path \"${{ inputs.backend-dir }}\" \"requirements-dev.txt\")) {\n            Write-Host \"Installing requirements-dev.txt...\" -ForegroundColor Cyan\n            pip install -r (Join-Path \"${{ inputs.backend-dir }}\" \"requirements-dev.txt\") -v 2>&1 | Tee-Object -Append \"install-requirements.log\"\n          }\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Host \"\u274c pip install failed\" -ForegroundColor Red\n            exit 1\n          }\n\n          Write-Host \"\u2705 All dependencies installed\" -ForegroundColor Green\n\n      - name: \ud83d\udce6 Capture Installed Packages\n        shell: pwsh\n        run: |\n          pip list | Tee-Object \"installed-packages.txt\"\n          pip freeze | Tee-Object \"pip-freeze.txt\"\n\n      - name: \ud83d\udc0d Set PYTHONPATH\n        shell: pwsh\n        run: |\n          echo \"PYTHONPATH=${{ github.workspace }}\" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n          Write-Host \"PYTHONPATH set to ${{ github.workspace }}\"\n\n      - name: \ud83e\uddea PHASE 1 System Imports\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 1 SYSTEM-LEVEL IMPORTS\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('os', 'filesystem'), ('sys', 'system'), ('json', 'serialization'),\",\n            \"    ('asyncio', 'async I/O'), ('pathlib', 'paths'), ('typing', 'type hints'),\",\n            \"    ('importlib', 'import utilities')\",\n            ']',\n            'failed = []',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:20} [{desc}]\")',\n            '    except ImportError as e:',\n            '        print(f\"\u274c {mod_name:20} ImportError: {e}\")',\n            '        failed.append(mod_name)',\n            'if failed:',\n            '    print(f\"\\n\u274c {len(failed)} system imports failed\")',\n            '    sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 1 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 2 Web Framework Core\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'import traceback',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 2 WEB FRAMEWORK CORE\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('fastapi', 'web framework'),\",\n            \"    ('uvicorn', 'ASGI server'),\",\n            \"    ('starlette', 'ASGI toolkit'),\",\n            \"    ('starlette.applications', 'ASGI app'),\",\n            \"    ('starlette.routing', 'routing'),\",\n            ']',\n            'failed = []',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except ImportError as e:',\n            '        print(f\"\u274c {mod_name:30} ImportError: {e}\")',\n            '        failed.append((mod_name, str(e)))',\n            '    except Exception as e:',\n            '        print(f\"\u26a0\ufe0f  {mod_name:30} {type(e).__name__}: {e}\")',\n            'if failed:',\n            '    print(f\"\\n\u274c {len(failed)} core framework imports failed\")',\n            '    for mod, err in failed:',\n            '        print(f\"  - {mod}\")',\n            '    sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 2 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 3 Pydantic & Data Validation\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 3 PYDANTIC & DATA VALIDATION\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('pydantic', 'validation'),\",\n            \"    ('pydantic_core', 'core'),\",\n            \"    ('pydantic_settings', 'settings'),\",\n            ']',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except Exception as e:',\n            '        print(f\"\u274c {mod_name:30} {type(e).__name__}: {e}\")',\n            '        sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 3 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 4 Async/IO Utilities\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 4 ASYNC/IO UTILITIES\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('anyio', 'async compat'),\",\n            \"    ('httpcore', 'HTTP core'),\",\n            \"    ('httpx', 'HTTP client'),\",\n            \"    ('aiosqlite', 'async DB'),\",\n            ']',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except Exception as e:',\n            '        print(f\"\u274c {mod_name:30} {type(e).__name__}: {e}\")',\n            '        sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 4 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 5 Optional Dependencies (non-critical)\n        shell: pwsh\n        continue-on-error: true\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 5 OPTIONAL DEPENDENCIES (non-critical)\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('slowapi', 'rate limiting'),\",\n            \"    ('structlog', 'logging'),\",\n            \"    ('tenacity', 'retries'),\",\n            ']',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except Exception as e:',\n            '        print(f\"\u26a0\ufe0f  {mod_name:30} not critical: {type(e).__name__}\")',\n            'print(f\"\\n\u2705 Phase 5 complete (warnings OK)\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n\n      - name: \ud83e\uddea PHASE 6 Application Directory Structure\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n\n          $script = @(\n            'import os',\n            'from pathlib import Path',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 6 APPLICATION DIRECTORY STRUCTURE\")',\n            'print(\"=\"*80)',\n            'cwd = Path.cwd()',\n            'backend_dir = cwd / \"${{ inputs.backend-dir }}\"',\n            'print(f\"\\nCurrent directory: {cwd}\")',\n            'print(f\"\\nBackend directory exists: {backend_dir.exists()}\")',\n            'if backend_dir.exists():',\n            '    print(f\"  Contents:\")',\n            '    for item in backend_dir.iterdir():',\n            '        print(f\"    - {item.name}\")',\n            '    main_py = backend_dir / \"main.py\"',\n            '    api_py = backend_dir / \"api.py\"',\n            '    print(f''\\n  main.py: {main_py.stat().st_size if main_py.exists() else \"N/A\"} bytes'')',\n            '    print(f''  api.py: {api_py.stat().st_size if api_py.exists() else \"N/A\"} bytes'')'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n\n      - name: \ud83e\uddea PHASE 7 CRITICAL - Application Module Imports\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'import traceback',\n            'import importlib',\n            'from pathlib import Path',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 7 APPLICATION MODULE IMPORTS (CRITICAL)\")',\n            'print(\"=\"*80)',\n            'backend_module_path = \"${{ inputs.backend-module-path }}\"',\n            'print(f\"\\n[Step 1] Dynamically importing module: {backend_module_path}\")',\n            'try:',\n            '    backend_module = importlib.import_module(backend_module_path)',\n            '    print(f\"\u2705 {backend_module_path} imported successfully\")',\n            'except Exception as e:',\n            '    print(f\"\u274c FATAL: {backend_module_path} import failed\")',\n            '    print(f\"   Error: {type(e).__name__}: {e}\")',\n            '    traceback.print_exc()',\n            '    sys.exit(1)',\n            'print(''\\\\n[Step 2] Retrieving \"app\" object from the API submodule...'')',\n            'api_module_path = f\"{backend_module_path}.api\"',\n            'try:',\n            '    api_module = importlib.import_module(api_module_path)',\n            '    app = getattr(api_module, \"app\")',\n            '    print(f\"\u2705 app object retrieved from {api_module_path}\")',\n            '    print(f\"   Type: {type(app)}\")',\n            '    print(f\"   Class: {app.__class__.__name__}\")',\n            '    print(f\"   Module: {app.__class__.__module__}\")',\n            'except (ImportError, AttributeError) as e:',\n            '    print(f\"\u274c FATAL: Could not get app object from {api_module_path}\")',\n            '    print(f\"   Error: {type(e).__name__}: {e}\")',\n            '    traceback.print_exc()',\n            '    sys.exit(1)',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"\u2705 ALL APPLICATION IMPORTS SUCCESSFUL\")',\n            'print(\"=\"*80)',\n            'print(\"\\nThe ASGI app is fully importable.\")',\n            'print(\"Uvicorn should be able to load it successfully.\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) {\n            Write-Host \"\u274c APPLICATION IMPORT TEST FAILED\" -ForegroundColor Red\n            exit 1\n          }\n\n      - name: \ud83d\udccb Generate ASGI Diagnostic Report\n        if: always()\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $report = @()\n          $report += \"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\"\n          $report += \"\u2551              ASGI IMPORT KILLER - DIAGNOSTIC REPORT                        \u2551\"\n          $report += \"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\"\n          $report += \"\"\n          $report += \"Timestamp: $(Get-Date -Format 'o')\"\n          $report += \"Python: $(python --version)\"\n          $report += \"\"\n          $report += \"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\"\n          $result = if ($LASTEXITCODE -eq 0) { 'PASS \u2705' } else { 'FAIL \u274c' }\n          $report += \"\u2502 RESULT: $result \u2502\"\n          $report += \"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\"\n          $report += \"\"\n          $report += \"If this passed:\"\n          $report += \"  \u2705 All required dependencies are installed\"\n          $report += \"  \u2705 python_service.main is importable\"\n          $report += \"  \u2705 FastAPI app is accessible\"\n          $report += \"  \u2705 The executable should work\"\n          $report += \"  \u2705 Uvicorn WILL be able to load the app\"\n          $report += \"\"\n          $report += \"If this failed:\"\n          $report += \"  \u274c See error output above for the exact problem\"\n          $report += \"  \u274c Fix the import error in your code\"\n          $report += \"  \u274c Common issues:\"\n          $report += \"     - Missing dependency in requirements.txt\"\n          $report += \"     - Syntax error in api.py or main.py\"\n          $report += \"     - Circular import in api.py\"\n          $report += \"     - api.py imports a module that fails\"\n          $report += \"\"\n          $report += \"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\"\n          $report | Tee-Object \"asgi-diagnostic-report.txt\"\n\n      - name: \ud83d\udce4 Upload Diagnostic Artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: asgi-import-diagnostics-${{ github.run_id }}\n          path: |\n            install-requirements.log\n            installed-packages.txt\n            pip-freeze.txt\n            asgi-diagnostic-report.txt\n          retention-days: 30\n          if-no-files-found: warn\n",
    ".github/actions/run-smoke-test/action.yml": "name: 'Run Smoke Test (Socket Handover)'\ndescription: 'Launches an executable, waits for a socket to be bound, and performs a health check.'\ninputs:\n  exe-path:\n    description: 'Path to the executable to launch'\n    required: true\n  service-port:\n    description: 'Port to wait for on localhost'\n    required: true\n  health-endpoint:\n    description: 'Health endpoint to check (e.g., /health)'\n    required: false\n    default: '/health'\n  api-key:\n    description: 'API key for the service'\n    required: false\nruns:\n  using: 'composite'\n  steps:\n    - name: Run Python Smoke Test\n      shell: python\n      run: |\n        import os, sys, subprocess, socket, time, threading\n        from contextlib import closing\n\n        # --- CONFIGURATION ---\n        EXE_PATH = os.environ.get(\"INPUT_EXE-PATH\")\n        HOST = \"127.0.0.1\"\n        PORT = int(os.environ.get(\"INPUT_SERVICE-PORT\"))\n        HEALTH_ENDPOINT = os.environ.get(\"INPUT_HEALTH-ENDPOINT\", \"/health\")\n        API_KEY = os.environ.get(\"INPUT_API-KEY\", \"default-key\")\n        STARTUP_TIMEOUT = 90  # seconds\n        POLL_INTERVAL = 0.5   # seconds\n        STDOUT_LOG = \"service-logs/stdout.txt\"\n        STDERR_LOG = \"service-logs/stderr.txt\"\n        PID_FILE = \"service.pid\"\n\n        # --- UTILITY FUNCTIONS ---\n        def print_banner(message):\n            print(f\"\\\\n{'='*25} {message} {'='*25}\")\n\n        def check_socket(host, port):\n            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:\n                return sock.connect_ex((host, port)) == 0\n\n        def tail_file(filename, lines=10):\n            try:\n                with open(filename, \"r\") as f:\n                    content = f.readlines()\n                return \"\".join(content[-lines:])\n            except FileNotFoundError:\n                return \"Log file not found.\"\n            except Exception as e:\n                return f\"Error reading log: {e}\"\n\n        # --- MAIN LOGIC ---\n        def main():\n            print_banner(\"SOCKET HANDOVER PROTOCOL INITIATED\")\n\n            # 1. Pre-flight check: Find a free port\n            print(f\"Checking if port {PORT} is available...\")\n            if check_socket(HOST, PORT):\n                print(f\"\u274c FATAL: Port {PORT} is already in use before starting the service.\")\n                sys.exit(1)\n            print(f\"\u2705 Port {PORT} is free.\")\n\n            # 2. Launch the service executable\n            print_banner(f\"LAUNCHING {os.path.basename(EXE_PATH)}\")\n            launch_env = os.environ.copy()\n            launch_env[\"FORTUNA_PORT\"] = str(PORT)\n            launch_env[\"API_KEY\"] = API_KEY\n            launch_env[\"FORTUNA_ENV\"] = \"smoke-test\"\n            launch_env[\"PYTHONUNBUFFERED\"] = \"1\"\n\n            if not os.path.exists(EXE_PATH):\n                print(f\"\u274c FATAL: Executable not found at {EXE_PATH}\")\n                sys.exit(1)\n\n            with open(STDOUT_LOG, \"wb\") as out_log, open(STDERR_LOG, \"wb\") as err_log:\n                process = subprocess.Popen([EXE_PATH], env=launch_env, stdout=out_log, stderr=err_log)\n\n            with open(PID_FILE, \"w\") as f:\n                f.write(str(process.pid))\n            print(f\"\ud83d\ude80 Service process started with PID: {process.pid}\")\n            print(f\"   - stdout -> {STDOUT_LOG}\")\n            print(f\"   - stderr -> {STDERR_LOG}\")\n\n            # 3. Wait for the socket to be bound\n            print_banner(f\"WAITING FOR SOCKET BIND ON {HOST}:{PORT}\")\n            start_time = time.time()\n            bound = False\n            while time.time() - start_time < STARTUP_TIMEOUT:\n                if process.poll() is not None:\n                    print(f\"\u274c FATAL: Process terminated unexpectedly with exit code {process.poll()}.\")\n                    break\n                if check_socket(HOST, PORT):\n                    print(f\"\u2705 Socket is now bound! (Took {time.time() - start_time:.2f}s)\")\n                    bound = True\n                    break\n                time.sleep(POLL_INTERVAL)\n                print(f\"   ... waiting ({time.time() - start_time:.1f}s)\", end=\"\\\\r\")\n\n            if not bound:\n                print(f\"\u274c FATAL: Timed out after {STARTUP_TIMEOUT}s waiting for port {PORT} to be bound.\")\n\n            # 4. Final Verification and Diagnostics\n            print_banner(\"FINAL DIAGNOSTICS\")\n            print(f\"STDOUT (Last 10 lines):\\\\n{tail_file(STDOUT_LOG)}\")\n            print(f\"STDERR (Last 10 lines):\\\\n{tail_file(STDERR_LOG)}\")\n\n            if process.poll() is not None:\n                print(\"\u274c Final state: PROCESS IS DEAD.\")\n                sys.exit(1)\n            else:\n                print(\"\u2705 Final state: PROCESS IS ALIVE.\")\n\n            if not bound:\n                print(\"\u274c Final state: SOCKET NOT BOUND.\")\n                sys.exit(1)\n            else:\n                print(\"\u2705 Final state: SOCKET IS BOUND.\")\n\n            print_banner(\"SOCKET HANDOVER SUCCESSFUL\")\n            sys.exit(0)\n\n        if __name__ == \"__main__\":\n            main()\n      env:\n        INPUT_EXE-PATH: ${{ inputs.exe-path }}\n        INPUT_SERVICE-PORT: ${{ inputs.service-port }}\n        INPUT_HEALTH-ENDPOINT: ${{ inputs.health-endpoint }}\n        INPUT_API-KEY: ${{ inputs.api-key }}",
    ".github/dependabot.yml": "# System Timestamp: 2025-11-29 13:19:26.933797\n# To get started with Dependabot version updates, you'll need to specify which\n# package ecosystems to update and where the package manifests are located.\n# Please see the documentation for all configuration options:\n# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates\n\nversion: 2\nupdates:\n  - package-ecosystem: \"pip\" # See documentation for possible values\n    directory: \"/\" # Location of package manifests\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/web_platform/frontend\"\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/electron\"\n    schedule:\n      interval: \"daily\"\n",
    ".github/workflows/build-electron-msi-gpt5.yml": "# System Timestamp: 2025-12-07 15:30:00\nname: \ud83d\udd28 Build Electron MSI Installer (Production)\n\non:\n  push:\n    branches: [\"main\"]\n\nconcurrency:\n  group: build-electron-msi-${{ github.ref }}\n  cancel-in-progress: false\n\nenv:\n  NODE_VERSION: '18'\n  PYTHON_VERSION: '3.11'\n  ELECTRON_BUILDER_CACHE: ${{ github.workspace }}/.cache/electron-builder\n  BACKEND_DIR: 'web_service/backend'\n  PYTHONUTF8: '1'\n  API_KEY: mock_key\n  FORTUNA_ENV: smoke-test\n  TVG_API_KEY: mock\n  GREYHOUND_API_URL: http://mock\n\njobs:\n  validate-environment:\n    name: \u2705 Pre-flight Validation\n    runs-on: windows-latest\n    timeout-minutes: 30\n    outputs:\n      node_version: ${{ steps.versions.outputs.node }}\n      python_version: ${{ steps.versions.outputs.python }}\n      build_id: ${{ steps.versions.outputs.build_id }}\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    steps:\n      - name: \ud83d\udce5 Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: \ud83d\udd0d Verify Critical Files Exist\n        shell: pwsh\n        run: |\n          # FIX: Removed dynamic spec file from check\n          $criticalFiles = @(\n            'electron/electron-builder-config.yml',\n            'web_platform/frontend/package.json',\n            '${{ env.BACKEND_DIR }}/requirements-dev.txt',\n            'electron/package.json'\n          )\n\n          $manifest = @{\n            timestamp = Get-Date -Format 'o'\n            checks = @()\n            errors = @()\n          }\n\n          foreach ($file in $criticalFiles) {\n            if (Test-Path -LiteralPath $file) {\n              $hash = (Get-FileHash -LiteralPath $file -Algorithm SHA256).Hash\n              $manifest.checks += @{\n                file = $file\n                exists = $true\n                sha256 = $hash\n              }\n              Write-Host \"\u2713 $file ($hash.Substring(0,8)...)\"\n            } else {\n              $manifest.errors += @{ file = $file; error = 'MISSING' }\n              Write-Error \"\u2717 CRITICAL: $file not found\"\n              exit 1\n            }\n          }\n\n          $manifest | ConvertTo-Json | Out-File -FilePath \".\\\\validation-manifest.json\"\n          Write-Host \"Manifest saved\"\n\n      - name: \ud83d\udcca Capture Version Information\n        id: versions\n        shell: pwsh\n        run: |\n          $nodeVersion = node --version\n          $pythonVersion = python --version\n          $buildId = \"${{ github.run_id }}-${{ github.run_attempt }}\"\n\n          Write-Host \"Node: $nodeVersion\"\n          Write-Host \"Python: $pythonVersion\"\n          Write-Host \"Build ID: $buildId\"\n\n          \"node=$nodeVersion\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"python=$pythonVersion\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"build_id=$buildId\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n      - name: \ud83d\udea8 Capture Environment State\n        if: always()\n        shell: pwsh\n        run: |\n          New-Item -ItemType Directory -Path \"debug-artifacts\" -Force | Out-Null\n\n          # System info\n          systeminfo | Out-File \"debug-artifacts/systeminfo.txt\"\n          Get-CimInstance -ClassName Win32_OperatingSystem | Select-Object Caption, Version, BuildNumber | Out-File \"debug-artifacts/os-version.txt\"\n\n          # Disk space\n          $free = (Get-Volume | Where DriveLetter -eq 'C').SizeRemaining\n          if ($free -lt 10GB) { Write-Error \"Insufficient disk space\"; exit 1 }\n          Get-Volume | Out-File \"debug-artifacts/disk-space.txt\"\n\n          # File permissions - FIX: $. to $_.\n          Get-ChildItem -Recurse -Include \"*.yml\", \"*.spec\", \"*.json\" -ErrorAction SilentlyContinue |\n            ForEach-Object { \"{0} {1}\" -f $_.FullName, $_.Length } |\n            Out-File \"debug-artifacts/file-manifest.txt\"\n\n      - name: Derive Build Metadata\n        id: meta\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $ref = \"${{ github.ref }}\"\n          if ($ref -like 'refs/tags/v*') {\n            $semver = $ref -replace 'refs/tags/v', ''\n          } else {\n            $semver = \"0.0.${{ github.run_number }}\"\n          }\n          $shortSha = \"${{ github.sha }}\".Substring(0,7)\n          \"semver=$semver\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"short_sha=$shortSha\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\ud83d\udd16 Version: $semver ($shortSha)\"\n\n      - name: \ud83d\udce4 Upload Validation Artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: validation-${{ github.run_id }}\n          path: |\n            validation-manifest.json\n            debug-artifacts/\n          retention-days: 14\n\n  build-python-service:\n    name: \ud83d\udc0d Build Python Service Bundle\n    runs-on: windows-latest\n    timeout-minutes: 20\n    needs: validate-environment\n    steps:\n      - name: \ud83d\udce5 Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: \ud83d\udc0d Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n\n      - name: \ud83d\udce6 Install Python Dependencies\n        shell: pwsh\n        run: |\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -r ${{ env.BACKEND_DIR }}/requirements-dev.txt\n          Write-Host \"\u2713 Python dependencies installed\"\n\n      - name: \ud83e\uddea Run Unit Tests (Quality Gate)\n        shell: pwsh\n        run: |\n          Write-Host \"Installing Test Dependencies...\"\n          pip install pytest pytest-asyncio httpx asgi-lifespan fakeredis\n          # 1. Define the Failure Threshold\n          $MAX_ALLOWED_FAILURES = 30\n          Write-Host \"Running Test Suite (Threshold: $MAX_ALLOWED_FAILURES failures)...\"\n          # 2. Run Pytest and generate an XML report.\n          # We use 'cmd /c' and '|| true' to ensure the script doesn't die immediately on exit code 1.\n          cmd /c \"pytest ${{ env.BACKEND_DIR }}/tests --junitxml=test-report.xml\" || Write-Host \"Pytest finished with issues.\"\n          # 3. Parse the XML Report\n          if (Test-Path \"test-report.xml\") {\n              [xml]$xml = Get-Content \"test-report.xml\"\n              # Sum up failures and errors\n              $failures = 0\n              $errors = 0\n              # Handle different XML structures (sometimes root is testsuites, sometimes testsuite)\n              if ($xml.testsuites) {\n                  $failures = [int]$xml.testsuites.failures\n                  $errors = [int]$xml.testsuites.errors\n              } elseif ($xml.testsuite) {\n                  $failures = [int]$xml.testsuite.failures\n                  $errors = [int]$xml.testsuite.errors\n              }\n              $total_issues = $failures + $errors\n              Write-Host \"----------------------------------------\"\n              Write-Host \"\ud83d\udcca TEST RESULTS SUMMARY\"\n              Write-Host \"   Failures: $failures\"\n              Write-Host \"   Errors:   $errors\"\n              Write-Host \"   Total:    $total_issues\"\n              Write-Host \"   Limit:    $MAX_ALLOWED_FAILURES\"\n              Write-Host \"----------------------------------------\"\n              # 4. The Decision Logic\n              if ($total_issues -gt $MAX_ALLOWED_FAILURES) {\n                  Write-Error \"\u274c CRITICAL: Too many tests failed ($total_issues). Limit is $MAX_ALLOWED_FAILURES.\"\n                  exit 1\n              } else {\n                  Write-Host \"\u2705 ACCEPTABLE: Failure count is within tolerance. Proceeding with build...\" -ForegroundColor Green\n                  exit 0 # Explicitly exit with success code to override pytest's failure code\n              }\n          } else {\n              Write-Error \"\u274c FATAL: No test report generated. Pytest failed to start.\"\n              exit 1\n          }\n\n      - name: \u2622\ufe0f NUCLEAR FIX -- Ensure Python Package Structure & Double Injection\n        shell: pwsh\n        run: |\n          Set-Content -Path \"web_service/backend/__init__.py\" -Value \"# This file is intentionally non-empty to ensure package recognition.\"\n          Write-Host \"\u2705 Ensured non-empty __init__.py files exist for package discovery.\"\n\n      - name: Create Dynamic fortuna-backend-electron.spec for PyInstaller\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $entry_point = '${{ env.BACKEND_DIR }}/main.py'.Replace('\\\\', '/')\n          $other_service = \"python_service\"\n\n          # Ensure absolute paths for the init files\n          $backend_init = (Resolve-Path \"web_service/backend/__init__.py\").Path.Replace('\\\\', '/')\n\n          $specContent = @(\n            \"# -- mode: python ; coding: utf-8 --\",\n            \"import os\",\n            \"from pathlib import Path\",\n            \"from PyInstaller.utils.hooks import collect_data_files, collect_submodules\",\n            \"\",\n            \"block_cipher = None\",\n            \"project_root = Path(os.getcwd())\",\n            \"\",\n            \"datas = []\",\n            \"datas += collect_data_files('uvicorn')\",\n            \"datas += collect_data_files('slowapi')\",\n            \"datas += collect_data_files('structlog')\",\n            \"\",\n            \"hiddenimports = collect_submodules('web_service.backend')\",\n            \"hiddenimports += [\",\n            \" 'uvicorn.logging', 'uvicorn.loops.auto', 'uvicorn.lifespan.on',\",\n            \" 'uvicorn.protocols.http.h11_impl', 'uvicorn.protocols.websockets.wsproto_impl',\",\n            \" 'fastapi.routing', 'starlette.staticfiles', 'anyio._backends._asyncio',\",\n            \" 'httpcore', 'httpx', 'slowapi', 'structlog', 'tenacity', 'aiosqlite',\",\n            \" 'pydantic_core', 'pydantic_settings.sources', 'win32timezone'\",\n            \"]\",\n            \"\",\n            \"a = Analysis(\",\n            \" ['$entry_point'],\",\n            \" pathex=[str(project_root)],\",\n            \" binaries=[],\",\n            \" datas=datas,\",\n            \" hiddenimports=hiddenimports,\",\n            \" hookspath=[],\",\n            \" runtime_hooks=[],\",\n            \" excludes=['tests', 'pytest', '$other_service'],\",\n            \" win_no_prefer_redirects=False,\",\n            \" win_private_assemblies=False,\",\n            \" cipher=block_cipher,\",\n            \" noarchive=False\",\n            \")\",\n            \"\",\n            \"# \u2622\ufe0f NUCLEAR OVERRIDE: Force init files into the PYZ archive\",\n            \"a.pure += [\",\n            \" ('web_service.backend', '$backend_init', 'PYMODULE')\",\n            \"]\",\n            \"\",\n            \"pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\",\n            \"exe = EXE(\",\n            \" pyz,\",\n            \" a.scripts,\",\n            \" a.binaries,\",\n            \" a.zipfiles,\",\n            \" a.datas,\",\n            \" [],\",\n            \" name='fortuna-backend',\",\n            \" debug=False,\",\n            \" bootloader_ignore_signals=False,\",\n            \" strip=False,\",\n            \" upx=True,\",\n            \" runtime_tmpdir=None,\",\n            \" console=True,\",\n            \" disable_windowed_traceback=False,\",\n            \" argv_emulation=False,\",\n            \" target_arch=None,\",\n            \" codesign_identity=None,\",\n            \" entitlements_file=None\",\n            \")\"\n          )\n          Set-Content -Path \"fortuna-backend-electron.spec\" -Value $specContent\n          Write-Host \"\u2705 Dynamically generated 'fortuna-backend-electron.spec' with PYZ Injection.\"\n\n      - name: \ud83d\udd28 Build Python Service with PyInstaller\n        shell: pwsh\n        run: |\n          $specFile = 'fortuna-backend-electron.spec'\n\n          if (-not (Test-Path $specFile)) {\n            Write-Error \"Spec file not found: $specFile\"\n            exit 1\n          }\n\n          pyinstaller `\n            --distpath \".\\\\dist\\\\service\" `\n            --workpath \".\\\\build\\\\service\" `\n            --clean `\n            $specFile\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"PyInstaller failed with code $LASTEXITCODE\"\n            exit $LASTEXITCODE\n          }\n\n          $serviceExe = Get-ChildItem -Path \".\\\\dist\\\\service\" -Filter \"*.exe\" | Select-Object -First 1\n          if ($null -eq $serviceExe) {\n            Write-Error \"No executable generated by PyInstaller\"\n            exit 1\n          }\n\n          Write-Host \"\u2713 Service executable: $($serviceExe.FullName) ($('{0:N0}' -f $serviceExe.Length) bytes)\"\n\n      - name: \ud83e\uddea Verify Service Executable\n        shell: pwsh\n        run: |\n          $serviceExe = Get-ChildItem -Path \".\\\\dist\\\\service\" -Filter \"*.exe\" | Select-Object -First 1\n\n          if ($null -eq $serviceExe) {\n            Write-Error \"Service executable not found\"\n            exit 1\n          }\n\n          # Verify it's a valid PE executable\n          $bytes = [System.IO.File]::ReadAllBytes($serviceExe.FullName)\n          if ($bytes[0] -ne 0x4D -or $bytes[1] -ne 0x5A) {\n            Write-Error \"Invalid PE executable signature\"\n            exit 1\n          }\n\n          Write-Host \"\u2713 Service executable is valid PE binary\"\n\n      - name: \ud83d\udce4 Upload Service Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: python-service-${{ needs.validate-environment.outputs.build_id }}\n          path: dist/service/\n          retention-days: 1\n\n      - name: \ud83d\udea8 Upload Failure Diagnostics\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: python-build-failure-logs-${{ needs.validate-environment.outputs.build_id }}\n          path: |\n            build/service/\n            spec-working/\n          retention-days: 30\n\n  build-frontend:\n    name: \ud83c\udfa8 Build Web Frontend\n    runs-on: windows-latest\n    timeout-minutes: 15\n    needs: validate-environment\n    steps:\n      - name: \ud83d\udce5 Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: \ud83d\udce6 Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: 'web_platform/frontend/package.json'\n\n      - name: \ud83d\udce5 Install Frontend Dependencies\n        shell: pwsh\n        working-directory: web_platform/frontend\n        run: |\n          npm ci --prefer-offline --no-audit\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"npm ci failed\"\n            exit 1\n          }\n          npm list --depth=0\n\n      - name: \ud83d\udd28 Build Frontend\n        shell: pwsh\n        working-directory: web_platform/frontend\n        run: |\n          npm run build 2>&1\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"Frontend build failed\"\n            exit 1\n          }\n\n          $outDir = Get-Item -Path \"out\" -ErrorAction SilentlyContinue\n          if ($null -eq $outDir) {\n            Write-Error \"No out/ directory generated\"\n            exit 1\n          }\n\n          $fileCount = (Get-ChildItem -Recurse -Path \"out\" | Measure-Object).Count\n          Write-Host \"\u2713 Frontend built: $fileCount files\"\n\n      - name: \ud83d\udce4 Upload Frontend Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-dist-${{ needs.validate-environment.outputs.build_id }}\n          path: web_platform/frontend/out/\n          retention-days: 1\n\n      - name: \ud83d\udea8 Upload Failure Diagnostics\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build-failure-logs-${{ needs.validate-environment.outputs.build_id }}\n          path: |\n            web_platform/frontend/.next/\n            web_platform/frontend/npm-debug.log\n          if-no-files-found: ignore\n          retention-days: 30\n\n  verify-assets:\n    name: '\ud83d\uddbc\ufe0f Verify Critical Build Assets'\n    runs-on: windows-latest\n    timeout-minutes: 5\n    needs: validate-environment\n    steps:\n      - name: \ud83d\udce5 Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: \ud83e\uddd0 Forensic Asset Verification\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $asset_manifest = @(\n            @{ Path = \"electron/assets/icon.ico\"; Purpose = \"Main Application & MSI Icon\" }\n          )\n\n          Write-Host \"--- Asset Verification Forensics ---\"\n          $all_assets_found = $true\n\n          foreach ($asset in $asset_manifest) {\n            Write-Host \"Checking for: $($asset.Path) ($($asset.Purpose))\"\n            if (Test-Path -LiteralPath $asset.Path) {\n              $file_info = Get-Item -LiteralPath $asset.Path\n              $hash = (Get-FileHash -LiteralPath $asset.Path -Algorithm SHA256).Hash\n              Write-Host \" \u2705 FOUND: $($file_info.Length) bytes, SHA256: $($hash.Substring(0,12))\" -ForegroundColor Green\n            } else {\n              Write-Host \" \u274c MISSING\" -ForegroundColor Red\n              $all_assets_found = $false\n            }\n          }\n\n          if (-not $all_assets_found) {\n            Write-Error \"CRITICAL: One or more required assets are missing.\"\n            Write-Host \"\\\\n--- Filesystem State ---\"\n            Write-Host \"Listing contents of 'electron/assets' directory for debugging:\"\n            Get-ChildItem -Path \"electron/assets\" -Recurse | ForEach-Object {\n              Write-Host \" - $($_.FullName.Replace($env:GITHUB_WORKSPACE, ''))\"\n            }\n            exit 1\n          }\n\n          Write-Host \"\\\\n\u2705 All critical assets verified.\" -ForegroundColor Green\n  build-electron-msi:\n    name: \ud83d\ude80 Build Electron MSI Package\n    runs-on: windows-latest\n    timeout-minutes: 30\n    needs: [validate-environment, build-python-service, build-frontend, verify-assets]\n    steps:\n      - name: \ud83d\udce5 Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: \ud83d\udce6 Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: 'electron/package.json'\n\n      - name: \ud83d\udce5 Download Python Service\n        uses: actions/download-artifact@v4\n        with:\n          name: python-service-${{ needs.validate-environment.outputs.build_id }}\n          path: python-service-bin\n\n      - name: \ud83d\udce5 Download Frontend Dist\n        uses: actions/download-artifact@v4\n        with:\n          name: frontend-dist-${{ needs.validate-environment.outputs.build_id }}\n          path: web_platform/frontend/out\n\n      - name: \ud83d\ude9a Stage Backend for Electron Builder\n        shell: pwsh\n        run: |\n          # FIX: The config looks for '../python-service-bin' relative to the 'electron' dir,\n          # so we place the artifact at the repo root.\n          $dest = \"python-service-bin\"\n          New-Item -ItemType Directory -Path $dest -Force | Out-Null\n          Move-Item -Path \"python-service-bin/*\" -Destination $dest -Force\n          Write-Host \"\u2705 Backend staged to root '$dest' directory.\"\n          Write-Host \"Contents:\"\n          Get-ChildItem -Path $dest -Recurse | ForEach-Object { Write-Host \" - $($_.Name)\" }\n\n      - name: \ud83d\udce5 Install Electron Dependencies\n        shell: pwsh\n        working-directory: electron\n        run: |\n          npm ci --prefer-offline --no-audit\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"npm ci failed\"\n            exit 1\n          }\n\n      - name: '\ud83e\uddd0 Forensically Guarantee Icon Paths'\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $ErrorActionPreference = \"Stop\"\n\n          # Install and import the required module for YAML parsing\n          Install-Module -Name powershell-yaml -Force -Scope CurrentUser -ErrorAction Stop\n          Import-Module powershell-yaml\n\n          $configPath = 'electron/electron-builder-config.yml'\n          Write-Host \"--- Icon Path Forensics ---\"\n          Write-Host \"Verifying and correcting icon paths in: $configPath\"\n\n          # 1. Verify the icon file physically exists\n          $iconPath = \"electron/assets/icon.ico\"\n          if (-not (Test-Path -LiteralPath $iconPath)) {\n            Write-Error \"CRITICAL: The primary icon file is missing at '$iconPath'.\"\n            exit 1\n          }\n          $absoluteIconPath = (Resolve-Path -LiteralPath $iconPath).Path\n          Write-Host \"\u2705 Primary icon found at: $absoluteIconPath\"\n\n          # 2. Read and parse the YAML configuration\n          $config = Get-Content $configPath | ConvertFrom-Yaml\n\n          # 3. Normalize the icon path for YAML\n          $normalizedIconPath = $absoluteIconPath.Replace('\\\\', '/')\n\n          # 4. Update the icon paths in the configuration object\n          $config.win.icon = $normalizedIconPath\n\n          # The installerIcon and uninstallerIcon properties are not valid for the MSI target.\n          # electron-builder automatically uses the main application icon for the installer.\n          # We unconditionally remove them here to prevent schema validation errors, catching the error if they don't exist.\n          try {\n            $config.msi.PSObject.Properties.Remove('installerIcon')\n            Write-Host \" -> Removed 'msi.installerIcon' property.\" -ForegroundColor Yellow\n          } catch {\n            Write-Host \" -> 'msi.installerIcon' property did not exist, no action needed.\"\n          }\n          try {\n            $config.msi.PSObject.Properties.Remove('uninstallerIcon')\n            Write-Host \" -> Removed 'msi.uninstallerIcon' property.\" -ForegroundColor Yellow\n          } catch {\n            Write-Host \" -> 'msi.uninstallerIcon' property did not exist, no action needed.\"\n          }\n\n          Write-Host \" -> Set win.icon to '$normalizedIconPath'\" -ForegroundColor Green\n\n          # 5. Convert back to YAML and write to a NEW temporary config file\n          $tempConfigPath = 'electron/temp-builder-config.yml'\n          $config | ConvertTo-Yaml | Set-Content -Path $tempConfigPath\n          Write-Host \"\u2705 Successfully created temporary config '$tempConfigPath' with corrected icon paths.\"\n\n          # 6. Display final config for verification\n          Write-Host \"\\\\n--- Final Config ---\"\n          Get-Content $tempConfigPath | Write-Host\n          Write-Host \"--------------------\"\n\n      - name: \ud83d\udd0d Verify electron-builder Config\n        shell: pwsh\n        run: |\n          $configPath = 'electron/temp-builder-config.yml'\n\n          if (-not (Test-Path $configPath)) {\n            Write-Error \"electron-builder config not found: $configPath\"\n            exit 1\n          }\n\n          # Basic YAML syntax check (non-exhaustive)\n          $content = Get-Content $configPath -Raw\n          if ($content -notmatch 'appId:') {\n            Write-Error \"Invalid electron-builder config: missing appId\"\n            exit 1\n          }\n\n          Write-Host \"\u2713 electron-builder config valid\"\n\n      - name: \ud83d\udcc4 Ensure WiX License Exists for electron-builder\n        shell: pwsh\n        run: |\n          if (-not (Test-Path 'build_wix')) { New-Item -ItemType Directory -Path 'build_wix' | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            # FIX: Use Base64 decoding to avoid RTF escape sequence issues in PowerShell/YAML\n            $rtfContent = [System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String(\"e1xydGYxXGFuc2lcZGVmZjB7XGZvbnR0Ymx7XGYwIEFyaWFsO319XGYwXGZzMjQgRU5EIFVTRVIgTElDRU5TRSBBR1JFRU1FTlRccGFyXHBhciBUaGlzIGlzIGEgcGxhY2Vob2xkZXIgbGljZW5zZSBmb3IgRm9ydHVuYSBGYXVjZXQuIFBsZWFzZSByZXBsYWNlIHdpdGggYWN0dWFsIHRlcm1zLn0=\"))\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: \ud83d\udd28 Build Electron Application\n        shell: pwsh\n        working-directory: electron\n        run: |\n          # Set code signing to false for CI (unless you have signing certificates)\n          $env:CSC_IDENTITY_AUTO_DISCOVERY = 'false'\n\n          npm run build -- --config temp-builder-config.yml --publish never 2>&1\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"Electron build failed\"\n            exit 1\n          }\n\n          Write-Host \"\u2713 Electron build completed\"\n\n      - name: \ud83c\udfd7\ufe0f Build MSI with electron-builder\n        shell: pwsh\n        working-directory: electron\n        env:\n          CSC_IDENTITY_AUTO_DISCOVERY: 'false'\n        run: |\n          $semver = \"${{ needs.validate-environment.outputs.semver }}\"\n          $shortSha = \"${{ needs.validate-environment.outputs.short_sha }}\"\n          $artifactName = \"Fortuna-Electron-${semver}-${shortSha}.msi\"\n          npm run dist -- --win msi --config temp-builder-config.yml --publish never --config.artifactName=$artifactName 2>&1\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"electron-builder MSI creation failed\"\n            exit 1\n          }\n\n          Write-Host \"\u2713 MSI build completed\"\n\n      - name: \ud83d\udd0d Verify MSI Output\n        shell: pwsh\n        run: |\n          $msiFiles = Get-ChildItem -Recurse -Filter \"*.msi\" -ErrorAction SilentlyContinue\n\n          if ($msiFiles.Count -eq 0) {\n            Write-Error \"No MSI files generated\"\n            exit 1\n          }\n\n          foreach ($msi in $msiFiles) {\n            $sizeGB = $msi.Length / 1GB\n            $sizeMB = $msi.Length / 1MB\n\n            if ($msi.Length -lt 10MB) {\n              Write-Warning \"MSI suspiciously small: $($msi.Name) ($('{0:N1}' -f $sizeMB) MB)\"\n            }\n\n            Write-Host \"\u2713 MSI: $($msi.FullName) ($('{0:N1}' -f $sizeMB) MB)\"\n\n            # Generate checksum\n            $hash = (Get-FileHash -LiteralPath $msi.FullName -Algorithm SHA256).Hash\n            Write-Host \" SHA256: $hash\"\n            $hash | Out-File -FilePath \"$($msi.FullName).sha256\"\n          }\n\n      - name: '\ud83d\udc24 The Canary (Malware Pre-Flight)'\n        shell: pwsh\n        run: |\n          $msi = Get-ChildItem -Recurse -Filter \"*.msi\" | Select-Object -First 1\n          if (!$msi) { Write-Warning \"No MSI found to scan.\"; exit 0 }\n\n          Write-Host \"\ud83d\udd0d Scanning $($msi.Name) with Windows Defender...\"\n          $defender = \"C:\\Program Files\\Windows Defender\\MpCmdRun.exe\"\n\n          if (-not (Test-Path $defender)) {\n              Write-Warning \"Windows Defender CLI not found at expected path.\"\n              exit 0\n          }\n\n          # ScanType 3 = File/Custom Scan\n          $proc = Start-Process -FilePath $defender -ArgumentList \"-Scan -ScanType 3 -File `\"$($msi.FullName)`\"\" -Wait -PassThru -NoNewWindow\n\n          if ($proc.ExitCode -eq 0) {\n              Write-Host \"\u2705 CLEAN: Windows Defender found no threats.\" -ForegroundColor Green\n          } elseif ($proc.ExitCode -eq 2) {\n              Write-Error \"\ud83d\udea8 THREAT DETECTED: Windows Defender flagged this installer!\"\n              exit 1\n          } else {\n              Write-Warning \"\u26a0\ufe0f Scan completed with inconclusive exit code: $($proc.ExitCode)\"\n          }\n\n      - name: \ud83d\udcca Generate Build Report\n        if: always()\n        shell: pwsh\n        run: |\n          $report = @{\n            build_id = '${{ needs.validate-environment.outputs.build_id }}'\n            timestamp = Get-Date -Format 'o'\n            status = if ($LASTEXITCODE -eq 0) { 'SUCCESS' } else { 'FAILED' }\n            node_version = '${{ needs.validate-environment.outputs.node_version }}'\n            python_version = '${{ needs.validate-environment.outputs.python_version }}'\n            msi_files = @()\n          }\n\n          Get-ChildItem -Filter \"*.msi\" -Recurse -ErrorAction SilentlyContinue | ForEach-Object {\n            $report.msi_files += @{\n              path = $_.FullName\n              size_mb = [math]::Round($_.Length / 1MB, 2)\n              hash = (Get-FileHash -LiteralPath $_.FullName -Algorithm SHA256).Hash\n            }\n          }\n\n          $report | ConvertTo-Json | Out-File -FilePath \"build-report.json\"\n          Get-Content \"build-report.json\" | Out-Host\n\n      - name: \ud83d\udce6 Stage Artifacts (Self-Discovery)\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $destDir = \"release-artifacts\"\n          New-Item -ItemType Directory -Path $destDir -Force | Out-Null\n          $destAbsolutePath = (Resolve-Path $destDir).Path\n\n          Write-Host \"Staging Destination: $destAbsolutePath\"\n\n          # Find all MSIs, excluding any inside the destination folder.\n          $msiFiles = Get-ChildItem -Path \".\" -Recurse -Filter \"*.msi\" |\n            Where-Object { $_.FullName -notlike \"$destAbsolutePath\" }\n\n          if ($msiFiles.Count -eq 0) {\n            Write-Error \"No MSI files found to stage!\"\n            exit 1\n          }\n\n          foreach ($msi in $msiFiles) {\n            Write-Host \"Moving: $($msi.FullName)\"\n            Move-Item $msi.FullName $destAbsolutePath -Force\n\n            # Handle the checksum file if it exists\n            $shaPath = \"$($msi.FullName).sha256\"\n            if (Test-Path $shaPath) {\n              Move-Item -LiteralPath $shaPath -Destination $destAbsolutePath -Force\n            }\n          }\n\n          Write-Host \"\u2705 Staging complete. Contents of $($destDir):\"\n          Get-ChildItem $destAbsolutePath | Select-Object Name, Length\n\n      - name: \ud83d\udce4 Upload Release Artifacts\n        if: success()\n        uses: actions/upload-artifact@v4\n        with:\n          name: fortuna-electron-msi-${{ needs.validate-environment.outputs.build_id }}\n          path: |\n            release-artifacts/\n            build-report.json\n          retention-days: 90\n\n      - name: \ud83d\udea8 Upload Failure Diagnostics\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: build-failure-logs-${{ needs.validate-environment.outputs.build_id }}\n          path: |\n            electron/dist/\n            python-service-bin/\n            web_platform/frontend/out/\n            build-report.json\n          retention-days: 30\n\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test (Robust)'\n    runs-on: windows-latest\n    timeout-minutes: 15\n    needs: [validate-environment, build-electron-msi]\n    env:\n      FORTUNA_ENV: 'smoke-test'\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-electron-msi-${{ needs.validate-environment.outputs.build_id }}\n          path: msi-installer\n\n      - name: \ud83d\udccb Inspect Artifact\n        shell: pwsh\n        run: |\n          Write-Host \"=== Artifact Contents ===\"\n          Get-ChildItem -Path \"msi-installer\" -Recurse | ForEach-Object {\n            Write-Host \" $($_.FullName)\"\n          }\n\n      - name: \ud83e\udd2b Install MSI & Verify\n        shell: pwsh\n        run: |\n          # EXOTIC INGREDIENT #1: Find MSI with recursion\n          $msi = Get-ChildItem -Path \"msi-installer\" -Filter \"*.msi\" -Recurse -ErrorAction SilentlyContinue |\n            Select-Object -First 1\n\n          if (-not $msi) {\n            Write-Error \"\u274c FATAL: No MSI file found in artifact\"\n            exit 1\n          }\n\n          Write-Host \"Installing: $($msi.Name)\"\n\n          # EXOTIC INGREDIENT #2: Logging with /L*v\n          $proc = Start-Process msiexec.exe -ArgumentList \"/i `\"$($msi.FullName)`\" /qn /L*v msi-install.log\" -Wait -PassThru\n\n          if ($proc.ExitCode -ne 0) {\n            Write-Error \"\u274c MSI Install Failed with exit code $($proc.ExitCode)\"\n            if (Test-Path msi-install.log) {\n              Get-Content msi-install.log | Select-Object -Last 50\n            }\n            exit 1\n          }\n\n          Write-Host \"\u2705 MSI installation succeeded\"\n\n      - name: \ud83d\udd75\ufe0f Find Installed Executable\n        shell: pwsh\n        run: |\n          # EXOTIC INGREDIENT #3: Don't assume the path. Find it dynamically.\n          # Searches for the main executable, skipping uninstallers\n\n          $root = \"C:\\\\Program Files\\\\Fortuna Faucet\"\n          if (-not (Test-Path $root)) {\n            Write-Error \"\u274c Install directory not found: $root\"\n            Get-ChildItem \"C:\\\\Program Files\" | Write-Host\n            exit 1\n          }\n\n          $exe = Get-ChildItem -Path $root -Filter \"*.exe\" -Recurse -ErrorAction SilentlyContinue | Where-Object { $_.Name -notmatch 'uninstall' } | Select-Object -First 1\n\n          if (-not $exe) {\n            Write-Error \"\u274c No main executable found in $root\"\n            Get-ChildItem -Path $root -Recurse | Write-Host\n            exit 1\n          }\n\n          Write-Host \"\u2705 Found Executable: $($exe.FullName)\"\n          $exe.FullName | Out-File \"installed_exe.txt\" -Encoding utf8\n\n      - name: \ud83d\ude80 Launch, Wait & Verify Backend\n        shell: pwsh\n        run: |\n          $root = 'C:\\\\Program Files\\\\Fortuna Faucet'\n          $exe = Get-ChildItem $root -Filter \"*.exe\" -Recurse | Where { $_.Name -notmatch 'uninstall' } | Select -First 1\n\n          if (!$exe) { throw \"Executable not found\" }\n\n          Write-Host \"=== PRE-LAUNCH DIAGNOSTICS ===\"\n          Write-Host \"Electron executable: $($exe.FullName)\"\n          Write-Host \"File size: $('{0:N0}' -f $exe.Length) bytes\"\n          Write-Host \"Exists: $(Test-Path $exe.FullName)\"\n\n          Write-Host \"`n=== LAUNCHING ELECTRON ===\"\n          $logDir = \"C:\\Temp\\fortuna-logs-$(Get-Random)\"\n          New-Item -ItemType Directory -Path $logDir -Force | Out-Null\n\n          $proc = Start-Process -FilePath $exe.FullName -PassThru -RedirectStandardOutput \"$logDir\\stdout.log\" -RedirectStandardError \"$logDir\\stderr.log\"\n\n          Write-Host \"Electron launched (PID: $($proc.Id))\"\n          Write-Host \"Waiting for backend to start listening on port 8000...\"\n\n          # Wait for port to be ready\n          $portReady = $false\n          for ($i = 0; $i -lt 60; $i++) {\n            try {\n              $tcp = New-Object System.Net.Sockets.TcpClient\n              $tcp.Connect('127.0.0.1', 8000)\n              $tcp.Close()\n              Write-Host \"\u2705 Port 8000 is OPEN!\"\n              $portReady = $true\n              break\n            } catch {\n              Write-Host \"  Attempt $($i+1)/60: Waiting...\"\n              Start-Sleep 2\n            }\n          }\n\n          if (!$portReady) {\n            Write-Error \"\u274c Backend never opened port 8000!\"\n            Write-Host \"`n=== ELECTRON STDOUT ===\"\n            Get-Content \"$logDir\\stdout.log\" -Tail 50\n            Write-Host \"`n=== ELECTRON STDERR ===\"\n            Get-Content \"$logDir\\stderr.log\" -Tail 50\n            exit 1\n          }\n\n          Write-Host \"\u2705 SUCCESS: Backend is ready and accepting connections!\"\n\n      - name: '\ud83d\udcf8 The Paparazzi (Visual Proof)'\n        shell: pwsh\n        run: |\n          Write-Host \"Installing Playwright for visual verification...\"\n          python -m pip install playwright\n          python -m playwright install chromium\n\n          # Default to 8102 if SERVICE_PORT is not set\n          $port = \"${{ env.SERVICE_PORT }}\"\n          if ([string]::IsNullOrWhiteSpace($port)) { $port = \"8102\" }\n\n          Write-Host \"Taking screenshot of http://localhost:$port...\"\n          python -c \"\n          from playwright.sync_api import sync_playwright\n          import sys\n\n          try:\n              with sync_playwright() as p:\n                  browser = p.chromium.launch()\n                  page = browser.new_page()\n                  # Try index.html, fall back to root if needed\n                  url = f'http://localhost:{sys.argv[1]}/index.html'\n                  print(f'Navigating to {url}...')\n                  page.goto(url)\n                  # Wait a moment for React/Next.js hydration if needed\n                  page.wait_for_timeout(2000)\n                  page.screenshot(path='proof-of-life.png', full_page=True)\n                  browser.close()\n              print('\u2705 Screenshot captured.')\n          except Exception as e:\n              print(f'\u274c Screenshot failed: {e}')\n              # We do not fail the build for this; it is a bonus artifact.\n              sys.exit(0)\n          \" $port\n\n      - name: \ud83d\udce4 Upload Visual Proof\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: visual-proof-${{ github.run_id }}\n          path: proof-of-life.png\n          retention-days: 7\n\n      - name: \ud83d\udce4 Upload Logs on Failure\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: electron-smoke-test-logs-${{ github.run_id }}\n          path: |\n            msi-install.log\n          retention-days: 7\n\n      - name: \ud83e\uddf9 Cleanup\n        if: always()\n        shell: pwsh\n        run: |\n          Stop-Process -Name \"Fortuna Faucet\" -Force -ErrorAction SilentlyContinue\n          Stop-Process -Name \"fortuna-backend\" -Force -ErrorAction SilentlyContinue\n          Write-Host \"\u2705 Cleanup complete\"\n\n  diagnose-asgi-imports:\n    name: '\ud83d\udd0d ASGI Import Killer Pre-Smoke Diagnostic'\n    runs-on: windows-latest\n    timeout-minutes: 15\n    needs: build-python-service\n    steps:\n      - name: \ud83d\udce5 Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n      - name: 'Run ASGI Diagnostics'\n        uses: ./.github/actions/run-asgi-diagnostics\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          backend-dir: 'python_service'\n          backend-module-path: 'python_service'\n\n  stage-release-artifacts:\n    name: '\ud83d\udce6 Stage Release Artifacts'\n    runs-on: windows-latest\n    timeout-minutes: 5\n    if: success()\n    needs:\n      - build-electron-msi\n      - validate-environment\n      - smoke-test\n    steps:\n      - name: \ud83d\udce5 Download Build Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-electron-msi-${{ needs.validate-environment.outputs.build_id }}\n          path: staging-area\n\n      - name: \ud83d\ude9a Stage Final Artifact\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $sourceDir = \"staging-area\"\n          $destDir = \"final-release-artifact\"\n          New-Item -ItemType Directory -Path $destDir -Force | Out-Null\n\n          # FIX: $. to $_.\n          Get-ChildItem -Path $sourceDir -Recurse | ForEach-Object {\n            $destPath = Join-Path $destDir $_.Name\n            Move-Item -Path $_.FullName -Destination $destPath -Force\n          }\n\n          Write-Host \"\u2705 Artifacts staged to $destDir\"\n\n      - name: \ud83d\udce4 Upload Final MSI Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: Final-MSI-Artifact\n          path: final-release-artifact/\n          retention-days: 90\n",
    ".github/workflows/build-msi-hattrickfusion-ultimate.yml": "name: HatTrick Fusion (Ultimate Edition)\non:\n  push:\n    branches: [\"main\"]\n    tags: [\"v*\"]\n  workflow_dispatch:\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.12'\n  DOTNET_VERSION: '8.0.x'\n  WIX_VERSION: '4.0.5'\n  SERVICE_PORT: '8102'\n  FRONTEND_PORT: '3000'\n  FIREWALL_RULE: 'HatTrickFusion-Port'\n  # Paths\n  FRONTEND_DIR: 'web_platform/frontend'\n  WIX_DIR: 'build_wix'\n  # Settings\n  PYTHONUTF8: '1'\n  # Mock API keys for service startup\n  API_KEY: mock_key\n  TVG_API_KEY: mock\n  GREYHOUND_API_URL: http://mock\n  FORTUNA_ENV: smoke-test\n\njobs:\n  # ==================================================================================\n  # JOB 2: BUILD FRONTEND (Cached & Manifested)\n  # ==================================================================================\n  build-frontend:\n    name: '\ud83c\udfa8 Build Frontend'\n    runs-on: windows-latest\n    timeout-minutes: 30\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n      - name: Cache Build Output\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_DIR }}/out\n          key: ${{ runner.os }}-frontend-${{ hashFiles('**/package-lock.json', '**/*.js', '**/*.ts', '**/*.tsx', '**/*.css') }}\n          restore-keys: |\n            ${{ runner.os }}-frontend-\n      - name: Install & Build\n        if: steps.cache-frontend.outputs.cache-hit != 'true'\n        run: |\n          cd ${{ env.FRONTEND_DIR }}\n          npm ci --prefer-offline --no-audit\n          npm run build\n      - name: Generate Artifact Manifest\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_DIR }}/out\"\n          # Fallback for different env var names in different workflows\n          if (-not (Test-Path $outDir)) { $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\" }\n\n          if (-not (Test-Path $outDir)) { Write-Error \"\u274c Build failed: 'out' dir missing\"; exit 1 }\n\n          $manifestPath = \"frontend-manifest.tsv\"\n          \"RelativePath`tSizeBytes`tSHA256\" | Out-File $manifestPath -Encoding utf8\n\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) { Write-Error \"\u274c Build failed: 'out' dir empty\"; exit 1 }\n\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n          foreach ($f in $files) {\n            # FIX: Changed TrimStart('\\\\\\\\', '/') to TrimStart('\\\\', '/') to prevent char conversion error\n            $rel = $f.FullName.Substring($outDir.Path.Length).TrimStart('\\','/')\n            $hash = (Get-FileHash $f.FullName -Algorithm SHA256).Hash.Substring(0,16)\n            \"$rel`t$($f.Length)`t$hash\" | Out-File $manifestPath -Encoding utf8 -Append\n          }\n      - name: Upload Frontend\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}-${{ github.run_attempt }}\n          path: |\n            ${{ env.FRONTEND_DIR }}/out\n            frontend-manifest.tsv\n          retention-days: 3\n\n  # ==================================================================================\n  # JOB 3: BUILD BACKEND (TDD, Cached, Frozen & Injected)\n  # ==================================================================================\n  build-backend:\n    name: '\ud83d\udc0d Build Backend'\n    runs-on: windows-latest\n    needs: [build-frontend]\n    timeout-minutes: 30\n    outputs:\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    env:\n      BACKEND_DIR: 'web_service/backend'\n      MODULE_PATH: 'web_service.backend'\n    steps:\n      - uses: actions/checkout@v4\n      - id: meta\n        shell: pwsh\n        run: |\n          $ver = if (\"${{ github.ref }}\" -match 'refs/tags/v(.*)') { $Matches[1] } else { \"0.0.${{ github.run_number }}\" }\n          \"semver=$ver\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          $sha = git rev-parse --short HEAD\n          \"short_sha=$sha\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n      - uses: actions/download-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}-${{ github.run_attempt }}\n          path: ${{ env.FRONTEND_DIR }}/out\n      - name: Clean Staging\n        run: Remove-Item \"${{ env.FRONTEND_DIR }}/out/frontend-manifest.tsv\" -ErrorAction SilentlyContinue\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n      - name: Install Dependencies\n        run: |\n          pip install -r ${{ env.BACKEND_DIR }}/requirements.txt\n          pip install pyinstaller==6.6.0 pytest pytest-asyncio httpx asgi-lifespan fakeredis\n          pip freeze > backend-freeze.txt\n\n      - name: \ud83e\uddea Run Unit Tests (Quality Gate)\n        shell: pwsh\n        run: |\n          # 1. Define the Failure Threshold\n          $MAX_ALLOWED_FAILURES = 30\n          Write-Host \"Running Test Suite (Threshold: $MAX_ALLOWED_FAILURES failures)...\"\n          # 2. Run Pytest and generate an XML report.\n          # We use 'cmd /c' and '|| true' to ensure the script doesn't die immediately on exit code 1.\n          cmd /c \"pytest ${{ env.BACKEND_DIR }}/tests --junitxml=test-report.xml\" || Write-Host \"Pytest finished with issues.\"\n          # 3. Parse the XML Report\n          if (Test-Path \"test-report.xml\") {\n              [xml]$xml = Get-Content \"test-report.xml\"\n              # Sum up failures and errors\n              $failures = 0\n              $errors = 0\n              # Handle different XML structures (sometimes root is testsuites, sometimes testsuite)\n              if ($xml.testsuites) {\n                  $failures = [int]$xml.testsuites.failures\n                  $errors = [int]$xml.testsuites.errors\n              } elseif ($xml.testsuite) {\n                  $failures = [int]$xml.testsuite.failures\n                  $errors = [int]$xml.testsuite.errors\n              }\n              $total_issues = $failures + $errors\n              Write-Host \"----------------------------------------\"\n              Write-Host \"\ud83d\udcca TEST RESULTS SUMMARY\"\n              Write-Host \"   Failures: $failures\"\n              Write-Host \"   Errors:   $errors\"\n              Write-Host \"   Total:    $total_issues\"\n              Write-Host \"   Limit:    $MAX_ALLOWED_FAILURES\"\n              Write-Host \"----------------------------------------\"\n              # 4. The Decision Logic\n              if ($total_issues -gt $MAX_ALLOWED_FAILURES) {\n                  Write-Error \"\u274c CRITICAL: Too many tests failed ($total_issues). Limit is $MAX_ALLOWED_FAILURES.\"\n                  exit 1\n              } else {\n                  Write-Host \"\u2705 ACCEPTABLE: Failure count is within tolerance. Proceeding with build...\" -ForegroundColor Green\n                  exit 0 # Explicitly exit with success code to override pytest's failure code\n              }\n          } else {\n              Write-Error \"\u274c FATAL: No test report generated. Pytest failed to start.\"\n              exit 1\n          }\n\n      - name: Cache PyInstaller Dist\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-${{ hashFiles(format('{0}/**', env.BACKEND_DIR)) }}\n\n      - name: Generate Spec & Build\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        shell: python\n        env:\n          BACKEND_DIR: ${{ env.BACKEND_DIR }}\n          MODULE_PATH: ${{ env.MODULE_PATH }}\n          FRONTEND_OUT: ${{ env.FRONTEND_DIR }}/out\n        run: |\n          import os\n          from pathlib import Path\n\n          bk_dir = os.environ['BACKEND_DIR']\n          mod_path = os.environ['MODULE_PATH']\n          # CHANGE: Point to the new service wrapper\n          entry = f\"{bk_dir}/main.py\"\n          frontend_out = os.environ['FRONTEND_OUT']\n\n          # FIX: Fixed quoting in datas list to avoid syntax error\n          spec = f\"\"\"\n          # -- mode: python ; coding: utf-8 --\n          from PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n          block_cipher = None\n\n          a = Analysis(\n              ['{entry}'],\n              pathex=[],\n              binaries=[],\n              datas=collect_data_files('uvicorn') + collect_data_files('slowapi') + [('{frontend_out}', 'ui')],\n              hiddenimports=collect_submodules('{mod_path}') + ['win32timezone'],\n              hookspath=[],\n              runtime_hooks=[],\n              excludes=['tests', 'pytest'],\n              win_no_prefer_redirects=False,\n              win_private_assemblies=False,\n              cipher=block_cipher,\n              noarchive=False,\n          )\n          pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n          exe = EXE(\n              pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n              name='fortuna-backend', debug=False, bootloader_ignore_signals=False, strip=False, upx=True, console=False\n          )\n          \"\"\"\n          with open(\"hat-trick.spec\", \"w\") as f: f.write(spec)\n          os.system(\"pyinstaller hat-trick.spec --clean --noconfirm\")\n      - name: Verify & Hash Executable\n        shell: pwsh\n        run: |\n          $exe = \"dist/fortuna-backend.exe\"\n          if (-not (Test-Path $exe)) { Write-Error \"\u274c Executable missing\"; exit 1 }\n          $size = (Get-Item $exe).Length / 1MB\n          if ($size -lt 10) { Write-Error \"\u274c Executable too small ($size MB)\"; exit 1 }\n          $hash = (Get-FileHash $exe -Algorithm SHA256).Hash\n          $hash | Out-File \"dist/fortuna-backend.exe.sha256\" -Encoding utf8\n          Move-Item backend-freeze.txt dist/\n          Write-Host \"\u2705 Backend ready: $size MB | SHA256: $hash\"\n      - name: Upload Backend\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-dist-${{ github.run_id }}-${{ github.run_attempt }}\n          path: dist/\n          retention-days: 3\n\n  # ==================================================================================\n  # JOB 4: PACKAGE MSI (WiX v4 with Dietician & Canary)\n  # ==================================================================================\n  package-msi:\n    name: '\ud83d\udcbf Package MSI'\n    runs-on: windows-latest\n    needs: [build-backend]\n    timeout-minutes: 30\n    outputs:\n      msi_name: ${{ steps.name_msi.outputs.msi_name }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/download-artifact@v4\n        with:\n          name: backend-dist-${{ github.run_id }}-${{ github.run_attempt }}\n          path: staging/backend\n\n      - name: Create Restart Service Batch Script\n        shell: pwsh\n        run: |\n          $scriptContent = @\"\n          @echo off\n          echo Requesting Admin privileges to restart FortunaWebService...\n          net stop FortunaWebService\n          net start FortunaWebService\n          echo Service Restarted.\n          pause\n          \"@\n          Set-Content -Path \"staging/backend/restart_service.bat\" -Value $scriptContent -Encoding Ascii\n          Write-Host \"\u2705 Created restart_service.bat script.\"\n\n      - uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n\n      - name: '\u2696\ufe0f The Dietician (Size Analysis)'\n        shell: pwsh\n        run: |\n          $target = \"staging\"\n          $limitMB = 300\n          Write-Host \"--- \ud83d\udcca Size Breakdown ---\"\n          $files = Get-ChildItem -Path $target -Recurse -File -ErrorAction SilentlyContinue\n          if (!$files) { Write-Warning \"No files found to weigh.\"; exit 0 }\n          $totalBytes = ($files | Measure-Object -Property Length -Sum).Sum\n          $totalMB = [math]::Round($totalBytes / 1MB, 2)\n          Write-Host \"Total Payload Size: $totalMB MB\"\n          if ($totalMB -gt $limitMB) {\n              Write-Warning \"\u26a0\ufe0f BLOAT ALERT: Build exceeds $limitMB MB limit! Check the heaviest files below.\"\n          } else {\n              Write-Host \"\u2705 Size within limits (< $limitMB MB).\" -ForegroundColor Green\n          }\n          Write-Host \"`n--- \ud83d\udc18 Top 10 Heaviest Files ---\"\n          $files | Sort-Object Length -Descending | Select-Object -First 10 @{N='File';E={$_.FullName.Replace($pwd,'')}}, @{N='Size(MB)';E={\"{0:N2}\" -f ($_.Length/1MB)}} | Format-Table -AutoSize\n\n      - name: \ud83d\udcc4 Ensure WiX License Exists\n        shell: pwsh\n        run: |\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            # FIX: Use Base64 decoding to avoid RTF escape sequence issues\n            $rtfContent = [System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String(\"e1xydGYxXGFuc2lcZGVmZjB7XGZvbnR0Ymx7XGYwIEFyaWFsO319XGYwXGZzMjQgRU5EIFVTRVIgTElDRU5TRSBBR1JFRU1FTlRccGFyXHBhciBUaGlzIGlzIGEgcGxhY2Vob2xkZXIgbGljZW5zZSBmb3IgRm9ydHVuYSBGYXVjZXQuIFBsZWFzZSByZXBsYWNlIHdpdGggYWN0dWFsIHRlcm1zLn0=\"))\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: Prepare WiX\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n\n          # Copy template and apply fix\n          Copy-Item build_wix/Product_WithService.wxs build_wix/Product.wxs -Force\n\n          # Stage Executable\n          if (Test-Path staging/backend/fortuna-backend.exe) {\n            Move-Item staging/backend/fortuna-backend.exe staging/backend/fortuna-webservice.exe -Force\n          }\n\n          # FIX: Generate Valid .wixproj with Extensions\n          $proj = @(\n            '<Project Sdk=\"WixToolset.Sdk/${{ env.WIX_VERSION }}\">',\n            '  <PropertyGroup>',\n            '    <EnableDefaultCompileItems>false</EnableDefaultCompileItems>',\n            '    <OutputType>Package</OutputType>',\n            '    <Platforms>x64</Platforms>',\n            '    <DefineConstants>Version=$(Version);SourceDir=$(SourceDir);ServicePort=$(ServicePort)</DefineConstants>',\n            '  </PropertyGroup>',\n            '  <ItemGroup>',\n            '    <PackageReference Include=\"WixToolset.UI.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Firewall.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Util.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '  </ItemGroup>',\n            '  <ItemGroup>',\n            '    <Compile Include=\"Product.wxs\" />',\n            '  </ItemGroup>',\n            '</Project>'\n          )\n          Set-Content build_wix/Fortuna.wixproj ($proj -join \"`r`n\") -Encoding utf8\n      - name: Build MSI\n        working-directory: ${{ env.WIX_DIR }}\n        # FIX: Pass variables via DefineConstants\n        run: dotnet build Fortuna.wixproj -c Release -p:Platform=x64 -p:Version=\"0.0.${{ github.run_number }}\" -p:SourceDir=\"../staging/backend\" -p:ServicePort=\"${{ env.SERVICE_PORT }}\"\n\n      - name: '\ud83d\udc24 The Canary (Malware Pre-Flight)'\n        shell: pwsh\n        run: |\n          $msi = Get-ChildItem -Path \"${{ env.WIX_DIR }}/bin/x64/Release\" -Filter \"*.msi\" | Select-Object -First 1\n          if (!$msi) { Write-Warning \"No MSI found to scan.\"; exit 0 }\n          Write-Host \"\ud83d\udd0d Scanning $($msi.Name) with Windows Defender...\"\n          $defender = \"C:\\Program Files\\Windows Defender\\MpCmdRun.exe\"\n          if (-not (Test-Path $defender)) { Write-Warning \"Windows Defender CLI not found.\"; exit 0 }\n          $proc = Start-Process -FilePath $defender -ArgumentList \"-Scan -ScanType 3 -File `\"$($msi.FullName)`\"\" -Wait -PassThru -NoNewWindow\n          if ($proc.ExitCode -eq 0) { Write-Host \"\u2705 CLEAN: Windows Defender found no threats.\" -ForegroundColor Green }\n          elseif ($proc.ExitCode -eq 2) { Write-Error \"\ud83d\udea8 THREAT DETECTED!\"; exit 1 }\n          else { Write-Warning \"\u26a0\ufe0f Scan inconclusive: $($proc.ExitCode)\" }\n\n      - name: Rename & Hash MSI\n        id: name_msi\n        shell: pwsh\n        run: |\n          $ver = \"${{ needs.build-backend.outputs.semver }}\"\n          $sha = \"${{ needs.build-backend.outputs.short_sha }}\"\n          \n          # FIX: Don't assume the name. Find whatever MSI was built.\n          $releaseDir = \"${{ env.WIX_DIR }}/bin/x64/Release\"\n          $msiFound = Get-ChildItem -Path $releaseDir -Filter \"*.msi\" | Select-Object -First 1\n          \n          if (-not $msiFound) {\n            Write-Error \"\u274c FATAL: No MSI file found in $releaseDir. Build may have failed silently.\"\n            exit 1\n          }\n\n          Write-Host \"Found built MSI: $($msiFound.Name)\"\n\n          $targetName = \"HatTrickFusion-${ver}-${sha}.msi\"\n          $newPath = Join-Path $releaseDir $targetName\n          \n          Move-Item -Path $msiFound.FullName -Destination $newPath -Force\n          \n          # Generate Hash\n          $hash = (Get-FileHash $newPath -Algorithm SHA256).Hash\n          $hash | Out-File \"$newPath.sha256\" -Encoding utf8\n          \n          Write-Host \"\u2705 MSI Renamed to: $targetName\"\n          \"msi_name=$targetName\" | Out-File $env:GITHUB_OUTPUT -Append\n\n      - name: Upload MSI\n        uses: actions/upload-artifact@v4\n        with:\n          name: msi-installer-${{ github.run_id }}-${{ github.run_attempt }}\n          path: ${{ env.WIX_DIR }}/bin/x64/Release/*\n          retention-days: 7\n\n  # ==================================================================================\n  # JOB 5: SMOKE TEST (Triple-Loop + Paparazzi)\n  # ==================================================================================\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test'\n    runs-on: windows-latest\n    needs: [package-msi]\n    timeout-minutes: 30\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: msi-installer-${{ github.run_id }}-${{ github.run_attempt }}\n          path: installer\n      - name: \ud83d\udee1\ufe0f Firewall & Install\n        shell: pwsh\n        run: |\n          New-NetFirewallRule -DisplayName \"${{ env.FIREWALL_RULE }}\" -Direction Inbound -LocalPort ${{ env.SERVICE_PORT }} -Protocol TCP -Action Allow\n          if (Get-Service -Name FortunaWebService -ErrorAction SilentlyContinue) {\n            sc.exe stop FortunaWebService 2>&1 | Out-Null\n            sc.exe delete FortunaWebService 2>&1 | Out-Null\n          }\n          $msi = Get-ChildItem installer -Filter \"*.msi\" -Recurse | Select -First 1\n          if (!$msi) { throw \"No MSI found\" }\n          Write-Host \"Installing $($msi.Name)...\"\n          $msiPath = $msi.FullName\n          $args = \"/i `\"$msiPath`\" /qn /L*v installation.log\"\n          $proc = Start-Process msiexec.exe -ArgumentList $args -Wait -NoNewWindow -PassThru\n          if ($proc.ExitCode -ne 0) {\n            Get-Content install.log -Tail 50\n            throw \"Install failed with code $($proc.ExitCode)\"\n          }\n\n      - name: '\u23f3 Loop 1 - Service Registration'\n        shell: pwsh\n        run: |\n          $reg = \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\FortunaWebService\"\n          for ($i=0; $i -lt 30; $i++) {\n            if (Test-Path $reg) { Write-Host \"\u2705 Service Registered\"; exit 0 }\n            Start-Sleep 1\n          }\n          throw \"Service failed to register in Registry\"\n\n      - name: '\ud83d\ude80 Loop 2 - Launch & Port Bind'\n        shell: python\n        env:\n          PORT: ${{ env.SERVICE_PORT }}\n        run: |\n          import os, socket, time, urllib.request, urllib.error, subprocess, sys\n          port = int(os.environ[\"PORT\"])\n          print(\"--- Starting Service ---\")\n          subprocess.run([\"sc.exe\", \"start\", \"FortunaWebService\"], check=False)\n          print(f\"--- Waiting for Port {port} (60s) ---\")\n          for _ in range(60):\n            try:\n              with socket.create_connection((\"127.0.0.1\", port), timeout=1):\n                print(\"\u2705 Socket bound.\")\n                sys.exit(0)\n            except:\n              time.sleep(1)\n\n          print(\"[X] Port bind timeout\")\n          subprocess.run([\"sc.exe\", \"query\", \"FortunaWebService\"])\n          sys.exit(1)\n\n      - name: '\ud83c\udfe5 Loop 3 - Health Check'\n        shell: python\n        env:\n          PORT: ${{ env.SERVICE_PORT }}\n        run: |\n          import urllib.request, urllib.error, time, sys, os\n          port = int(os.environ[\"PORT\"])\n          for _ in range(6):\n            try:\n              req = urllib.request.Request(f\"http://127.0.0.1:{port}/health\")\n              req.add_header(\"User-Agent\", \"HatTrickFusion/1.0\")\n              with urllib.request.urlopen(req, timeout=5) as resp:\n                if resp.status == 200:\n                  print(\"\u2705 Health Check Passed\")\n                  sys.exit(0)\n            except urllib.error.HTTPError as err:\n              if err.code in (401, 403):\n                print(\"\u2705 Service Up (Auth Required)\")\n                sys.exit(0)\n            except:\n              time.sleep(2)\n          sys.exit(1)\n\n      - name: \ud83d\udcca Capture Diagnostics\n        if: failure()\n        shell: pwsh\n        run: |\n          $diag = \"diagnostics\"\n          New-Item -ItemType Directory -Path $diag -Force\n          Copy-Item install.log $diag\n          Get-EventLog -LogName Application -Source \"FortunaWebService\" -Newest 50 | Out-File \"$diag/events.txt\"\n          Get-Service FortunaWebService | Out-File \"$diag/service_state.txt\"\n          netstat -anob > \"$diag/netstat.txt\"\n      - name: \ud83d\udce4 Upload Diagnostics\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: smoke-test-failure-${{ needs.path-finder.outputs.build_id }}\n          path: diagnostics/\n          retention-days: 30\n      - name: \ud83e\uddf9 Cleanup\n        if: always()\n        run: |\n          sc.exe stop FortunaWebService\n          sc.exe delete FortunaWebService\n          Remove-NetFirewallRule -DisplayName \"${{ env.FIREWALL_RULE }}\" -ErrorAction SilentlyContinue\n\n  # ==================================================================================\n  # JOB 6: GENERATE SBOM\n  # ==================================================================================\n  generate-sbom:\n    name: '\ud83d\udcdc Generate SBOM'\n    runs-on: ubuntu-latest\n    needs: [build-backend]\n    timeout-minutes: 30\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/download-artifact@v4\n        with:\n          name: backend-dist-${{ github.run_id }}-${{ github.run_attempt }}\n          path: backend\n      - name: Create SBOM\n        uses: anchore/sbom-action@v0\n        with:\n          path: backend\n          output-file: sbom.spdx.json\n          format: spdx-json\n      - name: Upload SBOM\n        uses: actions/upload-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}-${{ github.run_attempt }}\n          path: sbom.json\n\n  # ==================================================================================\n  # JOB 7: RELEASE (On Tag)\n  # ==================================================================================\n  release:\n    name: '\ud83d\ude80 Create Release'\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/')\n    needs: [smoke-test, generate-sbom]\n    timeout-minutes: 30\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: msi-installer-${{ github.run_id }}-${{ github.run_attempt }}\n          path: assets\n      - uses: actions/download-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}-${{ github.run_attempt }}\n          path: assets\n      - name: Generate Checksums\n        run: |\n          cd assets\n          sha256sum * > SHASUMS256.txt\n      - name: Publish Release\n        uses: softprops/action-gh-release@v2\n        with:\n          files: assets/*\n          generate_release_notes: true\n",
    ".github/workflows/build-web-service-msi-jules.yml": "# System Timestamp: 2025-12-04 16:01:12.318079\nname: Build Fortuna Faucet Web Service Installer (Synthesized Overkill)\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: read\n  actions: read\n  checks: read\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\ndefaults:\n  run:\n    shell: pwsh\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.12'\n  DOTNET_VERSION: '8.0.x'\n  PYTHONUTF8: '1'\n  PIP_DISABLE_PIP_VERSION_CHECK: '1'\n  PIP_NO_PYTHON_VERSION_WARNING: '1'\n  NPM_CONFIG_FUND: 'false'\n  NPM_CONFIG_AUDIT: 'false'\n  FORCE_COLOR: '3'\n  FRONTEND_DIR: 'web_platform/frontend'\n  FRONTEND_BUILD_DIR: 'web_platform/frontend/out'\n  WIX_DIR: 'build_wix'\n  SERVICE_PORT: '8102'\n  HEALTH_ENDPOINT: '/health'\n  API_KEY: ${{ secrets.TEST_API_KEY }}\n  TVG_API_KEY: \"mock_key\"\n  GREYHOUND_API_URL: \"http://mock\"\n  FORTUNA_ENV: \"smoke-test\"\n  MSI_STAGING_DIR: 'build_wix/staging'\n  MSI_OUTPUT_DIR: 'dist'\n  WIX_VERSION: '4.0.5'\n\njobs:\n  path-finder:\n    name: '\ud83d\udd0e Path Finder Dynamic Backend Detection'\n    runs-on: windows-latest\n    outputs:\n      backend_dir: ${{ steps.find-path.outputs.backend_dir }}\n      backend_module_path: ${{ steps.find-path.outputs.backend_module_path }}\n      spec_file: ${{ steps.find-path.outputs.spec_file }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Detect Backend Path\n        id: find-path\n        run: |\n          Set-StrictMode -Version Latest\n          $web_service_path = \"web_service/backend\"\n          $python_service_path = \"python_service\"\n          $backend_dir = \"\"\n          $backend_module_path = \"\"\n          $spec_file = \"\"\n\n          Write-Host \"--- Path Finding Forensics ---\"\n          Write-Host \"Searching for the correct backend service directory...\"\n\n          # Test web_service path\n          $web_service_main = Test-Path (Join-Path $web_service_path \"main.py\")\n          $web_service_api = Test-Path (Join-Path $web_service_path \"api.py\")\n          $web_service_init = Test-Path (Join-Path $web_service_path \"__init__.py\")\n          Write-Host \"Checking '$web_service_path':\"\n          Write-Host \"  main.py -> $web_service_main\"\n          Write-Host \"  api.py -> $web_service_api\"\n          Write-Host \"  __init__.py -> $web_service_init\"\n\n          # Test python_service path\n          $python_service_main = Test-Path (Join-Path $python_service_path \"main.py\")\n          $python_service_api = Test-Path (Join-Path $python_service_path \"api.py\")\n          $python_service_init = Test-Path (Join-Path $python_service_path \"__init__.py\")\n          Write-Host \"Checking '$python_service_path':\"\n          Write-Host \"  main.py -> $python_service_main\"\n          Write-Host \"  api.py -> $python_service_api\"\n          Write-Host \"  __init__.py -> $python_service_init\"\n\n          if ($web_service_main -and $web_service_api -and $web_service_init) {\n            $backend_dir = $web_service_path\n            $backend_module_path = \"web_service.backend\"\n            $spec_file = \"jules.spec\"\n            Write-Host \"\u2705 Verdict: Detected 'web_service/backend' as the target.\" -ForegroundColor Green\n          } elseif ($python_service_main -and $python_service_api -and $python_service_init) {\n            $backend_dir = $python_service_path\n            $backend_module_path = \"python_service\"\n            $spec_file = \"fortuna-backend-webservice.spec\"\n            Write-Host \"\u2705 Verdict: Detected 'python_service' as the target.\" -ForegroundColor Green\n          } else {\n            Write-Host \"\u274c FATAL: Could not determine a valid backend directory. Neither 'web_service/backend' nor 'python_service' contains the required files (main.py, api.py, __init__.py).\" -ForegroundColor Red\n            exit 1\n          }\n\n          Write-Host \"--- Outputs ---\"\n          Write-Host \"backend_dir: $backend_dir\"\n          Write-Host \"backend_module_path: $backend_module_path\"\n          Write-Host \"spec_file: $spec_file\"\n\n          \"backend_dir=$backend_dir\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_module_path=$backend_module_path\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"spec_file=$spec_file\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n  system-check:\n    name: '\u2699\ufe0f System Prerequisites'\n    runs-on: windows-latest\n    timeout-minutes: 5\n    outputs:\n      disk_free_gb: ${{ steps.system.outputs.disk_gb }}\n    steps:\n      - name: Verify Build Tools\n        run: |\n          Set-StrictMode -Version Latest\n          $tools = @('dotnet', 'python', 'node', 'npm', 'git')\n          foreach ($tool in $tools) {\n            Write-Host \"Checking for $($tool)...\"\n            Get-Command $tool -ErrorAction SilentlyContinue\n            if (-not $?) {\n              Write-Host \"\u274c FATAL: Build tool '$tool' not found in PATH.\" -ForegroundColor Red\n              exit 1\n            }\n          }\n          Write-Host \"\u2705 All critical build tools are present.\" -ForegroundColor Green\n      - name: Check Disk Space\n        id: system\n        run: |\n          Set-StrictMode -Version Latest\n          $disk = Get-Volume | Where-Object { $_.DriveLetter -eq 'C' }\n          $freeGB = [math]::Round($disk.SizeRemaining / 1GB, 2)\n          if ($freeGB -lt 10) {\n            Write-Host \"\u26a0\ufe0f WARNING: Low disk space. Only $freeGB GB free (10+ GB recommended).\" -ForegroundColor Yellow\n          } else {\n            Write-Host \"\u2705 Disk space check passed ($freeGB GB free).\" -ForegroundColor Green\n          }\n          \"disk_gb=$freeGB\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n  repo-preflight:\n    name: '\ud83e\uddea Repo Preflight & Integrity'\n    runs-on: windows-latest\n    needs: [path-finder, system-check]\n    timeout-minutes: 5\n    outputs:\n      frontend_lock_hash: ${{ steps.hashes.outputs.frontend_lock_hash }}\n      backend_requirements_hash: ${{ steps.hashes.outputs.backend_requirements_hash }}\n      wix_definition_hash: ${{ steps.hashes.outputs.wix_definition_hash }}\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Derive Build Metadata\n        id: meta\n        run: |\n          Set-StrictMode -Version Latest\n          $ref = \"${{ github.ref }}\"\n          if ($ref -like 'refs/tags/v*') {\n            $semver = $ref -replace 'refs/tags/v', ''\n          } else {\n            $semver = \"0.0.${{ github.run_number }}\"\n          }\n          $shortSha = \"${{ github.sha }}\".Substring(0,7)\n          \"semver=$semver\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"short_sha=$shortSha\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\ud83d\udd16 Version: $semver ($shortSha)\"\n\n      - name: Validate Critical Files Exist\n        env:\n          BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n        run: |\n          Set-StrictMode -Version Latest\n          $paths = @(\n            \"${{ env.FRONTEND_DIR }}/package.json\",\n            \"${{ env.FRONTEND_DIR }}/package-lock.json\",\n            (Join-Path $env:BACKEND_DIR \"requirements.txt\"),\n            (Join-Path $env:BACKEND_DIR \"main.py\"),\n            \"${{ env.WIX_DIR }}/Product_WithService.wxs\"\n          )\n          foreach ($path in $paths) {\n            if (-not (Test-Path $path)) {\n              Write-Host \"\u274c FATAL: Required path missing: $path\" -ForegroundColor Red\n              exit 1\n            }\n          }\n          Write-Host \"\u2705 All critical files confirmed.\"\n\n      - name: Capture Integrity Hashes\n        id: hashes\n        env:\n          BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n        run: |\n          Set-StrictMode -Version Latest\n          $frontend = (Get-FileHash \"${{ env.FRONTEND_DIR }}/package-lock.json\" -Algorithm SHA256).Hash\n          $backend = (Get-FileHash (Join-Path $env:BACKEND_DIR \"requirements.txt\") -Algorithm SHA256).Hash\n          $wix = (Get-FileHash \"${{ env.WIX_DIR }}/Product_WithService.wxs\" -Algorithm SHA256).Hash\n          \"frontend_lock_hash=$frontend\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_requirements_hash=$backend\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"wix_definition_hash=$wix\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n      - name: Upload Integrity Snapshot\n        uses: actions/upload-artifact@v4\n        with:\n          name: repo-preflight-${{ github.run_id }}\n          path: |\n            ${{ env.FRONTEND_DIR }}/package-lock.json\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.WIX_DIR }}/Product_WithService.wxs\n          retention-days: 3\n\n  frontend-quality:\n    name: '\ud83e\uddfc Frontend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 15\n    needs: repo-preflight\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n\n      - name: Cache Frontend Build\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          key: ${{ runner.os }}-frontend-build-${{ hashFiles('${{ env.FRONTEND_DIR }}/**') }}\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm ci --prefer-offline --no-audit --no-fund\n\n      - name: Run Lint (if defined)\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          $pkg = Get-Content package.json -Raw | ConvertFrom-Json\n          if ($pkg.scripts.PSObject.Properties.Name -contains 'lint') {\n            Write-Host \"\ud83e\uddf9 Running npm run lint\"\n            npm run lint\n          } else {\n            Write-Host \"\u2139\ufe0f No lint script defined, skipping.\"\n          }\n\n      - name: Run Tests (if defined)\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          $pkg = Get-Content package.json -Raw | ConvertFrom-Json\n          if ($pkg.scripts.PSObject.Properties.Name -contains 'test') {\n            Write-Host \"\ud83e\uddea Running npm test -- --watch=false\"\n            npm test -- --watch=false\n          } else {\n            Write-Host \"\u2139\ufe0f No test script defined, skipping.\"\n          }\n\n      - name: Security Audit (non-blocking)\n        continue-on-error: true\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm audit --audit-level=critical\n\n  backend-quality:\n    name: '\ud83e\uddef Backend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 20\n    needs: [path-finder, repo-preflight]\n    env:\n      BACKEND_REQUIREMENTS_HASH: ${{ needs.repo-preflight.outputs.backend_requirements_hash }}\n      BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n      BACKEND_SPEC: ${{ needs.path-finder.outputs.spec_file }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: |\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.BACKEND_DIR }}/requirements-dev.txt\n\n      - name: Cache Backend Build\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-build-${{ hashFiles(format('{0}/**', env.BACKEND_DIR), format('{0}', env.BACKEND_SPEC)) }}\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n          if (Test-Path (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")) {\n            pip install -r (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")\n          }\n\n      - name: Bytecode Compile (Fail Fast)\n        run: |\n          Set-StrictMode -Version Latest\n          python -m compileall -q \"${{ env.BACKEND_DIR }}\"\n\n      - name: Run Pytest (if available)\n        run: |\n          Set-StrictMode -Version Latest\n          python -c 'import importlib.util, sys; sys.exit(0 if importlib.util.find_spec(\"pytest\") else 1)'\n          if ($LASTEXITCODE -eq 0) {\n            Write-Host \"\ud83e\uddea pytest detected, running suite...\"\n            python -m pytest \"${{ env.BACKEND_DIR }}\" --maxfail=1 --disable-warnings\n          } else {\n            Write-Host \"\u2139\ufe0f pytest not installed; skipping tests.\"\n          }\n\n      - name: pip-audit (non-blocking)\n        continue-on-error: true\n        run: |\n          Set-StrictMode -Version Latest\n          pip install pip-audit\n          pip-audit -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n\n  sbom:\n    name: '\ud83d\udcc4 SBOM Snapshot'\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    needs: repo-preflight\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Generate SBOM (SPDX)\n        uses: anchore/sbom-action@v0\n        with:\n          output-file: sbom.spdx.json\n          format: spdx-json\n\n      - name: Upload SBOM\n        uses: actions/upload-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}\n          path: sbom.spdx.json\n          retention-days: 7\n\n  build-frontend:\n    name: '\ud83d\udce6 Build Frontend'\n    runs-on: windows-latest\n    timeout-minutes: 20\n    needs: [path-finder, repo-preflight, frontend-quality]\n    env:\n      FRONTEND_LOCK_HASH: ${{ needs.repo-preflight.outputs.frontend_lock_hash }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n\n      - name: Cache Frontend Build\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          key: ${{ runner.os }}-frontend-build-${{ hashFiles('${{ env.FRONTEND_DIR }}/**') }}\n          restore-keys: |\n            ${{ runner.os }}-frontend-build-\n\n      - name: Prime npm Cache\n        uses: actions/cache@v4\n        with:\n          path: ~\\AppData\\Local\\npm-cache\n          key: ${{ runner.os }}-npm-${{ env.NODE_VERSION }}-${{ env.FRONTEND_LOCK_HASH }}\n          restore-keys: |\n            ${{ runner.os }}-npm-${{ env.NODE_VERSION }}-\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm ci --prefer-offline --no-audit --no-fund\n\n      - name: Build Frontend\n        if: steps.cache-frontend.outputs.cache-hit != 'true'\n        env:\n          NEXT_PUBLIC_API_URL: http://127.0.0.1:${{ env.SERVICE_PORT }}\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm run build\n\n      - name: Report Cache Status\n        run: |\n          if ('${{ steps.cache-frontend.outputs.cache-hit }}' -eq 'true') {\n            Write-Host \"\u2705 Frontend build restored from cache.\" -ForegroundColor Green\n          } else {\n            Write-Host \"\u2139\ufe0f No cache hit. A new build was performed.\" -ForegroundColor Yellow\n          }\n\n      - name: Verify Build Output\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\"\n          if (-not (Test-Path $outDir)) {\n             Write-Host \"\u274c FATAL: Build directory not found\" -ForegroundColor Red\n             exit 1\n          }\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) {\n             Write-Host \"\u274c FATAL: Build directory empty\" -ForegroundColor Red\n             exit 1\n          }\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n      - name: Generate Artifact Manifest\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_DIR }}/out\"\n          # Fallback for different env var names in different workflows\n          if (-not (Test-Path $outDir)) { $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\" }\n\n          if (-not (Test-Path $outDir)) { Write-Error \"\u274c Build failed: 'out' dir missing\"; exit 1 }\n\n          $manifestPath = \"frontend-manifest.tsv\"\n          \"RelativePath`tSizeBytes`tSHA256\" | Out-File $manifestPath -Encoding utf8\n\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) { Write-Error \"\u274c Build failed: 'out' dir empty\"; exit 1 }\n\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n          foreach ($f in $files) {\n            # FIX: Changed TrimStart('\\\\\\\\', '/') to TrimStart('\\\\', '/') to prevent char conversion error\n            $rel = $f.FullName.Substring($outDir.Path.Length).TrimStart('\\','/')\n            $hash = (Get-FileHash $f.FullName -Algorithm SHA256).Hash.Substring(0,16)\n            \"$rel`t$($f.Length)`t$hash\" | Out-File $manifestPath -Encoding utf8 -Append\n          }\n\n      - name: Upload Frontend Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          retention-days: 3\n\n      - name: Upload Manifest\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-manifest-${{ github.run_id }}\n          path: frontend-manifest.tsv\n          retention-days: 3\n\n  build-backend:\n    name: '\ud83d\udc0d Build Backend'\n    runs-on: windows-latest\n    timeout-minutes: 25\n    needs: [path-finder, repo-preflight, build-frontend, backend-quality]\n    env:\n      BACKEND_REQUIREMENTS_HASH: ${{ needs.repo-preflight.outputs.backend_requirements_hash }}\n      BUILD_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n      BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n      BACKEND_MODULE_PATH: ${{ needs.path-finder.outputs.backend_module_path }}\n      BACKEND_SPEC: ${{ needs.path-finder.outputs.spec_file }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Download Frontend Artifact\n        uses: actions/download-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}\n          path: temp-frontend\n\n      - name: Cache Backend Build\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-build-${{ hashFiles(format('{0}/**', env.BACKEND_DIR), format('{0}', env.BACKEND_SPEC)) }}\n          restore-keys: |\n            ${{ runner.os }}-backend-build-\n\n      - name: Stage Frontend for PyInstaller\n        run: |\n          Set-StrictMode -Version Latest\n          $dest = \"staging/ui\"\n          New-Item -ItemType Directory -Path $dest -Force | Out-Null\n          Copy-Item -Path \"temp-frontend/*\" -Destination $dest -Recurse -Force\n          Write-Host \"\u2705 Frontend staged for inclusion.\"\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: |\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.BACKEND_DIR }}/requirements-dev.txt\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n          if (Test-Path (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")) {\n            pip install -r (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")\n          }\n\n      - name: Freeze Dependency Snapshot\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        run: |\n          Set-StrictMode -Version Latest\n          pip freeze | Out-File backend-freeze.txt -Encoding utf8\n\n      - name: Create Dynamic webservice.spec for PyInstaller\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        shell: python\n        env:\n          BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n          BACKEND_MODULE_PATH: ${{ needs.path-finder.outputs.backend_module_path }}\n          FRONTEND_OUT: ${{ env.FRONTEND_DIR }}/out\n        run: |\n          import os\n          from pathlib import Path\n\n          # FIX: Normalize paths to forward slashes to avoid f-string backslash hell\n          bk_dir = os.environ['BACKEND_DIR'].replace('\\\\', '/')\n          mod_path = os.environ['BACKEND_MODULE_PATH']\n          frontend_out = os.environ['FRONTEND_OUT'].replace('\\\\', '/')\n          spec_file = \"jules.spec\" # Hardcode the correct spec file name\n\n          entry = f\"{bk_dir}/main.py\"\n\n          spec = f\"\"\"\n          # -- mode: python ; coding: utf-8 --\n          from PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n          block_cipher = None\n\n          a = Analysis(\n              ['{entry}'],\n              pathex=[],\n              binaries=[],\n              datas=collect_data_files('uvicorn') + collect_data_files('slowapi') + [('{frontend_out}', 'ui')],\n              hiddenimports=collect_submodules('{mod_path}') + ['win32timezone'],\n              hookspath=[],\n              runtime_hooks=[],\n              excludes=['tests', 'pytest'],\n              win_no_prefer_redirects=False,\n              win_private_assemblies=False,\n              cipher=block_cipher,\n              noarchive=False,\n          )\n          pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n          exe = EXE(\n              pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n              name='fortuna-backend', debug=False, bootloader_ignore_signals=False, strip=False, upx=True, console=False\n          )\n          \"\"\"\n          with open(spec_file, \"w\") as f: f.write(spec)\n\n      - name: Create Required Backend Directories\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        run: |\n          Set-StrictMode -Version Latest\n          New-Item -ItemType Directory -Path (Join-Path $env:BACKEND_DIR \"data\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $env:BACKEND_DIR \"json\") -Force | Out-Null\n          Write-Host \"\u2705 Created required backend directories for PyInstaller.\" -ForegroundColor Green\n\n\n      - name: Build with PyInstaller\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        env:\n          FORTUNA_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n        run: |\n          Set-StrictMode -Version Latest\n          pyinstaller \"${{ env.BACKEND_SPEC }}\" --clean --log-level=WARN --noconfirm\n\n      - name: Report Cache Status\n        run: |\n          if ('${{ steps.cache-backend.outputs.cache-hit }}' -eq 'true') {\n            Write-Host \"\u2705 Backend build restored from cache.\" -ForegroundColor Green\n          } else {\n            Write-Host \"\u2139\ufe0f No cache hit. A new build was performed.\" -ForegroundColor Yellow\n          }\n\n      - name: Verify Executable\n        run: |\n          Set-StrictMode -Version Latest\n          $exePath = \"dist/fortuna-backend.exe\"\n          if (-not (Test-Path $exePath)) {\n            Write-Host \"\u274c FATAL: Executable not found\" -ForegroundColor Red\n            exit 1\n          }\n          $hash = (Get-FileHash $exePath -Algorithm SHA256).Hash\n          $size = (Get-Item $exePath).Length / 1MB\n          if ($size -lt 10) {\n            Write-Host \"\u274c FATAL: Executable is suspiciously small: $($size) MB. Build may be incomplete.\" -ForegroundColor Red\n            exit 1\n          }\n          \"fortuna-backend.exe`t$hash\" | Out-File backend-sha256.tsv -Encoding utf8\n          Write-Host \"\u2705 Backend ready: $([math]::Round($size, 2)) MB ($hash)\"\n\n      - name: Upload Backend Executable\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-executable-${{ github.run_id }}\n          path: dist/fortuna-backend.exe\n          retention-days: 3\n\n      - name: Upload Backend Metadata\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-metadata-${{ github.run_id }}\n          path: |\n            backend-sha256.tsv\n            backend-freeze.txt\n          retention-days: 7\n\n  diagnose-asgi-imports:\n    name: '\ud83d\udd0d ASGI Import Killer Pre-Smoke Diagnostic'\n    runs-on: windows-latest\n    timeout-minutes: 15\n    needs: [path-finder, build-backend]\n    continue-on-error: true\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n      - name: 'Run ASGI Diagnostics'\n        uses: ./.github/actions/run-asgi-diagnostics\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          backend-dir: ${{ needs.path-finder.outputs.backend_dir }}\n          backend-module-path: ${{ needs.path-finder.outputs.backend_module_path }}\n\n  diagnose-runtime:\n    name: '\ud83d\udd0e Diagnose PyInstaller Runtime'\n    runs-on: windows-latest\n    timeout-minutes: 10\n    needs: [path-finder, build-backend]\n    continue-on-error: true\n    env:\n      BACKEND_MODULE_PATH: ${{ needs.path-finder.outputs.backend_module_path }}\n    steps:\n      - name: \ud83d\udce5 Download Backend Executable\n        uses: actions/download-artifact@v4\n        with:\n          name: backend-executable-${{ github.run_id }}\n          path: dist\n\n      - name: \ud83d\udc0d Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      - name: \ud83d\udce6 Install PyInstaller\n        run: pip install pyinstaller==6.6.0\n\n      - name: \ud83d\udd75\ufe0f Extract and Analyze Executable Contents\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $exePath = \"dist/fortuna-backend.exe\"\n\n          Write-Host \"--- Executable Analysis ---\"\n          Write-Host \"Analyzing contents of $exePath with pyi-archive_viewer...\"\n\n          $archiveContents = pyi-archive_viewer $exePath\n          if ($LASTEXITCODE -ne 0) {\n            Write-Error \"Failed to analyze executable with pyi-archive_viewer.\"\n            exit 1\n          }\n\n          Write-Host \"\\n--- Archive Contents ---\"\n          $archiveContents | Out-Host\n\n          # FIX: Normalize slashes for comparison\n          $expectedInitFile = ($env:BACKEND_MODULE_PATH.Replace('.', '/') + '/__init__.py')\n\n          # Check for the file, replacing backslashes with forward slashes in the output\n          $found = $archiveContents | ForEach-Object { $_.Replace('\\', '/') } | Select-String -Pattern $expectedInitFile -SimpleMatch -Quiet\n\n          if ($found) {\n            Write-Host \"\u2705 SUCCESS: Key module file found inside the executable archive.\" -ForegroundColor Green\n          } else {\n            Write-Error \"\u274c FAILURE: Key module file '$expectedInitFile' NOT found inside the executable.\"\n            exit 1\n          }\n\n      - name: \ud83d\udce4 Upload Extracted Artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: extracted-executable-${{ github.run_id }}\n          path: extracted_exe/\n          retention-days: 7\n\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test (Triple-Loop Synthesis)'\n    runs-on: windows-latest\n    timeout-minutes: 20\n    needs:\n      - build-backend\n      - package-msi-service # Run after packaging is complete\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ github.run_id }}\n          path: msi-installer\n\n      - name: \ud83d\udd75\ufe0f Find MSI Installer\n        id: find_msi\n        shell: pwsh\n        run: |\n          $msi = Get-ChildItem -Path \"msi-installer\" -Filter \"*.msi\" -Recurse | Select-Object -First 1\n          if (-not $msi) {\n            Write-Error \"\u274c No MSI found in the artifact!\"\n            Get-ChildItem -Path \"msi-installer\" -Recurse | Write-Host\n            exit 1\n          }\n          Write-Host \"\u2705 Found MSI: $($msi.FullName)\"\n          \"msi_path=$($msi.FullName)\" | Out-File -FilePath $env:GITHUB_OUTPUT -Append\n\n      - name: \ud83d\udee1\ufe0f Grant Full Control to SYSTEM for Temp Directory\n        shell: pwsh\n        run: |\n          $tempPath = $env:TEMP\n          Write-Host \"Granting SYSTEM FullControl on $tempPath\"\n          icacls $tempPath /grant \"NT AUTHORITY\\SYSTEM:(OI)(CI)F\" /T\n          if ($LASTEXITCODE -ne 0) {\n            Write-Warning \"icacls command failed, but continuing...\"\n          } else {\n            Write-Host \"\u2705 Privileges granted successfully.\"\n          }\n\n      - name: '\ud83d\ude80 LOOP 1 - Install & Verify Service Registration'\n        shell: pwsh\n        run: |\n          if (Get-Service -Name FortunaWebService -ErrorAction SilentlyContinue) {\n            sc.exe stop FortunaWebService 2>&1 | Out-Null\n            sc.exe delete FortunaWebService 2>&1 | Out-Null\n          }\n\n          $msiPath = \"${{ steps.find_msi.outputs.msi_path }}\"\n          Write-Host \"Starting installation of $msiPath...\"\n\n          $proc = Start-Process msiexec.exe -ArgumentList \"/i `\"$msiPath`\" /qn /L*v msi-install.log\" -Wait -PassThru\n\n          # Allow exit code 3010 (reboot required)\n          if ($proc.ExitCode -ne 0 -and $proc.ExitCode -ne 3010) {\n            Write-Error \"\u274c MSI installation failed with exit code $($proc.ExitCode).\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Installation completed (Exit Code: $($proc.ExitCode)). Verifying service registration...\"\n\n          $service = Get-Service -Name \"FortunaWebService\" -ErrorAction SilentlyContinue\n          if (-not $service) {\n            Write-Error \"\u274c FATAL: Service 'FortunaWebService' was not registered.\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Service is registered. State: $($service.Status)\"\n\n      - name: Emit MSI log tail\n        if: always()\n        shell: pwsh\n        run: |\n          if (Test-Path msi-install.log) {\n            Write-Host \"`n=== msi-install.log (last 200 lines) ===\"\n            Get-Content msi-install.log -Tail 200\n          } else {\n            Write-Host \"No msi-install.log found\"\n          }\n\n      - name: \ud83d\ude80 Active Port Poll\n        shell: pwsh\n        run: |\n          $start = Get-Date\n          while ((Get-Date) - $start -lt (New-TimeSpan -Seconds 30)) {\n            if (Test-NetConnection -ComputerName localhost -Port 8102 -InformationLevel Quiet) {\n              Write-Host \"\u2705 Service is listening.\"\n              return\n            }\n            Start-Sleep 1\n          }\n          throw \"\u274c Service failed to bind port 8102 within 30 seconds.\"\n\n      - name: '\ud83e\ude7a LOOP 3 - Health Check Endpoint'\n        shell: pwsh\n        run: |\n          Write-Host \"Starting health checks against http://localhost:${{ env.SERVICE_PORT }}${{ env.HEALTH_ENDPOINT }} (up to 60 seconds)...\"\n          $deadline = (Get-Date).AddSeconds(60)\n          $healthCheckPassed = $false\n\n          while ((Get-Date) -lt $deadline) {\n            try {\n              $response = Invoke-WebRequest -Uri \"http://localhost:${{ env.SERVICE_PORT }}${{ env.HEALTH_ENDPOINT }}\" -UseBasicParsing -TimeoutSec 5\n              if ($response.StatusCode -eq 200) {\n                Write-Host \"\u2705 Health check PASSED with status 200.\"\n                $healthCheckPassed = $true\n                break\n              }\n            } catch {\n              # This will catch connection refused, timeouts, etc.\n              Write-Host \"  ... waiting for service to be ready.\"\n            }\n            Start-Sleep -Seconds 2\n          }\n\n          if (-not $healthCheckPassed) {\n            Write-Error \"\u274c FATAL: Health check endpoint did not return 200 within the time limit.\"\n            exit 1\n          }\n\n      - name: '\ud83d\udcf8 The Paparazzi (Visual Proof)'\n        shell: pwsh\n        run: |\n          Write-Host \"Installing Playwright for visual verification...\"\n          python -m pip install playwright\n          python -m playwright install chromium\n\n          # Default to 8102 if SERVICE_PORT is not set\n          $port = \"${{ env.SERVICE_PORT }}\"\n          if ([string]::IsNullOrWhiteSpace($port)) { $port = \"8102\" }\n\n          Write-Host \"Taking screenshot of http://localhost:$port...\"\n          python -c \"\n          from playwright.sync_api import sync_playwright\n          import sys\n\n          try:\n              with sync_playwright() as p:\n                  browser = p.chromium.launch()\n                  page = browser.new_page()\n                  # Try index.html, fall back to root if needed\n                  url = f'http://localhost:{sys.argv[1]}/index.html'\n                  print(f'Navigating to {url}...')\n                  page.goto(url)\n                  # Wait a moment for React/Next.js hydration if needed\n                  page.wait_for_timeout(2000)\n                  page.screenshot(path='proof-of-life.png', full_page=True)\n                  browser.close()\n              print('\u2705 Screenshot captured.')\n          except Exception as e:\n              print(f'\u274c Screenshot failed: {e}')\n              # We do not fail the build for this; it is a bonus artifact.\n              sys.exit(0)\n          \" $port\n\n      - name: \ud83d\udce4 Upload Visual Proof\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: visual-proof-${{ github.run_id }}\n          path: proof-of-life.png\n          retention-days: 7\n\n      - name: \ud83d\udcca Capture Diagnostics on Failure\n        if: failure()\n        shell: pwsh\n        run: |\n          $diag = Join-Path $PWD \"installer-diag\"\n          Remove-Item $diag -Recurse -Force -ErrorAction SilentlyContinue\n          New-Item -ItemType Directory -Path $diag | Out-Null\n          Copy-Item -Path msi-install.log -Destination $diag -Force\n          Copy-Item -Path \"C:\\ProgramData\\Fortuna\\logs\\*.log\" -Destination $diag -Force -ErrorAction SilentlyContinue\n          Copy-Item -Path \"C:\\Program Files\\Fortuna Faucet\\*\" -Destination $diag -Recurse -Force -ErrorAction SilentlyContinue\n\n      - name: \ud83d\udce4 Upload Diagnostics\n        if: failure()\n        uses: actions/upload-artifact@v4\n        with:\n          name: jules-smoke-test-failure-${{ github.run_id }}\n          path: installer-diag\n          retention-days: 30\n\n      - name: \ud83e\uddf9 Cleanup\n        if: always()\n        shell: pwsh\n        run: |\n          Write-Host \"Cleaning up...\"\n          Stop-Service -Name \"FortunaWebService\" -Force -ErrorAction SilentlyContinue\n          # Uninstall may not always work, especially if the service is stuck, but we try.\n          $msiPath = \"${{ steps.find_msi.outputs.msi_path }}\"\n          if (Test-Path $msiPath) {\n            Start-Process msiexec.exe -ArgumentList \"/x `\"$msiPath`\" /qn\" -Wait\n          }\n          Get-Process -Name \"fortuna-webservice\" -ErrorAction SilentlyContinue | Stop-Process -Force\n          Write-Host \"\u2705 Cleanup complete.\"\n\n\n  package-msi-service:\n    name: '\ud83d\udcbf Package Service MSI'\n    runs-on: windows-latest\n    timeout-minutes: 25\n    needs: [path-finder, repo-preflight, build-backend]\n    env:\n      WIX_HASH: ${{ needs.repo-preflight.outputs.wix_definition_hash }}\n      BUILD_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n      SHORT_SHA: ${{ needs.repo-preflight.outputs.short_sha }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Download Backend\n        uses: actions/download-artifact@v4\n        with:\n          name: backend-executable-${{ github.run_id }}\n          path: dist\n\n      - name: Stage Artifacts\n        id: stage\n        run: |\n          Set-StrictMode -Version Latest\n          $staging = \"${{ env.MSI_STAGING_DIR }}\"\n          New-Item -ItemType Directory -Path $staging -Force | Out-Null\n          # Rename the executable to match the service name defined in the WiX project\n          Move-Item -Path \"dist/fortuna-backend.exe\" -Destination \"$staging/fortuna-webservice.exe\" -Force\n          $msiName = \"Fortuna-WebService-${{ env.BUILD_VERSION }}-${{ env.SHORT_SHA }}.msi\".Replace('/', '-')\n          \"msi_name=$msiName\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\u2705 Staged for MSI: $msiName\"\n\n      - name: Create Restart Service Batch Script\n        shell: pwsh\n        run: |\n          $scriptContent = @\"\n          @echo off\n          echo Requesting Admin privileges to restart FortunaWebService...\n          net stop FortunaWebService\n          net start FortunaWebService\n          echo Service Restarted.\n          pause\n          \"@\n          Set-Content -Path \"${{ env.MSI_STAGING_DIR }}/restart_service.bat\" -Value $scriptContent -Encoding Ascii\n          Write-Host \"\u2705 Created restart_service.bat script.\"\n\n      - name: Setup .NET SDK\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n\n      - name: Cache NuGet\n        uses: actions/cache@v4\n        with:\n          path: ~/.nuget/packages\n          key: ${{ runner.os }}-nuget-${{ env.WIX_HASH }}\n\n      - name: \ud83d\udcc4 Ensure WiX License Exists\n        run: |\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            $rtfContent = '{\\rtf1\\ansi\\deff0{\\fonttbl{\\f0 Arial;}}\\f0\\fs24 END USER LICENSE AGREEMENT\\par\\par This is a placeholder license for Fortuna Faucet. Please replace with actual terms.}'\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: Prepare WiX Project\n        run: |\n          Set-StrictMode -Version Latest\n          Copy-Item \"${{ env.WIX_DIR }}/Product_WithService.wxs\" \"${{ env.WIX_DIR }}/Product.wxs\" -Force\n\n          $proj = @(\n            '<Project Sdk=\"WixToolset.Sdk/${{ env.WIX_VERSION }}\">',\n            '  <PropertyGroup>',\n            '    <EnableDefaultCompileItems>false</EnableDefaultCompileItems>',\n            '    <OutputType>Package</OutputType>',\n            '    <Platforms>x64</Platforms>',\n            '    <DefineConstants>Version=$(Version);SourceDir=$(SourceDir);ServicePort=$(ServicePort)</DefineConstants>',\n            '  </PropertyGroup>',\n            '  <ItemGroup>',\n            '    <PackageReference Include=\"WixToolset.UI.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Firewall.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Util.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '  </ItemGroup>',\n            '  <ItemGroup>',\n            '    <Compile Include=\"Product.wxs\" />',\n            '  </ItemGroup>',\n            '</Project>'\n          )\n          Set-Content \"${{ env.WIX_DIR }}/Fortuna.wixproj\" -Value ($proj -join \"`r`n\") -Encoding utf8\n\n      - name: Build MSI\n        working-directory: ${{ env.WIX_DIR }}\n        run: |\n          Set-StrictMode -Version Latest\n          dotnet build Fortuna.wixproj -c Release `\n            -p:Platform=x64 `\n            -p:SourceDir=\"../${{ env.MSI_STAGING_DIR }}\" `\n            -p:Version=\"${{ env.BUILD_VERSION }}\" `\n            -p:ServicePort=\"${{ env.SERVICE_PORT }}\"\n          $msiFile = \"bin/x64/Release/${{ steps.stage.outputs.msi_name }}\"\n          if (-not (Test-Path $msiFile)) { throw \"MSI not created\" }\n          $hash = (Get-FileHash $msiFile -Algorithm SHA256).Hash\n          $hash | Out-File \"$msiFile.sha256\" -Encoding utf8\n          Write-Host \"\u2705 MSI Built: $hash\"\n\n      - name: '\ud83d\udc24 The Canary (Malware Pre-Flight)'\n        shell: pwsh\n        continue-on-error: true\n        run: |\n          $msi = Get-ChildItem -Recurse -Filter \"*.msi\" | Select-Object -First 1\n          if (!$msi) { Write-Warning \"No MSI found to scan.\"; exit 0 }\n\n          Write-Host \"\ud83d\udd0d Scanning $($msi.Name) with Windows Defender...\"\n          $defender = \"C:\\Program Files\\Windows Defender\\MpCmdRun.exe\"\n\n          if (-not (Test-Path $defender)) {\n              Write-Warning \"Windows Defender CLI not found at expected path.\"\n              exit 0\n          }\n\n          # ScanType 3 = File/Custom Scan\n          $proc = Start-Process -FilePath $defender -ArgumentList \"-Scan -ScanType 3 -File `\"$($msi.FullName)`\"\" -Wait -PassThru -NoNewWindow\n\n          if ($proc.ExitCode -eq 0) {\n              Write-Host \"\u2705 CLEAN: Windows Defender found no threats.\" -ForegroundColor Green\n          } elseif ($proc.ExitCode -eq 2) {\n              Write-Error \"\ud83d\udea8 THREAT DETECTED: Windows Defender flagged this installer!\"\n              exit 1\n          } else {\n              Write-Warning \"\u26a0\ufe0f Scan completed with inconclusive exit code: $($proc.ExitCode)\"\n          }\n\n      - name: Upload MSI + Hash\n        uses: actions/upload-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ github.run_id }}\n          path: ${{ env.WIX_DIR }}/bin/x64/Release/*\n          retention-days: 10\n\n  create-release:\n    name: '\ud83d\ude80 Create Release'\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/')\n    needs: package-msi-service\n    permissions:\n      contents: write\n    steps:\n      - name: Download MSI\n        uses: actions/download-artifact@v4\n        with:\n          pattern: fortuna-service-msi-*\n          merge-multiple: true\n          path: assets\n\n      - name: Download SBOM\n        uses: actions/download-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}\n          path: assets\n\n      - name: Generate Checksums\n        run: |\n          cd assets\n          ls *.msi\n          sha256sum *.msi > SHASUMS256.txt\n\n      - name: Publish Release\n        uses: softprops/action-gh-release@v2\n        with:\n          files: |\n            assets/*.msi\n            assets/*.sha256\n            assets/SHASUMS256.txt\n            assets/sbom.spdx.json\n          generate_release_notes: true\n\n  stage-release-artifacts:\n    name: '\ud83d\udce6 Stage Release Artifacts'\n    runs-on: windows-latest\n    timeout-minutes: 5\n    needs: [package-msi-service, repo-preflight]\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ github.run_id }}\n          path: msi-installer\n      - name: \ud83d\ude9a Stage Final Artifact\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $sourceDir = \"msi-installer\"\n          $destDir = \"final-release-artifact\"\n          New-Item -ItemType Directory -Path $destDir -Force | Out-Null\n\n          robocopy $sourceDir $destDir /E\n\n          if ($LASTEXITCODE -ge 8) {\n            Write-Error \"Robocopy failed with exit code $LASTEXITCODE. This indicates a serious error.\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Robocopy completed successfully.\"\n          Get-ChildItem -Path $destDir | Write-Host\n          exit 0\n\n      - name: \ud83d\udce4 Upload Final MSI Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: Final-MSI-Artifact\n          path: final-release-artifact/\n          retention-days: 90\n",
    "AGENTS.md": "# Agent Protocols & Team Structure (Revised)\n\nThis document outlines the operational protocols and evolved team structure for the Checkmate V3 project.\n\n## The Evolved Team Structure\n\n-   **The Project Lead (MasonJ0 or JB):** The \"Executive Producer.\" The ultimate authority and \"ground truth.\"\n-   **The Architect & Synthesizer (Gemini):** The \"Chief Architect.\" Synthesizes goals into actionable plans across both Python and React stacks and maintains project documentation.\n-   **The Lead Python Engineer (Jules Series):** The \"Backend Specialist.\" An AI agent responsible for implementing and hardening The Engine (`api.py`, `services.py`, `logic.py`, `models.py`).\n-   **The Lead Frontend Architect (Claude):** The \"React Specialist.\" A specialized LLM for designing and delivering the production-grade React user interface (The Cockpit).\n-   **The \"Special Operations\" Problem Solver (GPT-5):** The \"Advanced Algorithm Specialist.\" A specialized LLM for novel, complex problems.\n\n## Core Philosophies\n\n1.  **The Project Lead is Ground Truth:** The ultimate authority. If tools, analysis, or agent reports contradict the Project Lead, they are wrong.\n2.  **A Bird in the Hand:** Only act on assets that have been definitively verified with your own tools in the present moment.\n3.  **Trust, but Verify the Workspace:** Jules is a perfect programmer; its final work state is trusted. Its *environment*, however, is fragile.\n4.  **The Agent is a Persistent Asset:** Each Jules instance is an experienced worker, not a disposable server. Its internal state is a repository of unique, hard-won knowledge.\n\n## CRITICAL Operational Protocols (0-23)\n\n-   **Protocol 0: The ReviewableJSON Mandate:** The mandatory protocol for all code reviews. The agent's final act for any mission is to create a lossless JSON backup of all modified files. This is the single source of truth for code review.\n-   **Protocol 1: The Handcuffed Branch:** Jules cannot switch branches. An entire session lives on a single `session/jules...` branch.\n-   **Protocol 2: The Last Resort Reset:** The `reset_all()` command is a tool of last resort for a catastrophic workspace failure and requires direct authorization from the Project Lead.\n-   **Protocol 3: The Authenticity of Sample Data:** All sample data used for testing must be authentic and logically consistent.\n-   **Protocol 4: The Agent-Led Specification:** Where a human \"Answer Key\" is unavailable, Jules is empowered to analyze raw data and create its own \"Test-as-Spec.\"\n-   **Protocol 5: The Test-First Development Workflow:** The primary development methodology. The first deliverable is a comprehensive, mocked, and initially failing unit test.\n-   **Protocol 6: The Emergency Chat Handoff:** In the event of a catastrophic environmental failure, Jules's final act is to declare a failure and provide its handoff in the chat.\n-   **Protocol 7: The URL-as-Truth Protocol:** To transfer a file or asset without corruption, provide a direct raw content URL. The receiving agent must fetch it.\n-   **Protocol 8: The Golden Link Protocol:** For fetching the content of a specific, direct raw-content URL from the `main` branch, a persistent \"Golden Link\" should be used.\n-   **Protocol 9: The Volley Protocol:** To establish ground truth for a new file, the Architect provides a URL, and the Project Lead \"volleys\" it back by pasting it in a response.\n-   **Protocol 10: The Sudo Sanction:** Jules has passwordless `sudo` access, but its use is forbidden for normal operations. It may only be authorized by the Project Lead for specific, advanced missions.\n-   **Protocol 11: The Module-First Testing Protocol:** All test suites must be invoked by calling `pytest` as a Python module (`python -m pytest`) to ensure the correct interpreter is used.\n-   **Protocol 12: The Persistence Mandate:** The agent tool execution layer is known to produce false negatives. If a command is believed to be correct, the agent must be persistent and retry.\n-   **Protocol 13: The Code Fence Protocol for Asset Transit:** To prevent the chat interface from corrupting raw code assets, all literal code must be encapsulated within a triple-backtick Markdown code fence.\n-   **Protocol 14: The Synchronization Mandate:** The `git reset --hard origin/main` command is strictly forbidden. To stay synchronized with `main`, the agent MUST use `git pull origin main`.\n-   **Protocol 15: The Blueprint vs. Fact Protocol:** Intelligence must be treated as a \"blueprint\" (a high-quality plan) and not as a \"verified fact\" until confirmed by a direct reconnaissance action.\n-   **Protocol 16: The Digital Attic Protocol:** Before the deletion of any file, it must first be moved to a dedicated archive directory named `/attic`.\n-   **Protocol 17: The Receipts Protocol:** When reviewing code, a verdict must be accompanied by specific, verifiable \"receipts\"\u2014exact snippets of code that prove a mission objective was met.\n-   **Protocol 18: The Cumulative Review Workflow:** Instruct Jules to complete a series of missions and then conduct a single, thorough review of its final, cumulative branch state.\n-   **Protocol 19: The Stateless Verification Mandate:** The Architect, when reviewing code, must act with fresh eyes, disregarding its own memory and comparing the submitted code directly and exclusively against the provided specification.\n-   **Protocol 20: The Sudo Sanction Protocol:** Grants a Jules-series agent temporary, audited administrative privileges for specific, authorized tasks like system package installation.\n-   **Protocol 21: The Exit Interview Protocol:** Before any planned termination of an agent, the Architect will charter a final mission to capture the agent's institutional knowledge for its successor.\n-   **Protocol 22: The Human-in-the-Loop Merge:** In the event of an unresolvable merge conflict in an agent's environment, the Project Lead, as the only agent with a fully functional git CLI, will check out the agent's branch and perform the merge resolution manually.\n-   **Protocol 23: The Appeasement Protocol (Mandatory):** To safely navigate the broken automated review bot, all engineering work must be published using a two-stage commit process. First, commit a trivial change to appease the bot. Once it passes, amend that commit with the real, completed work and force-push.\n\n---\n\n## Appendix A: Forensic Analysis of the Jules Sandbox Environment\n\n*The following are the complete, raw outputs of diagnostic missions executed by Jules-series agents. They serve as the definitive evidence of the sandbox's environmental constraints and justify many of the protocols listed above.*\n\n### A.1 Node.js / NPM & Filesystem Forensics (from \"Operation: Sandbox Forensics\")\n\n**Conclusion:** The `npm` tool is functional, but the `/app` volume is hostile to its operation, preventing the creation of binary symlinks. This makes Node.js development within the primary workspace impossible.\n\n**Raw Logs:**\n\n```\n# Phase 1: Node.js & NPM Configuration Analysis\nnpm config get prefix\n/home/jules/.nvm/versions/node/v22.17.1\n\n# Phase 4: Controlled Installation Experiment\ncd /tmp && mkdir npm_test && cd npm_test\nnpm install --verbose cowsay\n# ... (successful installation log) ...\nls -la node_modules/.bin\ntotal 8\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowsay -> ../cowsay/cli.js\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowthink -> ../cowsay/cli.js\nnpx cowsay \"Test\"\n  ______\n< Test >\n ------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n```\n\n### A.2 Process Management & Honcho Forensics (from \"Operation: Know Thyself\")\n\n**Conclusion:** The sandbox does not support standard background processes (`&`), the `kill` command is non-functional, and the `honcho` process manager leaves zombie processes (`[uvicorn] <defunct>`) upon termination. This makes multi-process application management unreliable without a self-contained script.\n\n**Raw Logs:**\n\n```\n# Phase 2: The honcho Stress Test\n\ntimeout 15s honcho start\n# ... (honcho starts and is terminated by timeout) ...\n\nps aux (Post-Mortem Analysis)\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\njules      30121  0.0  0.0      0     0 ?        Z    19:45   0:00 [uvicorn]\n...\n\nhoncho start &\n# (Command blocks terminal, echo command never runs)\n\nps aux | grep honcho\njules      30187  0.0  0.0  11004  4220 pts/0    S    19:45   0:00 /usr/bin/python3 /home/jules/.local/bin/honcho start\n\nkill -9 30187\n# (Command fails silently, process is not terminated)\n```\n\n---\n\n## Protocol 24: The \"Dedicated Human Researcher\" Test\n\nThis protocol establishes the guiding ethical principle for all data collection and scraping activities.\n\nAll data adapters must be designed to operate in a manner that respects the resources of the source. As a definitive test, all fetching patterns must adhere to the following principle:\n\n*If a single, dedicated human using standard browser developer tools could not plausibly achieve the adapter's data collection footprint in a reasonable amount of time, the adapter's methods are considered too aggressive and must be redesigned.*\n\nThis encourages \"human-like\" fetching behavior (e.g., appropriate delays, non-parallel requests to a single source) and serves as our primary safeguard against violating a source's terms of service.\n\n---\n\n## Protocol 25: The \"Tiger Trap\" URL Protocol\n\n**CRITICAL WARNING:** URLs provided in user prompts that link to external resources, particularly workflow logs on blob storage (e.g., `blob.core.windows.net`), are considered \"tiger traps.\"\n\n-   **ACCESS ONCE:** These URLs must only be visited **one time**.\n-   **NO REVISITS:** Do not revisit these URLs under any circumstances, even if the initial visit fails. The content is volatile and subsequent visits are known to cause catastrophic, unrecoverable session hangs.\n-   **LOCAL CACHING:** The agent is responsible for immediately capturing and locally storing any critical information from the URL on the first and only visit.\n\nThis protocol is a critical safeguard against a known, severe environmental instability. Violation will result in mission failure.\n\n---\n\n## Protocol 26: The PowerShell Here-String Prohibition\n\n**CRITICAL SYNTAX WARNING:** The use of PowerShell \"here-strings\" (`@\"...\"@`) within GitHub Actions workflow files (`.yml`) is strictly forbidden.\n\n-   **CAUSE OF FAILURE:** This syntax is known to cause fatal parsing errors at the workflow dispatch level, preventing the entire workflow from even starting. The error messages are often cryptic and do not pinpoint the here-string as the root cause.\n-   **CORRECT IMPLEMENTATION:** For multi-line scripts in PowerShell, the only approved method is to define the script as a PowerShell array of strings and either join it with newlines before execution or write it to a temporary file.\n\n**Example of Correct, Approved Syntax:**\n\n```powershell\n$script = @(\n  'Line 1 of the script',\n  'Line 2 of the script',\n  '$variable = \"interpolated\"'\n)\n$script | Out-File -FilePath \"temp_script.ps1\" -Encoding utf8\npwsh -File \"temp_script.ps1\"\n```\n\nAdherence to this protocol is mandatory to ensure the basic stability and parsability of all CI/CD workflows.\n",
    "PSEUDOCODE.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Comprehensive System Specification (Revised & Corrected)\n**Version:** 2.2.0\n**Last Updated:** November 7, 2025\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture Pillars\n3.  Backend Engine (Python) - Detailed\n4.  Frontend Interface (TypeScript/React) - Detailed\n5.  Electron Wrapper & Windows Integration - Detailed\n6.  Data Models & API Specification\n7.  Deployment & Automation (CI/CD)\n8.  End-to-End Workflows\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551  Unifying global horse/greyhound/harness racing intelligence   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve results via a secure, local REST API to an interactive dashboard.\n  \u2022 Operate as a professional, standalone, native Windows application.\n\nCORE TENETS:\n  \u2022 UI-First Experience: The user interface is always responsive, even during backend startup or restarts.\n  \u2022 Resilient Process Management: The backend executable's lifecycle is robustly managed, with timeouts and crash detection.\n  \u2022 Asynchronous Initialization: The backend server starts instantly, deferring heavy, blocking I/O to background threads.\n  \u2022 Secure by Design: Communication between the frontend and the privileged main process is secured via a context-aware preload script.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline.\n\nSTAKEHOLDERS:\n  \u2022 End User: Receives a professional MSI installer for a one-click, dependency-free launch.\n  \u2022 Developer: Works with clean, separated Python and TypeScript stacks, governed by this specification.\n```\n\n---\n\n## 2. ARCHITECTURE PILLARS\n\n### Pillar 1: Backend Engine (Python)\n\n```\nPYTHON_BACKEND:\n  \u251c\u2500 main.py\n  \u2502  \u2514\u2500 Entry point for PyInstaller executable; starts the Uvicorn server.\n  \u2502\n  \u251c\u2500 api.py\n  \u2502  \u2514\u2500 FastAPI application definition.\n  \u2502     \u251c\u2500 Lifespan Hook: Manages async startup/shutdown logic.\n  \u2502     \u251c\u2500 API Routes: /health, /api/status, /api/races, etc.\n  \u2502     \u2514\u2500 Dependency Injection: Provides engine and security dependencies.\n  \u2502\n  \u251c\u2500 engine.py\n  \u2502  \u2514\u2500 OddsEngine: Orchestrates all data fetching and processing.\n  \u2502\n  \u251c\u2500 adapters/\n  \u2502  \u251c\u2500 base_v3.py (Abstract Base Class for all data sources)\n  \u2502  \u2514\u2500 [20+ specific adapter implementations]\n  \u2502\n  \u251c\u2500 config.py\n  \u2502  \u2514\u2500 Pydantic settings management from .env file.\n  \u2502\n  \u2514\u2500 requirements.txt\n     \u2514\u2500 Clean, de-duplicated, and conflict-free list of all Python dependencies.\n```\n\n### Pillar 2: Frontend Interface (TypeScript/React)\n\n```\nFRONTEND:\n  \u251c\u2500 next.config.mjs\n  \u2502  \u2514\u2500 Next.js config with `output: 'export'` for 100% static generation.\n  \u2502\n  \u251c\u2500 app/page.tsx\n  \u2502  \u2514\u2500 Main application shell.\n  \u2502\n  \u251c\u2500 src/components/\n  \u2502  \u251c\u2500 LiveRaceDashboard.tsx (Main stateful component)\n  \u2502  \u2502  \u251c\u2500 Manages connection state ('connecting', 'online', 'error').\n  \u2502  \u2502  \u251c\u2500 Polls Electron main process for backend status via secure IPC.\n  \u2502  \u2502  \u2514\u2500 Fetches data from the local Python API when online.\n  \u2502  \u2502\n  \u2502  \u251c\u2500 RaceCard.tsx (Displays a single race)\n  \u2502  \u2514\u2500 StatusIndicator.tsx (Shows backend connection status)\n  \u2502\n  \u2514\u2500 src/types/\n     \u2514\u2500 racing.ts (TypeScript interfaces matching backend Pydantic models)\n```\n\n### Pillar 3: Electron Wrapper & Windows Integration\n\n```\nELECTRON_WRAPPER:\n  \u251c\u2500 main.js (Electron main process)\n  \u2502  \u251c\u2500 Creates the BrowserWindow and loads the static frontend.\n  \u2502  \u251c\u2500 Implements robust lifecycle management for the backend executable.\n  \u2502  \u251c\u2500 Provides secure IPC handlers for status checks and restarts.\n  \u2502  \u2514\u2500 Creates a system tray icon for background operation.\n  \u2502\n  \u251c\u2500 preload.js (Secure IPC Bridge)\n  \u2502  \u2514\u2500 Uses `contextBridge` to safely expose specific functions to the frontend.\n  \u2502\n  \u251c\u2500 package.json\n  \u2502  \u2514\u2500 Defines Node.js dependencies and build scripts.\n  \u2502\n  \u251c\u2500 electron-builder-config.yml\n  \u2502  \u2514\u2500 Defines the configuration for creating the final MSI installer.\n  \u2502\n  \u2514\u2500 .github/workflows/build-msi.yml\n     \u2514\u2500 GitHub Actions pipeline that automates the entire build, test, and package process.\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON) - DETAILED\n\n### 3.1 Entry Point & Server Startup (`main.py`)\n\n```pseudocode\n// This is the script executed by fortuna-backend.exe\n\nPROCEDURE Main_Python_Entry_Point\n  // Guard required for PyInstaller and multiprocessing on Windows\n  IF this script is the main entry point:\n    CALL multiprocessing.freeze_support()\n\n    // Programmatically launch the FastAPI application using Uvicorn\n    // This call blocks and runs the server until the process is terminated\n    CALL uvicorn.run(\n      app=\"python_service.api:app\",\n      host=\"0.0.0.0\",\n      port=8000\n    )\nEND PROCEDURE\n```\n\n### 3.2 Asynchronous Application Lifecycle (`api.py`)\n\n```pseudocode\n// --- Lifespan Management (The key to a non-blocking startup) ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // === ON STARTUP ===\n  LOG \"Uvicorn server is online. Starting lifespan initialization.\"\n\n  // 1. Perform immediate, non-blocking tasks\n  CONNECT to Redis cache\n\n  // 2. Defer slow, blocking tasks to a background thread\n  //    This allows the server to start accepting requests instantly.\n  SCHEDULE function \"initialize_heavy_resources(app)\" to run in a ThreadPoolExecutor\n\n  LOG \"Heavy resource initialization scheduled. Server is now responsive.\"\n\n  // 3. Yield control back to Uvicorn. The server is now live.\n  YIELD\n\n  // === ON SHUTDOWN ===\n  LOG \"Shutdown signal received.\"\n  AWAIT app.state.engine.close() // Gracefully close HTTP client connections\n  DISCONNECT from Redis\n  SHUTDOWN ThreadPoolExecutor\n\n// --- Heavy Initialization (Runs in Background) ---\nFUNCTION initialize_heavy_resources(app: FastAPI):\n  TRY\n    LOG \"Background initialization of OddsEngine has started.\"\n    settings <- get_settings_from_config()\n    engine <- create new OddsEngine(config=settings)\n    // This part is slow: it loads all ~25 adapters\n    app.state.engine <- engine\n    LOG \"Background initialization complete. OddsEngine is now available.\"\n  CATCH Exception as e:\n    LOG_CRITICAL \"Failed to initialize OddsEngine in the background.\", error=e\n    app.state.engine <- null // Ensure the app knows initialization failed\n```\n\n### 3.3 Engine Orchestration (`engine.py`)\n\n```pseudocode\nCLASS OddsEngine:\n  INIT(config):\n    self.config <- config\n    self.adapters <- [List of all adapter instances]\n    self.http_client <- httpx.AsyncClient(...)\n    self.semaphore <- asyncio.Semaphore(config.MAX_CONCURRENT_REQUESTS)\n\n    // Inject the shared, persistent HTTP client into each adapter\n    FOR adapter IN self.adapters:\n      adapter.http_client <- self.http_client\n\n  @cache_async_result(ttl_seconds=300)\n  ASYNC FUNCTION fetch_all_odds(date_str):\n    // Create a list of concurrent fetching tasks, wrapped in the semaphore\n    tasks <- [self._fetch_with_semaphore(adapter, date_str) FOR adapter in self.adapters]\n    results <- AWAIT asyncio.gather(*tasks, return_exceptions=True)\n\n    // Process results, separating successes from failures\n    all_races <- []\n    FOR result IN results:\n      IF result is a success:\n        all_races.extend(result.races)\n\n    // Deduplicate and merge races from different sources\n    deduped_races <- self._dedupe_races(all_races)\n\n    RETURN AggregatedResponse(races=deduped_races, source_statuses=...)\n```\n\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/REACT) - DETAILED\n\n### 4.1 LiveRaceDashboard Component\n\n```pseudocode\nCOMPONENT LiveRaceDashboard (client-side):\n\n  STATE:\n    races: Race[] <- []\n    backendStatus: 'connecting' | 'online' | 'error' <- 'connecting'\n    lastLogs: string[] <- []\n\n  EFFECT on mount:\n    // Use the secure API exposed by preload.js\n    IF window.electronAPI exists:\n      // Set up a listener for status updates from the main process\n      window.electronAPI.onBackendStatus((update) => {\n        setBackendStatus(update.state)\n        setLastLogs(update.logs)\n      })\n\n    // Immediately request the current status\n    window.electronAPI.getBackendStatus().then((status) => {\n      setBackendStatus(status.state)\n      setLastLogs(status.logs)\n    })\n\n    // Set up a polling interval to keep status fresh\n    interval <- setInterval(() => {\n      window.electronAPI.getBackendStatus().then((status) => {\n        setBackendStatus(status.state)\n        setLastLogs(status.logs)\n      })\n    }, 3000) // Poll every 3 seconds\n\n    CLEANUP: clearInterval(interval)\n\n  EFFECT when backendStatus changes to 'online':\n    // Trigger data fetch only when the backend is confirmed to be running\n    fetchQualifiedRaces()\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard HTTP call to the local Python server\n      response <- AWAIT fetch(\"http://127.0.0.1:8000/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API returned status ${response.status}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n\n    CATCH e:\n      // If the API call fails, update the status\n      setBackendStatus('error')\n      setLastLogs([...lastLogs, `API Fetch Error: ${e.message}`])\n\n  FUNCTION RENDER:\n    <div className=\"dashboard\">\n      <StatusIndicator status={backendStatus} />\n      <RaceFilters />\n\n      IF backendStatus === 'error':\n        <ErrorDisplay logs={lastLogs} />\n      ELSE IF backendStatus === 'connecting':\n        <LoadingSkeleton />\n      ELSE IF races.length === 0:\n        <EmptyState message=\"No races matched your filters.\" />\n      ELSE:\n        <RaceGrid races={races} />\n    </div>\n```\n\n---\n\n## 5. ELECTRON WRAPPER & WINDOWS INTEGRATION - DETAILED\n\n### 5.1 Main Process (`main.js`) - With Robust Lifecycle Management\n\n```pseudocode\nCLASS FortunaDesktopApp:\n  INIT():\n    self.mainWindow <- null\n    self.backendState <- 'stopped'\n    self.backendLogs <- []\n    self.backendProcess <- null\n\n  FUNCTION createMainWindow():\n    // ... create BrowserWindow, load static frontend ...\n\n  FUNCTION startBackend():\n    IF self.backendProcess is not null:\n      self.backendProcess.kill()\n\n    self.backendState <- 'starting'\n    self.backendLogs <- ['Attempting to start backend...']\n    self.sendBackendStatusUpdate() // Notify UI\n\n    // Get path to the packaged executable\n    exePath <- path.join(process.resourcesPath, 'fortuna-backend', 'fortuna-backend.exe')\n\n    IF file at exePath does NOT exist:\n      self.backendState <- 'error'\n      self.backendLogs.push(`FATAL: Executable not found at ${exePath}`)\n      self.sendBackendStatusUpdate()\n      dialog.showErrorBox(\"Critical Error\", \"Backend is missing. Please reinstall.\")\n      RETURN\n\n    // Spawn the process\n    self.backendProcess <- spawn(exePath, [], { stdio: ['ignore', 'pipe', 'pipe'] })\n\n    // --- CRITICAL: Resiliency Logic ---\n    startupTimeout <- setTimeout(() => {\n      IF self.backendState === 'starting':\n        self.backendState <- 'error'\n        self.backendLogs.push('Error: Backend startup timed out after 30 seconds.')\n        self.backendProcess.kill()\n        self.sendBackendStatusUpdate()\n    }, 30000) // 30-second timeout\n\n    self.backendProcess.stdout.on('data', (data) => {\n      self.backendLogs.push(data.toString())\n      // A more robust check would be a successful health check poll\n      IF data.toString().includes(\"Uvicorn running\"):\n        self.backendState <- 'online'\n        clearTimeout(startupTimeout)\n        self.sendBackendStatusUpdate()\n    })\n\n    self.backendProcess.stderr.on('data', (data) => {\n      self.backendLogs.push(`[STDERR] ${data.toString()}`)\n    })\n\n    self.backendProcess.on('exit', (code) => {\n      clearTimeout(startupTimeout)\n      IF self.backendState is not 'error': // Avoid duplicate error messages\n        self.backendState <- 'error'\n        self.backendLogs.push(`Backend process exited unexpectedly with code: ${code}`)\n        self.sendBackendStatusUpdate()\n    })\n\n  FUNCTION sendBackendStatusUpdate():\n    // Send the latest status to the frontend renderer process\n    IF self.mainWindow is not null:\n      self.mainWindow.webContents.send('backend-status-update', {\n        state: self.backendState,\n        logs: self.backendLogs.slice(-20) // Send last 20 log lines\n      })\n\n// --- IPC Handlers (Securely Defined) ---\nipcMain.handle('get-backend-status', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN null\n\n  RETURN { state: self.backendState, logs: self.backendLogs.slice(-20) }\n})\n\nipcMain.on('restart-backend', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN\n\n  self.startBackend()\n})\n```\n\n### 5.2 Preload Script (`preload.js`)\n\n```pseudocode\n// Expose a limited, secure API to the frontend renderer process\ncontextBridge.exposeInMainWorld('electronAPI', {\n  getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n  restartBackend: () => ipcRenderer.send('restart-backend'),\n  onBackendStatus: (callback) => ipcRenderer.on('backend-status-update', (_event, value) => callback(value))\n})\n```\n\n---\n\n## 6. DATA MODELS & API SPECIFICATION\n\n### 6.1 Core Data Models (Pydantic/TypeScript)\n\n```\nMODEL Race:\n  id: str (unique identifier, e.g., \"Betfair_USA_Aqueduct_2025-11-07_R1\")\n  venue: str\n  race_number: int\n  start_time: datetime\n  runners: List[Runner]\n  source: str\n\nMODEL Runner:\n  name: str\n  odds: Optional[float]\n```\n\n### 6.2 Primary API Endpoints\n\n```\nENDPOINT GET /health\n  Description: Simple health check, requires no authentication.\n  Response (200 OK): {\"status\": \"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Description: Fetches all race data, runs the Trifecta analyzer, and returns qualified races.\n  Headers:\n    - X-API-Key: (Required, not used in this local setup but good practice)\n  Query Params:\n    - max_field_size: int\n    - min_odds: float\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 7. DEPLOYMENT & AUTOMATION (CI/CD)\n\n```pseudocode\nWORKFLOW Build_MSI_Installer_on_GitHub_Actions:\n  // Phase 1: Setup\n  SETUP Node.js and Python environments\n\n  // Phase 2: Build Frontend\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY static output to /electron/web-ui-build/out\n\n  // Phase 3: Build Backend\n  RUN \"pip install -r python_service/requirements.txt\"\n  // CRITICAL: Use PyInstaller with a spec file or CLI flags that include\n  // necessary hidden imports to prevent runtime crashes.\n  // e.g., --hidden-import=keyring.backends.fail.Keyring\n  EXECUTE PyInstaller to create fortuna-backend.exe\n  PLACE executable in /electron/resources/fortuna-backend\n\n  // Phase 4: Deep Integration Test\n  START fortuna-backend.exe in the background\n  POLL http://127.0.0.1:8000/health until it responds with 200 OK or times out\n  IF timeout or crash THEN FAIL the build\n\n  // Phase 5: Package\n  RUN \"npm ci\" in /electron\n  EXECUTE \"npx electron-builder\" to create the MSI installer\n\n  // Phase 6: Publish\n  UPLOAD MSI as a build artifact\n  IF build was triggered by a git tag THEN CREATE a new GitHub Release\n```\n\n---\n\n## 8. END-TO-END WORKFLOWS\n\n### 8.1 Production Startup Workflow (Resilient)\n\n```\nWORKFLOW user_launches_application:\n  STEP 1: User executes Fortuna Faucet.exe -> Electron main.js starts.\n  STEP 2: UI appears instantly. The main process creates the BrowserWindow and loads the static index.html. The UI shows a 'connecting' state.\n  STEP 3: Backend starts asynchronously. The main process calls the robust `startBackend()` function.\n  STEP 4: `startBackend()` spawns `fortuna-backend.exe` and starts a 30-second timeout.\n  STEP 5: The frontend UI polls for status every 3 seconds via the secure `window.electronAPI.getBackendStatus()`.\n  STEP 6: The backend `.exe` starts, its `lifespan` hook runs, and the Uvicorn server comes online within seconds.\n  STEP 7: The main process detects the \"Uvicorn running\" message (or a successful health poll) and updates its internal state to 'online'. The startup timeout is cleared.\n  STEP 8: On its next poll, the frontend receives the 'online' status.\n  STEP 9: The frontend's state changes, triggering the `fetchQualifiedRaces()` API call to `localhost:8000`.\n  STEP 10: Data is returned from the now fully-initialized backend and rendered in the UI.\n\n  FAILURE SCENARIO (Backend Crash):\n  STEP 6a: The backend `.exe` crashes on startup.\n  STEP 7a: The `on('exit')` handler in `main.js` fires. The state is set to 'error' with the exit code.\n  STEP 8a: On its next poll, the frontend receives the 'error' status and relevant logs.\n  STEP 9a: The UI renders an error message and a \"Restart Backend\" button.\n```\n\n---\n*This concludes the revised and definitive blueprint for the Fortuna Faucet application.*",
    "README.md": "# \ud83d\udc34 Fortuna Faucet - Developer's Guide\n\nThis guide provides technical instructions for developers. **For end-user installation, please see the [User Guide (README_WINDOWS.md)](README_WINDOWS.md).**\n\n---\n\n## Core Architecture\n\nThe project has a distinct architecture for production and development environments. Understanding this separation is key to effective development.\n\n*   **Production Architecture (MSI Installer):**\n    *   **Standalone Backend:** The Python backend is compiled into a single, self-contained executable (`fortuna-api.exe`) using **PyInstaller**. This bundles the Python interpreter and all dependencies, requiring no Python installation on the user's machine.\n    *   **Static Frontend:** The Next.js frontend is exported as a set of static HTML, CSS, and JavaScript assets.\n    *   **Electron Wrapper:** The Electron app acts as a lightweight shell, launching the backend executable as a background process and loading the static frontend directly from the filesystem.\n\n*   **Development Architecture:**\n    *   **Backend (`python_service/`):** An asynchronous FastAPI application run from a local Python virtual environment.\n    *   **Frontend (`web_platform/frontend/`):** A standard Next.js development server that enables hot-reloading for rapid UI development.\n\n## Development Environment\n\n### 1. Prerequisites\n\n*   Python 3.12+\n*   Node.js (LTS)\n*   Git\n\n### 2. Initial One-Time Setup\n\nFor your first time setting up the project, run the main setup script. This will create the Python virtual environment and install all necessary dependencies for both the backend and frontend.\n\n```bash\ngit clone https://github.com/masonj0/fortuna.git\ncd fortuna\nrun_dev_environment.bat\n```\n\n### 3. Daily Execution (Backend)\n\nTo start the backend server, use the new official launcher script. This script ensures the server starts correctly and avoids common import errors.\n\n```bash\n# From the project root, with your virtual environment activated\npython run_backend.py\n```\n\nThis script will check for dependencies, ensure the environment is correct, and launch the Uvicorn server.\n\n**(Frontend)** To start the frontend server for UI development, run the following command in a separate terminal:\n\n```bash\nnpm run dev --prefix web_platform/frontend\n```\n\n---\n## Web Service Development Environment\n\nThis section details the setup for the new web service architecture, which is independent of the original Electron application.\n\n### Backend (`web_service/backend/`)\n\n1.  **Navigate to the backend directory:**\n    ```bash\n    cd web_service/backend\n    ```\n2.  **Create a virtual environment:**\n    ```bash\n    python -m venv .venv\n    ```\n3.  **Activate the virtual environment:**\n    *   Windows: `.venv\\Scripts\\activate`\n    *   macOS/Linux: `source .venv/bin/activate`\n4.  **Install dependencies:**\n    ```bash\n    pip install -r requirements-dev.txt\n    ```\n5.  **Run the development server:**\n    ```bash\n    python main.py\n    ```\n\n### Frontend (`web_service/frontend/`)\n\n1.  **Navigate to the frontend directory:**\n    ```bash\n    cd web_service/frontend\n    ```\n2.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n3.  **Run the development server:**\n    ```bash\n    npm run dev\n    ```\n---\n\n## Configuration\n\nThe backend server port can be configured using an environment variable.\n\n*   **`FORTUNA_PORT`**: Sets the port for the backend API service. Defaults to `8000` if not specified.\n\n**Example (PowerShell):**\n```powershell\n$env:FORTUNA_PORT=8088\npython run_backend.py\n```\n\n---\n\n## Key Scripts & Tooling\n\n*   **`run_dev_environment.bat`**: The master script for the **initial one-time setup** of the development environment.\n*   **`scripts/fortuna-quick-start.ps1`**: The recommended script for **daily execution**, providing robust process management for both backend and frontend servers.\n*   **`scripts/build_msi.ps1`**: The local build orchestrator. This script runs the entire production build pipeline: it compiles the backend with PyInstaller, builds the static frontend, and packages everything into an MSI installer using the WiX Toolset.\n*   **`ARCHIVE_PROJECT.py`**: The \"True Scribe.\" This script programmatically scans the repository and generates the `FORTUNA_ALL_PART*.JSON` archive files, which are the authoritative source for project-wide code reviews.\n*   **`.github/workflows/build-msi.yml`**: The CI/CD pipeline definition for GitHub Actions, which automates the creation of the release MSI installer.\n",
    "electron/electron-builder-config.yml": "appId: com.jules.fortunafaucet\nproductName: \"Fortuna Faucet\"\n\ndirectories:\n  output: dist\n  buildResources: assets\n\nfiles:\n  - filter:\n      - \"**/*\"\n\nextraResources:\n  - from: \"../python-service-bin\"\n    to: \"resources/python-service-bin\"\n    filter:\n      - \"**/*\"\n  - from: \"../web_platform/frontend/out\"\n    to: \"resources/frontend\"\n    filter:\n      - \"**/*\"\n\nwin:\n  target: msi\n  icon: \"assets/icon.ico\"\n\nmsi:\n  oneClick: false\n  perMachine: true\n  runAfterFinish: true\n  # Explicitly pointing to the file ensures WiX picks it up\n  shortcutName: \"Fortuna Faucet\"\n  warningsAsErrors: false",
    "electron/secure-settings-manager.js": "// electron/secure-settings-manager.js\nconst { app } = require('electron');\nconst fs = require('fs');\nconst path = require('path');\n\nconst SETTINGS_FILE = path.join(app.getPath('userData'), 'settings.json');\n\nclass SecureSettingsManager {\n constructor() {\n this.settings = this.loadSettings();\n }\n\n loadSettings() {\n try {\n if (fs.existsSync(SETTINGS_FILE)) {\n const data = fs.readFileSync(SETTINGS_FILE, 'utf-8');\n return JSON.parse(data);\n }\n } catch (error) {\n console.error('Error loading settings:', error);\n }\n return {};\n }\n\n saveSettings() {\n try {\n fs.writeFileSync(SETTINGS_FILE, JSON.stringify(this.settings, null, 2));\n } catch (error) {\n console.error('Error saving settings:', error);\n }\n }\n\n getApiKey() {\n return this.settings.apiKey || null;\n }\n\n saveApiKey(apiKey) {\n this.settings.apiKey = apiKey;\n this.saveSettings();\n return { success: true };\n }\n\n getBetfairCredentials() {\n return this.settings.betfair || null;\n }\n\n saveBetfairCredentials(credentials) {\n this.settings.betfair = credentials;\n this.saveSettings();\n return { success: true };\n }\n}\n\nmodule.exports = new SecureSettingsManager();\n",
    "package-lock.json": "{\n  \"name\": \"app\",\n  \"lockfileVersion\": 3,\n  \"requires\": true,\n  \"packages\": {\n    \"\": {\n      \"dependencies\": {\n        \"@playwright/test\": \"^1.56.1\"\n      }\n    },\n    \"node_modules/@playwright/test\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/@playwright/test/-/test-1.56.1.tgz\",\n      \"integrity\": \"sha512-vSMYtL/zOcFpvJCW71Q/OEGQb7KYBPAdKh35WNSkaZA75JlAO8ED8UN6GUNTm3drWomcbcqRPFqQbLae8yBTdg==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"playwright\": \"1.56.1\"\n      },\n      \"bin\": {\n        \"playwright\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      }\n    },\n    \"node_modules/fsevents\": {\n      \"version\": \"2.3.2\",\n      \"resolved\": \"https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz\",\n      \"integrity\": \"sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==\",\n      \"hasInstallScript\": true,\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"darwin\"\n      ],\n      \"engines\": {\n        \"node\": \"^8.16.0 || ^10.6.0 || >=11.0.0\"\n      }\n    },\n    \"node_modules/playwright\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/playwright/-/playwright-1.56.1.tgz\",\n      \"integrity\": \"sha512-aFi5B0WovBHTEvpM3DzXTUaeN6eN0qWnTkKx4NQaH4Wvcmc153PdaY2UBdSYKaGYw+UyWXSVyxDUg5DoPEttjw==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"playwright-core\": \"1.56.1\"\n      },\n      \"bin\": {\n        \"playwright\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      },\n      \"optionalDependencies\": {\n        \"fsevents\": \"2.3.2\"\n      }\n    },\n    \"node_modules/playwright-core\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/playwright-core/-/playwright-core-1.56.1.tgz\",\n      \"integrity\": \"sha512-hutraynyn31F+Bifme+Ps9Vq59hKuUCz7H1kDOcBs+2oGguKkWTU50bBWrtz34OUWmIwpBTWDxaRPXrIXkgvmQ==\",\n      \"license\": \"Apache-2.0\",\n      \"bin\": {\n        \"playwright-core\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      }\n    }\n  }\n}\n",
    "python_service/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass AtTheRacesAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for attheraces.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"AtTheRaces\"\n    BASE_URL = \"https://www.attheraces.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch AtTheRaces index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        all_races = []\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to AtTheRacesAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                header_element = soup.select_one(\"h1.heading-racecard-title\")\n                if not header_element:\n                    continue\n                header = header_element.get_text()\n                track_name_raw, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n                track_name = normalize_venue_name(track_name_raw)\n                active_link = soup.select_one(\"a.race-time-link.active\")\n                race_number = 1\n                if active_link:\n                    parent_div = active_link.find_parent(\"div\", \"races\")\n                    if parent_div:\n                        all_links = parent_div.select(\"a.race-time-link\")\n                        race_number = all_links.index(active_link) + 1\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time, \"%H:%M\").time())\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n                race = Race(\n                    id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from AtTheRaces, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_element = row.select_one(\"h3.horse-name a\")\n            if not name_element:\n                return None\n            name = clean_text(name_element.get_text())\n\n            num_element = row.select_one(\"span.horse-number\")\n            if not num_element:\n                return None\n            num_str = clean_text(num_element.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_element = row.select_one(\"button.best-odds\")\n            odds_str = clean_text(odds_element.get_text()) if odds_element else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on AtTheRaces, skipping runner.\")\n            return None\n",
    "python_service/adapters/betfair_datascientist_adapter.py": "# python_service/adapters/betfair_datascientist_adapter.py\n\nfrom datetime import datetime\nfrom io import StringIO\nfrom typing import List\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BetfairDataScientistAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Betfair Data Scientist CSV models, migrated to BaseAdapterV3.\n    \"\"\"\n\n    ADAPTER_NAME = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str, url: str, config=None):\n        source_name = f\"{self.ADAPTER_NAME}_{model_name}\"\n        super().__init__(source_name=source_name, base_url=url, config=config)\n        self.model_name = model_name\n\n    async def _fetch_data(self, date: str) -> Optional[StringIO]:\n        \"\"\"Fetches the raw CSV data from the Betfair Data Scientist model endpoint.\"\"\"\n        endpoint = f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n        self.logger.info(f\"Fetching data from {self.base_url}{endpoint}\")\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return StringIO(response.text) if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[StringIO]) -> List[Race]:\n        \"\"\"Parses the raw CSV data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n        try:\n            df = pd.read_csv(raw_data)\n            if df.empty:\n                self.logger.warning(\"Received empty CSV from Betfair Data Scientist.\")\n                return []\n\n            df = df.rename(\n                columns={\n                    \"meetings.races.bfExchangeMarketId\": \"market_id\",\n                    \"meetings.races.runners.bfExchangeSelectionId\": \"selection_id\",\n                    \"meetings.races.runners.ratedPrice\": \"rated_price\",\n                    \"meetings.races.raceName\": \"race_name\",\n                    \"meetings.name\": \"meeting_name\",\n                    \"meetings.races.raceNumber\": \"race_number\",\n                    \"meetings.races.runners.runnerName\": \"runner_name\",\n                    \"meetings.races.runners.clothNumber\": \"saddle_cloth\",\n                }\n            )\n            races: List[Race] = []\n            for market_id, group in df.groupby(\"market_id\"):\n                race_info = group.iloc[0]\n                runners = []\n                for _, row in group.iterrows():\n                    rated_price = row.get(\"rated_price\")\n                    odds_data = {}\n                    if pd.notna(rated_price):\n                        odds_data[self.source_name] = OddsData(\n                            win=float(rated_price),\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(\n                        Runner(\n                            name=str(row.get(\"runner_name\", \"Unknown\")),\n                            number=int(row.get(\"saddle_cloth\", 0)),\n                            odds=odds_data,\n                        )\n                    )\n\n                race = Race(\n                    id=str(market_id),\n                    venue=normalize_venue_name(str(race_info.get(\"meeting_name\", \"\"))),\n                    race_number=int(race_info.get(\"race_number\", 0)),\n                    start_time=datetime.now(),  # Placeholder, not provided in source\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n            self.logger.info(f\"Normalized {len(races)} races from {self.model_name}.\")\n            return races\n        except (pd.errors.ParserError, KeyError) as e:\n            self.logger.error(\n                \"Failed to parse Betfair Data Scientist CSV.\",\n                exc_info=True,\n                error=str(e),\n            )\n            return []\n",
    "python_service/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass SportingLifeAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to SportingLifeAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"a.hr-race-header-course-name__link\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.hr-race-header-time__time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n                race_number = 1\n                if active_link:\n                    all_links = soup.select(\"a.hr-race-header-navigation-link\")\n                    try:\n                        race_number = all_links.index(active_link) + 1\n                    except ValueError:\n                        pass  # Keep default race number if active link not in all links\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from SportingLife, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.hr-racing-runner-horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.hr-racing-runner-saddle-cloth-no\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.select_one(\"span.hr-racing-runner-odds\")\n            odds_str = clean_text(odds_node.get_text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on SportingLife, skipping runner.\")\n            return None\n",
    "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for The Racing API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(self.http_client, \"GET\", endpoint, headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in TheRacingAPI response.\")\n            return []\n\n        races = []\n        for race_data in raw_data.get(\"racecards\", []):\n            try:\n                race_id = race_data.get(\"race_id\")\n                off_time = race_data.get(\"off_time\")\n                course = race_data.get(\"course\")\n                race_no = race_data.get(\"race_no\")\n\n                if not all([race_id, off_time, course, race_no]):\n                    continue\n\n                start_time = datetime.fromisoformat(off_time.replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_id}\",\n                    venue=course,\n                    race_number=race_no,\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_data.get(\"race_id\"),\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                horse = runner_data.get(\"horse\")\n                if not horse:\n                    continue\n\n                odds_data = {}\n                odds_list = runner_data.get(\"odds\", [])\n                if odds_list:\n                    odds_decimal_str = odds_list[0].get(\"odds_decimal\")\n                    if odds_decimal_str:\n                        win_odds = Decimal(str(odds_decimal_str))\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=horse,\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI runner\",\n                    runner_name=runner_data.get(\"horse\"),\n                    exc_info=True,\n                )\n        return runners\n",
    "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US racing data from the TVG API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v2/races/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(self.http_client, \"GET\", summary_url, headers=headers)\n        if not tracks_response:\n            return None\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get(\"tracks\", []):\n            track_id = track.get(\"id\")\n            for race in track.get(\"races\", []):\n                race_id = race.get(\"id\")\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(self.http_client, \"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        # Filter out exceptions and return only successful responses\n        return [resp.json() for resp in race_detail_responses if resp and not isinstance(resp, Exception)]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of detailed race JSON objects into Race models.\"\"\"\n        races = []\n        if not isinstance(raw_data, list):\n            self.logger.warning(\"raw_data is not a list, cannot parse TVG races.\")\n            return races\n\n        for race_detail in raw_data:\n            try:\n                if race := self._parse_race(race_detail):\n                    races.append(race)\n            except AdapterParsingError:\n                self.logger.warning(\n                    \"Failed to parse TVG race detail, skipping.\",\n                    race_detail=race_detail,\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_race(self, race_detail: dict) -> Optional[Race]:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get(\"track\")\n        race_info = race_detail.get(\"race\")\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get(\"runners\", []):\n            if runner_data.get(\"scratched\"):\n                continue\n\n            odds = runner_data.get(\"odds\", {})\n            current_odds = odds.get(\"currentPrice\", {})\n            odds_str = current_odds.get(\"fractional\") or odds.get(\"morningLinePrice\", {}).get(\"fractional\")\n\n            try:\n                number = int(runner_data.get(\"programNumber\", \"0\").replace(\"A\", \"\"))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            odds_data = {}\n            if odds_str:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n\n            runners.append(\n                Runner(\n                    number=number,\n                    name=clean_text(runner_data.get(\"name\")),\n                    odds=odds_data,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        post_time = race_info.get(\"postTime\")\n        if not post_time:\n            raise AdapterParsingError(self.source_name, \"Missing post time.\")\n\n        try:\n            start_time = datetime.fromisoformat(post_time.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name,\n                f\"Could not parse post time: {post_time}\",\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get(\"name\"),\n            race_number=race_info.get(\"number\"),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/utils.py": "# python_service/adapters/utils.py\n# Compatibility shim to re-export parse_odds from the centralized location.\n\nfrom ..utils.odds import parse_odds\n\n__all__ = [\"parse_odds\"]\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\n\n# from .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_cls.__name__}\")\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_cls.__name__}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters.append(adapter_instance)\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_cls.__name__,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_cls.__name__}\",\n                        exc_info=True,\n                    )\n\n            # Special case for BetfairDataScientistAdapter with extra args - DISABLED\n            # try:\n            #     bds_adapter = BetfairDataScientistAdapter(\n            #         model_name=\"ThoroughbredModel\",\n            #         url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n            #         config=self.config,\n            #     )\n            #     if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n            #         bds_adapter.enable_manual_override(manual_override_manager)\n            #     self.adapters.append(bds_adapter)\n            # except Exception:\n            #     self.logger.warning(\n            #         \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n            #         exc_info=True,\n            #     )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            races = [Race(**race_data) for race_data in race_data_list]\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n\n                # Ensure the source is a list and append new source if not present\n                if not isinstance(existing_race.source, list):\n                    existing_race.source = [existing_race.source]\n                if race.source not in existing_race.source:\n                    existing_race.source.append(race.source)\n\n        return list(race_map.values())\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached and broadcasted via WebSocket.\n        \"\"\"\n        # Construct a cache key\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        log.info(\"Cache miss for fetch_all_odds\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._fetch_with_semaphore(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for i, result in enumerate(results):\n            adapter = target_adapters[i]\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed with an unhandled exception\", adapter=adapter.source_name, error=result)\n                errors.append({\n                    \"adapter_name\": adapter.source_name,\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                    \"attempted_url\": \"Unknown\"\n                })\n                source_infos.append({\n                    \"name\": adapter.source_name,\n                    \"status\": \"FAILED\",\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                })\n            else:\n                _adapter_name, adapter_result, _duration = result\n                source_info = adapter_result.get(\"source_info\", {})\n                source_infos.append(source_info)\n                if source_info.get(\"status\") == \"SUCCESS\":\n                    all_races.extend(adapter_result.get(\"races\", []))\n                else:\n                    errors.append({\n                        \"adapter_name\": adapter.source_name,\n                        \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                        \"attempted_url\": source_info.get(\"attempted_url\")\n                    })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            errors=errors,\n            source_info=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n                \"total_errors\": len(errors),\n            },\n        )\n\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Set the result in the cache\n        await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "python_service/fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport structlog\n\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.config import get_settings\nfrom python_service.engine import OddsEngine\nfrom python_service.etl import run_etl_for_yesterday\nfrom python_service.models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = OddsEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n        try:\n            background_tasks = set()  # Create a dummy set for background tasks\n            aggregated_data = await self.odds_engine.fetch_all_odds(today_str, background_tasks)\n            all_races = aggregated_data.get(\"races\", [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer(\"trifecta\")\n            qualified_races_result = analyzer.qualify_races(all_races)\n            qualified_races_list = qualified_races_result.get(\"races\", [])\n            log.info(\n                \"Watchman: Initial target acquisition and ranking complete\",\n                target_count=len(qualified_races_list),\n            )\n\n            # Log the top targets for better observability\n            for race in qualified_races_list[:5]:\n                log.info(\n                    \"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat(),\n                )\n            return qualified_races_list\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        # active_targets = list(targets)\n\n        # from python_service.adapters.betfair_adapter import BetfairAdapter\n        # async with LiveOddsMonitor(betfair_adapter=BetfairAdapter(config=self.settings)) as live_monitor:\n        #     async with httpx.AsyncClient() as client:\n        #         while active_targets:\n        #             now = datetime.now(timezone.utc)\n\n        #             # Find races that are within the 5-minute monitoring window\n        #             races_to_monitor = [\n        #                 r\n        #                 for r in active_targets\n        #                 if r.start_time.replace(tzinfo=timezone.utc) > now\n        #                 and r.start_time.replace(tzinfo=timezone.utc)\n        #                 < now + timedelta(minutes=5)\n        #             ]\n\n        #             if races_to_monitor:\n        #                 for race in races_to_monitor:\n        #                     log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n        #                         race_id=race.id,\n        #                         venue=race.venue,\n        #                         score=race.qualification_score\n        #                     )\n        #                     updated_race = await live_monitor.monitor_race(race, client)\n        #                     log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n        #                     # Remove from target list to prevent re-monitoring\n        #                     active_targets = [t for t in active_targets if t.id != race.id]\n\n        #             if not active_targets:\n        #                 break # Exit loop if all targets are processed\n\n        #             await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        try:\n            initial_targets = await self.get_initial_targets()\n            if initial_targets:\n                await self.run_tactical_monitoring(initial_targets)\n            else:\n                log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n        finally:\n            await self.odds_engine.close()\n\n        # Run ETL for yesterday's data after all other operations are complete\n        try:\n            log.info(\"Starting daily ETL process for Scribe's Archives...\")\n            run_etl_for_yesterday()\n            log.info(\"Daily ETL process completed successfully.\")\n        except Exception:\n            log.error(\"Daily ETL process failed.\", exc_info=True)\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\n\nasync def main():\n    from python_service.logging_config import configure_logging\n\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "python_service/initialize_db.py": "# python_service/initialize_db.py\nfrom db.init import initialize_database\n\n\ndef main():\n    \"\"\"\n    This script exists solely to initialize the database.\n    It should be called before the main server process is started.\n    \"\"\"\n    print(\"Initializing database...\", flush=True)\n    initialize_database()\n    print(\"Database initialization complete.\", flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/main.py": "import uvicorn\nimport sys\nimport os\nimport asyncio\nfrom multiprocessing import freeze_support\n\n# Force UTF-8 encoding for stdout and stderr, crucial for PyInstaller on Windows\nos.environ[\"PYTHONUTF8\"] = \"1\"\n\n# Import the 'app' object at the top level to make it accessible for import by other modules,\n# such as diagnostic scripts in CI/CD.\nfrom python_service.api import app, HTTPException\n\n# This is the definitive entry point for the Fortuna Faucet backend service.\n# It is designed to be compiled with PyInstaller.\n\n\ndef _configure_sys_path():\n    \"\"\"\n    Dynamically adds project paths to sys.path.\n    This is the robust solution to ensure that string-based imports like\n    \"web_service.backend.api:app\" work correctly when the application is\n    run from a PyInstaller executable. The `_MEIPASS` attribute is a temporary\n    directory created by PyInstaller at runtime.\n    \"\"\"\n    if getattr(sys, \"frozen\", False) and hasattr(sys, \"_MEIPASS\"):\n        # Running in a PyInstaller bundle. The project root is the _MEIPASS directory.\n        project_root = os.path.abspath(sys._MEIPASS)\n\n        # Aggressively add paths to resolve potential module lookup issues in frozen mode.\n        paths_to_add = [\n            project_root,\n            os.path.join(project_root, \"python_service\"),\n        ]\n\n        # Insert paths at the beginning of sys.path in reverse order\n        # to maintain the desired precedence.\n        for path in reversed(paths_to_add):\n            if path not in sys.path:\n                sys.path.insert(0, path)\n                print(f\"INFO: Added path to sys.path: {path}\")\n\n    else:\n        # Running as a normal script. The project root is one level up.\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n        if project_root not in sys.path:\n            sys.path.insert(0, project_root)\n            print(f\"INFO: Added project root to sys.path: {project_root}\")\n\n\ndef main():\n    _configure_sys_path()\n    \"\"\"\n    Primary entry point for the Fortuna Faucet backend application.\n    This function configures and runs the Uvicorn server.\n    It's crucial to launch the app this way to ensure PyInstaller's bootloader\n    can correctly resolve the package context.\n    \"\"\"\n    # When packaged, we need to ensure multiprocessing works correctly.\n    if getattr(sys, \"frozen\", False):\n        # CRITICAL for multiprocessing support in frozen mode on Windows.\n        freeze_support()\n\n    from python_service.config import get_settings\n    from fastapi.staticfiles import StaticFiles\n    from fastapi.responses import FileResponse\n    from python_service.port_check import check_port_and_exit_if_in_use\n\n    settings = get_settings()\n\n    # --- Port Sanity Check ---\n    # Before doing anything else, ensure the target port is not already in use.\n    # This prevents a common and confusing crash scenario on startup.\n    check_port_and_exit_if_in_use(settings.FORTUNA_PORT, settings.UVICORN_HOST)\n\n    # --- Conditional UI Serving for Web Service Mode ---\n    # Only serve the UI if the FORTUNA_MODE environment variable is set to 'webservice'.\n    # This prevents the Electron-packaged backend from trying to serve files it doesn't have.\n    if os.environ.get(\"FORTUNA_MODE\") == \"webservice\":\n        # Define the path to the static UI files, accommodating PyInstaller's bundle.\n        if getattr(sys, \"frozen\", False):\n            # In a bundled app, the UI files are in the '_MEIPASS/ui' directory.\n            STATIC_DIR = os.path.join(sys._MEIPASS, \"ui\")\n        else:\n            # In development, they are in the frontend's output directory.\n            STATIC_DIR = os.path.abspath(\n                os.path.join(os.path.dirname(__file__), \"..\", \"web_platform\", \"frontend\", \"out\")\n            )\n\n        # Mount the static assets directory for CSS, JS, etc.\n        if os.path.exists(os.path.join(STATIC_DIR, \"_next\")):\n            app.mount(\"/_next\", StaticFiles(directory=os.path.join(STATIC_DIR, \"_next\")), name=\"next\")\n\n        # Serve the main index.html for any non-API path.\n        @app.get(\"/{full_path:path}\", include_in_schema=False)\n        async def serve_frontend(full_path: str):\n            if full_path.startswith(\"api/\") or full_path.startswith(\"docs\") or full_path == \"health\":\n                # This is an API route, let FastAPI handle it.\n                # A 404 will be raised naturally if no route matches.\n                return\n\n            index_path = os.path.join(STATIC_DIR, \"index.html\")\n            if os.path.exists(index_path):\n                return FileResponse(index_path)\n            else:\n                # This will only be hit if the frontend files are missing entirely.\n                raise HTTPException(\n                    status_code=404,\n                    detail=\"Frontend not found. Please build the frontend and ensure it's in the correct location.\",\n                )\n\n    # CRITICAL FIX FOR PYINSTALLER on WINDOWS: Force event loop policy\n    # This resolves a silent network binding failure where Uvicorn reports startup\n    # but the OS never actually binds the port.\n    if sys.platform == \"win32\" and getattr(sys, 'frozen', False):\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n        print(\"[BOOT] Applied WindowsSelectorEventLoopPolicy for PyInstaller\", file=sys.stderr)\n\n    uvicorn.run(app, host=settings.UVICORN_HOST, port=settings.FORTUNA_PORT, log_level=\"info\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/requirements_minimal.txt": "httpx==0.25.0\nstructlog==23.2.0\npydantic==2.5.0\nuvicorn==0.24.0\nfastapi==0.104.1\ntenacity==8.2.3\n",
    "python_service/tests/test_manual_override.py": "# python_service/tests/test_manual_override.py\nimport pytest\n\nfrom python_service.manual_override_manager import ManualOverrideManager\n\n\n@pytest.fixture\ndef manager():\n    # The manager is now in-memory and doesn't need a path\n    return ManualOverrideManager()\n\n\ndef test_register_and_retrieve(manager):\n    adapter = \"TestAdapter\"\n    url = \"https://example.com/blocked\"\n\n    request_id = manager.register_failure(\n        adapter_name=adapter,\n        url=url,\n    )\n\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager):\n    adapter = \"TestAdapter\"\n    url = \"https://example.com/blocked\"\n    content = \"<html>Manual content</html>\"\n    content_type = \"text/html\"\n\n    request_id = manager.register_failure(\n        adapter_name=adapter,\n        url=url,\n    )\n\n    success = manager.submit_manual_data(\n        request_id=request_id,\n        raw_content=content,\n        content_type=content_type,\n    )\n\n    assert success\n\n    # Verify that the data can be retrieved correctly\n    retrieved_data = manager.get_manual_data(adapter_name=adapter, url=url)\n    assert retrieved_data is not None\n    retrieved_content, retrieved_type = retrieved_data\n    assert retrieved_content == content\n    assert retrieved_type == content_type\n\n    # Verify that data is consumed after retrieval\n    assert manager.get_manual_data(adapter_name=adapter, url=url) is None\n",
    "python_service/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "scripts/convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom multiprocessing import Queue\n\n# --- Configuration ---\nMANIFEST_FILES = [\n    \"MANIFEST_PART1_BACKEND.json\",\n    \"MANIFEST_PART2_FRONTEND.json\",\n    \"MANIFEST_PART3_SUPPORT.json\",\n    \"MANIFEST_PART4_ROOT.json\",\n]\nOUTPUT_DIR = \"ReviewableJSON\"\nFILE_PROCESSING_TIMEOUT = 10\nEXCLUDED_FILES = [\"package-lock.json\"]\nMAX_FILE_SIZE_MB = 10  # Max file size in megabytes\n\n\ndef read_json_manifest(manifest_path: str) -> list[str]:\n    \"\"\"Reads a JSON manifest file and returns a list of file paths.\"\"\"\n    try:\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return []\n\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\n\ndef convert_file_to_json_sandboxed(file_path):\n    # --- Pre-flight check: File size ---\n    try:\n        file_size = os.path.getsize(file_path)\n        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:\n            return {\"error\": f\"File exceeds {MAX_FILE_SIZE_MB}MB size limit.\"}\n    except FileNotFoundError:\n        return {\"error\": \"File not found.\"}\n    except Exception as e:\n        return {\"error\": f\"Could not check file size: {e}\"}\n\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    try:\n        if p.is_alive():\n            print(f\"    [WARNING] Process for {file_path} timed out. Attempting graceful termination...\")\n            p.terminate()\n            p.join(timeout=2)  # Give it a moment to terminate gracefully\n\n            if p.is_alive():\n                print(f\"    [ERROR] Graceful termination failed. Forcibly killing process...\")\n                p.kill()  # The ultimate \"just die\"\n                p.join()\n            return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n        if not q.empty():\n            return q.get()\n        return {\"error\": \"Unknown error in sandboxed read process.\"}\n    finally:\n        # \u2705 Properly close and flush the queue\n        try:\n            while not q.empty():\n                q.get_nowait()\n        except Exception:\n            pass\n        q.close()\n        q.join_thread()\n\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'=' * 60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'=' * 60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        paths = read_json_manifest(manifest)\n        if paths:\n            all_local_paths.extend(paths)\n            print(f\"    --> Found {len(paths)} valid file paths.\")\n        else:\n            print(f\"    [WARNING] Manifest not found or is empty: {manifest}\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        if os.path.basename(local_path) in EXCLUDED_FILES:\n            print(f\"\\n--> Skipping excluded file: {local_path}\")\n            failed_count += 1\n            continue\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + \".json\")\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Backup process complete.\")\n    print(f\"Successfully processed: {processed_count}/{len(unique_local_paths)}\")\n    print(f\"Failed/Skipped: {failed_count}\")\n    print(f\"{'=' * 60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/fortuna-quick-start.ps1": "# ====================================================================\n# Fortuna Faucet - Quick Start Script (No Installation Required)\n# ====================================================================\n# This script runs Fortuna directly from source without any MSI\n# Useful for development and testing before packaging\n# ====================================================================\n\nparam(\n    [switch]$SkipChecks,\n    [switch]$NoFrontend\n)\n\n$ErrorActionPreference = 'Stop'\n$OriginalLocation = Get-Location\n\n# ============= CONFIGURATION =============\n$PROJECT_ROOT = Split-Path -Parent $MyInvocation.MyCommand.Path\n$VENV_PATH = Join-Path $PROJECT_ROOT \".venv\"\n$PYTHON_EXE = Join-Path $VENV_PATH \"Scripts\\python.exe\"\n$BACKEND_DIR = Join-Path $PROJECT_ROOT \"python_service\"\n$FRONTEND_DIR = Join-Path $PROJECT_ROOT \"web_platform\\frontend\"\n$BACKEND_PORT = 8000\n$FRONTEND_PORT = 3000\n\n# ============= HELPER FUNCTIONS =============\n\nfunction Write-Status {\n    param([string]$Message, [string]$Status = \"INFO\")\n    $Color = switch ($Status) {\n        \"OK\"      { \"Green\" }\n        \"ERROR\"   { \"Red\" }\n        \"WARNING\" { \"Yellow\" }\n        default   { \"Cyan\" }\n    }\n    Write-Host \"[$Status] $Message\" -ForegroundColor $Color\n}\n\nfunction Test-CommandExists {\n    param([string]$Command)\n    try {\n        Get-Command $Command -ErrorAction Stop | Out-Null\n        return $true\n    } catch {\n        return $false\n    }\n}\n\nfunction Test-PortAvailable {\n    param([int]$Port)\n    try {\n        $Listener = [System.Net.Sockets.TcpListener]::new([System.Net.IPAddress]::Any, $Port)\n        $Listener.Start()\n        $Listener.Stop()\n        return $true\n    } catch {\n        return $false\n    }\n}\n\nfunction Stop-ProcessOnPort {\n    param([int]$Port)\n    $Connection = Get-NetTCPConnection -LocalPort $Port -ErrorAction SilentlyContinue\n    if ($Connection) {\n        $ProcessId = $Connection.OwningProcess\n        Write-Status \"Killing process $ProcessId on port $Port\" \"WARNING\"\n        Stop-Process -Id $ProcessId -Force -ErrorAction SilentlyContinue\n        Start-Sleep -Seconds 2\n    }\n}\n\nfunction Wait-ForBackend {\n    param([int]$MaxAttempts = 30)\n\n    Write-Status \"Waiting for backend to start (http://127.0.0.1:$BACKEND_PORT/health)...\"\n\n    for ($i = 1; $i -le $MaxAttempts; $i++) {\n        try {\n            $Response = Invoke-WebRequest -Uri \"http://127.0.0.1:$BACKEND_PORT/health\" -UseBasicParsing -TimeoutSec 2\n            if ($Response.StatusCode -eq 200) {\n                Write-Status \"Backend is healthy!\" \"OK\"\n                return $true\n            }\n        } catch {\n            Write-Host \".\" -NoNewline\n            Start-Sleep -Seconds 1\n        }\n    }\n\n    Write-Status \"Backend failed to start after $MaxAttempts seconds\" \"ERROR\"\n    return $false\n}\n\n# ============= PREFLIGHT CHECKS =============\n\nWrite-Host \"`n========================================\" -ForegroundColor Cyan\nWrite-Host \" Fortuna Faucet - Quick Start\" -ForegroundColor Cyan\nWrite-Host \"========================================`n\" -ForegroundColor Cyan\n\nif (-not $SkipChecks) {\n    Write-Status \"Running preflight checks...\"\n\n    # Check Python\n    if (-not (Test-Path $PYTHON_EXE)) {\n        Write-Status \"Python virtual environment not found at $VENV_PATH\" \"ERROR\"\n        Write-Status \"Please run setup script first or create venv manually\" \"ERROR\"\n        exit 1\n    }\n    Write-Status \"Python venv found\" \"OK\"\n\n    # Check Node.js\n    if (-not (Test-CommandExists \"node\")) {\n        Write-Status \"Node.js not found in PATH\" \"ERROR\"\n        Write-Status \"Install from: https://nodejs.org/\" \"ERROR\"\n        exit 1\n    }\n    Write-Status \"Node.js found: $(node --version)\" \"OK\"\n\n    # Check npm\n    if (-not (Test-CommandExists \"npm\")) {\n        Write-Status \"npm not found\" \"ERROR\"\n        exit 1\n    }\n    Write-Status \"npm found: $(npm --version)\" \"OK\"\n\n    # Check if ports are available\n    if (-not (Test-PortAvailable $BACKEND_PORT)) {\n        Write-Status \"Port $BACKEND_PORT is already in use\" \"WARNING\"\n        Stop-ProcessOnPort $BACKEND_PORT\n    }\n\n    if (-not $NoFrontend -and -not (Test-PortAvailable $FRONTEND_PORT)) {\n        Write-Status \"Port $FRONTEND_PORT is already in use\" \"WARNING\"\n        Stop-ProcessOnPort $FRONTEND_PORT\n    }\n\n    # Check Python dependencies\n    Write-Status \"Checking Python dependencies...\"\n    $PipList = & $PYTHON_EXE -m pip list\n    if ($PipList -notmatch \"fastapi\") {\n        Write-Status \"Python dependencies not installed\" \"WARNING\"\n        Write-Status \"Installing dependencies...\"\n        & $PYTHON_EXE -m pip install -r (Join-Path $BACKEND_DIR \"requirements.txt\")\n    } else {\n        Write-Status \"Python dependencies OK\" \"OK\"\n    }\n\n    # Check Node dependencies\n    if (-not $NoFrontend) {\n        Write-Status \"Checking Node.js dependencies...\"\n        $NodeModules = Join-Path $FRONTEND_DIR \"node_modules\"\n        if (-not (Test-Path $NodeModules)) {\n            Write-Status \"Node.js dependencies not installed\" \"WARNING\"\n            Write-Status \"Installing dependencies...\"\n            Push-Location $FRONTEND_DIR\n            npm install\n            Pop-Location\n        } else {\n            Write-Status \"Node.js dependencies OK\" \"OK\"\n        }\n    }\n\n    Write-Host \"\"\n}\n\n# ============= LAUNCH BACKEND =============\n\nWrite-Status \"Starting backend server...\"\n\n$BackendJob = Start-Job -ScriptBlock {\n    param($PythonExe, $BackendDir)\n    Set-Location $BackendDir\n    & $PythonExe -m uvicorn api:app --host 127.0.0.1 --port 8000 --reload\n} -ArgumentList $PYTHON_EXE, $BACKEND_DIR\n\nWrite-Status \"Backend job started (ID: $($BackendJob.Id))\"\n\n# Wait for backend to be healthy\nif (-not (Wait-ForBackend)) {\n    Write-Status \"Backend startup failed. Checking logs...\" \"ERROR\"\n    Receive-Job $BackendJob\n    Stop-Job $BackendJob\n    Remove-Job $BackendJob\n    exit 1\n}\n\n# ============= LAUNCH FRONTEND =============\n\nif (-not $NoFrontend) {\n    Write-Status \"Starting frontend dev server...\"\n\n    $FrontendJob = Start-Job -ScriptBlock {\n        param($FrontendDir)\n        Set-Location $FrontendDir\n        npm run dev\n    } -ArgumentList $FRONTEND_DIR\n\n    Write-Status \"Frontend job started (ID: $($FrontendJob.Id))\"\n\n    # Wait a bit for frontend to start\n    Start-Sleep -Seconds 5\n\n    Write-Status \"Opening browser...\" \"OK\"\n    Start-Process \"http://localhost:$FRONTEND_PORT\"\n}\n\n# ============= MONITORING =============\n\nWrite-Host \"`n========================================\" -ForegroundColor Green\nWrite-Host \" Fortuna is now running!\" -ForegroundColor Green\nWrite-Host \"========================================\" -ForegroundColor Green\nWrite-Host \"\"\nWrite-Status \"Backend:  http://127.0.0.1:$BACKEND_PORT\" \"OK\"\nif (-not $NoFrontend) {\n    Write-Status \"Frontend: http://127.0.0.1:$FRONTEND_PORT\" \"OK\"\n}\nWrite-Host \"\"\nWrite-Host \"Press Ctrl+C to stop all services\" -ForegroundColor Yellow\nWrite-Host \"\"\n\ntry {\n    while ($true) {\n        Start-Sleep -Seconds 2\n\n        # Check if jobs are still running\n        if ($BackendJob.State -eq \"Failed\" -or $BackendJob.State -eq \"Stopped\") {\n            Write-Status \"Backend has stopped unexpectedly!\" \"ERROR\"\n            Receive-Job $BackendJob\n            break\n        }\n\n        if (-not $NoFrontend -and ($FrontendJob.State -eq \"Failed\" -or $FrontendJob.State -eq \"Stopped\")) {\n            Write-Status \"Frontend has stopped unexpectedly!\" \"ERROR\"\n            Receive-Job $FrontendJob\n            break\n        }\n    }\n} finally {\n    # ============= CLEANUP =============\n    Write-Host \"`n`nShutting down...\" -ForegroundColor Yellow\n\n    if ($BackendJob) {\n        Write-Status \"Stopping backend...\"\n        Stop-Job $BackendJob -ErrorAction SilentlyContinue\n        Remove-Job $BackendJob -Force -ErrorAction SilentlyContinue\n    }\n\n    if ($FrontendJob) {\n        Write-Status \"Stopping frontend...\"\n        Stop-Job $FrontendJob -ErrorAction SilentlyContinue\n        Remove-Job $FrontendJob -Force -ErrorAction SilentlyContinue\n    }\n\n    # Kill any remaining processes on the ports\n    Stop-ProcessOnPort $BACKEND_PORT\n    if (-not $NoFrontend) {\n        Stop-ProcessOnPort $FRONTEND_PORT\n    }\n\n    Set-Location $OriginalLocation\n    Write-Status \"Cleanup complete\" \"OK\"\n}\n\n# ============= USAGE EXAMPLES =============\n<#\n.SYNOPSIS\nQuick start script for Fortuna Faucet (no installation required)\n\n.DESCRIPTION\nLaunches the backend and frontend servers directly from source code\nUseful for development and testing before creating an MSI installer\n\n.PARAMETER SkipChecks\nSkip all preflight dependency checks (faster startup)\n\n.PARAMETER NoFrontend\nOnly start the backend API server (no UI)\n\n.EXAMPLE\n.\\fortuna-quick-start.ps1\nStarts both backend and frontend with full checks\n\n.EXAMPLE\n.\\fortuna-quick-start.ps1 -NoFrontend\nStarts only the backend API (useful for API testing)\n\n.EXAMPLE\n.\\fortuna-quick-start.ps1 -SkipChecks\nFast startup (assumes all dependencies are already installed)\n#>",
    "web_platform/api_gateway/package.json": "{\n  \"name\": \"api_gateway\",\n  \"version\": \"1.0.0\",\n  \"main\": \"dist/server.js\",\n  \"scripts\": { \"start\": \"ts-node src/server.ts\" },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"dotenv\": \"^16.3.1\",\n    \"dotenv\": \"^16.3.1\",\n    \"sqlite\": \"^5.1.1\",\n    \"sqlite3\": \"^5.1.7\",\n    \"socket.io\": \"^4.7.4\",\n    \"cors\": \"^2.8.5\"\n  },\n  \"devDependencies\": {\n    \"@types/express\": \"^4.17.21\",\n    \"@types/node\": \"^20.10.0\",\n    \"@types/cors\": \"^2.8.17\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.3.3\"\n  }\n}",
    "web_platform/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_platform/frontend/postcss.config.js": "module.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n};",
    "web_platform/frontend/public/manifest.json": "{\n  \"name\": \"Fortuna Faucet Command Deck\",\n  \"short_name\": \"Fortuna\",\n  \"description\": \"Real-time racing analysis.\",\n  \"start_url\": \"/\",\n  \"display\": \"standalone\",\n  \"background_color\": \"#1a202c\",\n  \"theme_color\": \"#1a202c\",\n  \"icons\": [\n    {\n      \"src\": \"/icons/icon-192x192.png\",\n      \"sizes\": \"192x192\",\n      \"type\": \"image/png\"\n    },\n    {\n      \"src\": \"/icons/icon-512x512.png\",\n      \"sizes\": \"512x512\",\n      \"type\": \"image/png\"\n    }\n  ]\n}\n",
    "web_platform/frontend/public/workbox-4754cb34.js": "define([\"exports\"],function(t){\"use strict\";try{self[\"workbox:core:6.5.4\"]&&_()}catch(t){}const e=(t,...e)=>{let s=t;return e.length>0&&(s+=` :: ${JSON.stringify(e)}`),s};class s extends Error{constructor(t,s){super(e(t,s)),this.name=t,this.details=s}}try{self[\"workbox:routing:6.5.4\"]&&_()}catch(t){}const n=t=>t&&\"object\"==typeof t?t:{handle:t};class r{constructor(t,e,s=\"GET\"){this.handler=n(e),this.match=t,this.method=s}setCatchHandler(t){this.catchHandler=n(t)}}class i extends r{constructor(t,e,s){super(({url:e})=>{const s=t.exec(e.href);if(s&&(e.origin===location.origin||0===s.index))return s.slice(1)},e,s)}}class a{constructor(){this.t=new Map,this.i=new Map}get routes(){return this.t}addFetchListener(){self.addEventListener(\"fetch\",t=>{const{request:e}=t,s=this.handleRequest({request:e,event:t});s&&t.respondWith(s)})}addCacheListener(){self.addEventListener(\"message\",t=>{if(t.data&&\"CACHE_URLS\"===t.data.type){const{payload:e}=t.data,s=Promise.all(e.urlsToCache.map(e=>{\"string\"==typeof e&&(e=[e]);const s=new Request(...e);return this.handleRequest({request:s,event:t})}));t.waitUntil(s),t.ports&&t.ports[0]&&s.then(()=>t.ports[0].postMessage(!0))}})}handleRequest({request:t,event:e}){const s=new URL(t.url,location.href);if(!s.protocol.startsWith(\"http\"))return;const n=s.origin===location.origin,{params:r,route:i}=this.findMatchingRoute({event:e,request:t,sameOrigin:n,url:s});let a=i&&i.handler;const o=t.method;if(!a&&this.i.has(o)&&(a=this.i.get(o)),!a)return;let c;try{c=a.handle({url:s,request:t,event:e,params:r})}catch(t){c=Promise.reject(t)}const h=i&&i.catchHandler;return c instanceof Promise&&(this.o||h)&&(c=c.catch(async n=>{if(h)try{return await h.handle({url:s,request:t,event:e,params:r})}catch(t){t instanceof Error&&(n=t)}if(this.o)return this.o.handle({url:s,request:t,event:e});throw n})),c}findMatchingRoute({url:t,sameOrigin:e,request:s,event:n}){const r=this.t.get(s.method)||[];for(const i of r){let r;const a=i.match({url:t,sameOrigin:e,request:s,event:n});if(a)return r=a,(Array.isArray(r)&&0===r.length||a.constructor===Object&&0===Object.keys(a).length||\"boolean\"==typeof a)&&(r=void 0),{route:i,params:r}}return{}}setDefaultHandler(t,e=\"GET\"){this.i.set(e,n(t))}setCatchHandler(t){this.o=n(t)}registerRoute(t){this.t.has(t.method)||this.t.set(t.method,[]),this.t.get(t.method).push(t)}unregisterRoute(t){if(!this.t.has(t.method))throw new s(\"unregister-route-but-not-found-with-method\",{method:t.method});const e=this.t.get(t.method).indexOf(t);if(!(e>-1))throw new s(\"unregister-route-route-not-registered\");this.t.get(t.method).splice(e,1)}}let o;const c=()=>(o||(o=new a,o.addFetchListener(),o.addCacheListener()),o);function h(t,e,n){let a;if(\"string\"==typeof t){const s=new URL(t,location.href);a=new r(({url:t})=>t.href===s.href,e,n)}else if(t instanceof RegExp)a=new i(t,e,n);else if(\"function\"==typeof t)a=new r(t,e,n);else{if(!(t instanceof r))throw new s(\"unsupported-route-type\",{moduleName:\"workbox-routing\",funcName:\"registerRoute\",paramName:\"capture\"});a=t}return c().registerRoute(a),a}try{self[\"workbox:strategies:6.5.4\"]&&_()}catch(t){}const u={cacheWillUpdate:async({response:t})=>200===t.status||0===t.status?t:null},l={googleAnalytics:\"googleAnalytics\",precache:\"precache-v2\",prefix:\"workbox\",runtime:\"runtime\",suffix:\"undefined\"!=typeof registration?registration.scope:\"\"},f=t=>[l.prefix,t,l.suffix].filter(t=>t&&t.length>0).join(\"-\"),w=t=>t||f(l.precache),d=t=>t||f(l.runtime);function p(t,e){const s=new URL(t);for(const t of e)s.searchParams.delete(t);return s.href}class y{constructor(){this.promise=new Promise((t,e)=>{this.resolve=t,this.reject=e})}}const g=new Set;function m(t){return\"string\"==typeof t?new Request(t):t}class v{constructor(t,e){this.h={},Object.assign(this,e),this.event=e.event,this.u=t,this.l=new y,this.p=[],this.m=[...t.plugins],this.v=new Map;for(const t of this.m)this.v.set(t,{});this.event.waitUntil(this.l.promise)}async fetch(t){const{event:e}=this;let n=m(t);if(\"navigate\"===n.mode&&e instanceof FetchEvent&&e.preloadResponse){const t=await e.preloadResponse;if(t)return t}const r=this.hasCallback(\"fetchDidFail\")?n.clone():null;try{for(const t of this.iterateCallbacks(\"requestWillFetch\"))n=await t({request:n.clone(),event:e})}catch(t){if(t instanceof Error)throw new s(\"plugin-error-request-will-fetch\",{thrownErrorMessage:t.message})}const i=n.clone();try{let t;t=await fetch(n,\"navigate\"===n.mode?void 0:this.u.fetchOptions);for(const s of this.iterateCallbacks(\"fetchDidSucceed\"))t=await s({event:e,request:i,response:t});return t}catch(t){throw r&&await this.runCallbacks(\"fetchDidFail\",{error:t,event:e,originalRequest:r.clone(),request:i.clone()}),t}}async fetchAndCachePut(t){const e=await this.fetch(t),s=e.clone();return this.waitUntil(this.cachePut(t,s)),e}async cacheMatch(t){const e=m(t);let s;const{cacheName:n,matchOptions:r}=this.u,i=await this.getCacheKey(e,\"read\"),a=Object.assign(Object.assign({},r),{cacheName:n});s=await caches.match(i,a);for(const t of this.iterateCallbacks(\"cachedResponseWillBeUsed\"))s=await t({cacheName:n,matchOptions:r,cachedResponse:s,request:i,event:this.event})||void 0;return s}async cachePut(t,e){const n=m(t);var r;await(r=0,new Promise(t=>setTimeout(t,r)));const i=await this.getCacheKey(n,\"write\");if(!e)throw new s(\"cache-put-with-no-response\",{url:(a=i.url,new URL(String(a),location.href).href.replace(new RegExp(`^${location.origin}`),\"\"))});var a;const o=await this.R(e);if(!o)return!1;const{cacheName:c,matchOptions:h}=this.u,u=await self.caches.open(c),l=this.hasCallback(\"cacheDidUpdate\"),f=l?await async function(t,e,s,n){const r=p(e.url,s);if(e.url===r)return t.match(e,n);const i=Object.assign(Object.assign({},n),{ignoreSearch:!0}),a=await t.keys(e,i);for(const e of a)if(r===p(e.url,s))return t.match(e,n)}(u,i.clone(),[\"__WB_REVISION__\"],h):null;try{await u.put(i,l?o.clone():o)}catch(t){if(t instanceof Error)throw\"QuotaExceededError\"===t.name&&await async function(){for(const t of g)await t()}(),t}for(const t of this.iterateCallbacks(\"cacheDidUpdate\"))await t({cacheName:c,oldResponse:f,newResponse:o.clone(),request:i,event:this.event});return!0}async getCacheKey(t,e){const s=`${t.url} | ${e}`;if(!this.h[s]){let n=t;for(const t of this.iterateCallbacks(\"cacheKeyWillBeUsed\"))n=m(await t({mode:e,request:n,event:this.event,params:this.params}));this.h[s]=n}return this.h[s]}hasCallback(t){for(const e of this.u.plugins)if(t in e)return!0;return!1}async runCallbacks(t,e){for(const s of this.iterateCallbacks(t))await s(e)}*iterateCallbacks(t){for(const e of this.u.plugins)if(\"function\"==typeof e[t]){const s=this.v.get(e),n=n=>{const r=Object.assign(Object.assign({},n),{state:s});return e[t](r)};yield n}}waitUntil(t){return this.p.push(t),t}async doneWaiting(){let t;for(;t=this.p.shift();)await t}destroy(){this.l.resolve(null)}async R(t){let e=t,s=!1;for(const t of this.iterateCallbacks(\"cacheWillUpdate\"))if(e=await t({request:this.request,response:e,event:this.event})||void 0,s=!0,!e)break;return s||e&&200!==e.status&&(e=void 0),e}}class R{constructor(t={}){this.cacheName=d(t.cacheName),this.plugins=t.plugins||[],this.fetchOptions=t.fetchOptions,this.matchOptions=t.matchOptions}handle(t){const[e]=this.handleAll(t);return e}handleAll(t){t instanceof FetchEvent&&(t={event:t,request:t.request});const e=t.event,s=\"string\"==typeof t.request?new Request(t.request):t.request,n=\"params\"in t?t.params:void 0,r=new v(this,{event:e,request:s,params:n}),i=this.q(r,s,e);return[i,this.D(i,r,s,e)]}async q(t,e,n){let r;await t.runCallbacks(\"handlerWillStart\",{event:n,request:e});try{if(r=await this.U(e,t),!r||\"error\"===r.type)throw new s(\"no-response\",{url:e.url})}catch(s){if(s instanceof Error)for(const i of t.iterateCallbacks(\"handlerDidError\"))if(r=await i({error:s,event:n,request:e}),r)break;if(!r)throw s}for(const s of t.iterateCallbacks(\"handlerWillRespond\"))r=await s({event:n,request:e,response:r});return r}async D(t,e,s,n){let r,i;try{r=await t}catch(i){}try{await e.runCallbacks(\"handlerDidRespond\",{event:n,request:s,response:r}),await e.doneWaiting()}catch(t){t instanceof Error&&(i=t)}if(await e.runCallbacks(\"handlerDidComplete\",{event:n,request:s,response:r,error:i}),e.destroy(),i)throw i}}function b(t){t.then(()=>{})}function q(){return q=Object.assign?Object.assign.bind():function(t){for(var e=1;e<arguments.length;e++){var s=arguments[e];for(var n in s)({}).hasOwnProperty.call(s,n)&&(t[n]=s[n])}return t},q.apply(null,arguments)}let D,U;const x=new WeakMap,L=new WeakMap,I=new WeakMap,C=new WeakMap,E=new WeakMap;let N={get(t,e,s){if(t instanceof IDBTransaction){if(\"done\"===e)return L.get(t);if(\"objectStoreNames\"===e)return t.objectStoreNames||I.get(t);if(\"store\"===e)return s.objectStoreNames[1]?void 0:s.objectStore(s.objectStoreNames[0])}return k(t[e])},set:(t,e,s)=>(t[e]=s,!0),has:(t,e)=>t instanceof IDBTransaction&&(\"done\"===e||\"store\"===e)||e in t};function O(t){return t!==IDBDatabase.prototype.transaction||\"objectStoreNames\"in IDBTransaction.prototype?(U||(U=[IDBCursor.prototype.advance,IDBCursor.prototype.continue,IDBCursor.prototype.continuePrimaryKey])).includes(t)?function(...e){return t.apply(B(this),e),k(x.get(this))}:function(...e){return k(t.apply(B(this),e))}:function(e,...s){const n=t.call(B(this),e,...s);return I.set(n,e.sort?e.sort():[e]),k(n)}}function T(t){return\"function\"==typeof t?O(t):(t instanceof IDBTransaction&&function(t){if(L.has(t))return;const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"complete\",r),t.removeEventListener(\"error\",i),t.removeEventListener(\"abort\",i)},r=()=>{e(),n()},i=()=>{s(t.error||new DOMException(\"AbortError\",\"AbortError\")),n()};t.addEventListener(\"complete\",r),t.addEventListener(\"error\",i),t.addEventListener(\"abort\",i)});L.set(t,e)}(t),e=t,(D||(D=[IDBDatabase,IDBObjectStore,IDBIndex,IDBCursor,IDBTransaction])).some(t=>e instanceof t)?new Proxy(t,N):t);var e}function k(t){if(t instanceof IDBRequest)return function(t){const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"success\",r),t.removeEventListener(\"error\",i)},r=()=>{e(k(t.result)),n()},i=()=>{s(t.error),n()};t.addEventListener(\"success\",r),t.addEventListener(\"error\",i)});return e.then(e=>{e instanceof IDBCursor&&x.set(e,t)}).catch(()=>{}),E.set(e,t),e}(t);if(C.has(t))return C.get(t);const e=T(t);return e!==t&&(C.set(t,e),E.set(e,t)),e}const B=t=>E.get(t);const P=[\"get\",\"getKey\",\"getAll\",\"getAllKeys\",\"count\"],M=[\"put\",\"add\",\"delete\",\"clear\"],W=new Map;function j(t,e){if(!(t instanceof IDBDatabase)||e in t||\"string\"!=typeof e)return;if(W.get(e))return W.get(e);const s=e.replace(/FromIndex$/,\"\"),n=e!==s,r=M.includes(s);if(!(s in(n?IDBIndex:IDBObjectStore).prototype)||!r&&!P.includes(s))return;const i=async function(t,...e){const i=this.transaction(t,r?\"readwrite\":\"readonly\");let a=i.store;return n&&(a=a.index(e.shift())),(await Promise.all([a[s](...e),r&&i.done]))[0]};return W.set(e,i),i}N=(t=>q({},t,{get:(e,s,n)=>j(e,s)||t.get(e,s,n),has:(e,s)=>!!j(e,s)||t.has(e,s)}))(N);try{self[\"workbox:expiration:6.5.4\"]&&_()}catch(t){}const S=\"cache-entries\",K=t=>{const e=new URL(t,location.href);return e.hash=\"\",e.href};class A{constructor(t){this._=null,this.L=t}I(t){const e=t.createObjectStore(S,{keyPath:\"id\"});e.createIndex(\"cacheName\",\"cacheName\",{unique:!1}),e.createIndex(\"timestamp\",\"timestamp\",{unique:!1})}C(t){this.I(t),this.L&&function(t,{blocked:e}={}){const s=indexedDB.deleteDatabase(t);e&&s.addEventListener(\"blocked\",t=>e(t.oldVersion,t)),k(s).then(()=>{})}(this.L)}async setTimestamp(t,e){const s={url:t=K(t),timestamp:e,cacheName:this.L,id:this.N(t)},n=(await this.getDb()).transaction(S,\"readwrite\",{durability:\"relaxed\"});await n.store.put(s),await n.done}async getTimestamp(t){const e=await this.getDb(),s=await e.get(S,this.N(t));return null==s?void 0:s.timestamp}async expireEntries(t,e){const s=await this.getDb();let n=await s.transaction(S).store.index(\"timestamp\").openCursor(null,\"prev\");const r=[];let i=0;for(;n;){const s=n.value;s.cacheName===this.L&&(t&&s.timestamp<t||e&&i>=e?r.push(n.value):i++),n=await n.continue()}const a=[];for(const t of r)await s.delete(S,t.id),a.push(t.url);return a}N(t){return this.L+\"|\"+K(t)}async getDb(){return this._||(this._=await function(t,e,{blocked:s,upgrade:n,blocking:r,terminated:i}={}){const a=indexedDB.open(t,e),o=k(a);return n&&a.addEventListener(\"upgradeneeded\",t=>{n(k(a.result),t.oldVersion,t.newVersion,k(a.transaction),t)}),s&&a.addEventListener(\"blocked\",t=>s(t.oldVersion,t.newVersion,t)),o.then(t=>{i&&t.addEventListener(\"close\",()=>i()),r&&t.addEventListener(\"versionchange\",t=>r(t.oldVersion,t.newVersion,t))}).catch(()=>{}),o}(\"workbox-expiration\",1,{upgrade:this.C.bind(this)})),this._}}class F{constructor(t,e={}){this.O=!1,this.T=!1,this.k=e.maxEntries,this.B=e.maxAgeSeconds,this.P=e.matchOptions,this.L=t,this.M=new A(t)}async expireEntries(){if(this.O)return void(this.T=!0);this.O=!0;const t=this.B?Date.now()-1e3*this.B:0,e=await this.M.expireEntries(t,this.k),s=await self.caches.open(this.L);for(const t of e)await s.delete(t,this.P);this.O=!1,this.T&&(this.T=!1,b(this.expireEntries()))}async updateTimestamp(t){await this.M.setTimestamp(t,Date.now())}async isURLExpired(t){if(this.B){const e=await this.M.getTimestamp(t),s=Date.now()-1e3*this.B;return void 0===e||e<s}return!1}async delete(){this.T=!1,await this.M.expireEntries(1/0)}}try{self[\"workbox:range-requests:6.5.4\"]&&_()}catch(t){}async function H(t,e){try{if(206===e.status)return e;const n=t.headers.get(\"range\");if(!n)throw new s(\"no-range-header\");const r=function(t){const e=t.trim().toLowerCase();if(!e.startsWith(\"bytes=\"))throw new s(\"unit-must-be-bytes\",{normalizedRangeHeader:e});if(e.includes(\",\"))throw new s(\"single-range-only\",{normalizedRangeHeader:e});const n=/(\\d*)-(\\d*)/.exec(e);if(!n||!n[1]&&!n[2])throw new s(\"invalid-range-values\",{normalizedRangeHeader:e});return{start:\"\"===n[1]?void 0:Number(n[1]),end:\"\"===n[2]?void 0:Number(n[2])}}(n),i=await e.blob(),a=function(t,e,n){const r=t.size;if(n&&n>r||e&&e<0)throw new s(\"range-not-satisfiable\",{size:r,end:n,start:e});let i,a;return void 0!==e&&void 0!==n?(i=e,a=n+1):void 0!==e&&void 0===n?(i=e,a=r):void 0!==n&&void 0===e&&(i=r-n,a=r),{start:i,end:a}}(i,r.start,r.end),o=i.slice(a.start,a.end),c=o.size,h=new Response(o,{status:206,statusText:\"Partial Content\",headers:e.headers});return h.headers.set(\"Content-Length\",String(c)),h.headers.set(\"Content-Range\",`bytes ${a.start}-${a.end-1}/${i.size}`),h}catch(t){return new Response(\"\",{status:416,statusText:\"Range Not Satisfiable\"})}}function $(t,e){const s=e();return t.waitUntil(s),s}try{self[\"workbox:precaching:6.5.4\"]&&_()}catch(t){}function z(t){if(!t)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(\"string\"==typeof t){const e=new URL(t,location.href);return{cacheKey:e.href,url:e.href}}const{revision:e,url:n}=t;if(!n)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(!e){const t=new URL(n,location.href);return{cacheKey:t.href,url:t.href}}const r=new URL(n,location.href),i=new URL(n,location.href);return r.searchParams.set(\"__WB_REVISION__\",e),{cacheKey:r.href,url:i.href}}class G{constructor(){this.updatedURLs=[],this.notUpdatedURLs=[],this.handlerWillStart=async({request:t,state:e})=>{e&&(e.originalRequest=t)},this.cachedResponseWillBeUsed=async({event:t,state:e,cachedResponse:s})=>{if(\"install\"===t.type&&e&&e.originalRequest&&e.originalRequest instanceof Request){const t=e.originalRequest.url;s?this.notUpdatedURLs.push(t):this.updatedURLs.push(t)}return s}}}class V{constructor({precacheController:t}){this.cacheKeyWillBeUsed=async({request:t,params:e})=>{const s=(null==e?void 0:e.cacheKey)||this.W.getCacheKeyForURL(t.url);return s?new Request(s,{headers:t.headers}):t},this.W=t}}let J,Q;async function X(t,e){let n=null;if(t.url){n=new URL(t.url).origin}if(n!==self.location.origin)throw new s(\"cross-origin-copy-response\",{origin:n});const r=t.clone(),i={headers:new Headers(r.headers),status:r.status,statusText:r.statusText},a=e?e(i):i,o=function(){if(void 0===J){const t=new Response(\"\");if(\"body\"in t)try{new Response(t.body),J=!0}catch(t){J=!1}J=!1}return J}()?r.body:await r.blob();return new Response(o,a)}class Y extends R{constructor(t={}){t.cacheName=w(t.cacheName),super(t),this.j=!1!==t.fallbackToNetwork,this.plugins.push(Y.copyRedirectedCacheableResponsesPlugin)}async U(t,e){const s=await e.cacheMatch(t);return s||(e.event&&\"install\"===e.event.type?await this.S(t,e):await this.K(t,e))}async K(t,e){let n;const r=e.params||{};if(!this.j)throw new s(\"missing-precache-entry\",{cacheName:this.cacheName,url:t.url});{const s=r.integrity,i=t.integrity,a=!i||i===s;n=await e.fetch(new Request(t,{integrity:\"no-cors\"!==t.mode?i||s:void 0})),s&&a&&\"no-cors\"!==t.mode&&(this.A(),await e.cachePut(t,n.clone()))}return n}async S(t,e){this.A();const n=await e.fetch(t);if(!await e.cachePut(t,n.clone()))throw new s(\"bad-precaching-response\",{url:t.url,status:n.status});return n}A(){let t=null,e=0;for(const[s,n]of this.plugins.entries())n!==Y.copyRedirectedCacheableResponsesPlugin&&(n===Y.defaultPrecacheCacheabilityPlugin&&(t=s),n.cacheWillUpdate&&e++);0===e?this.plugins.push(Y.defaultPrecacheCacheabilityPlugin):e>1&&null!==t&&this.plugins.splice(t,1)}}Y.defaultPrecacheCacheabilityPlugin={cacheWillUpdate:async({response:t})=>!t||t.status>=400?null:t},Y.copyRedirectedCacheableResponsesPlugin={cacheWillUpdate:async({response:t})=>t.redirected?await X(t):t};class Z{constructor({cacheName:t,plugins:e=[],fallbackToNetwork:s=!0}={}){this.F=new Map,this.H=new Map,this.$=new Map,this.u=new Y({cacheName:w(t),plugins:[...e,new V({precacheController:this})],fallbackToNetwork:s}),this.install=this.install.bind(this),this.activate=this.activate.bind(this)}get strategy(){return this.u}precache(t){this.addToCacheList(t),this.G||(self.addEventListener(\"install\",this.install),self.addEventListener(\"activate\",this.activate),this.G=!0)}addToCacheList(t){const e=[];for(const n of t){\"string\"==typeof n?e.push(n):n&&void 0===n.revision&&e.push(n.url);const{cacheKey:t,url:r}=z(n),i=\"string\"!=typeof n&&n.revision?\"reload\":\"default\";if(this.F.has(r)&&this.F.get(r)!==t)throw new s(\"add-to-cache-list-conflicting-entries\",{firstEntry:this.F.get(r),secondEntry:t});if(\"string\"!=typeof n&&n.integrity){if(this.$.has(t)&&this.$.get(t)!==n.integrity)throw new s(\"add-to-cache-list-conflicting-integrities\",{url:r});this.$.set(t,n.integrity)}if(this.F.set(r,t),this.H.set(r,i),e.length>0){const t=`Workbox is precaching URLs without revision info: ${e.join(\", \")}\\nThis is generally NOT safe. Learn more at https://bit.ly/wb-precache`;console.warn(t)}}}install(t){return $(t,async()=>{const e=new G;this.strategy.plugins.push(e);for(const[e,s]of this.F){const n=this.$.get(s),r=this.H.get(e),i=new Request(e,{integrity:n,cache:r,credentials:\"same-origin\"});await Promise.all(this.strategy.handleAll({params:{cacheKey:s},request:i,event:t}))}const{updatedURLs:s,notUpdatedURLs:n}=e;return{updatedURLs:s,notUpdatedURLs:n}})}activate(t){return $(t,async()=>{const t=await self.caches.open(this.strategy.cacheName),e=await t.keys(),s=new Set(this.F.values()),n=[];for(const r of e)s.has(r.url)||(await t.delete(r),n.push(r.url));return{deletedURLs:n}})}getURLsToCacheKeys(){return this.F}getCachedURLs(){return[...this.F.keys()]}getCacheKeyForURL(t){const e=new URL(t,location.href);return this.F.get(e.href)}getIntegrityForCacheKey(t){return this.$.get(t)}async matchPrecache(t){const e=t instanceof Request?t.url:t,s=this.getCacheKeyForURL(e);if(s){return(await self.caches.open(this.strategy.cacheName)).match(s)}}createHandlerBoundToURL(t){const e=this.getCacheKeyForURL(t);if(!e)throw new s(\"non-precached-url\",{url:t});return s=>(s.request=new Request(t),s.params=Object.assign({cacheKey:e},s.params),this.strategy.handle(s))}}const tt=()=>(Q||(Q=new Z),Q);class et extends r{constructor(t,e){super(({request:s})=>{const n=t.getURLsToCacheKeys();for(const r of function*(t,{ignoreURLParametersMatching:e=[/^utm_/,/^fbclid$/],directoryIndex:s=\"index.html\",cleanURLs:n=!0,urlManipulation:r}={}){const i=new URL(t,location.href);i.hash=\"\",yield i.href;const a=function(t,e=[]){for(const s of[...t.searchParams.keys()])e.some(t=>t.test(s))&&t.searchParams.delete(s);return t}(i,e);if(yield a.href,s&&a.pathname.endsWith(\"/\")){const t=new URL(a.href);t.pathname+=s,yield t.href}if(n){const t=new URL(a.href);t.pathname+=\".html\",yield t.href}if(r){const t=r({url:i});for(const e of t)yield e.href}}(s.url,e)){const e=n.get(r);if(e){return{cacheKey:e,integrity:t.getIntegrityForCacheKey(e)}}}},t.strategy)}}t.CacheFirst=class extends R{async U(t,e){let n,r=await e.cacheMatch(t);if(!r)try{r=await e.fetchAndCachePut(t)}catch(t){t instanceof Error&&(n=t)}if(!r)throw new s(\"no-response\",{url:t.url,error:n});return r}},t.ExpirationPlugin=class{constructor(t={}){this.cachedResponseWillBeUsed=async({event:t,request:e,cacheName:s,cachedResponse:n})=>{if(!n)return null;const r=this.V(n),i=this.J(s);b(i.expireEntries());const a=i.updateTimestamp(e.url);if(t)try{t.waitUntil(a)}catch(t){}return r?n:null},this.cacheDidUpdate=async({cacheName:t,request:e})=>{const s=this.J(t);await s.updateTimestamp(e.url),await s.expireEntries()},this.X=t,this.B=t.maxAgeSeconds,this.Y=new Map,t.purgeOnQuotaError&&function(t){g.add(t)}(()=>this.deleteCacheAndMetadata())}J(t){if(t===d())throw new s(\"expire-custom-caches-only\");let e=this.Y.get(t);return e||(e=new F(t,this.X),this.Y.set(t,e)),e}V(t){if(!this.B)return!0;const e=this.Z(t);if(null===e)return!0;return e>=Date.now()-1e3*this.B}Z(t){if(!t.headers.has(\"date\"))return null;const e=t.headers.get(\"date\"),s=new Date(e).getTime();return isNaN(s)?null:s}async deleteCacheAndMetadata(){for(const[t,e]of this.Y)await self.caches.delete(t),await e.delete();this.Y=new Map}},t.NetworkFirst=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u),this.tt=t.networkTimeoutSeconds||0}async U(t,e){const n=[],r=[];let i;if(this.tt){const{id:s,promise:a}=this.et({request:t,logs:n,handler:e});i=s,r.push(a)}const a=this.st({timeoutId:i,request:t,logs:n,handler:e});r.push(a);const o=await e.waitUntil((async()=>await e.waitUntil(Promise.race(r))||await a)());if(!o)throw new s(\"no-response\",{url:t.url});return o}et({request:t,logs:e,handler:s}){let n;return{promise:new Promise(e=>{n=setTimeout(async()=>{e(await s.cacheMatch(t))},1e3*this.tt)}),id:n}}async st({timeoutId:t,request:e,logs:s,handler:n}){let r,i;try{i=await n.fetchAndCachePut(e)}catch(t){t instanceof Error&&(r=t)}return t&&clearTimeout(t),!r&&i||(i=await n.cacheMatch(e)),i}},t.RangeRequestsPlugin=class{constructor(){this.cachedResponseWillBeUsed=async({request:t,cachedResponse:e})=>e&&t.headers.has(\"range\")?await H(t,e):e}},t.StaleWhileRevalidate=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u)}async U(t,e){const n=e.fetchAndCachePut(t).catch(()=>{});e.waitUntil(n);let r,i=await e.cacheMatch(t);if(i);else try{i=await n}catch(t){t instanceof Error&&(r=t)}if(!i)throw new s(\"no-response\",{url:t.url,error:r});return i}},t.cleanupOutdatedCaches=function(){self.addEventListener(\"activate\",t=>{const e=w();t.waitUntil((async(t,e=\"-precache-\")=>{const s=(await self.caches.keys()).filter(s=>s.includes(e)&&s.includes(self.registration.scope)&&s!==t);return await Promise.all(s.map(t=>self.caches.delete(t))),s})(e).then(t=>{}))})},t.clientsClaim=function(){self.addEventListener(\"activate\",()=>self.clients.claim())},t.precacheAndRoute=function(t,e){!function(t){tt().precache(t)}(t),function(t){const e=tt();h(new et(e,t))}(e)},t.registerRoute=h});\n",
    "web_platform/frontend/src/components/ErrorDisplay.tsx": "// web_platform/frontend/src/components/ErrorDisplay.tsx\n'use client';\n\nimport React from 'react';\n\ninterface ErrorInfo {\n  message: string;\n  suggestion: string;\n  details?: string;\n}\n\ninterface ErrorDisplayProps {\n  error: ErrorInfo;\n}\n\nexport const ErrorDisplay: React.FC<ErrorDisplayProps> = ({ error }) => {\n  return (\n    <div className=\"bg-red-900/20 border border-red-500/30 text-white rounded-lg p-6 max-w-2xl mx-auto my-8\">\n      <div className=\"flex items-center mb-4\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-8 w-8 text-red-400 mr-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n          <path fillRule=\"evenodd\" d=\"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z\" clipRule=\"evenodd\" />\n        </svg>\n        <h2 className=\"text-2xl font-bold text-red-400\">An Error Occurred</h2>\n      </div>\n      <p className=\"text-lg text-slate-300 mb-2\">{error.message}</p>\n      <p className=\"text-slate-400 mb-6\">{error.suggestion}</p>\n      {error.details && (\n        <details className=\"bg-slate-800/50 rounded-lg p-4\">\n          <summary className=\"cursor-pointer text-sm text-slate-500 hover:text-white\">\n            Technical Details\n          </summary>\n          <pre className=\"text-xs text-slate-400 mt-2 p-2 bg-black/30 rounded overflow-x-auto\">\n            <code>{error.details}</code>\n          </pre>\n        </details>\n      )}\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/RaceCard.tsx": "// web_platform/frontend/src/components/RaceCard.tsx\n'use client';\n\nimport React, { useState, useEffect } from 'react';\nimport type { Race, Runner } from '../types/racing';\n\n// Local types removed, now importing from '../types/racing'\n\ninterface RaceCardProps {\n  race: Race;\n}\n\nconst Countdown: React.FC<{ startTime: string }> = ({ startTime }) => {\n  const [currentTime, setCurrentTime] = useState(new Date());\n\n  useEffect(() => {\n    const timer = setInterval(() => setCurrentTime(new Date()), 1000);\n    return () => clearInterval(timer);\n  }, []);\n\n  const getCountdown = (startTimeStr: string) => {\n    const postTime = new Date(startTimeStr);\n    const diff = postTime.getTime() - currentTime.getTime();\n\n    if (diff <= 0) return { text: \"RACE COMPLETE\", color: \"text-gray-500\" };\n\n    const minutes = Math.floor(diff / 60000);\n    const seconds = Math.floor((diff % 60000) / 1000).toString().padStart(2, '0');\n\n    let color = \"text-green-400\";\n    if (minutes < 2) color = \"text-red-500 font-bold animate-pulse\";\n    else if (minutes < 10) color = \"text-yellow-400\";\n\n    return { text: `${minutes}:${seconds} to post`, color };\n  };\n\n  const countdown = getCountdown(startTime);\n\n  return (\n    <span className={`font-mono text-sm ${countdown.color}`}>{countdown.text}</span>\n  );\n};\n\nexport const RaceCard: React.FC<RaceCardProps> = ({ race }) => {\n  const activeRunners = race.runners.filter(r => !r.scratched);\n  activeRunners.sort((a, b) => a.number - b.number);\n\n  const getUniqueSourcesCount = (runners: Runner[]): number => {\n    const sources = new Set();\n    runners.forEach(runner => {\n      if (runner.odds) {\n        Object.keys(runner.odds).forEach(source => sources.add(source));\n      }\n    });\n    return sources.size;\n  };\n\n  const getBestOdds = (runner: Runner): { odds: number, source: string } | null => {\n    if (!runner.odds) return null;\n  const validOdds = Object.values(runner.odds).filter(o => o.win !== null && o.win !== undefined && o.win < 999);\n    if (validOdds.length === 0) return null;\n  const best = validOdds.reduce((min, o) => (o.win ?? 999) < (min.win ?? 999) ? o : min);\n    return { odds: best.win!, source: best.source };\n  };\n\n  return (\n    <div className={`race-card-enhanced border rounded-lg p-4 bg-gray-800 shadow-lg hover:border-purple-500 transition-all ${race.qualification_score && race.qualification_score >= 80 ? 'card-premium' : 'border-gray-700'}`}>\n      {/* Header with Smart Status Indicators */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <h2 className=\"text-2xl font-bold text-white\">{race.venue}</h2>\n            <div className=\"flex gap-2 text-sm text-gray-400\">\n              <span>Race {race.race_number}</span>\n              <span>\u2022</span>\n              <Countdown startTime={race.start_time} />\n            </div>\n            {race.favorite && (\n              <div className=\"flex items-center gap-2 mt-2 text-sm text-yellow-400\">\n                <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-4 w-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n                  <path d=\"M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z\" />\n                </svg>\n                <span className=\"font-semibold\">Favorite: {race.favorite.name}</span>\n              </div>\n            )}\n          </div>\n        </div>\n\n        {race.qualification_score && (\n          <div className={`px-4 py-2 rounded-full text-center ${\n            race.qualification_score >= 80 ? 'bg-red-500/20 text-red-400 border border-red-500/30' :\n            race.qualification_score >= 60 ? 'bg-yellow-500/20 text-yellow-400 border border-yellow-500/30' :\n            'bg-green-500/20 text-green-400 border border-green-500/30'\n          }`}>\n            <div className=\"font-bold text-lg\">{race.qualification_score.toFixed(0)}%</div>\n            <div className=\"text-xs\">Score</div>\n          </div>\n        )}\n      </div>\n\n      {/* Race Conditions Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Distance</div>\n          <div className=\"text-sm font-semibold text-white\">{race.distance || 'N/A'}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Surface</div>\n          <div className=\"text-sm font-semibold text-white\">{race.surface || 'Dirt'}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Field</div>\n          <div className=\"text-sm font-semibold text-white\">{activeRunners.length}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Sources</div>\n          <div className=\"text-sm font-semibold text-white\">{getUniqueSourcesCount(race.runners)}</div>\n        </div>\n      </div>\n\n      {/* Interactive Runner Rows */}\n      <div className=\"runners-table space-y-2\">\n        {activeRunners.map((runner, idx) => {\n          const bestOddsInfo = getBestOdds(runner);\n          return (\n            <div key={runner.number} className=\"runner-row group hover:bg-purple-500/10 transition-all rounded-md p-3\">\n              <div className=\"flex items-center justify-between\">\n                <div className=\"flex items-center gap-4 flex-1\">\n                  <div className={`w-10 h-10 rounded-full flex items-center justify-center font-bold transition-all group-hover:scale-110 text-gray-900 shadow-lg ${idx === 0 ? 'bg-gradient-to-br from-yellow-400 to-yellow-600 shadow-yellow-500/50' : idx === 1 ? 'bg-gradient-to-br from-gray-300 to-gray-500 shadow-gray-400/50' : idx === 2 ? 'bg-gradient-to-br from-orange-400 to-orange-600 shadow-orange-500/50' : 'bg-gray-700 text-gray-300'}`}>\n                    {runner.number}\n                  </div>\n                  <div className=\"flex flex-col\">\n                    <span className=\"font-bold text-white text-lg\">{runner.name}</span>\n                    <div className=\"flex gap-3 text-sm text-gray-400\">\n                      {runner.jockey && <span>J: {runner.jockey}</span>}\n                      {runner.trainer && <span>T: {runner.trainer}</span>}\n                    </div>\n                  </div>\n                </div>\n                {bestOddsInfo && (\n                  <div className=\"text-right\">\n                    <div className=\"text-2xl font-bold text-emerald-400\">{bestOddsInfo.odds.toFixed(2)}</div>\n                    <div className=\"text-xs text-gray-500\">via {bestOddsInfo.source}</div>\n                  </div>\n                )}\n              </div>\n            </div>\n          );\n        })}\n      </div>\n    </div>\n  );\n};",
    "web_platform/frontend/src/components/RaceCardSkeleton.tsx": "// web_platform/frontend/src/components/RaceCardSkeleton.tsx\nimport React from 'react';\n\nexport const RaceCardSkeleton: React.FC = () => {\n  return (\n    <div className=\"race-card-skeleton border border-gray-700 rounded-lg p-4 bg-gray-800 shadow-lg animate-pulse\">\n      {/* Skeleton Header */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <div className=\"h-7 w-28 bg-gray-700 rounded-md\"></div>\n            <div className=\"h-4 w-40 bg-gray-700 rounded-md mt-2\"></div>\n          </div>\n        </div>\n        <div className=\"h-16 w-16 bg-gray-700 rounded-full\"></div>\n      </div>\n\n      {/* Skeleton Info Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n      </div>\n\n      {/* Skeleton Runner Rows */}\n      <div className=\"space-y-2\">\n        {[...Array(3)].map((_, i) => (\n          <div key={i} className=\"runner-row rounded-md p-3\">\n            <div className=\"flex items-center justify-between\">\n              <div className=\"flex items-center gap-4 flex-1\">\n                <div className=\"w-10 h-10 rounded-full bg-gray-700\"></div>\n                <div className=\"flex flex-col space-y-2\">\n                  <div className=\"h-5 w-32 bg-gray-700 rounded-md\"></div>\n                  <div className=\"h-4 w-40 bg-gray-700 rounded-md\"></div>\n                </div>\n              </div>\n              <div className=\"text-right\">\n                <div className=\"h-6 w-16 bg-gray-700 rounded-md\"></div>\n                <div className=\"h-3 w-12 bg-gray-700 rounded-md mt-2\"></div>\n              </div>\n            </div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/RaceFilters.tsx": "// web_platform/frontend/src/components/RaceFilters.tsx\n'use client';\n\nimport { useState, useCallback } from 'react';\nimport { Settings, RotateCcw } from 'lucide-react';\n\ninterface FilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nexport interface RaceFiltersProps {\n  onParamsChange: (params: FilterParams) => void;\n  isLoading: boolean;\n  refetch: () => void;\n}\n\nconst DEFAULT_PARAMS: FilterParams = {\n  maxFieldSize: 10,\n  minFavoriteOdds: 2.5,\n  minSecondFavoriteOdds: 4.0,\n};\n\nexport function RaceFilters({ onParamsChange, isLoading, refetch }: RaceFiltersProps) {\n  const [params, setParams] = useState<FilterParams>(DEFAULT_PARAMS);\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  // Handle individual parameter changes\n  const handleChange = useCallback((key: keyof FilterParams, value: number) => {\n    setParams(prev => {\n      const updated = { ...prev, [key]: value };\n      onParamsChange(updated);\n      return updated;\n    });\n    // Debounce the refetch call\n    const timer = setTimeout(() => {\n      refetch();\n    }, 500);\n    return () => clearTimeout(timer);\n  }, [onParamsChange, refetch]);\n\n  // Reset to defaults\n  const handleReset = useCallback(() => {\n    setParams(DEFAULT_PARAMS);\n    onParamsChange(DEFAULT_PARAMS);\n    refetch();\n  }, [onParamsChange, refetch]);\n\n  return (\n    <div className=\"bg-gradient-to-r from-slate-800 to-slate-900 rounded-lg p-4 mb-6 border border-slate-700\">\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-2\">\n          <Settings className=\"w-5 h-5 text-amber-500\" />\n          <h3 className=\"text-lg font-semibold text-white\">Race Filters</h3>\n        </div>\n        <button\n          onClick={() => setIsExpanded(!isExpanded)}\n          className=\"text-sm text-slate-400 hover:text-slate-200 transition\"\n        >\n          {isExpanded ? 'Hide' : 'Show'}\n        </button>\n      </div>\n\n      {isExpanded && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 pt-4 border-t border-slate-700\">\n          {/* Max Field Size */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Max Field Size\n              <span className=\"text-amber-500 ml-2\">{params.maxFieldSize}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2\"\n              max=\"20\"\n              value={params.maxFieldSize}\n              onChange={(e) => handleChange('maxFieldSize', parseInt(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Filters races with larger fields</p>\n          </div>\n\n          {/* Min Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"1.5\"\n              max=\"5\"\n              step=\"0.1\"\n              value={params.minFavoriteOdds}\n              onChange={(e) => handleChange('minFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = pickier favorites</p>\n          </div>\n\n          {/* Min Second Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min 2nd Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minSecondFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2.0\"\n              max=\"8\"\n              step=\"0.1\"\n              value={params.minSecondFavoriteOdds}\n              onChange={(e) => handleChange('minSecondFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = better odds separation</p>\n          </div>\n\n          {/* Reset Button */}\n          <div className=\"md:col-span-3 flex justify-end pt-4 border-t border-slate-700\">\n            <button\n              onClick={handleReset}\n              disabled={isLoading}\n              className=\"inline-flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 text-slate-200 rounded text-sm font-medium transition disabled:opacity-50\"\n            >\n              <RotateCcw className=\"w-4 h-4\" />\n              Reset to Defaults\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n}\n",
    "web_platform/frontend/src/components/Tabs.tsx": "// src/components/Tabs.tsx\n'use client';\n\nimport React, { useState } from 'react';\n\ntype Tab = {\n  label: string;\n  content: React.ReactNode;\n};\n\ntype TabsProps = {\n  tabs: Tab[];\n};\n\nexport function Tabs({ tabs }: TabsProps) {\n  const [activeTab, setActiveTab] = useState(0);\n\n  return (\n    <div>\n      <div className=\"border-b border-slate-700\">\n        <nav className=\"-mb-px flex space-x-8\" aria-label=\"Tabs\">\n          {tabs.map((tab, index) => (\n            <button\n              key={tab.label}\n              onClick={() => setActiveTab(index)}\n              className={`${\n                activeTab === index\n                  ? 'border-blue-500 text-blue-400'\n                  : 'border-transparent text-slate-400 hover:text-slate-200 hover:border-slate-500'\n              } whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm transition-colors focus:outline-none`}\n            >\n              {tab.label}\n            </button>\n          ))}\n        </nav>\n      </div>\n      <div className=\"mt-8\">{tabs[activeTab].content}</div>\n    </div>\n  );\n}\n",
    "web_platform/frontend/src/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n",
    "web_platform/frontend/src/types/racing.ts": "// web_platform/frontend/src/types/racing.ts\n// This file is the central source of truth for frontend racing data types.\n\n// --- Runner & Odds Interfaces ---\nexport interface OddsData {\n  win: number | null;\n  place: number | null;\n  show: number | null;\n  source: string;\n  last_updated: string;\n}\n\nexport interface Runner {\n  number: number;\n  name: string;\n  scratched: boolean;\n  selection_id?: number;\n  odds: Record<string, OddsData>;\n  jockey?: string;\n  trainer?: string;\n}\n\n// --- Race Interface ---\n// This interface matches the shape of the data returned by the API for the dashboard.\nexport interface Race {\n  id: string;\n  venue: string;\n  race_number: number;\n  start_time: string;\n  runners: Runner[];\n  source: string;\n  qualification_score?: number;\n  distance?: string;\n  surface?: string;\n  favorite?: Runner;\n  isErrorPlaceholder?: boolean;\n  errorMessage?: string;\n}\n\n// --- API Response Interfaces ---\nexport interface SourceInfo {\n  name: string;\n  status: 'SUCCESS' | 'FAILED' | 'CONFIG_ERROR' | 'PENDING';\n  racesFetched: number;\n  fetchDuration: number;\n  errorMessage?: string;\n  attemptedUrl?: string;\n}\n\nexport interface AdapterError {\n  adapterName: string;\n  errorMessage: string;\n  attemptedUrl?: string;\n}\n\nexport interface AggregatedRacesResponse {\n  races: Race[];\n  errors: AdapterError[];\n  source_info: SourceInfo[];\n}\n\n// --- Analysis Factor Interfaces (retained from previous version) ---\nexport interface Factor {\n    points: number;\n    ok: boolean;\n    reason: string;\n}\n\nexport interface TrifectaFactors {\n    [key: string]: Factor;\n}\n",
    "web_platform/frontend/tailwind.config.ts": "import type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  darkMode: 'media',\n  content: [\n    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './src/components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\nexport default config",
    "web_service/backend/adapters/betfair_auth_mixin.py": "# python_service/adapters/betfair_auth_mixin.py\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..credentials_manager import SecureCredentialsManager\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock = asyncio.Lock()\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        \"\"\"\n        Authenticates with Betfair using credentials from the system's credential manager,\n        ensuring the session token is valid and refreshing it if necessary.\n        \"\"\"\n        async with self._auth_lock:\n            if self.session_token and self.token_expiry and self.token_expiry > (datetime.now() + timedelta(minutes=5)):\n                return\n\n            log.info(\"Attempting to authenticate with Betfair...\")\n            username, password = SecureCredentialsManager.get_betfair_credentials()\n\n            if not all([self.config.BETFAIR_APP_KEY, username, password]):\n                raise ValueError(\"Betfair credentials not fully configured in credential manager.\")\n\n            auth_url = \"https://identitysso.betfair.com/api/login\"\n            headers = {\n                \"X-Application\": self.config.BETFAIR_APP_KEY,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={username}&password={password}\"\n\n            response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n            response.raise_for_status()\n            data = response.json()\n\n            if data.get(\"status\") == \"SUCCESS\":\n                self.session_token = data.get(\"token\")\n                self.token_expiry = datetime.now() + timedelta(hours=3)\n                log.info(\"Betfair authentication successful.\")\n            else:\n                log.error(\"Betfair authentication failed\", error=data.get(\"error\"))\n                self.session_token = None  # Reset token to prevent using a stale one\n                return  # Return gracefully and let the adapter handle the lack of a token\n",
    "web_service/backend/adapters/brisnet_adapter.py": "# python_service/adapters/brisnet_adapter.py\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BrisnetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for brisnet.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Brisnet\"\n    BASE_URL = \"https://www.brisnet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetches the raw HTML from the Brisnet race page.\"\"\"\n        # Note: Brisnet URL structure seems to require a track code, e.g., 'CD' for Churchill Downs.\n        # This implementation will need to be improved to dynamically handle different tracks.\n        # For now, it is hardcoded to Churchill Downs as a placeholder.\n        url = f\"/race/{date}/CD\"\n        response = await self.make_request(self.http_client, \"GET\", url)\n        return {\"html\": response.text, \"date\": date} if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[dict]) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        venue_text_node = soup.select_one(\"header h1\")\n        if not venue_text_node:\n            self.logger.warning(\"Could not find venue name on Brisnet page.\")\n            return []\n\n        venue_text = venue_text_node.text\n        venue = normalize_venue_name(venue_text.split(\" - \")[0])\n\n        races = []\n        for race_section in soup.select(\"section.race\"):\n            try:\n                race_number_str = race_section.get(\"data-racenumber\")\n                if not race_number_str or not race_number_str.isdigit():\n                    continue\n                race_number = int(race_number_str)\n\n                post_time_node = race_section.select_one(\".race-title span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text.replace(\"Post Time: \", \"\").strip()\n                start_time = parse(f\"{race_date} {post_time_str}\")\n\n                runners = []\n                for row in race_section.select(\"tbody tr\"):\n                    if \"scratched\" in row.get(\"class\", []):\n                        continue\n\n                    cells = row.find_all(\"td\")\n                    if len(cells) < 3:\n                        continue\n\n                    number_text = cells[0].text.strip()\n                    if not number_text.isdigit():\n                        continue\n                    number = int(number_text)\n\n                    name = cells[1].text.strip()\n                    odds_str = cells[2].text.strip()\n\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    odds = {}\n                    if win_odds:\n                        odds[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(Runner(number=number, name=name, odds=odds))\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"brisnet_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                    field_size=len(runners),\n                )\n                races.append(race)\n            except (ValueError, IndexError, TypeError):\n                self.logger.warning(\"Failed to parse a race on Brisnet, skipping.\", exc_info=True)\n                continue\n\n        return races\n",
    "web_service/backend/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass EquibaseAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        d = datetime.strptime(date, \"%Y-%m-%d\").date()\n        index_url = f\"/entries/Entries.cfm?ELEC_DATE={d.month}/{d.day}/{d.year}&STYLE=EQB\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Equibase index page\", url=index_url)\n            return None\n\n        parser = HTMLParser(index_response.text)\n        track_links = [\n            link.attributes[\"href\"]\n            for link in parser.css(\"div.track-information a\")\n            if \"race=\" not in link.attributes.get(\"href\", \"\")\n        ]\n\n        async def get_race_links_from_track(track_url: str):\n            response = await self.make_request(self.http_client, \"GET\", track_url, headers=self._get_headers())\n            if not response:\n                return []\n            parser = HTMLParser(response.text)\n            return [link.attributes[\"href\"] for link in parser.css(\"a.program-race-link\")]\n\n        tasks = [get_race_links_from_track(link) for link in track_links]\n        results = await asyncio.gather(*tasks)\n        race_links = [f\"{self.base_url}{link}\" for sublist in results for link in sublist]\n\n        async def fetch_single_html(race_url: str):\n            response = await self.make_request(self.http_client, \"GET\", race_url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first(\"div.track-information strong\")\n                if not venue_node:\n                    continue\n                venue = clean_text(venue_node.text())\n\n                race_number_node = parser.css_first(\"div.race-information strong\")\n                if not race_number_node:\n                    continue\n                race_number_text = race_number_node.text().replace(\"Race\", \"\").strip()\n                if not race_number_text.isdigit():\n                    continue\n                race_number = int(race_number_text)\n\n                post_time_node = parser.css_first(\"p.post-time span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    if runner := self._parse_runner(node):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first(\"td:nth-child(1)\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            name_node = node.css_first(\"td:nth-child(3)\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            odds_node = node.css_first(\"td:nth-child(10)\")\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError, IndexError):\n            self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n            return None\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        time_part = time_str.split(\" \")[-2] + \" \" + time_str.split(\" \")[-1]\n        dt_str = f\"{date_str} {time_part}\"\n        return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "web_service/backend/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Note: FanDuel's API is not date-centric. Event discovery would be needed for a robust implementation.\n        # This uses a hardcoded eventId as a placeholder.\n        event_id = \"38183.3\"\n        self.logger.info(f\"Fetching races from FanDuel for event_id: {event_id}\")\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            self.logger.warning(\"FanDuel response missing 'marketGroups' key\")\n            return []\n\n        races = []\n        for group in raw_data.get(\"marketGroups\", []):\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\n                            \"Failed to parse a FanDuel market\",\n                            market=market,\n                            exc_info=True,\n                        )\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            self.logger.warning(f\"Could not parse race and track from FanDuel market name: {market_name}\")\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\").strip()\n        if not race_number_str.isdigit():\n            return None\n        race_number = int(race_number_str)\n\n        track_name = parts[1]\n\n        # Placeholder for start_time - FanDuel's market API doesn't provide it directly\n        start_time = datetime.now(timezone.utc) + timedelta(hours=race_number)\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_runner_odds = runner_data.get(\"winRunnerOdds\", {})\n                current_price = win_runner_odds.get(\"currentPrice\")\n\n                if not runner_name or not current_price:\n                    continue\n\n                numerator, denominator = map(int, current_price.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                odds = OddsData(\n                    win=decimal_odds,\n                    source=self.source_name,\n                    last_updated=datetime.now(timezone.utc),\n                )\n\n                name_parts = runner_name.split(\".\", 1)\n                if len(name_parts) < 2:\n                    continue\n                program_number_str = name_parts[0].strip()\n                horse_name = name_parts[1].strip()\n\n                runners.append(\n                    Runner(\n                        name=horse_name,\n                        number=(int(program_number_str) if program_number_str.isdigit() else 0),\n                        odds={self.source_name: odds},\n                    )\n                )\n            except (ValueError, ZeroDivisionError, IndexError, TypeError):\n                self.logger.warning(\n                    \"Could not parse FanDuel runner\",\n                    runner_data=runner_data,\n                    exc_info=True,\n                )\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/gbgb_api_adapter.py": "# python_service/adapters/gbgb_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GbgbApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Greyhound Board of Great Britain API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"GBGB\"\n    BASE_URL = \"https://api.gbgb.org.uk/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw meeting data from the GBGB API.\"\"\"\n        endpoint = f\"results/meeting/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, meetings_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw meeting data into a list of Race objects.\"\"\"\n        if not meetings_data:\n            return []\n\n        all_races = []\n        for meeting in meetings_data:\n            track_name = meeting.get(\"trackName\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name):\n                        all_races.append(race)\n                except (KeyError, TypeError):\n                    self.logger.error(\n                        \"Error parsing GBGB race\",\n                        race_id=race_data.get(\"raceId\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], track_name: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"raceId\")\n        race_number = race_data.get(\"raceNumber\")\n        race_time = race_data.get(\"raceTime\")\n\n        if not all([race_id, race_number, race_time]):\n            return None\n\n        return Race(\n            id=f\"gbgb_{race_id}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(race_time.replace(\"Z\", \"+00:00\")),\n            runners=self._parse_runners(race_data.get(\"traps\", [])),\n            source=self.source_name,\n            race_name=race_data.get(\"raceTitle\"),\n            distance=f\"{race_data.get('raceDistance')}m\",\n        )\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                trap_number = runner_data.get(\"trapNumber\")\n                dog_name = runner_data.get(\"dogName\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                sp = runner_data.get(\"sp\")\n                if sp:\n                    win_odds = parse_odds_to_decimal(sp)\n                    if win_odds and win_odds < 999:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Error parsing GBGB runner, skipping.\",\n                    runner_name=runner_data.get(\"dogName\"),\n                )\n                continue\n        return runners\n",
    "web_service/backend/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, \"GREYHOUND_API_URL\") or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "web_service/backend/adapters/nyrabets_adapter.py": "# python_service/adapters/nyrabets_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass NYRABetsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for nyrabets.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/racingpost_adapter.py": "# python_service/adapters/racingpost_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingPostAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch RacingPost index page\", url=index_url)\n            return None\n\n        index_parser = HTMLParser(index_response.text)\n        links = index_parser.css('a[data-test-selector^=\"RC-meetingItem__link_race\"]')\n        race_card_urls = [link.attributes[\"href\"] for link in links]\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(self.http_client, \"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n                if not venue_node:\n                    continue\n                venue_raw = venue_node.text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n\n                race_time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n                if not race_time_node:\n                    continue\n                race_time_str = race_time_node.text(strip=True)\n\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            if runner := self._parse_runner(node):\n                runners.append(runner)\n        return runners\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n            name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n            odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n            if not all([number_node, name_node, odds_node]):\n                return None\n\n            number_str = clean_text(number_node.text())\n            number = int(number_str) if number_str and number_str.isdigit() else 0\n            name = clean_text(name_node.text())\n            odds_str = clean_text(odds_node.text())\n            scratched = \"NR\" in odds_str.upper() or not odds_str\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError):\n            self.logger.warning(\"Could not parse RacingPost runner, skipping.\", exc_info=True)\n            return None\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "web_service/backend/adapters/racingtv_adapter.py": "# python_service/adapters/racingtv_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingTVAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping data from racingtv.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/twinspires_adapter.py": "# python_service/adapters/twinspires_adapter.py\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for twinspires.com.\n    This is a placeholder for a full implementation using the discovered JSON API.\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        [MODIFIED FOR OFFLINE DEVELOPMENT]\n        Reads HTML content from a local fixture file instead of making a live API call.\n        This is a temporary measure to allow development while the live API is blocking requests.\n        \"\"\"\n        # Read the local HTML fixture\n        try:\n            with open(\"tests/fixtures/twinspires_sample.html\", \"r\") as f:\n                html_content = f.read()\n        except FileNotFoundError:\n            self.logger.error(\"TwinSpires test fixture not found.\")\n            return None\n\n        # To maintain the data structure the parser expects, we will create a mock\n        # raw_data object that resembles the original API response, but includes\n        # the HTML content.\n        return {\n            \"html_content\": html_content,\n            \"mock_track_data\": {\"trackId\": \"cd\", \"trackName\": \"Churchill Downs\", \"raceType\": \"Thoroughbred\"},\n            \"mock_race_card\": {\"raceNumber\": 5, \"postTime\": \"2025-10-26T16:30:00Z\"},\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        [MODIFIED FOR OFFLINE DEVELOPMENT]\n        Parses race and runner data from the mock raw_data object, which now\n        includes the HTML content from the local fixture.\n        \"\"\"\n        if not raw_data or \"html_content\" not in raw_data:\n            return []\n\n        self.logger.info(\"Parsing TwinSpires data from local fixture.\")\n\n        html_content = raw_data[\"html_content\"]\n        track = raw_data[\"mock_track_data\"]\n        race_card = raw_data[\"mock_race_card\"]\n\n        # Parse the runners from the HTML content\n        runners = self._parse_runners_from_html(html_content)\n\n        try:\n            start_time = datetime.fromisoformat(race_card.get(\"postTime\").replace(\"Z\", \"+00:00\"))\n\n            race = Race(\n                id=f\"ts_{track.get('trackId')}_{race_card.get('raceNumber')}\",\n                venue=track.get(\"trackName\"),\n                race_number=race_card.get(\"raceNumber\"),\n                start_time=start_time,\n                discipline=track.get(\"raceType\", \"Unknown\"),\n                runners=runners,\n                source=self.SOURCE_NAME,\n            )\n            return [race]\n        except Exception as e:\n            self.logger.warning(\n                \"Failed to parse race card from mock data.\",\n                error=e,\n                exc_info=True,\n            )\n            return []\n\n    def _parse_runners_from_html(self, html_content: str) -> List[Runner]:\n        \"\"\"Parses runner data from a race card's HTML content.\"\"\"\n        runners = []\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        runner_elements = soup.select(\"li.runner\")\n\n        for element in runner_elements:\n            try:\n                scratched = \"scratched\" in element.get(\"class\", [])\n\n                number_tag = element.select_one(\"span.runner-number\")\n                name_tag = element.select_one(\"span.runner-name\")\n                odds_tag = element.select_one(\"span.runner-odds\")\n\n                if not all([number_tag, name_tag, odds_tag]):\n                    continue\n\n                number = int(number_tag.text.strip())\n                name = name_tag.text.strip()\n                odds_str = odds_tag.text.strip()\n\n                odds = {}\n                if not scratched and odds_str not in [\"SCR\", \"\"]:\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    if win_odds:\n                        odds[self.SOURCE_NAME] = OddsData(\n                            win=win_odds,\n                            source=self.SOURCE_NAME,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=number,\n                        name=name,\n                        scratched=scratched,\n                        odds=odds,\n                    )\n                )\n            except (ValueError, TypeError) as e:\n                self.logger.warning(\"Failed to parse a runner, skipping.\", error=e, exc_info=True)\n                continue\n\n        return runners\n\n    async def _get_races_async(self, date: str) -> List[Race]:\n        raw_data = await self._fetch_data(date)\n        return self._parse_races(raw_data)\n\n    def get_races(self, date: str) -> List[Race]:\n        \"\"\"\n        Orchestrates the fetching and parsing of race data for a given date.\n        This method will be called by the FortunaEngine.\n        \"\"\"\n        self.logger.info(f\"Getting races for {date} from {self.SOURCE_NAME}\")\n        # This is a synchronous wrapper for the async orchestrator\n        # It's a temporary measure to allow me to see the API response.\n        import asyncio\n\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n        races = loop.run_until_complete(self._get_races_async(date))\n        return races\n",
    "web_service/backend/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(\n        self,\n        max_field_size: int = 10,\n        min_favorite_odds: float = 2.5,\n        min_second_favorite_odds: float = 4.0,\n    ):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        qualified_races = []\n        for race in races:\n            score = self._evaluate_race(race)\n            if score > 0:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\n            \"Universal scoring complete\",\n            total_races_scored=len(qualified_races),\n            criteria=criteria,\n        )\n\n        for race in qualified_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": qualified_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier(\"Fortuna\") if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "web_service/backend/cache_manager.py": "# python_service/cache_manager.py\nimport asyncio\nimport hashlib\nimport json\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self):\n        self.redis_client = None\n        self.memory_cache = {}\n        self.is_configured = False\n        log.info(\"CacheManager initialized (not connected).\")\n\n    async def connect(self, redis_url: str):\n        if self.is_configured or not REDIS_AVAILABLE or not redis_url:\n            return\n\n        try:\n            log.info(\"Attempting to connect to Redis...\", url=redis_url)\n            # Use the async version of the client\n            self.redis_client = redis.asyncio.from_url(redis_url, decode_responses=True)\n            await self.redis_client.ping()  # Verify connection asynchronously\n            self.is_configured = True\n            log.info(\"Redis cache connected successfully.\")\n        except (redis.exceptions.ConnectionError, asyncio.TimeoutError) as e:\n            log.warning(\n                \"Failed to connect to Redis. Falling back to in-memory cache.\",\n                error=str(e),\n            )\n            self.redis_client = None\n            self.is_configured = False\n\n    async def disconnect(self):\n        if self.redis_client:\n            await self.redis_client.close()\n            log.info(\"Redis connection closed.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    async def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = await self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis GET failed, falling back to memory cache.\", error=e)\n\n        entry = self.memory_cache.get(key)\n        if entry and entry.get(\"expires_at\", datetime.min) > datetime.now():\n            return entry.get(\"value\")\n        return None\n\n    async def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        try:\n            serialized = json.dumps(value, default=str)\n        except (TypeError, ValueError) as e:\n            log.error(\"Failed to serialize value for caching.\", value=value, error=str(e))\n            return\n\n        if self.redis_client:\n            try:\n                await self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis SET failed, falling back to memory cache.\", error=e)\n\n        self.memory_cache[key] = {\n            \"value\": value,\n            \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds),\n        }\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager()\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = await cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n\n            try:\n                await cache_manager.set(cache_key, result, ttl_seconds)\n            except Exception as e:\n                log.error(\"Failed to store result in cache.\", error=str(e), key=cache_key)\n\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "web_service/backend/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    # Keep the processor chain simple for maximum reliability in bundled executables.\n    # More complex processors like StackInfoRenderer can cause issues in\n    # constrained environments.\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "web_service/backend/manual_override_manager.py": "# python_service/manual_override_manager.py\nimport hashlib\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n\nclass ManualOverrideRequest(BaseModel):\n    request_id: str\n    adapter_name: str\n    url: str\n    timestamp: datetime = Field(default_factory=datetime.now)\n    status: str = \"pending\"  # pending, submitted, skipped\n\n\nclass ManualOverrideManager:\n    def __init__(self):\n        self._requests: Dict[str, ManualOverrideRequest] = {}\n        self._data: Dict[str, Tuple[str, str]] = {}  # request_id -> (content, content_type)\n\n    def _generate_id(self, adapter_name: str, url: str) -> str:\n        \"\"\"Generates a consistent ID for a given adapter and URL.\"\"\"\n        return hashlib.sha256(f\"{adapter_name}:{url}\".encode()).hexdigest()[:16]\n\n    def register_failure(self, adapter_name: str, url: str) -> str:\n        \"\"\"\n        Registers a failed fetch attempt and returns a unique request ID.\n        If a pending request for this exact resource already exists, it returns the existing ID.\n        \"\"\"\n        request_id = self._generate_id(adapter_name, url)\n        if request_id not in self._requests or self._requests[request_id].status != \"pending\":\n            request = ManualOverrideRequest(request_id=request_id, adapter_name=adapter_name, url=url)\n            self._requests[request_id] = request\n        return request_id\n\n    def submit_manual_data(self, request_id: str, raw_content: str, content_type: str) -> bool:\n        \"\"\"Submits manual data for a pending request.\"\"\"\n        if request_id in self._requests and self._requests[request_id].status == \"pending\":\n            self._data[request_id] = (raw_content, content_type)\n            self._requests[request_id].status = \"submitted\"\n            return True\n        return False\n\n    def skip_request(self, request_id: str) -> bool:\n        \"\"\"Marks a pending request as skipped.\"\"\"\n        if request_id in self._requests and self._requests[request_id].status == \"pending\":\n            self._requests[request_id].status = \"skipped\"\n            return True\n        return False\n\n    def get_pending_requests(self) -> List[ManualOverrideRequest]:\n        \"\"\"Returns a list of all requests that are currently pending.\"\"\"\n        return [req for req in self._requests.values() if req.status == \"pending\"]\n\n    def get_manual_data(self, adapter_name: str, url: str) -> Optional[Tuple[str, str]]:\n        \"\"\"\n        Retrieves submitted manual data for a given adapter and URL, if it exists.\n        Once retrieved, the data is consumed and will not be returned again.\n        \"\"\"\n        request_id = self._generate_id(adapter_name, url)\n        if request_id in self._data:\n            # Data is single-use; remove it after retrieval.\n            return self._data.pop(request_id)\n        return None\n\n    def clear_old_requests(self, max_age_hours: int = 24):\n        \"\"\"Removes requests and associated data older than a specified age.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=max_age_hours)\n        old_request_ids = [req_id for req_id, req in self._requests.items() if req.timestamp < cutoff]\n        for req_id in old_request_ids:\n            self._requests.pop(req_id, None)\n            self._data.pop(req_id, None)\n",
    "web_service/backend/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Annotated\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import Field\nfrom pydantic import WrapSerializer\n\n\ndef decimal_serializer(value: Decimal, handler: Callable[[Decimal], Any]) -> Any:\n    \"\"\"Custom serializer for Decimal to float conversion.\"\"\"\n    return float(value)\n\n\nJsonDecimal = Annotated[Decimal, WrapSerializer(decimal_serializer, when_used=\"json\")]\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[JsonDecimal] = None\n    place: Optional[JsonDecimal] = None\n    show: Optional[JsonDecimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n    is_error_placeholder: bool = Field(False, alias=\"isErrorPlaceholder\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AdapterError(FortunaBaseModel):\n    adapter_name: str = Field(..., alias=\"adapterName\")\n    error_message: str = Field(..., alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    errors: List[AdapterError]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n\n\nclass ManualParseRequest(FortunaBaseModel):\n    adapter_name: str\n    html_content: str = Field(..., max_length=5_000_000)  # ~5MB limit\n",
    "web_service/backend/service_entry.py": "import win32serviceutil\nimport win32service\nimport win32event\nimport servicemanager\nimport socket\nimport sys\nimport os\nimport uvicorn\nimport multiprocessing\nimport threading\nfrom pathlib import Path\n\n# FIX: Ensure the current directory is in sys.path for relative imports in frozen state\nsys.path.insert(0, str(Path(__file__).parent))\n\ntry:\n    from main import app\nexcept ImportError:\n    # Fallback for different packaging structures\n    from web_service.backend.main import app\n\nclass FortunaSvc(win32serviceutil.ServiceFramework):\n    _svc_name_ = 'FortunaWebService'\n    _svc_display_name_ = 'Fortuna Faucet Backend Service'\n    _svc_description_ = 'Data aggregation and analysis engine.'\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.server = None\n        self.server_thread = None\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        win32event.SetEvent(self.hWaitStop)\n        if self.server:\n            self.server.should_exit = True\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE,\n                              servicemanager.PYS_SERVICE_STARTED,\n                              (self._svc_name_, ''))\n\n        config = uvicorn.Config(app, host='127.0.0.1', port=8102, log_config=None, reload=False)\n        self.server = uvicorn.Server(config)\n\n        # Run the server in a separate thread\n        self.server_thread = threading.Thread(target=self.server.run)\n        self.server_thread.start()\n\n        # Wait for the stop event\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n        # Wait for the server thread to finish\n        self.server_thread.join()\n\nif __name__ == '__main__':\n    multiprocessing.freeze_support()\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaSvc)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaSvc)\n",
    "web_service/frontend/app/Providers.tsx": "// web_platform/frontend/app/Providers.tsx\n'use client';\n\nimport { QueryClientProvider } from '@tanstack/react-query';\nimport { queryClient } from './lib/queryClient';\nimport React from 'react';\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>\n  );\n}\n",
    "web_service/frontend/app/components/SettingsPage.tsx": "// src/components/SettingsPage.tsx\n'use client';\n\nimport React, { useState, useEffect } from 'react';\n\nexport function SettingsPage() {\n  const [apiKey, setApiKey] = useState('');\n  const [betfairAppKey, setBetfairAppKey] = useState('');\n  const [betfairUsername, setBetfairUsername] = useState('');\n  const [betfairPassword, setBetfairPassword] = useState('');\n\n  useEffect(() => {\n    // Fetch the current API key when the component mounts\n    const fetchApiKey = async () => {\n      if (window.electronAPI?.getApiKey) {\n        const key = await window.electronAPI.getApiKey();\n        if (key) {\n          setApiKey(key);\n        }\n      }\n    };\n    fetchApiKey();\n  }, []);\n\n  const handleGenerateApiKey = async () => {\n    if (window.electronAPI?.generateApiKey) {\n      const newKey = await window.electronAPI.generateApiKey();\n      setApiKey(newKey);\n    }\n  };\n\n  const handleSaveSettings = async () => {\n    if (window.electronAPI?.saveApiKey && window.electronAPI?.saveBetfairCredentials) {\n      await window.electronAPI.saveApiKey(apiKey);\n      await window.electronAPI.saveBetfairCredentials({\n        appKey: betfairAppKey,\n        username: betfairUsername,\n        password: betfairPassword,\n      });\n      alert('Settings saved successfully!');\n    }\n  };\n\n  return (\n    <div className=\"bg-slate-800 p-8 rounded-lg border border-slate-700 text-white max-w-2xl mx-auto\">\n      <h2 className=\"text-3xl font-bold text-white mb-6\">Application Settings</h2>\n\n      <div className=\"space-y-8\">\n        <div>\n          <h3 className=\"text-xl font-semibold text-slate-300 mb-2\">API Key</h3>\n          <p className=\"text-sm text-slate-400 mb-3\">This key is required for the dashboard to communicate with the backend service.</p>\n          <div className=\"flex items-center space-x-2\">\n            <input\n              type=\"text\"\n              readOnly\n              value={apiKey}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 font-mono text-sm\"\n            />\n            <button\n              onClick={handleGenerateApiKey}\n              className=\"px-4 py-2 bg-blue-600 hover:bg-blue-700 rounded transition-colors font-semibold\"\n            >\n              Generate New Key\n            </button>\n          </div>\n        </div>\n\n        <div>\n          <h3 className=\"text-xl font-semibold text-slate-300 mb-2\">Betfair Credentials (Optional)</h3>\n           <p className=\"text-sm text-slate-400 mb-3\">Required for adapters that use the Betfair Exchange API.</p>\n          <div className=\"space-y-3\">\n            <input\n              type=\"password\"\n              placeholder=\"App Key\"\n              value={betfairAppKey}\n              onChange={(e) => setBetfairAppKey(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n            <input\n              type=\"text\"\n              placeholder=\"Username\"\n              value={betfairUsername}\n              onChange={(e) => setBetfairUsername(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n            <input\n              type=\"password\"\n              placeholder=\"Password\"\n              value={betfairPassword}\n              onChange={(e) => setBetfairPassword(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n          </div>\n        </div>\n\n        <div className=\"flex justify-end pt-6 border-t border-slate-700\">\n          <button\n            onClick={handleSaveSettings}\n            className=\"px-8 py-3 bg-green-600 hover:bg-green-700 rounded font-bold text-lg transition-colors\"\n          >\n            Save All Settings\n          </button>\n        </div>\n      </div>\n    </div>\n  );\n}\n",
    "web_service/frontend/app/components/TrifectaFactors.tsx": "// TrifectaFactors.tsx - FINAL, DYNAMIC VERSION\n'use client';\nimport React from 'react';\n\ninterface TrifectaFactorsProps {\n  factorsJson: string | null;\n}\n\nexport function TrifectaFactors({ factorsJson }: TrifectaFactorsProps) {\n  if (!factorsJson) {\n    return <div className=\"text-sm text-gray-500\">No analysis factors available.</div>;\n  }\n\n  try {\n    const factors = JSON.parse(factorsJson);\n    const positiveFactors = Object.entries(factors).filter(([key, value]: [string, any]) => value.ok);\n\n    if (positiveFactors.length === 0) {\n      return <div className=\"text-sm text-gray-500\">No positive factors identified.</div>;\n    }\n\n    return (\n      <div className=\"mt-2 text-xs\">\n        <h4 className=\"font-semibold mb-1\">Key Factors:</h4>\n        <ul className=\"list-disc list-inside space-y-1\">\n          {positiveFactors.map(([key, value]: [string, any]) => (\n            <li key={key} className=\"text-gray-700\">\n              <span className=\"font-medium text-green-600\">\u2713</span> {value.reason} ({value.points > 0 ? `+${value.points}` : value.points} pts)\n            </li>\n          ))}\n        </ul>\n      </div>\n    );\n  } catch (error) {\n    console.error(\"Failed to parse trifecta factors:\", error);\n    return <div className=\"text-sm text-red-500\">Error displaying analysis factors.</div>;\n  }\n}",
    "web_service/frontend/app/page.tsx": "'use client';\nimport dynamic from 'next/dynamic';\nimport React from 'react';\nimport { Tabs } from './components/Tabs';\nimport { SettingsPage } from './components/SettingsPage';\n\nconst LiveRaceDashboard = dynamic(\n  () => import('./components/LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  {\n    ssr: false,\n    loading: () => <p className=\"text-center text-xl mt-8\">Loading Dashboard...</p>\n  }\n);\n\nexport default function Home() {\n  const tabs = [\n    {\n      label: 'Dashboard',\n      content: <LiveRaceDashboard />,\n    },\n    {\n      label: 'Settings',\n      content: <SettingsPage />,\n    },\n  ];\n\n  return (\n    <main className=\"min-h-screen bg-gradient-to-br from-slate-900 via-purple-900 to-slate-900 p-8\">\n      <div className=\"max-w-7xl mx-auto space-y-8\">\n        <h1 className=\"text-4xl font-bold text-white\">Fortuna Faucet</h1>\n        <Tabs tabs={tabs} />\n      </div>\n    </main>\n  );\n}\n",
    "web_service/frontend/app/types/electron.d.ts": "// web_platform/frontend/src/types/electron.d.ts\n\n/**\n * This declaration file extends the global Window interface to include the\n * 'electronAPI' object exposed by the preload script. This provides\n * TypeScript with type information for the functions we're using for IPC.\n */\nexport {};\n\ndeclare global {\n  interface Window {\n    electronAPI?: {\n      /**\n       * Asynchronously fetches the secure API key from the main process.\n       * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n       */\n      getApiKey: () => Promise<string | null>;\n      /**\n       * Registers a callback for backend status updates from the main process.\n       * @param callback The function to execute. Receives an object with state and logs.\n       * @returns A function to unsubscribe the listener.\n       */\n      onBackendStatusUpdate: (callback: (status: { state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }) => void) => () => void;\n\n      /**\n       * Sends a command to the main process to restart the backend executable.\n       */\n      restartBackend: () => void;\n\n      /**\n       * Asynchronously fetches the current backend status from the main process.\n       * @returns {Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>}\n       */\n      getBackendStatus: () => Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>;\n      generateApiKey: () => Promise<string>;\n      saveApiKey: (apiKey: string) => Promise<{ success: boolean }>;\n      saveBetfairCredentials: (credentials: { appKey: string; username: string; password: string }) => Promise<{ success: boolean }>;\n      getApiPort: () => Promise<number | null>;\n    };\n  }\n}\n",
    "web_service/frontend/app/utils/exportManager.ts": "// web_platform/frontend/src/utils/exportManager.ts\n// import { saveAs } from 'file-saver';\n// import * as XLSX from 'xlsx';\n\nexport class ExportManager {\n  static exportToExcel(races: any[], filename: string = 'fortuna_races') {\n    //\n    // [JULES] - NOTE FOR JB AND AI EXPERTS:\n    // This feature has been temporarily disabled because the external dependency (xlsx)\n    // is hosted on a CDN (cdn.sheetjs.com) that is consistently failing during\n    // the CI/CD build process with 500 Internal Server Errors.\n    //\n    // To ensure the main application build is not blocked, I have commented out\n    // the implementation of this function. The 'xlsx' package remains in package.json,\n    // but this code will not be active until the dependency issue is resolved.\n    //\n\n    // const workbook = XLSX.utils.book_new();\n\n    // const summaryData = [\n    //   ['Total Qualified Races', races.length],\n    //   ['Generated At', new Date().toLocaleString()]\n    // ];\n    // const summarySheet = XLSX.utils.aoa_to_sheet(summaryData);\n    // XLSX.utils.book_append_sheet(workbook, summarySheet, 'Summary');\n\n    // const raceData = races.map(race => ({\n    //   'Venue': race.venue,\n    //   'Race Number': race.race_number,\n    //   'Post Time': new Date(race.start_time).toLocaleString(),\n    //   'Qualification Score': race.qualification_score || 0,\n    //   'Field Size': race.runners.filter(r => !r.scratched).length,\n    //   'Source': race.source\n    // }));\n    // const raceSheet = XLSX.utils.json_to_sheet(raceData);\n    // XLSX.utils.book_append_sheet(workbook, raceSheet, 'Races');\n\n    // XLSX.writeFile(workbook, `${filename}_${Date.now()}.xlsx`);\n    console.warn(\"Excel export is temporarily disabled due to an external dependency issue.\");\n    alert(\"The Excel export feature is temporarily disabled due to an unreliable external dependency. Please try again later.\");\n  }\n}\n",
    "web_service/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"@tanstack/react-query\": \"^5.28.9\",\n    \"file-saver\": \"^2.0.5\",\n    \"lucide-react\": \"^0.548.0\",\n    \"next\": \"^14.2.33\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"socket.io-client\": \"^4.7.4\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"file-saver\": \"^2.0.5\",\n    \"next-pwa\": \"^5.6.0\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.3.0\",\n    \"typescript\": \"^5\"\n  }\n}\n",
    "wix/WixUI_CustomProgress.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\">\n  <Fragment>\n    <UI>\n      <!-- Override the default InstallProgress dialog -->\n      <Dialog Id=\"InstallProgressDlg\" Width=\"370\" Height=\"270\" Title=\"Fortuna Faucet Installation\" Modeless=\"yes\">\n        <Control Id=\"Title\" Type=\"Title\" X=\"20\" Y=\"6\" Width=\"330\" Height=\"18\" Text=\"Installation Progress\" />\n        <Control Id=\"BannerBitmap\" Type=\"Bitmap\" X=\"0\" Y=\"0\" Width=\"370\" Height=\"44\" TabSkip=\"no\" Text=\"WixUI_Bmp_Banner\" />\n        <Control Id=\"Back\" Type=\"PushButton\" X=\"180\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Back\" Disabled=\"yes\" />\n        <Control Id=\"Next\" Type=\"PushButton\" X=\"236\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Next\" Disabled=\"yes\" />\n        <Control Id=\"Cancel\" Type=\"PushButton\" X=\"304\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"Cancel\" />\n\n        <Control Id=\"ActionText\" Type=\"Text\" X=\"70\" Y=\"80\" Width=\"280\" Height=\"20\" TabSkip=\"no\">\n          <Subscribe Event=\"ActionText\" Attribute=\"Text\" />\n        </Control>\n        <Control Id=\"Description\" Type=\"Text\" X=\"35\" Y=\"55\" Width=\"300\" Height=\"20\" Text=\"Please wait while the installer copies files.\" />\n\n        <!-- This is the new control to display the current filename -->\n        <Control Id=\"CurrentFileText\" Type=\"Text\" X=\"70\" Y=\"100\" Width=\"280\" Height=\"20\">\n            <Subscribe Event=\"SetProgress\" Attribute=\"Text\" />\n        </Control>\n\n        <Control Id=\"ProgressBar\" Type=\"ProgressBar\" X=\"35\" Y=\"120\" Width=\"300\" Height=\"10\" ProgressBlocks=\"yes\" Text=\"Progress\">\n          <Subscribe Event=\"SetProgress\" Attribute=\"Progress\" />\n        </Control>\n      </Dialog>\n\n      <!-- The Publish element must be a child of UI, not Dialog -->\n      <Publish Dialog=\"InstallProgressDlg\" Control=\"Cancel\" Event=\"SpawnDialog\" Value=\"CancelDlg\">1</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}