{
    ".github/actions/run-smoke-test/action.yml": "name: 'Run 4-Step Diagnostic Smoke Test'\ndescription: 'Installs an MSI, then runs a 4-step diagnostic to verify file installation, service status, port binding, and API health, finishing with a Paparazzi screenshot.'\n\ninputs:\n  msi-artifact-name:\n    description: 'The name of the MSI artifact to download.'\n    required: true\n  service-name:\n    description: 'The name of the Windows Service to verify (e.g., FortunaWebService).'\n    required: true\n  executable-path:\n    description: 'The full, absolute path to the installed service executable to verify.'\n    required: true\n  port:\n    description: 'The port to check for a listener.'\n    required: true\n  firewall-rule-name:\n    description: 'The name of the firewall rule to create.'\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n    - name: \ud83d\udce5 Download MSI Artifact\n      uses: actions/download-artifact@v4\n      with:\n        name: ${{ inputs.msi-artifact-name }}\n        path: installer\n\n    - name: \ud83d\udee1\ufe0f Firewall & Install\n      shell: pwsh\n      run: |\n        New-NetFirewallRule -DisplayName \"${{ inputs.firewall-rule-name }}\" -Direction Inbound -LocalPort ${{ inputs.port }} -Protocol TCP -Action Allow\n        if (Get-Service -Name \"${{ inputs.service-name }}\" -ErrorAction SilentlyContinue) {\n          sc.exe stop \"${{ inputs.service-name }}\" 2>&1 | Out-Null\n          sc.exe delete \"${{ inputs.service-name }}\" 2>&1 | Out-Null\n        }\n        $msi = Get-ChildItem installer -Filter \"*.msi\" -Recurse | Select-Object -First 1\n        if (!$msi) { throw \"No MSI found\" }\n        Write-Host \"Installing $($msi.Name)...\"\n        $msiPath = $msi.FullName\n        $args = \"/i `\"$msiPath`\" /qn /L*v installation.log\"\n        $proc = Start-Process msiexec.exe -ArgumentList $args -Wait -NoNewWindow -PassThru\n        if ($proc.ExitCode -ne 0) {\n          Get-Content installation.log -Tail 50\n          throw \"Install failed with code $($proc.ExitCode)\"\n        }\n\n    - name: \"\u2705 Create Required Runtime Directories Post-Install\"\n      shell: pwsh\n      run: |\n        $installRoot = Split-Path -Path \"${{ inputs.executable-path }}\" -Parent\n        if (-not (Test-Path $installRoot)) {\n          Write-Error \"Installation directory not found at $installRoot. Cannot create runtime directories.\"\n          exit 1\n        }\n        New-Item -Path \"$installRoot\\data\" -ItemType Directory -Force | Out-Null\n        New-Item -Path \"$installRoot\\json\" -ItemType Directory -Force | Out-Null\n        New-Item -Path \"$installRoot\\logs\" -ItemType Directory -Force | Out-Null\n        Write-Host \"\u2705 Created data, json, and logs directories in $installRoot\"\n\n    - name: '\ud83d\udd2c Complete Smoke Test (3-Layer Defense)'\n      shell: pwsh\n      run: |\n        Set-StrictMode -Version Latest\n        $ErrorActionPreference = \"Stop\"\n\n        # --- LAYER 1: INSTALLATION & FILE VERIFICATION ---\n        Write-Host \"`n--- DEFENSE LAYER 1: VERIFYING INSTALLATION ---\"\n        $installRoot = Split-Path -Path \"${{ inputs.executable-path }}\" -Parent\n        if (-not (Test-Path $installRoot)) {\n          Write-Error \"\u274c LAYER 1 FAILED: Install directory not found: $installRoot\"\n          exit 1\n        }\n        $mainExe = Get-ChildItem -Path $installRoot -Filter \"*.exe\" -Recurse | Where-Object { $_.Name -notmatch 'uninstall' } | Select -First 1\n        if (-not $mainExe) { Write-Error \"\u274c LAYER 1 FAILED: Main executable not found.\"; exit 1 }\n        Write-Host \"\u2705 Layer 1 Passed: Found main executable ($($mainExe.Name)).\"\n\n        # --- LAYER 2: PROCESS VERIFICATION ---\n        Write-Host \"`n--- DEFENSE LAYER 2: VERIFYING PROCESS STARTUP ---\"\n        $svc = Get-Service \"${{ inputs.service-name }}\" -ErrorAction SilentlyContinue\n        if (!$svc) {\n          Write-Error \"\u274c LAYER 2 FAILED: Service '${{ inputs.service-name }}' is NOT registered!\"\n          exit 1\n        }\n        if ($svc.Status -ne 'Running') {\n          Start-Service \"${{ inputs.service-name }}\"\n          Start-Sleep -Seconds 10\n        }\n        $svc = Get-Service \"${{ inputs.service-name }}\"\n        if ($svc.Status -ne 'Running') {\n          Write-Error \"\u274c LAYER 2 FAILED: Service failed to start. Status: $($svc.Status)\"\n          Get-EventLog -LogName System -Source \"Service Control Manager\" -Newest 20 | Format-Table -AutoSize\n          exit 1\n        }\n        Write-Host \"\u2705 Layer 2 Passed: Service is RUNNING.\"\n\n        # Inserted Backend Alive Check\n        Write-Host \"--- Verifying backend process stability (10s alive check) ---\"\n        Start-Sleep -Seconds 10\n        $svcProcess = Get-CimInstance win32_service | where name -eq \"${{ inputs.service-name }}\" | select -ExpandProperty ProcessId\n        if ($null -eq (Get-Process -Id $svcProcess -ErrorAction SilentlyContinue)) {\n          Write-Error \"\u274c Backend process crashed within 10 seconds of starting.\"\n          exit 1\n        }\n        Write-Host \"\u2705 Backend process is still alive.\"\n\n        # --- LAYER 3: NETWORK VERIFICATION ---\n        Write-Host \"`n--- DEFENSE LAYER 3: VERIFYING NETWORK ENDPOINT ---\"\n        $maxAttempts = 10\n        $healthUrl = \"http://localhost:${{ inputs.port }}/health\"\n        for ($i = 1; $i -le $maxAttempts; $i++) {\n          try {\n            Write-Host \"Attempt $i/${maxAttempts}: Pinging $healthUrl...\"\n            $response = Invoke-WebRequest -Uri $healthUrl -Method Get -UseBasicParsing -ErrorAction Stop\n            if ($response.StatusCode -eq 200) {\n              Write-Host \"\u2705\u2705\u2705 SMOKE TEST PASSED ALL 3 DEFENSE LAYERS \u2705\u2705\u2705\"\n              exit 0\n            }\n          } catch {\n            Write-Host \"\u23f3 Waiting for service...\"\n            Start-Sleep -Seconds 5\n          }\n        }\n        Write-Error \"\u274c LAYER 3 FAILED: Service Failed Health Check after $maxAttempts attempts\"\n        exit 1\n\n    - name: '\ud83d\udcf8 The Paparazzi (Visual Proof)'\n      shell: pwsh\n      run: |\n        Write-Host \"Installing Playwright...\"\n        npm install playwright\n        npx playwright install chromium --with-deps\n\n        $url = \"http://127.0.0.1:${{ inputs.port }}/docs\"\n\n        node -e \"\n          const { chromium } = require('playwright');\n          (async () => {\n            try {\n              const browser = await chromium.launch();\n              const page = await browser.newPage();\n              await page.goto('$url', { timeout: 15000 });\n              await page.waitForSelector('.swagger-ui', { timeout: 5000 }).catch(() => console.log('UI not fully loaded, snapping anyway...'));\n              await page.screenshot({ path: 'proof-of-life.png', fullPage: true });\n              await browser.close();\n            } catch (e) {\n              console.error(e); process.exit(1);\n            }\n          })();\n        \"\n\n    - name: Upload Visual Proof\n      if: always()\n      uses: actions/upload-artifact@v4\n      with:\n        name: visual-proof-${{ github.run_id }}\n        path: proof-of-life.png\n\n    - name: \ud83e\uddf9 Cleanup\n      if: always()\n      shell: pwsh\n      run: |\n        sc.exe stop ${{ inputs.service-name }}\n        sc.exe delete ${{ inputs.service-name }}\n        Remove-NetFirewallRule -DisplayName \"${{ inputs.firewall-rule-name }}\" -ErrorAction SilentlyContinue\n",
    ".github/workflows/build-msi-hat-trick-fusion.yml": "# System Timestamp: 2025-12-24 18:00:00\nname: \ud83c\udfa9 HatTrick Fusion (Standard)\non:\n  workflow_dispatch:\n  push:\n    branches: [\"main\"]\n  workflow_call:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.9' # \ud83d\ude80 DOWNGRADED for x86\n  DOTNET_VERSION: '8.0.x'\n  WIX_VERSION: '4.0.5'\n  SERVICE_PORT: '8102'\n  FRONTEND_PORT: '3000'\n  MSI_NAME: 'HatTrickFusion.msi'\n  FIREWALL_RULE: 'HatTrickFusion-Port'\n  UPGRADE_CODE: 'FA689549-366B-4C5C-A482-1132F9A34B10'\n  FORTUNA_PORT: '8102'\n  # Mock API keys for service startup\n  API_KEY: mock_key\n  TVG_API_KEY: mock\n  GREYHOUND_API_URL: http://mock\n  FORTUNA_ENV: smoke-test\n\njobs:\n  # 1. Build Frontend First (so Backend can bundle it)\n  build-frontend:\n    name: Build Frontend\n    runs-on: windows-2022\n    timeout-minutes: 30\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '**/package-lock.json'\n      - name: Cache Build Output\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: web_platform/frontend/out\n          key: ${{ runner.os }}-frontend-${{ hashFiles('**/package-lock.json', 'web_platform/frontend/**/*.js', 'web_platform/frontend/**/*.ts', 'web_platform/frontend/**/*.tsx', 'web_platform/frontend/**/*.css') }}\n          restore-keys: |\n            ${{ runner.os }}-frontend-\n      - name: Install and Build\n        if: steps.cache-frontend.outputs.cache-hit != 'true'\n        run: |\n          cd web_platform/frontend\n          npm ci --prefer-offline --no-audit --no-fund\n          npm run build\n      - name: Generate Artifact Manifest\n        shell: pwsh\n        working-directory: web_platform/frontend\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"out\"\n          $manifestPath = \"frontend-manifest.tsv\"\n          \"RelativePath`tSizeBytes`tSHA256\" | Out-File $manifestPath -Encoding utf8\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          foreach ($f in $files) {\n            $rel = $f.FullName.Substring($outDir.Path.Length) -replace '^[\\\\\\/]', ''\n            $hash = (Get-FileHash $f.FullName -Algorithm SHA256).Hash.Substring(0,16)\n            \"$rel`t$($f.Length)`t$hash\" | Out-File $manifestPath -Encoding utf8 -Append\n          }\n          Write-Host \"\u2705 Generated frontend artifact manifest.\"\n      - name: Upload Frontend\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build\n          path: |\n            web_platform/frontend/out\n            web_platform/frontend/frontend-manifest.tsv\n\n  # 2. Build Backend (Downloads Frontend to bundle it)\n  build-backend:\n    name: '\ud83d\udc0d Build Backend (${{ matrix.arch }})'\n    runs-on: windows-2022\n    needs: build-frontend\n    timeout-minutes: 30\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    outputs:\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/download-artifact@v4\n        with:\n          name: frontend-build\n          path: web_platform/frontend/out\n      - id: meta\n        shell: pwsh\n        run: |\n          $ver = if (\"${{ github.ref }}\" -match 'refs/tags/v(.*)') { $Matches[1] } else { \"0.0.${{ github.run_number }}\" }\n          \"semver=$ver\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          $shortSha = \"${{ github.sha }}\".Substring(0,7)\n          \"short_sha=$shortSha\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_dir=web_service/backend\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"module_path=web_service.backend\" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          architecture: ${{ matrix.arch }}\n          cache: 'pip'\n      - name: \ud83e\uddfe Create Architecture Constraints\n        id: constraints\n        shell: pwsh\n        run: |\n          $constraintDir = \"temp-constraints\"\n          New-Item -ItemType Directory -Path $constraintDir -Force | Out-Null\n          $constraintFile = Join-Path $constraintDir \"constraint-${{ matrix.arch }}.txt\"\n          if ('${{ matrix.arch }}' -eq 'x86') {\n            Write-Host \"\ud83d\udee1\ufe0f ACTIVATING X86 SAFE MODE\"\n            @(\n              \"numpy==1.23.5\",\n              \"pandas==1.5.3\",\n              \"scipy==1.10.1\",\n              \"--only-binary=:all:\"\n            ) | Set-Content $constraintFile\n            Write-Host \"\u2705 x86 constraints: numpy 1.23.5, pandas 1.5.3, scipy 1.10.1, wheel-only\"\n          } else { New-Item $constraintFile -ItemType File -Force }\n          \"file=$constraintFile\" | Out-File $env:GITHUB_OUTPUT -Append\n\n      - name: CACHE-PYI Cache PyInstaller Build\n        id: cache-pyi-build\n        uses: actions/cache@v4\n        with:\n          path: dist/fortuna-core-service\n          key: ${{ runner.os }}-${{ matrix.arch }}-pyi-${{ hashFiles('web_service/backend/**/*.py', 'scripts/generate_spec_dual.py', 'web_service/backend/requirements*.txt') }}\n\n      - name: Install Dependencies\n        shell: pwsh\n        run: |\n          python -m pip install --upgrade pip\n          pip install uv\n\n          Write-Host \"[BUILD] Installing x86-constrained packages first...\"\n          uv pip install --system --only-binary=:all: `\n            \"sqlalchemy==1.4.46\" `\n            \"greenlet==1.1.2\" `\n            \"pandas==1.5.3\" `\n            \"numpy==1.23.5\" `\n            \"scipy==1.8.1\"\n\n          if ($LASTEXITCODE -ne 0) {\n            throw \"[BUILD] \u274c Failed to install x86-constrained packages\"\n          }\n\n          Write-Host \"[BUILD] Installing remaining dependencies...\"\n          uv pip install --system -r web_service/backend/requirements.txt --no-deps\n\n          if ($LASTEXITCODE -ne 0) {\n            throw \"[BUILD] \u274c Failed to install remaining requirements\"\n          }\n\n          Write-Host \"[BUILD] Verifying all dependencies are satisfied...\"\n          pip check\n\n          Write-Host \"[BUILD] Installing PyInstaller...\"\n          uv pip install --system pyinstaller==6.6.0 pywin32\n\n          Write-Host \"[BUILD] \u2705 All dependencies installed successfully\"\n\n      - name: Verify x86 package versions\n        if: matrix.arch == 'x86'\n        shell: pwsh\n        run: |\n          Write-Host \"[BUILD] Verifying x86-constrained packages...\"\n\n          # CRITICAL: These must match the versions installed in the previous step\n          $expectedVersions = @{\n            'sqlalchemy' = '1.4.46'  # \u2705 FIXED: Match installed version\n            'greenlet' = '1.1.2'     # \u2705 FIXED: Match installed version\n            'pandas' = '1.5.3'\n            'numpy' = '1.23.5'\n            'scipy' = '1.10.1'\n          }\n\n          $allVerified = $true\n\n          foreach ($pkg in $expectedVersions.Keys) {\n            $expectedVersion = $expectedVersions[$pkg]\n\n            # Get installed version\n            $installedVersion = pip show $pkg 2>&1 | Select-String \"Version:\" | ForEach-Object { $_.Line -replace 'Version: ', '' }\n\n            if (-not $installedVersion) {\n              Write-Error \"\u274c Package '$pkg' not installed!\"\n              $allVerified = $false\n              continue\n            }\n\n            # Trim whitespace for comparison\n            $installedVersion = $installedVersion.Trim()\n            $expectedVersion = $expectedVersion.Trim()\n\n            if ($installedVersion -ne $expectedVersion) {\n              Write-Error \"\u274c Package '$pkg' has wrong version: $installedVersion (expected $expectedVersion)\"\n              $allVerified = $false\n            } else {\n              Write-Host \"\u2705 $pkg==$installedVersion\"\n            }\n          }\n\n          if (-not $allVerified) {\n            throw \"[BUILD] \u274c x86 package verification failed\"\n          }\n\n          Write-Host \"[BUILD] \u2705 All x86 packages verified\"\n      - name: \ud83d\udd2c Diagnostic - Verify wheel installation method\n        shell: pwsh\n        run: |\n          Write-Host \"[DIAGNOSTIC] Checking installation metadata for x86 packages...\"\n\n          $packages = @('sqlalchemy', 'greenlet')\n\n          foreach ($pkg in $packages) {\n            $location = pip show $pkg 2>&1 | Select-String \"Location:\" | ForEach-Object { $_.Line -replace 'Location: ', '' }\n\n            if ($location) {\n              $location = $location.Trim()\n              Write-Host \"Package: $pkg\"\n              Write-Host \"  Location: $location\"\n\n              # Find the .dist-info directory\n              $distInfo = Get-ChildItem -Path $location -Directory -Filter \"${pkg}*.dist-info\" -ErrorAction SilentlyContinue | Select-Object -First 1\n\n              if ($distInfo) {\n                Write-Host \"  .dist-info: $($distInfo.Name)\"\n\n                # Check for WHEEL file (proves it was a wheel installation)\n                $wheelFile = Join-Path $distInfo.FullName \"WHEEL\"\n                if (Test-Path $wheelFile) {\n                  $wheelContent = Get-Content $wheelFile -Raw\n                  Write-Host \"  \u2705 Installed from wheel (WHEEL file exists)\"\n\n                  # Extract wheel tag\n                  if ($wheelContent -match \"Tag: ([^\\r\\n]+)\") {\n                    Write-Host \"  Wheel tag: $($Matches[1])\"\n                  }\n                } else {\n                  Write-Warning \"  \u26a0\ufe0f  No WHEEL file found - might be source install\"\n                }\n\n                # Check for INSTALLER file\n                $installerFile = Join-Path $distInfo.FullName \"INSTALLER\"\n                if (Test-Path $installerFile) {\n                  $installer = Get-Content $installerFile -Raw\n                  Write-Host \"  Installer: $installer\"\n                }\n              } else {\n                Write-Warning \"  \u26a0\ufe0f  No .dist-info directory found for $pkg\"\n              }\n\n              Write-Host \"\"\n            }\n          }\n      - name: Build Backend (PyInstaller)\n        if: steps.cache-pyi-build.outputs.cache-hit != 'true'\n        shell: pwsh\n        env:\n          PYTHONUTF8: '1'\n        run: |\n          pyinstaller --noconfirm --clean --log-level INFO fortuna-unified.spec\n      - name: \ud83d\udd0d Sanity Check Executable\n        shell: pwsh\n        run: |\n          dist/fortuna-core-service/fortuna-core-service.exe --help\n      - name: Upload Backend\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-dist-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist/fortuna-core-service/\n\n  package-msi:\n    name: '\ud83d\udcbf Package MSI (${{ matrix.arch }})'\n    runs-on: windows-2022\n    needs: build-backend\n    timeout-minutes: 30\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/download-artifact@v4\n        with:\n          name: backend-dist-${{ matrix.arch }}-${{ github.run_id }}\n          path: staging/backend\n\n      - name: '\ud83d\udd10 Verify Backend Executable Hash'\n        shell: pwsh\n        run: |\n          $exePath = \"staging/backend/fortuna-core-service.exe\"\n          if (Test-Path $exePath) {\n            $hash = (Get-FileHash $exePath -Algorithm SHA256).Hash\n            Write-Host \"\u2705 SHA-256 of fortuna-core-service.exe: $hash\"\n          } else {\n            throw \"\u274c Backend executable not found at $exePath\"\n          }\n\n      - name: '\u2696\ufe0f The Dietician (Size Analysis)'\n        shell: pwsh\n        run: |\n          $target = \"staging\"\n          $limitMB = 300\n          Write-Host \"--- \ud83d\udcca Size Breakdown ---\"\n          $files = Get-ChildItem -Path $target -Recurse -File -ErrorAction SilentlyContinue\n          if (!$files) { Write-Warning \"No files found to weigh.\"; exit 0 }\n          $totalBytes = ($files | Measure-Object -Property Length -Sum).Sum\n          $totalMB = [math]::Round($totalBytes / 1MB, 2)\n          Write-Host \"Total Payload Size: $totalMB MB\"\n          if ($totalMB -gt $limitMB) {\n              Write-Warning \"\u26a0\ufe0f BLOAT ALERT: Build exceeds $limitMB MB limit! Check the heaviest files below.\"\n          } else {\n              Write-Host \"\u2705 Size within limits (< $limitMB MB).\" -ForegroundColor Green\n          }\n          Write-Host \"`n--- \ud83d\udc18 Top 10 Heaviest Files ---\"\n          $files | Sort-Object Length -Descending | Select-Object -First 10 @{N='File';E={$_.FullName.Replace($pwd,'')}}, @{N='Size(MB)';E={\"{0:N2}\" -f ($_.Length/1MB)}} | Format-Table -AutoSize\n\n      - name: Create Restart Service Batch Script\n        shell: pwsh\n        run: |\n          $scriptContent = \"@echo off`r`necho Requesting Admin privileges to restart fortuna-core-service...`r`nnet stop fortuna-core-service`r`nnet start fortuna-core-service`r`necho Service Restarted.`r`npause\"\n          Set-Content -Path \"staging/backend/restart_service.bat\" -Value $scriptContent -Encoding Ascii\n          Write-Host \"\u2705 Created restart_service.bat script.\"\n\n      - uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n\n      - name: \ud83d\udcc4 Ensure WiX License Exists\n        shell: pwsh\n        run: |\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            # FIX: Use Base64 decoding to avoid RTF escape sequence issues in PowerShell/YAML\n            $rtfContent = [System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String(\"e1xydGYxXGFuc2lcZGVmZjB7XGZvbnR0Ymx7XGYwIEFyaWFsO319XGYwXGZzMjQgRU5EIFVTRVIgTElDRU5TRSBBR1JFRU1FTlRccGFyXHBhciBUaGlzIGlzIGEgcGxhY2Vob2xkZXIgbGljZW5zZSBmb3IgRm9ydHVuYSBGYXVjZXQuIFBsZWFzZSByZXBsYWNlIHdpdGggYWN0dWFsIHRlcm1zLn0=\"))\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: Prepare WiX\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          Copy-Item build_wix/Product_WebService.wxs build_wix/Product.wxs -Force\n          $wxsPath = 'build_wix/Product.wxs'\n          $wxsContent = [xml](Get-Content $wxsPath -Raw)\n          $serviceControl = $wxsContent.SelectSingleNode(\"//*[local-name()='ServiceControl']\")\n          if ($serviceControl -and $serviceControl.HasAttribute(\"Start\")) {\n              $serviceControl.RemoveAttribute(\"Start\")\n              $wxsContent.Save($wxsPath)\n              Write-Host \"\u2705 Dynamically removed 'Start=install' attribute from WiX template.\"\n          }\n          $sourceDir = \"staging/backend\"\n          $distDir = \"staging/backend/fortuna-core-service\" # PyInstaller one-dir output\n          $targetExe = \"fortuna-webservice.exe\"\n\n          if (Test-Path $distDir) {\n            Write-Host \"PyInstaller 'onedir' output found. Staging entire directory...\"\n            # Move all files from the 'dist' folder up one level\n            Move-Item -Path (Join-Path $distDir \"*\") -Destination $sourceDir -Force\n            # Clean up the now-empty dist directory\n            Remove-Item $distDir -Recurse -Force\n            # Rename the main executable for WiX\n            Rename-Item -Path (Join-Path $sourceDir \"fortuna-core-service.exe\") -NewName $targetExe -Force\n            Write-Host \"\u2705 Staging complete for WiX.\"\n          } else {\n            Write-Host \"##[error]Could not find fortuna-backend.exe in expected locations.\"\n            Write-Host \"Listing contents of staging directory for forensics:\"\n            Get-ChildItem -Path \"staging\" -Recurse\n            exit 1\n          }\n          $proj = @(\n            '<Project Sdk=\"WixToolset.Sdk/${{ env.WIX_VERSION }}\">',\n            '  <PropertyGroup>',\n            '    <EnableDefaultCompileItems>false</EnableDefaultCompileItems>',\n            '    <OutputType>Package</OutputType>',\n            '    <Platforms>x64;x86</Platforms>',\n            '    <!-- Platform is set via -p:Platform in build command, NOT in DefineConstants -->',\n            '    <DefineConstants>Version=$(Version);SourceDir=$(SourceDir);ServicePort=$(ServicePort)</DefineConstants>',\n            '  </PropertyGroup>',\n            '  <ItemGroup>',\n            '    <PackageReference Include=\"WixToolset.UI.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Firewall.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Util.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '  </ItemGroup>',\n            '  <ItemGroup>',\n            '    <Compile Include=\"Product.wxs\" />',\n            '  </ItemGroup>',\n            '</Project>'\n          )\n          Set-Content build_wix/Fortuna.wixproj ($proj -join \"`r`n\") -Encoding utf8\n      - name: Build MSI\n        working-directory: build_wix\n        run: |\n          dotnet build Fortuna.wixproj -c Release -p:Platform=${{ matrix.arch }} -p:Version=\"${{ needs.build-backend.outputs.semver }}\" -p:SourceDir=\"../staging/backend\" -p:ServicePort=\"${{ env.SERVICE_PORT }}\"\n      - name: Rename & Hash MSI\n        run: |\n          $ver = \"${{ needs.build-backend.outputs.semver }}\"\n          $releaseDir = \"build_wix/bin/${{ matrix.arch }}/Release\"\n          $msiFound = Get-ChildItem -Path $releaseDir -Filter \"*.msi\" | Select-Object -First 1\n          if (-not $msiFound) { throw \"MSI not found in $releaseDir\" }\n          $targetName = \"HatTrickFusion-${{ matrix.arch }}-${ver}.msi\"\n          $newPath = Join-Path $releaseDir $targetName\n          Move-Item -Path $msiFound.FullName -Destination $newPath -Force\n          Write-Host \"\u2705 MSI Ready: $targetName\"\n      - name: Upload MSI\n        uses: actions/upload-artifact@v4\n        with:\n          name: hat-trick-msi-${{ matrix.arch }}-${{ github.run_id }}\n          path: build_wix/bin/${{ matrix.arch }}/Release/*\n\n      - name: \ud83d\udcdd Generate Job Summary\n        shell: pwsh\n        run: |\n          $ver = \"${{ needs.build-backend.outputs.semver }}\"\n          $targetName = \"HatTrickFusion-${{ matrix.arch }}-${ver}.msi\"\n          $hash = (Get-FileHash \"build_wix/bin/${{ matrix.arch }}/Release/$targetName\").Hash\n          echo \"### \ud83d\udce6 Build Summary (`${{ matrix.arch }}`)\" >> $env:GITHUB_STEP_SUMMARY\n          echo \"| File | Version | SHA-256 |\" >> $env:GITHUB_STEP_SUMMARY\n          echo \"|---|---|---|\" >> $env:GITHUB_STEP_SUMMARY\n          echo \"| `$targetName` | `$ver` | `$hash` |\" >> $env:GITHUB_STEP_SUMMARY\n\n  generate-sbom:\n    name: '\ud83d\udcdc Generate SBOM (${{ matrix.arch }})'\n    runs-on: ubuntu-latest\n    needs: [build-backend]\n    timeout-minutes: 30\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: backend-dist-${{ matrix.arch }}-${{ github.run_id }}\n          path: backend\n      - name: Create SBOM\n        uses: anchore/sbom-action@v0\n        with:\n          path: backend\n          artifact-name: sbom-${{ matrix.arch }}-${{ github.run_id }}\n          format: spdx-json\n\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test (${{ matrix.arch }})'\n    runs-on: windows-2022\n    needs: package-msi\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: hat-trick-msi-${{ matrix.arch }}-${{ github.run_id }}\n          path: msi-installer\n      - name: \ud83d\udd25 Smoke Test\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $ErrorActionPreference = \"Stop\"\n          $msiPath = (Get-ChildItem -Path \"msi-installer\" -Filter \"*.msi\" -Recurse | Select-Object -First 1).FullName\n          if (-not $msiPath) { throw \"MSI not found!\" }\n          Write-Host \"Found MSI at $msiPath\"\n\n          # 1. PRE-INSTALL CLEANUP\n          Write-Host \"Attempting pre-emptive service removal...\"\n          $service = Get-Service -Name \"FortunaWebService\" -ErrorAction SilentlyContinue\n          if ($service) {\n            sc.exe delete \"FortunaWebService\"\n            Start-Sleep -Seconds 5\n          }\n\n          # 2. INSTALL\n          $logFile = \"msi-install.log\"\n          $msiArgs = \"/i `\"$msiPath`\" /qn /L*v `\"$logFile`\"\"\n          Write-Host \"Running: msiexec.exe $msiArgs\"\n          $proc = Start-Process msiexec.exe -ArgumentList $msiArgs -Wait -PassThru\n          if ($proc.ExitCode -ne 0 -and $proc.ExitCode -ne 3010) {\n            Get-Content $logFile -Tail 50\n            throw \"MSI installation failed with exit code $($proc.ExitCode).\"\n          }\n          Write-Host \"\u2705 MSI Installation successful.\"\n\n          # 3. VERIFY & RUN\n          $progFiles = ${env:ProgramFiles}\n          if ('${{ matrix.arch }}' -eq 'x86') { $progFiles = ${env:ProgramFiles(x86)} }\n          $installDir = Join-Path $progFiles \"Fortuna Faucet Service\"\n          if (-not (Test-Path $installDir)) { throw \"Installation directory not found!\" }\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"data\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"json\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"logs\") -Force | Out-Null\n          Start-Service -Name \"FortunaWebService\" -ErrorAction Stop\n          Start-Sleep -Seconds 10\n\n          # 4. HEALTH CHECK\n          $maxRetries = 5\n          $delay = 5\n          For ($i=0; $i -lt $maxRetries; $i++) {\n            try {\n              $response = Invoke-WebRequest -Uri \"http://localhost:${{ env.SERVICE_PORT }}/health\" -UseBasicParsing\n              if ($response.StatusCode -eq 200) {\n                Write-Host \"\u2705 Health check PASSED.\"\n                Stop-Service -Name \"FortunaWebService\"\n                exit 0\n              }\n            } catch { Write-Host \"Attempt $($i+1) failed. Retrying in $delay seconds...\" }\n            Start-Sleep -Seconds $delay\n          }\n          throw \"Health check failed after $maxRetries attempts.\"\n      - name: '\ud83d\udd75\ufe0f CSI: Windows Post-Mortem (On Failure)'\n        if: failure()\n        shell: pwsh\n        run: |\n          Get-Process | Where-Object { $_.ProcessName -match \"fortuna\" }\n          Get-EventLog -LogName Application -Newest 50 -EntryType Error,Warning\n",
    ".github/workflows/build-web-service-msi-jules.yml": "# System Timestamp: 2025-12-24 18:00:00\nname: \ud83d\udd2c Build Fortuna Faucet Web Service Installer (Synthesized Overkill)\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: read\n  actions: read\n  checks: read\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\ndefaults:\n  run:\n    shell: pwsh\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.9' # \ud83d\ude80 DOWNGRADE\n  DOTNET_VERSION: '8.0.x'\n  PYTHONUTF8: '1'\n  PIP_DISABLE_PIP_VERSION_CHECK: '1'\n  PIP_NO_PYTHON_VERSION_WARNING: '1'\n  NPM_CONFIG_FUND: 'false'\n  NPM_CONFIG_AUDIT: 'false'\n  FORCE_COLOR: '3'\n  FRONTEND_DIR: 'web_platform/frontend'\n  FRONTEND_BUILD_DIR: 'web_platform/frontend/out'\n  WIX_DIR: 'build_wix'\n  SERVICE_PORT: '8102'\n  HEALTH_ENDPOINT: '/health'\n  API_KEY: ${{ secrets.TEST_API_KEY }}\n  TVG_API_KEY: \"mock_key\"\n  GREYHOUND_API_URL: \"http://mock\"\n  FORTUNA_ENV: \"smoke-test\"\n  MSI_STAGING_DIR: 'build_wix/staging'\n  MSI_OUTPUT_DIR: 'dist'\n  WIX_VERSION: '4.0.5'\n\njobs:\n  system-check:\n    name: '\u2699\ufe0f System Prerequisites'\n    runs-on: windows-2022\n    timeout-minutes: 5\n    outputs:\n      disk_free_gb: ${{ steps.system.outputs.disk_gb }}\n    steps:\n      - name: Verify Build Tools\n        run: |\n          Set-StrictMode -Version Latest\n          $tools = @('dotnet', 'python', 'node', 'npm', 'git')\n          foreach ($tool in $tools) {\n            Write-Host \"Checking for $($tool)...\"\n            Get-Command $tool -ErrorAction SilentlyContinue\n            if (-not $?) {\n              Write-Host \"\u274c FATAL: Build tool '$tool' not found in PATH.\" -ForegroundColor Red\n              exit 1\n            }\n          }\n          Write-Host \"\u2705 All critical build tools are present.\" -ForegroundColor Green\n      - name: Check Disk Space\n        id: system\n        run: |\n          Set-StrictMode -Version Latest\n          $disk = Get-Volume | Where-Object { $_.DriveLetter -eq 'C' }\n          $freeGB = [math]::Round($disk.SizeRemaining / 1GB, 2)\n          if ($freeGB -lt 10) {\n            Write-Host \"\u26a0\ufe0f WARNING: Low disk space. Only $freeGB GB free (10+ GB recommended).\" -ForegroundColor Yellow\n          } else {\n            Write-Host \"\u2705 Disk space check passed ($freeGB GB free).\" -ForegroundColor Green\n          }\n          \"disk_gb=$freeGB\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n  repo-preflight:\n    name: '\ud83e\uddea Repo Preflight & Integrity'\n    runs-on: windows-2022\n    needs: [system-check]\n    timeout-minutes: 5\n    outputs:\n      frontend_lock_hash: ${{ steps.hashes.outputs.frontend_lock_hash }}\n      backend_requirements_hash: ${{ steps.hashes.outputs.backend_requirements_hash }}\n      wix_definition_hash: ${{ steps.hashes.outputs.wix_definition_hash }}\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Derive Build Metadata\n        id: meta\n        run: |\n          Set-StrictMode -Version Latest\n          $ref = \"${{ github.ref }}\"\n          if ($ref -like 'refs/tags/v*') {\n            $semver = $ref -replace 'refs/tags/v', ''\n          } else {\n            $semver = \"0.0.${{ github.run_number }}\"\n          }\n          $shortSha = \"${{ github.sha }}\".Substring(0,7)\n          \"semver=$semver\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"short_sha=$shortSha\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\ud83d\udd16 Version: $semver ($shortSha)\"\n\n      - name: Validate Critical Files Exist\n        env:\n          BACKEND_DIR: 'web_service/backend'\n        run: |\n          Set-StrictMode -Version Latest\n          $paths = @(\n            \"${{ env.FRONTEND_DIR }}/package.json\",\n            \"${{ env.FRONTEND_DIR }}/package-lock.json\",\n            (Join-Path $env:BACKEND_DIR \"requirements.txt\"),\n            (Join-Path $env:BACKEND_DIR \"main.py\"),\n            \"${{ env.WIX_DIR }}/Product_WebService.wxs\"\n          )\n          foreach ($path in $paths) {\n            if (-not (Test-Path $path)) {\n              Write-Host \"\u274c FATAL: Required path missing: $path\" -ForegroundColor Red\n              exit 1\n            }\n          }\n          Write-Host \"\u2705 All critical files confirmed.\"\n\n      - name: Capture Integrity Hashes\n        id: hashes\n        env:\n          BACKEND_DIR: 'web_service/backend'\n        run: |\n          Set-StrictMode -Version Latest\n          $frontend = (Get-FileHash \"${{ env.FRONTEND_DIR }}/package-lock.json\" -Algorithm SHA256).Hash\n          $backend = (Get-FileHash (Join-Path $env:BACKEND_DIR \"requirements.txt\") -Algorithm SHA256).Hash\n          $wix = (Get-FileHash \"${{ env.WIX_DIR }}/Product_WebService.wxs\" -Algorithm SHA256).Hash\n          \"frontend_lock_hash=$frontend\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_requirements_hash=$backend\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"wix_definition_hash=$wix\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n      - name: Upload Integrity Snapshot\n        uses: actions/upload-artifact@v4\n        with:\n          name: repo-preflight-${{ github.run_id }}\n          path: |\n            ${{ env.FRONTEND_DIR }}/package-lock.json\n            web_service/backend/requirements.txt\n            ${{ env.WIX_DIR }}/Product_WebService.wxs\n          retention-days: 3\n\n  frontend-quality:\n    name: '\ud83e\uddfc Frontend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 15\n    needs: repo-preflight\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n\n      - name: Cache Frontend Build\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          key: ${{ runner.os }}-frontend-build-${{ hashFiles('${{ env.FRONTEND_DIR }}/**') }}\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm ci --prefer-offline --no-audit --no-fund\n\n      - name: Run Lint (if defined)\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          $pkg = Get-Content package.json -Raw | ConvertFrom-Json\n          if ($pkg.scripts.PSObject.Properties.Name -contains 'lint') {\n            Write-Host \"\ud83e\uddf9 Running npm run lint\"\n            npm run lint\n          } else {\n            Write-Host \"\u2139\ufe0f No lint script defined, skipping.\"\n          }\n\n      - name: Run Tests (if defined)\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          $pkg = Get-Content package.json -Raw | ConvertFrom-Json\n          if ($pkg.scripts.PSObject.Properties.Name -contains 'test') {\n            Write-Host \"\ud83e\uddea Running npm test -- --watch=false\"\n            npm test -- --watch=false\n          } else {\n            Write-Host \"\u2139\ufe0f No test script defined, skipping.\"\n          }\n\n      - name: Security Audit (non-blocking)\n        continue-on-error: true\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm audit --audit-level=critical\n\n  backend-quality:\n    name: '\ud83e\uddef Backend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 20\n    needs: repo-preflight\n    env:\n      BACKEND_REQUIREMENTS_HASH: ${{ needs.repo-preflight.outputs.backend_requirements_hash }}\n      BACKEND_DIR: 'web_service/backend'\n      BACKEND_SPEC: 'jules.spec'\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: |\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.BACKEND_DIR }}/requirements-dev.txt\n\n      - name: Cache Backend Build\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-build-${{ hashFiles(format('{0}/**', env.BACKEND_DIR), format('{0}', env.BACKEND_SPEC)) }}\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel\n          pip install uv\n          uv pip install --system -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n          if (Test-Path (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")) {\n            uv pip install --system -r (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")\n          }\n\n      - name: Bytecode Compile (Fail Fast)\n        run: |\n          Set-StrictMode -Version Latest\n          python -m compileall -q \"${{ env.BACKEND_DIR }}\"\n\n      - name: Run Pytest (if available)\n        run: |\n          Set-StrictMode -Version Latest\n          python -c 'import importlib.util, sys; sys.exit(0 if importlib.util.find_spec(\"pytest\") else 1)'\n          if ($LASTEXITCODE -eq 0) {\n            Write-Host \"\ud83e\uddea pytest detected, running suite...\"\n            python -m pytest \"${{ env.BACKEND_DIR }}\" --maxfail=1 --disable-warnings\n          } else {\n            Write-Host \"\u2139\ufe0f pytest not installed; skipping tests.\"\n          }\n\n      - name: pip-audit (non-blocking)\n        continue-on-error: true\n        run: |\n          Set-StrictMode -Version Latest\n          uv pip install pip-audit\n          pip-audit -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n\n  sbom:\n    name: '\ud83d\udcc4 SBOM Snapshot'\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    needs: repo-preflight\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Generate SBOM (SPDX)\n        uses: anchore/sbom-action@v0\n        with:\n          output-file: sbom.spdx.json\n          format: spdx-json\n\n      - name: Upload SBOM\n        uses: actions/upload-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}\n          path: sbom.spdx.json\n          retention-days: 7\n\n  build-frontend:\n    name: '\ud83d\udce6 Build Frontend'\n    runs-on: windows-2022\n    timeout-minutes: 20\n    needs: [repo-preflight, frontend-quality]\n    env:\n      FRONTEND_LOCK_HASH: ${{ needs.repo-preflight.outputs.frontend_lock_hash }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n\n      - name: Cache Frontend Build\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          key: ${{ runner.os }}-frontend-build-${{ hashFiles('${{ env.FRONTEND_DIR }}/**') }}\n          restore-keys: |\n            ${{ runner.os }}-frontend-build-\n\n      - name: Prime npm Cache\n        uses: actions/cache@v4\n        with:\n          path: ~\\AppData\\Local\\npm-cache\n          key: ${{ runner.os }}-npm-${{ env.NODE_VERSION }}-${{ env.FRONTEND_LOCK_HASH }}\n          restore-keys: |\n            ${{ runner.os }}-npm-${{ env.NODE_VERSION }}-\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm ci --prefer-offline --no-audit --no-fund\n\n      - name: Build Frontend\n        if: steps.cache-frontend.outputs.cache-hit != 'true'\n        env:\n          NEXT_PUBLIC_API_URL: http://127.0.0.1:${{ env.SERVICE_PORT }}\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm run build\n\n      - name: Report Cache Status\n        run: |\n          if ('${{ steps.cache-frontend.outputs.cache-hit }}' -eq 'true') {\n            Write-Host \"\u2705 Frontend build restored from cache.\" -ForegroundColor Green\n          } else {\n            Write-Host \"\u2139\ufe0f No cache hit. A new build was performed.\" -ForegroundColor Yellow\n          }\n\n      - name: Verify Build Output\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\"\n          if (-not (Test-Path $outDir)) {\n             Write-Host \"\u274c FATAL: Build directory not found\" -ForegroundColor Red\n             exit 1\n          }\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) {\n             Write-Host \"\u274c FATAL: Build directory empty\" -ForegroundColor Red\n             exit 1\n          }\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n      - name: Generate Artifact Manifest\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_DIR }}/out\"\n          # Fallback for different env var names in different workflows\n          if (-not (Test-Path $outDir)) { $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\" }\n\n          if (-not (Test-Path $outDir)) { Write-Error \"\u274c Build failed: 'out' dir missing\"; exit 1 }\n\n          $manifestPath = \"frontend-manifest.tsv\"\n          \"RelativePath`tSizeBytes`tSHA256\" | Out-File $manifestPath -Encoding utf8\n\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) { Write-Error \"\u274c Build failed: 'out' dir empty\"; exit 1 }\n\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n          foreach ($f in $files) {\n            $rel = $f.FullName.Substring($outDir.Path.Length) -replace '^[\\\\\\/]', ''\n            $hash = (Get-FileHash $f.FullName -Algorithm SHA256).Hash.Substring(0,16)\n            \"$rel`t$($f.Length)`t$hash\" | Out-File $manifestPath -Encoding utf8 -Append\n          }\n\n      - name: Upload Frontend Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          retention-days: 3\n\n      - name: Upload Manifest\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-manifest-${{ github.run_id }}\n          path: frontend-manifest.tsv\n          retention-days: 3\n\n  build-backend:\n    name: '\ud83d\udc0d Build Backend (${{ matrix.arch }})'\n    runs-on: windows-2022\n    timeout-minutes: 25\n    needs: [repo-preflight, build-frontend, backend-quality]\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    env:\n      BACKEND_REQUIREMENTS_HASH: ${{ needs.repo-preflight.outputs.backend_requirements_hash }}\n      BUILD_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n      BACKEND_DIR: 'web_service/backend'\n      BACKEND_MODULE_PATH: 'web_service.backend'\n      BACKEND_SPEC: 'jules.spec'\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n      - name: Download Frontend Artifact\n        uses: actions/download-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}\n          path: temp-frontend\n      - name: Cache Backend Build\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-build-${{ matrix.arch }}-${{ hashFiles(format('{0}/**', env.BACKEND_DIR), format('{0}', env.BACKEND_SPEC)) }}\n          restore-keys: |\n            ${{ runner.os }}-backend-build-${{ matrix.arch }}-\n      - name: Stage Frontend for PyInstaller\n        run: |\n          Set-StrictMode -Version Latest\n          $dest = \"staging/ui\"\n          New-Item -ItemType Directory -Path $dest -Force | Out-Null\n          Copy-Item -Path \"temp-frontend/*\" -Destination $dest -Recurse -Force\n          Write-Host \"\u2705 Frontend staged for inclusion.\"\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          architecture: ${{ matrix.arch }}\n          cache: 'pip'\n          cache-dependency-path: |\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.BACKEND_DIR }}/requirements-dev.txt\n\n      - name: \ud83e\uddfe Create Architecture Constraints\n        id: constraints\n        shell: pwsh\n        run: |\n          $constraintDir = \"temp-constraints\"\n          New-Item -ItemType Directory -Path $constraintDir -Force | Out-Null\n          $constraintFile = Join-Path $constraintDir \"constraint-${{ matrix.arch }}.txt\"\n          if ('${{ matrix.arch }}' -eq 'x86') {\n            \"numpy==1.23.5`r`npandas==1.5.3`r`nscipy==1.10.1`r`nsqlalchemy==1.4.46`r`ngreenlet==1.1.2`r`n--only-binary=:all:\" | Set-Content $constraintFile\n          } else {\n            New-Item $constraintFile -ItemType File -Force\n          }\n          \"file=$constraintFile\" | Out-File $env:GITHUB_OUTPUT -Append\n\n      - name: CACHE-PYI Cache PyInstaller Build\n        id: cache-pyi-build\n        uses: actions/cache@v4\n        with:\n          path: dist/fortuna-core-service\n          key: ${{ runner.os }}-${{ matrix.arch }}-pyi-${{ hashFiles('web_service/backend/**/*.py', 'fortuna-unified.spec', 'web_service/backend/requirements*.txt') }}\n      - name: Install Dependencies\n        shell: pwsh\n        run: |\n          python -m pip install --upgrade pip\n          pip install uv\n\n          Write-Host \"[BUILD] Installing x86-constrained packages first...\"\n          uv pip install --system --only-binary=:all: `\n            \"sqlalchemy==1.4.46\" `\n            \"greenlet==1.1.2\" `\n            \"pandas==1.5.3\" `\n            \"numpy==1.23.5\" `\n            \"scipy==1.8.1\"\n\n          if ($LASTEXITCODE -ne 0) {\n            throw \"[BUILD] \u274c Failed to install x86-constrained packages\"\n          }\n\n          Write-Host \"[BUILD] Installing remaining dependencies...\"\n          uv pip install --system -r (Join-Path $env:BACKEND_DIR \"requirements.txt\") --no-deps\n\n          if ($LASTEXITCODE -ne 0) {\n            throw \"[BUILD] \u274c Failed to install remaining requirements\"\n          }\n\n          Write-Host \"[BUILD] Verifying all dependencies are satisfied...\"\n          pip check\n\n          Write-Host \"[BUILD] Installing PyInstaller and PyWin32...\"\n          uv pip install --system pyinstaller pywin32\n\n          Write-Host \"[BUILD] \u2705 All dependencies installed successfully\"\n\n      - name: Verify x86 package versions\n        if: matrix.arch == 'x86'\n        shell: pwsh\n        run: |\n          Write-Host \"[BUILD] Verifying x86-constrained packages...\"\n\n          # CRITICAL: These must match the versions installed in the previous step\n          $expectedVersions = @{\n            'sqlalchemy' = '1.4.46'  # \u2705 FIXED: Match installed version\n            'greenlet' = '1.1.2'     # \u2705 FIXED: Match installed version\n            'pandas' = '1.5.3'\n            'numpy' = '1.23.5'\n            'scipy' = '1.10.1'\n          }\n\n          $allVerified = $true\n\n          foreach ($pkg in $expectedVersions.Keys) {\n            $expectedVersion = $expectedVersions[$pkg]\n\n            # Get installed version\n            $installedVersion = pip show $pkg 2>&1 | Select-String \"Version:\" | ForEach-Object { $_.Line -replace 'Version: ', '' }\n\n            if (-not $installedVersion) {\n              Write-Error \"\u274c Package '$pkg' not installed!\"\n              $allVerified = $false\n              continue\n            }\n\n            # Trim whitespace for comparison\n            $installedVersion = $installedVersion.Trim()\n            $expectedVersion = $expectedVersion.Trim()\n\n            if ($installedVersion -ne $expectedVersion) {\n              Write-Error \"\u274c Package '$pkg' has wrong version: $installedVersion (expected $expectedVersion)\"\n              $allVerified = $false\n            } else {\n              Write-Host \"\u2705 $pkg==$installedVersion\"\n            }\n          }\n\n          if (-not $allVerified) {\n            throw \"[BUILD] \u274c x86 package verification failed\"\n          }\n\n          Write-Host \"[BUILD] \u2705 All x86 packages verified\"\n\n      - name: \ud83d\udd2c Diagnostic - Verify wheel installation method\n        if: matrix.arch == 'x86'\n        shell: pwsh\n        run: |\n          Write-Host \"[DIAGNOSTIC] Checking installation metadata for x86 packages...\"\n\n          $packages = @('sqlalchemy', 'greenlet')\n\n          foreach ($pkg in $packages) {\n            $location = pip show $pkg 2>&1 | Select-String \"Location:\" | ForEach-Object { $_.Line -replace 'Location: ', '' }\n\n            if ($location) {\n              $location = $location.Trim()\n              Write-Host \"Package: $pkg\"\n              Write-Host \"  Location: $location\"\n\n              # Find the .dist-info directory\n              $distInfo = Get-ChildItem -Path $location -Directory -Filter \"${pkg}*.dist-info\" -ErrorAction SilentlyContinue | Select-Object -First 1\n\n              if ($distInfo) {\n                Write-Host \"  .dist-info: $($distInfo.Name)\"\n\n                # Check for WHEEL file (proves it was a wheel installation)\n                $wheelFile = Join-Path $distInfo.FullName \"WHEEL\"\n                if (Test-Path $wheelFile) {\n                  $wheelContent = Get-Content $wheelFile -Raw\n                  Write-Host \"  \u2705 Installed from wheel (WHEEL file exists)\"\n\n                  # Extract wheel tag\n                  if ($wheelContent -match \"Tag: ([^\\r\\n]+)\") {\n                    Write-Host \"  Wheel tag: $($Matches[1])\"\n                  }\n                } else {\n                  Write-Warning \"  \u26a0\ufe0f  No WHEEL file found - might be source install\"\n                }\n\n                # Check for INSTALLER file\n                $installerFile = Join-Path $distInfo.FullName \"INSTALLER\"\n                if (Test-Path $installerFile) {\n                  $installer = Get-Content $installerFile -Raw\n                  Write-Host \"  Installer: $installer\"\n                }\n              } else {\n                Write-Warning \"  \u26a0\ufe0f  No .dist-info directory found for $pkg\"\n              }\n\n              Write-Host \"\"\n            }\n          }\n\n      - name: Generate SBOM (Software Bill of Materials)\n        uses: anchore/sbom-action@v0\n        with:\n          path: .\n          format: spdx-json\n          artifact-name: sbom-${{ github.job }}-${{ github.sha }}\n\n      - name: \ud83d\udd10 pip-audit\n        continue-on-error: true\n        run: |\n          uv pip install --system pip-audit\n          pip-audit -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n      - name: Build with PyInstaller\n        if: steps.cache-pyi-build.outputs.cache-hit != 'true'\n        env:\n          FORTUNA_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n        run: |\n          pyinstaller --noconfirm --clean --log-level INFO fortuna-unified.spec\n      - name: \ud83d\udd0d Sanity Check Executable\n        shell: pwsh\n        run: |\n          dist/fortuna-core-service/fortuna-core-service.exe --help\n      - name: Verify Executable\n        run: |\n          Set-StrictMode -Version Latest\n          $exePath = \"dist/fortuna-core-service.exe\"\n          if (-not (Test-Path $exePath)) { throw \"\u274c FATAL: Executable not found\" }\n          $size = (Get-Item $exePath).Length / 1MB\n          if ($size -lt 10) { throw \"\u274c FATAL: Executable is suspiciously small: $($size) MB.\" }\n          Write-Host \"\u2705 Backend ready: $([math]::Round($size, 2)) MB\"\n\n      - name: \ud83c\udf33 Tree Command for Debugging\n        run: |\n          tree /F dist\n      - name: Upload Backend Executable\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-executable-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist/fortuna-core-service.exe\n          retention-days: 3\n\n  diagnose-asgi-imports:\n    name: '\ud83d\udd0d ASGI Import Killer Pre-Smoke Diagnostic'\n    runs-on: windows-2022\n    timeout-minutes: 15\n    needs: [build-backend]\n    continue-on-error: true\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n      - name: 'Run ASGI Diagnostics'\n        uses: ./.github/actions/run-asgi-diagnostics\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          backend-dir: 'web_service/backend'\n          backend-module-path: 'web_service.backend'\n  diagnose-runtime:\n    name: '\ud83d\udd0e Diagnose PyInstaller Runtime'\n    runs-on: windows-2022\n    timeout-minutes: 10\n    needs: [build-backend]\n    continue-on-error: true\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    env:\n      BACKEND_MODULE_PATH: 'web_service.backend'\n    steps:\n      - name: \ud83d\udce5 Download Backend Executable\n        uses: actions/download-artifact@v4\n        with:\n          name: backend-executable-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist\n      - name: \ud83d\udc0d Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          architecture: ${{ matrix.arch }}\n      - name: \ud83d\udce6 Install PyInstaller\n        run: uv pip install pyinstaller==6.6.0\n      - name: \ud83d\udd75\ufe0f Extract and Analyze Executable Contents\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $exePath = \"dist/fortuna-backend.exe\"\n          Write-Host \"--- Executable Analysis ---\"\n          $archiveContents = pyi-archive_viewer $exePath\n          if ($LASTEXITCODE -ne 0) { throw \"Failed to analyze executable.\" }\n          Write-Host \"--- Archive Contents ---\"\n          $archiveContents | Out-Host\n          $expectedInitFile = ($env:BACKEND_MODULE_PATH.Replace('.', '/') + '/__init__.py')\n          $found = $archiveContents | ForEach-Object { $_.Replace('\\', '/') } | Select-String -Pattern $expectedInitFile -SimpleMatch -Quiet\n          if ($found) {\n            Write-Host \"\u2705 SUCCESS: Key module file found.\" -ForegroundColor Green\n          } else {\n            Write-Error \"\u274c FAILURE: Key module file '$expectedInitFile' NOT found.\"\n            exit 1\n          }\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test (${{ matrix.arch }})'\n    runs-on: windows-2022\n    timeout-minutes: 20\n    needs: [build-backend, package-msi-service]\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ matrix.arch }}-${{ github.run_id }}\n          path: msi-installer\n      - name: '\u2705 Create Required Runtime Directories & Start Service'\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $ErrorActionPreference = \"Stop\"\n\n          # Determine install path based on architecture\n          $progFiles = ${env:ProgramFiles}\n          if ('${{ matrix.arch }}' -eq 'x86') { $progFiles = ${env:ProgramFiles(x86)} }\n          $installDir = Join-Path $progFiles \"Fortuna Faucet Service\"\n\n          if (-not (Test-Path $installDir)) {\n            throw \"\u274c Installation directory not found at $installDir\"\n          }\n\n          # CRITICAL: Create runtime directories that service needs\n          $dirs = @('data', 'json', 'logs')\n          foreach ($dir in $dirs) {\n            $fullPath = Join-Path $installDir $dir\n            New-Item -ItemType Directory -Path $fullPath -Force | Out-Null\n            # Grant LocalSystem (the service account) full control\n            icacls $fullPath /grant \"NT AUTHORITY\\SYSTEM:(OI)(CI)F\" /T 2>&1 | Out-Null\n          }\n          Write-Host \"\u2705 Created runtime directories with proper permissions\"\n\n          # CRITICAL: Explicitly start the service (WiX removed auto-start)\n          Write-Host \"Starting FortunaWebService...\"\n          Start-Service -Name \"FortunaWebService\" -ErrorAction Stop\n          Start-Sleep -Seconds 10\n\n          # Verify service is running\n          $svc = Get-Service -Name \"FortunaWebService\"\n          if ($svc.Status -ne 'Running') {\n            throw \"\u274c Service failed to start. Status: $($svc.Status)\"\n          }\n          Write-Host \"\u2705 Service started successfully\"\n      - name: \ud83e\ude7a Health Check\n        shell: pwsh\n        run: |\n          $maxRetries = 5\n          $delay = 5\n          For ($i=0; $i -lt $maxRetries; $i++) {\n            try {\n              $response = Invoke-WebRequest -Uri \"http://localhost:${{ env.SERVICE_PORT }}/health\" -UseBasicParsing\n              if ($response.StatusCode -eq 200) {\n                Write-Host \"\u2705 Health check PASSED.\"\n                Stop-Service -Name \"FortunaWebService\"\n                exit 0\n              }\n            } catch { Write-Host \"Attempt $($i+1) failed. Retrying...\" }\n            Start-Sleep -Seconds $delay\n          }\n          throw \"Health check failed.\"\n\n\n  package-msi-service:\n    name: '\ud83d\udcbf Package Service MSI (${{ matrix.arch }})'\n    runs-on: windows-2022\n    timeout-minutes: 25\n    needs: [repo-preflight, build-backend]\n    continue-on-error: ${{ matrix.arch == 'x86' }}\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    env:\n      WIX_HASH: ${{ needs.repo-preflight.outputs.wix_definition_hash }}\n      BUILD_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n      SHORT_SHA: ${{ needs.repo-preflight.outputs.short_sha }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n      - name: Download Backend\n        uses: actions/download-artifact@v4\n        with:\n          name: backend-executable-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist\n      - name: Stage Artifacts\n        id: stage\n        run: |\n          Set-StrictMode -Version Latest\n          $staging = \"${{ env.MSI_STAGING_DIR }}\"\n          New-Item -ItemType Directory -Path $staging -Force | Out-Null\n          Move-Item -Path \"dist/fortuna-core-service.exe\" -Destination \"$staging/fortuna-webservice.exe\" -Force\n          $msiName = \"Fortuna-WebService-${{ matrix.arch }}-${{ env.BUILD_VERSION }}-${{ env.SHORT_SHA }}.msi\".Replace('/', '-')\n          \"msi_name=$msiName\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\u2705 Staged for MSI: $msiName\"\n      - name: Create Restart Service Batch Script\n        shell: pwsh\n        run: |\n          $scriptContent = \"@echo off`r`necho Requesting Admin privileges to restart FortunaWebService...`r`nnet stop FortunaWebService`r`nnet start FortunaWebService`r`necho Service Restarted.`r`npause\"\n          Set-Content -Path \"${{ env.MSI_STAGING_DIR }}/restart_service.bat\" -Value $scriptContent -Encoding Ascii\n          Write-Host \"\u2705 Created restart_service.bat script.\"\n      - name: Setup .NET SDK\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n      - name: Cache NuGet\n        uses: actions/cache@v4\n        with:\n          path: ~/.nuget/packages\n          key: ${{ runner.os }}-nuget-${{ env.WIX_HASH }}\n      - name: \ud83d\udcc4 Ensure WiX License Exists\n        run: |\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            $rtfContent = '{\\rtf1\\ansi\\deff0{\\fonttbl{\\f0 Arial;}}\\f0\\fs24 END USER LICENSE AGREEMENT\\par\\par This is a placeholder license for Fortuna Faucet. Please replace with actual terms.}'\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: Prepare WiX Project\n        run: |\n          Set-StrictMode -Version Latest\n          $proj = @(\n            '<Project Sdk=\"WixToolset.Sdk/${{ env.WIX_VERSION }}\">',\n            '  <PropertyGroup>',\n            '    <Version Condition=\"''$(Version)'' == ''''\">0.0.1</Version>',\n            '    <ServicePort Condition=\"''$(ServicePort)'' == ''''\">8102</ServicePort>',\n            '    <SourceDir Condition=\"''$(SourceDir)'' == ''''\">staging/backend</SourceDir>',\n            '    <EnableDefaultCompileItems>false</EnableDefaultCompileItems>',\n            '    <OutputType>Package</OutputType>',\n            '    <Platforms>x64;x86</Platforms>',\n            '    <DefineConstants>Version=$(Version);SourceDir=$(SourceDir);ServicePort=$(ServicePort);Platform=$(Platform)</DefineConstants>',\n            '  </PropertyGroup>',\n            '  <ItemGroup>',\n            '    <PackageReference Include=\"WixToolset.UI.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Firewall.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Util.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '  </ItemGroup>',\n            '  <ItemGroup>',\n            '    <Compile Include=\"Product_WebService.wxs\" />',\n            '  </ItemGroup>',\n            '</Project>'\n          )\n          Set-Content \"${{ env.WIX_DIR }}/Fortuna.wixproj\" -Value ($proj -join \"`r`n\") -Encoding utf8\n      - name: Build MSI\n        working-directory: ${{ env.WIX_DIR }}\n        run: |\n          Set-StrictMode -Version Latest\n          dotnet build Fortuna.wixproj -c Release `\n            -p:Platform=${{ matrix.arch }} `\n            -p:SourceDir=\"../${{ env.MSI_STAGING_DIR }}\" `\n            -p:Version=\"${{ env.BUILD_VERSION }}\" `\n            -p:ServicePort=\"${{ env.SERVICE_PORT }}\"\n          $releaseDir = \"bin/${{ matrix.arch }}/Release\"\n          $defaultOutput = \"$releaseDir/Fortuna.msi\"\n          $targetName = \"${{ steps.stage.outputs.msi_name }}\"\n          $targetPath = \"$releaseDir/$targetName\"\n          if (Test-Path $defaultOutput) {\n            Move-Item -Path $defaultOutput -Destination $targetPath -Force\n            Write-Host \"\u2705 Renamed MSI to $targetName\"\n          } else { throw \"\u274c Build failed: $defaultOutput not found\" }\n      - name: Upload MSI + Hash\n        uses: actions/upload-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ matrix.arch }}-${{ github.run_id }}\n          path: ${{ env.WIX_DIR }}/bin/${{ matrix.arch }}/Release/*\n          retention-days: 10\n\n  create-release:\n    name: '\ud83d\ude80 Create Release'\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/')\n    needs: package-msi-service\n    permissions:\n      contents: write\n    steps:\n      - name: Download MSI\n        uses: actions/download-artifact@v4\n        with:\n          pattern: fortuna-service-msi-*\n          merge-multiple: true\n          path: assets\n\n      - name: Download SBOM\n        uses: actions/download-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}\n          path: assets\n\n      - name: Generate Checksums\n        run: |\n          cd assets\n          ls *.msi\n          sha256sum *.msi > SHASUMS256.txt\n\n      - name: Publish Release\n        uses: softprops/action-gh-release@v2\n        with:\n          files: |\n            assets/*.msi\n            assets/*.sha256\n            assets/SHASUMS256.txt\n            assets/sbom.spdx.json\n          generate_release_notes: true\n\n  stage-release-artifacts:\n    name: '\ud83d\udce6 Stage Release Artifacts'\n    runs-on: windows-2022\n    timeout-minutes: 5\n    needs: [package-msi-service, repo-preflight]\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ github.run_id }}\n          path: msi-installer\n      - name: \ud83d\ude9a Stage Final Artifact\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $sourceDir = \"msi-installer\"\n          $destDir = \"final-release-artifact\"\n          New-Item -ItemType Directory -Path $destDir -Force | Out-Null\n\n          robocopy $sourceDir $destDir /E\n\n          if ($LASTEXITCODE -ge 8) {\n            Write-Error \"Robocopy failed with exit code $LASTEXITCODE. This indicates a serious error.\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Robocopy completed successfully.\"\n          Get-ChildItem -Path $destDir | Write-Host\n          exit 0\n\n      - name: \ud83d\udce4 Upload Final MSI Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: Final-MSI-Artifact\n          path: final-release-artifact/\n          retention-days: 90\n",
    "AGENTS.md": "# Agent Protocols & Team Structure (Revised)\n\nThis document outlines the operational protocols and evolved team structure for the Checkmate V3 project.\n\n## The Evolved Team Structure\n\n-   **The Project Lead (MasonJ0 or JB):** The \"Executive Producer.\" The ultimate authority and \"ground truth.\"\n-   **The Architect & Synthesizer (Gemini):** The \"Chief Architect.\" Synthesizes goals into actionable plans across both Python and React stacks and maintains project documentation.\n-   **The Lead Python Engineer (Jules Series):** The \"Backend Specialist.\" An AI agent responsible for implementing and hardening The Engine (`api.py`, `services.py`, `logic.py`, `models.py`).\n-   **The Lead Frontend Architect (Claude):** The \"React Specialist.\" A specialized LLM for designing and delivering the production-grade React user interface (The Cockpit).\n-   **The \"Special Operations\" Problem Solver (GPT-5):** The \"Advanced Algorithm Specialist.\" A specialized LLM for novel, complex problems.\n\n## Core Philosophies\n\n1.  **The Project Lead is Ground Truth:** The ultimate authority. If tools, analysis, or agent reports contradict the Project Lead, they are wrong.\n2.  **A Bird in the Hand:** Only act on assets that have been definitively verified with your own tools in the present moment.\n3.  **Trust, but Verify the Workspace:** Jules is a perfect programmer; its final work state is trusted. Its *environment*, however, is fragile.\n4.  **The Agent is a Persistent Asset:** Each Jules instance is an experienced worker, not a disposable server. Its internal state is a repository of unique, hard-won knowledge.\n\n## CRITICAL Operational Protocols (0-23)\n\n-   **Protocol 0: The ReviewableJSON Mandate:** The mandatory protocol for all code reviews. The agent's final act for any mission is to create a lossless JSON backup of all modified files. This is the single source of truth for code review.\n-   **Protocol 1: The Handcuffed Branch:** Jules cannot switch branches. An entire session lives on a single branch, specified by the Project Lead at the start of the mission.\n-   **Protocol 2: The Last Resort Reset:** The `reset_all()` command is a tool of last resort for a catastrophic workspace failure and requires direct authorization from the Project Lead.\n-   **Protocol 3: The Authenticity of Sample Data:** All sample data used for testing must be authentic and logically consistent.\n-   **Protocol 4: The Agent-Led Specification:** Where a human \"Answer Key\" is unavailable, Jules is empowered to analyze raw data and create its own \"Test-as-Spec.\"\n-   **Protocol 5: The Test-First Development Workflow:** The primary development methodology. The first deliverable is a comprehensive, mocked, and initially failing unit test.\n-   **Protocol 6: The Emergency Chat Handoff:** In the event of a catastrophic environmental failure, Jules's final act is to declare a failure and provide its handoff in the chat.\n-   **Protocol 7: The URL-as-Truth Protocol:** To transfer a file or asset without corruption, provide a direct raw content URL. The receiving agent must fetch it.\n-   **Protocol 8: The Golden Link Protocol:** For fetching the content of a specific, direct raw-content URL from the `main` branch, a persistent \"Golden Link\" should be used.\n-   **Protocol 9: The Volley Protocol:** To establish ground truth for a new file, the Architect provides a URL, and the Project Lead \"volleys\" it back by pasting it in a response.\n-   **Protocol 10: The Sudo Sanction:** Jules has passwordless `sudo` access, but its use is forbidden for normal operations. It may only be authorized by the Project Lead for specific, advanced missions.\n-   **Protocol 11: The Module-First Testing Protocol:** All test suites must be invoked by calling `pytest` as a Python module (`python -m pytest`) to ensure the correct interpreter is used.\n-   **Protocol 12: The Persistence Mandate:** The agent tool execution layer is known to produce false negatives. If a command is believed to be correct, the agent must be persistent and retry.\n-   **Protocol 13: The Code Fence Protocol for Asset Transit:** To prevent the chat interface from corrupting raw code assets, all literal code must be encapsulated within a triple-backtick Markdown code fence.\n-   **Protocol 14: The Synchronization Mandate:** The `git reset --hard origin/main` command is strictly forbidden. To stay synchronized with `main`, the agent MUST use `git pull origin main`.\n-   **Protocol 15: The Blueprint vs. Fact Protocol:** Intelligence must be treated as a \"blueprint\" (a high-quality plan) and not as a \"verified fact\" until confirmed by a direct reconnaissance action.\n-   **Protocol 16: The Digital Attic Protocol:** Before the deletion of any file, it must first be moved to a dedicated archive directory named `/attic`.\n-   **Protocol 17: The Receipts Protocol:** When reviewing code, a verdict must be accompanied by specific, verifiable \"receipts\"\u2014exact snippets of code that prove a mission objective was met.\n-   **Protocol 18: The Cumulative Review Workflow:** Instruct Jules to complete a series of missions and then conduct a single, thorough review of its final, cumulative branch state.\n-   **Protocol 19: The Stateless Verification Mandate:** The Architect, when reviewing code, must act with fresh eyes, disregarding its own memory and comparing the submitted code directly and exclusively against the provided specification.\n-   **Protocol 20: The Sudo Sanction Protocol:** Grants a Jules-series agent temporary, audited administrative privileges for specific, authorized tasks like system package installation.\n-   **Protocol 21: The Exit Interview Protocol:** Before any planned termination of an agent, the Architect will charter a final mission to capture the agent's institutional knowledge for its successor.\n-   **Protocol 22: The Human-in-the-Loop Merge:** In the event of an unresolvable merge conflict in an agent's environment, the Project Lead, as the only agent with a fully functional git CLI, will check out the agent's branch and perform the merge resolution manually.\n-   **Protocol 23: The Appeasement Protocol (Mandatory):** To safely navigate the broken automated review bot, all engineering work must be published using a two-stage commit process. First, commit a trivial change to appease the bot. Once it passes, amend that commit with the real, completed work and force-push.\n\n---\n\n## Appendix A: Forensic Analysis of the Jules Sandbox Environment\n\n*The following are the complete, raw outputs of diagnostic missions executed by Jules-series agents. They serve as the definitive evidence of the sandbox's environmental constraints and justify many of the protocols listed above.*\n\n### A.1 Node.js / NPM & Filesystem Forensics (from \"Operation: Sandbox Forensics\")\n\n**Conclusion:** The `npm` tool is functional, but the `/app` volume is hostile to its operation, preventing the creation of binary symlinks. This makes Node.js development within the primary workspace impossible.\n\n**Raw Logs:**\n\n```\n# Phase 1: Node.js & NPM Configuration Analysis\nnpm config get prefix\n/home/jules/.nvm/versions/node/v22.17.1\n\n# Phase 4: Controlled Installation Experiment\ncd /tmp && mkdir npm_test && cd npm_test\nnpm install --verbose cowsay\n# ... (successful installation log) ...\nls -la node_modules/.bin\ntotal 8\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowsay -> ../cowsay/cli.js\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowthink -> ../cowsay/cli.js\nnpx cowsay \"Test\"\n  ______\n< Test >\n ------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n```\n\n### A.2 Process Management & Honcho Forensics (from \"Operation: Know Thyself\")\n\n**Conclusion:** The sandbox does not support standard background processes (`&`), the `kill` command is non-functional, and the `honcho` process manager leaves zombie processes (`[uvicorn] <defunct>`) upon termination. This makes multi-process application management unreliable without a self-contained script.\n\n**Raw Logs:**\n\n```\n# Phase 2: The honcho Stress Test\n\ntimeout 15s honcho start\n# ... (honcho starts and is terminated by timeout) ...\n\nps aux (Post-Mortem Analysis)\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\njules      30121  0.0  0.0      0     0 ?        Z    19:45   0:00 [uvicorn]\n...\n\nhoncho start &\n# (Command blocks terminal, echo command never runs)\n\nps aux | grep honcho\njules      30187  0.0  0.0  11004  4220 pts/0    S    19:45   0:00 /usr/bin/python3 /home/jules/.local/bin/honcho start\n\nkill -9 30187\n# (Command fails silently, process is not terminated)\n```\n\n---\n\n## Protocol 24: The \"Dedicated Human Researcher\" Test\n\nThis protocol establishes the guiding ethical principle for all data collection and scraping activities.\n\nAll data adapters must be designed to operate in a manner that respects the resources of the source. As a definitive test, all fetching patterns must adhere to the following principle:\n\n*If a single, dedicated human using standard browser developer tools could not plausibly achieve the adapter's data collection footprint in a reasonable amount of time, the adapter's methods are considered too aggressive and must be redesigned.*\n\nThis encourages \"human-like\" fetching behavior (e.g., appropriate delays, non-parallel requests to a single source) and serves as our primary safeguard against violating a source's terms of service.\n\n---\n\n## Protocol 25: The \"Tiger Trap\" URL Protocol\n\n**CRITICAL WARNING:** URLs provided in user prompts that link to external resources, particularly workflow logs on blob storage (e.g., `blob.core.windows.net`), are considered \"tiger traps.\"\n\n-   **ACCESS ONCE:** These URLs must only be visited **one time**.\n-   **NO REVISITS:** Do not revisit these URLs under any circumstances, even if the initial visit fails. The content is volatile and subsequent visits are known to cause catastrophic, unrecoverable session hangs.\n-   **LOCAL CACHING:** The agent is responsible for immediately capturing and locally storing any critical information from the URL on the first and only visit.\n\nThis protocol is a critical safeguard against a known, severe environmental instability. Violation will result in mission failure.\n\n---\n\n## Protocol 26: The PowerShell Here-String Prohibition\n\n**CRITICAL SYNTAX WARNING:** The use of PowerShell \"here-strings\" (`@\"...\"@`) within GitHub Actions workflow files (`.yml`) is strictly forbidden.\n\n-   **CAUSE OF FAILURE:** This syntax is known to cause fatal parsing errors at the workflow dispatch level, preventing the entire workflow from even starting. The error messages are often cryptic and do not pinpoint the here-string as the root cause.\n-   **CORRECT IMPLEMENTATION:** For multi-line scripts in PowerShell, the only approved method is to define the script as a PowerShell array of strings and either join it with newlines before execution or write it to a temporary file.\n\n**Example of Correct, Approved Syntax:**\n\n```powershell\n$script = @(\n  'Line 1 of the script',\n  'Line 2 of the script',\n  '$variable = \"interpolated\"'\n)\n$script | Out-File -FilePath \"temp_script.ps1\" -Encoding utf8\npwsh -File \"temp_script.ps1\"\n```\n\nAdherence to this protocol is mandatory to ensure the basic stability and parsability of all CI/CD workflows.\n",
    "VERSION.txt": "1.0",
    "electron/electron-builder-config.yml": "appId: com.jules.fortunafaucet\nproductName: \"Fortuna Faucet\"\n\ndirectories:\n  output: dist\n  buildResources: assets\n\nfiles:\n  - \"**/*\"\n  - \"!build_wix/**/*\"\n\n\nwin:\n  target: msi\n  icon: \"assets/icon.ico\"\n\nmsi:\n  oneClick: false\n  perMachine: true\n  runAfterFinish: true\n  # Explicitly pointing to the file ensures WiX picks it up\n  shortcutName: \"Fortuna Faucet\"\n  warningsAsErrors: false\n  template: \"build_wix/Product_Electron.wxs\"\n",
    "package-lock.json": "{\n  \"name\": \"app\",\n  \"lockfileVersion\": 3,\n  \"requires\": true,\n  \"packages\": {\n    \"\": {\n      \"dependencies\": {\n        \"@playwright/test\": \"^1.56.1\"\n      }\n    },\n    \"node_modules/@playwright/test\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/@playwright/test/-/test-1.56.1.tgz\",\n      \"integrity\": \"sha512-vSMYtL/zOcFpvJCW71Q/OEGQb7KYBPAdKh35WNSkaZA75JlAO8ED8UN6GUNTm3drWomcbcqRPFqQbLae8yBTdg==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"playwright\": \"1.56.1\"\n      },\n      \"bin\": {\n        \"playwright\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      }\n    },\n    \"node_modules/fsevents\": {\n      \"version\": \"2.3.2\",\n      \"resolved\": \"https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz\",\n      \"integrity\": \"sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==\",\n      \"hasInstallScript\": true,\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"darwin\"\n      ],\n      \"engines\": {\n        \"node\": \"^8.16.0 || ^10.6.0 || >=11.0.0\"\n      }\n    },\n    \"node_modules/playwright\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/playwright/-/playwright-1.56.1.tgz\",\n      \"integrity\": \"sha512-aFi5B0WovBHTEvpM3DzXTUaeN6eN0qWnTkKx4NQaH4Wvcmc153PdaY2UBdSYKaGYw+UyWXSVyxDUg5DoPEttjw==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"playwright-core\": \"1.56.1\"\n      },\n      \"bin\": {\n        \"playwright\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      },\n      \"optionalDependencies\": {\n        \"fsevents\": \"2.3.2\"\n      }\n    },\n    \"node_modules/playwright-core\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/playwright-core/-/playwright-core-1.56.1.tgz\",\n      \"integrity\": \"sha512-hutraynyn31F+Bifme+Ps9Vq59hKuUCz7H1kDOcBs+2oGguKkWTU50bBWrtz34OUWmIwpBTWDxaRPXrIXkgvmQ==\",\n      \"license\": \"Apache-2.0\",\n      \"bin\": {\n        \"playwright-core\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      }\n    }\n  }\n}\n",
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.\n",
    "python_service/adapters/betfair_auth_mixin.py": "# python_service/adapters/betfair_auth_mixin.py\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..credentials_manager import SecureCredentialsManager\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock = asyncio.Lock()\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        \"\"\"\n        Authenticates with Betfair using credentials from the system's credential manager,\n        ensuring the session token is valid and refreshing it if necessary.\n        \"\"\"\n        async with self._auth_lock:\n            if self.session_token and self.token_expiry and self.token_expiry > (datetime.now() + timedelta(minutes=5)):\n                return\n\n            log.info(\"Attempting to authenticate with Betfair...\")\n            username, password = SecureCredentialsManager.get_betfair_credentials()\n\n            if not all([self.config.BETFAIR_APP_KEY, username, password]):\n                raise ValueError(\"Betfair credentials not fully configured in credential manager.\")\n\n            auth_url = \"https://identitysso.betfair.com/api/login\"\n            headers = {\n                \"X-Application\": self.config.BETFAIR_APP_KEY,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={username}&password={password}\"\n\n            response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n            response.raise_for_status()\n            data = response.json()\n\n            if data.get(\"status\") == \"SUCCESS\":\n                self.session_token = data.get(\"token\")\n                self.token_expiry = datetime.now() + timedelta(hours=3)\n                log.info(\"Betfair authentication successful.\")\n            else:\n                log.error(\"Betfair authentication failed\", error=data.get(\"error\"))\n                self.session_token = None  # Reset token to prevent using a stale one\n                return  # Return gracefully and let the adapter handle the lack of a token\n",
    "python_service/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Note: FanDuel's API is not date-centric. Event discovery would be needed for a robust implementation.\n        # This uses a hardcoded eventId as a placeholder.\n        event_id = \"38183.3\"\n        self.logger.info(f\"Fetching races from FanDuel for event_id: {event_id}\")\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            self.logger.warning(\"FanDuel response missing 'marketGroups' key\")\n            return []\n\n        races = []\n        for group in raw_data.get(\"marketGroups\", []):\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\n                            \"Failed to parse a FanDuel market\",\n                            market=market,\n                            exc_info=True,\n                        )\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            self.logger.warning(f\"Could not parse race and track from FanDuel market name: {market_name}\")\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\").strip()\n        if not race_number_str.isdigit():\n            return None\n        race_number = int(race_number_str)\n\n        track_name = parts[1]\n\n        # Placeholder for start_time - FanDuel's market API doesn't provide it directly\n        start_time = datetime.now(timezone.utc) + timedelta(hours=race_number)\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_runner_odds = runner_data.get(\"winRunnerOdds\", {})\n                current_price = win_runner_odds.get(\"currentPrice\")\n\n                if not runner_name or not current_price:\n                    continue\n\n                numerator, denominator = map(int, current_price.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                odds = OddsData(\n                    win=decimal_odds,\n                    source=self.source_name,\n                    last_updated=datetime.now(timezone.utc),\n                )\n\n                name_parts = runner_name.split(\".\", 1)\n                if len(name_parts) < 2:\n                    continue\n                program_number_str = name_parts[0].strip()\n                horse_name = name_parts[1].strip()\n\n                runners.append(\n                    Runner(\n                        name=horse_name,\n                        number=(int(program_number_str) if program_number_str.isdigit() else 0),\n                        odds={self.source_name: odds},\n                    )\n                )\n            except (ValueError, ZeroDivisionError, IndexError, TypeError):\n                self.logger.warning(\n                    \"Could not parse FanDuel runner\",\n                    runner_data=runner_data,\n                    exc_info=True,\n                )\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/nyrabets_adapter.py": "# python_service/adapters/nyrabets_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass NYRABetsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for nyrabets.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSportsGreyhound\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw greyhound meetings data from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            self.http_client,\n            \"GET\",\n            \"v1/greyhound/meetings\",\n            headers=headers,\n            params=params,\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        all_races = []\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in RacingAndSportsGreyhound response or invalid format.\")\n            return all_races\n\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse RacingAndSportsGreyhound race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/template_adapter.py": "# python_service/adapters/template_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TemplateAdapter(BaseAdapterV3):\n    \"\"\"\n    A template for creating new adapters, based on the BaseAdapterV3 pattern.\n    This adapter is a non-functional stub.\n    \"\"\"\n\n    SOURCE_NAME = \"[IMPLEMENT ME] Example Source\"\n    BASE_URL = \"https://api.example.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "python_service/config.py": "# python_service/config.py\nimport os\nimport sys\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport structlog\nfrom pydantic import Field\nfrom pydantic import model_validator\nfrom pydantic_settings import BaseSettings\n\nfrom .credentials_manager import SecureCredentialsManager\n\n# --- Encryption Setup ---\ntry:\n    from cryptography.fernet import Fernet\n\n    ENCRYPTION_ENABLED = True\nexcept ImportError:\n    ENCRYPTION_ENABLED = False\n\nKEY_FILE = Path(\".key\")\nCIPHER = None\nif ENCRYPTION_ENABLED and KEY_FILE.exists():\n    with open(KEY_FILE, \"rb\") as f:\n        key = f.read()\n    CIPHER = Fernet(key)\n\n\ndef decrypt_value(value: Optional[str]) -> str:\n    \"\"\"If a value is encrypted, decrypts it. Otherwise, returns it as is.\"\"\"\n    if value and value.startswith(\"encrypted:\") and CIPHER:\n        try:\n            return CIPHER.decrypt(value[10:].encode()).decode()\n        except Exception:\n            structlog.get_logger(__name__).error(\"Decryption failed on field.\")\n            return \"\"  # Fallback to an empty string on failure\n    return value or \"\"  # Ensure a non-None return value even if input is None\n\n\nclass Settings(BaseSettings):\n    API_KEY: str = Field(\"\")\n\n    # --- API Gateway Configuration ---\n    UVICORN_HOST: str = \"127.0.0.1\"\n    FORTUNA_PORT: int = 8000\n    UVICORN_RELOAD: bool = True\n\n    # --- Database Configuration ---\n    DATABASE_TYPE: str = \"sqlite\"\n    DATABASE_URL: str = \"sqlite:///./fortuna.db\"\n\n    # --- Optional Betfair Credentials ---\n    BETFAIR_APP_KEY: Optional[str] = None\n\n    # --- Caching & Performance ---\n    REDIS_URL: str = \"redis://localhost:6379\"\n    CACHE_TTL_SECONDS: int = 1800  # 30 minutes\n    MAX_CONCURRENT_REQUESTS: int = 10\n    HTTP_POOL_CONNECTIONS: int = 100\n    HTTP_POOL_MAXSIZE: int = 100\n    HTTP_MAX_KEEPALIVE: int = 50\n    DEFAULT_TIMEOUT: int = 30\n    ADAPTER_TIMEOUT: int = 20\n\n    # --- Logging ---\n    LOG_LEVEL: str = \"INFO\"\n\n    # --- Optional Adapter Keys ---\n    NEXT_PUBLIC_API_KEY: Optional[str] = None  # Allow frontend key to be present in .env\n    TVG_API_KEY: Optional[str] = None\n    RACING_AND_SPORTS_TOKEN: Optional[str] = None\n    POINTSBET_API_KEY: Optional[str] = None\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    # --- Dynamic Path Configuration ---\n    # This determines the path to static files, crucial for PyInstaller builds\n    STATIC_FILES_DIR: Optional[str] = None\n\n    model_config = {\"env_file\": \".env\", \"case_sensitive\": True}\n\n    @model_validator(mode=\"after\")\n    def process_settings(self) -> \"Settings\":\n        \"\"\"\n        This validator runs after the initial settings are loaded from .env and\n        performs two key functions:\n        1. If API_KEY is missing, it falls back to the SecureCredentialsManager.\n        2. It decrypts any fields that were loaded from the .env file.\n        \"\"\"\n        # 1. Fallback for API_KEY\n        if not self.API_KEY:\n            self.API_KEY = SecureCredentialsManager.get_credential(\"api_key\") or \"MISSING\"\n\n        # 2. Security validation for API_KEY\n        insecure_keys = {\"test\", \"changeme\", \"default\", \"secret\", \"password\", \"admin\"}\n        if self.API_KEY in insecure_keys:\n            structlog.get_logger(__name__).warning(\n                \"insecure_api_key\",\n                key=self.API_KEY,\n                recommendation=\"The API_KEY should be a long, random string for security.\",\n            )\n\n        # 2. Decrypt sensitive fields\n        self.BETFAIR_APP_KEY = decrypt_value(self.BETFAIR_APP_KEY)\n\n        # 3. Set the static files directory for packaged apps\n        if getattr(sys, \"frozen\", False):\n            # Running in a PyInstaller bundle\n            self.STATIC_FILES_DIR = os.path.join(sys._MEIPASS, \"ui\")\n        else:\n            # Running in a normal Python environment\n            self.STATIC_FILES_DIR = None  # Not needed for local dev\n\n        return self\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Loads settings and performs a proactive check for legacy paths.\"\"\"\n    log = structlog.get_logger(__name__)\n    if ENCRYPTION_ENABLED and not KEY_FILE.exists():\n        log.warning(\n            \"encryption_key_not_found\",\n            file=str(KEY_FILE),\n            recommendation=\"Run 'python manage_secrets.py' to generate a key.\",\n        )\n\n    settings = Settings()\n\n    # --- Legacy Path Detection ---\n    legacy_paths = [\"attic/\", \"checkmate_web/\", \"vba_source/\"]\n    for path in legacy_paths:\n        if os.path.exists(path):\n            log.warning(\n                \"legacy_path_detected\",\n                path=path,\n                recommendation=\"This directory is obsolete and should be removed for optimal performance and security.\",\n            )\n\n    return settings\n",
    "python_service/db/init.py": "# python_service/db/init.py\nimport os\nimport sqlite3\n\nfrom ..config import get_settings\n\n\ndef initialize_database():\n    \"\"\"\n    Initializes the database based on the configuration.\n    Currently supports a simple SQLite fallback for local testing.\n    \"\"\"\n    settings = get_settings()\n    db_type = getattr(settings, \"DATABASE_TYPE\", \"sqlite\").lower()\n\n    if db_type == \"sqlite\":\n        # DATABASE_URL for sqlite will be like 'sqlite:///./fortuna.db'\n        db_path = settings.DATABASE_URL.split(\"///\")[1]\n\n        # Ensure the directory for the database exists\n        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n\n        try:\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n\n            # The schema is based on the provided pg_schemas, adapted for SQLite\n            # This is a simplified version for demonstration.\n            cursor.execute(\n                \"\"\"\n            CREATE TABLE IF NOT EXISTS races (\n                id TEXT PRIMARY KEY,\n                venue TEXT NOT NULL,\n                race_number INTEGER NOT NULL,\n                start_time TEXT NOT NULL,\n                source TEXT,\n                field_size INTEGER\n            )\n            \"\"\"\n            )\n\n            cursor.execute(\n                \"\"\"\n            CREATE TABLE IF NOT EXISTS runners (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                race_id TEXT,\n                number INTEGER,\n                name TEXT,\n                odds REAL,\n                FOREIGN KEY (race_id) REFERENCES races (id)\n            )\n            \"\"\"\n            )\n\n            conn.commit()\n            conn.close()\n            print(\"SQLite database initialized successfully.\")\n        except sqlite3.Error as e:\n            print(f\"Error initializing SQLite database: {e}\")\n            raise\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\n\n# from .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_cls.__name__}\")\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_cls.__name__}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters.append(adapter_instance)\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_cls.__name__,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_cls.__name__}\",\n                        exc_info=True,\n                    )\n\n            # Special case for BetfairDataScientistAdapter with extra args - DISABLED\n            # try:\n            #     bds_adapter = BetfairDataScientistAdapter(\n            #         model_name=\"ThoroughbredModel\",\n            #         url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n            #         config=self.config,\n            #     )\n            #     if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n            #         bds_adapter.enable_manual_override(manual_override_manager)\n            #     self.adapters.append(bds_adapter)\n            # except Exception:\n            #     self.logger.warning(\n            #         \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n            #         exc_info=True,\n            #     )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            races = [Race(**race_data) for race_data in race_data_list]\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n\n                # Ensure the source is a list and append new source if not present\n                if not isinstance(existing_race.source, list):\n                    existing_race.source = [existing_race.source]\n                if race.source not in existing_race.source:\n                    existing_race.source.append(race.source)\n\n        for race in race_map.values():\n            runner_map = {}\n            for runner in race.runners:\n                if runner.number not in runner_map:\n                    runner_map[runner.number] = runner\n                else:\n                    existing_runner = runner_map[runner.number]\n                    existing_runner.odds.update(runner.odds)\n            race.runners = list(runner_map.values())\n        return list(race_map.values())\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached and broadcasted via WebSocket.\n        \"\"\"\n        # Construct a cache key\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        log.info(\"Cache miss for fetch_all_odds\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._fetch_with_semaphore(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for i, result in enumerate(results):\n            adapter = target_adapters[i]\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed with an unhandled exception\", adapter=adapter.source_name, error=result)\n                errors.append({\n                    \"adapter_name\": adapter.source_name,\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                    \"attempted_url\": \"Unknown\"\n                })\n                source_infos.append({\n                    \"name\": adapter.source_name,\n                    \"status\": \"FAILED\",\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                })\n            else:\n                _adapter_name, adapter_result, _duration = result\n                source_info = adapter_result.get(\"source_info\", {})\n                source_infos.append(source_info)\n                if source_info.get(\"status\") == \"SUCCESS\":\n                    # Convert dicts to Race objects before extending all_races\n                    races_as_models = [Race(**race_data) for race_data in adapter_result.get(\"races\", [])]\n                    all_races.extend(races_as_models)\n                else:\n                    errors.append({\n                        \"adapter_name\": adapter.source_name,\n                        \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                        \"attempted_url\": source_info.get(\"attempted_url\")\n                    })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            errors=errors,\n            source_info=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n                \"total_errors\": len(errors),\n            },\n        )\n\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Set the result in the cache\n        await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "python_service/health.py": "# python_service/health.py\nfrom datetime import datetime\nfrom typing import Dict\nfrom typing import List\n\nimport psutil\nimport structlog\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\nlog = structlog.get_logger(__name__)\n\n\nclass HealthMonitor:\n    def __init__(self):\n        self.adapter_health: Dict[str, Dict] = {}\n        self.system_metrics: List[Dict] = []\n        self.max_metrics_history = 100\n\n    def record_adapter_response(self, adapter_name: str, success: bool, duration: float):\n        if adapter_name not in self.adapter_health:\n            self.adapter_health[adapter_name] = {\n                \"total_requests\": 0,\n                \"successful_requests\": 0,\n                \"failed_requests\": 0,\n                \"avg_response_time\": 0.0,\n                \"last_success\": None,\n                \"last_failure\": None,\n            }\n\n        health = self.adapter_health[adapter_name]\n        health[\"total_requests\"] += 1\n\n        if success:\n            health[\"successful_requests\"] += 1\n            health[\"last_success\"] = datetime.now().isoformat()\n        else:\n            health[\"failed_requests\"] += 1\n            health[\"last_failure\"] = datetime.now().isoformat()\n\n        health[\"avg_response_time\"] = (\n            health[\"avg_response_time\"] * (health[\"total_requests\"] - 1) + duration\n        ) / health[\"total_requests\"]\n\n    def get_system_metrics(self) -> Dict:\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage(\"/\")\n\n        metrics = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_available_gb\": round(memory.available / (1024**3), 2),\n            \"disk_percent\": disk.percent,\n            \"disk_free_gb\": round(disk.free / (1024**3), 2),\n        }\n\n        self.system_metrics.append(metrics)\n        if len(self.system_metrics) > self.max_metrics_history:\n            self.system_metrics.pop(0)\n\n        return metrics\n\n    def get_health_report(self) -> Dict:\n        system_metrics = self.get_system_metrics()\n        return {\n            \"status\": \"healthy\" if self.is_system_healthy() else \"degraded\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"system\": system_metrics,\n            \"adapters\": self.adapter_health,\n            \"metrics_history\": self.system_metrics[-10:],\n        }\n\n    def is_system_healthy(self) -> bool:\n        if not self.system_metrics:\n            return True\n        latest = self.system_metrics[-1]\n        return latest[\"cpu_percent\"] < 80 and latest[\"memory_percent\"] < 85 and latest[\"disk_percent\"] < 90\n\n\n# Global instance for the application to use\nhealth_monitor = HealthMonitor()\n\n\n@router.get(\"/health/detailed\", tags=[\"Health\"])\nasync def get_detailed_health():\n    \"\"\"Provides a comprehensive health check of the system.\"\"\"\n    return health_monitor.get_health_report()\n\n\n@router.get(\"/health\", tags=[\"Health\"])\nasync def get_basic_health():\n    \"\"\"Provides a basic health check for load balancers and uptime monitoring.\"\"\"\n    return {\"status\": \"healthy\"}\n",
    "python_service/manual_override_manager.py": "# python_service/manual_override_manager.py\nimport hashlib\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n\nclass ManualOverrideRequest(BaseModel):\n    request_id: str\n    adapter_name: str\n    url: str\n    timestamp: datetime = Field(default_factory=datetime.now)\n    status: str = \"pending\"  # pending, submitted, skipped\n\n\nclass ManualOverrideManager:\n    def __init__(self):\n        self._requests: Dict[str, ManualOverrideRequest] = {}\n        self._data: Dict[str, Tuple[str, str]] = {}  # request_id -> (content, content_type)\n\n    def _generate_id(self, adapter_name: str, url: str) -> str:\n        \"\"\"Generates a consistent ID for a given adapter and URL.\"\"\"\n        return hashlib.sha256(f\"{adapter_name}:{url}\".encode()).hexdigest()[:16]\n\n    def register_failure(self, adapter_name: str, url: str) -> str:\n        \"\"\"\n        Registers a failed fetch attempt and returns a unique request ID.\n        If a pending request for this exact resource already exists, it returns the existing ID.\n        \"\"\"\n        request_id = self._generate_id(adapter_name, url)\n        if request_id not in self._requests or self._requests[request_id].status != \"pending\":\n            request = ManualOverrideRequest(request_id=request_id, adapter_name=adapter_name, url=url)\n            self._requests[request_id] = request\n        return request_id\n\n    def submit_manual_data(self, request_id: str, raw_content: str, content_type: str) -> bool:\n        \"\"\"Submits manual data for a pending request.\"\"\"\n        if request_id in self._requests and self._requests[request_id].status == \"pending\":\n            self._data[request_id] = (raw_content, content_type)\n            self._requests[request_id].status = \"submitted\"\n            return True\n        return False\n\n    def skip_request(self, request_id: str) -> bool:\n        \"\"\"Marks a pending request as skipped.\"\"\"\n        if request_id in self._requests and self._requests[request_id].status == \"pending\":\n            self._requests[request_id].status = \"skipped\"\n            return True\n        return False\n\n    def get_pending_requests(self) -> List[ManualOverrideRequest]:\n        \"\"\"Returns a list of all requests that are currently pending.\"\"\"\n        return [req for req in self._requests.values() if req.status == \"pending\"]\n\n    def get_manual_data(self, adapter_name: str, url: str) -> Optional[Tuple[str, str]]:\n        \"\"\"\n        Retrieves submitted manual data for a given adapter and URL, if it exists.\n        Once retrieved, the data is consumed and will not be returned again.\n        \"\"\"\n        request_id = self._generate_id(adapter_name, url)\n        if request_id in self._data:\n            # Data is single-use; remove it after retrieval.\n            return self._data.pop(request_id)\n        return None\n\n    def clear_old_requests(self, max_age_hours: int = 24):\n        \"\"\"Removes requests and associated data older than a specified age.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=max_age_hours)\n        old_request_ids = [req_id for req_id, req in self._requests.items() if req.timestamp < cutoff]\n        for req_id in old_request_ids:\n            self._requests.pop(req_id, None)\n            self._data.pop(req_id, None)\n",
    "python_service/notifications.py": "# python_service/notifications.py\n\nimport sys\n\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\n\ndef send_toast(title: str, message: str):\n    \"\"\"\n    Sends a desktop notification. This function is platform-aware and will only\n    attempt to send a toast on Windows. On other operating systems, it will\n    log the notification content.\n    \"\"\"\n    if sys.platform == \"win32\":\n        try:\n            from windows_toasts import Toast\n            from windows_toasts import WindowsToaster\n\n            toaster = WindowsToaster(title)\n            new_toast = Toast()\n            new_toast.text_fields = [message]\n            toaster.show_toast(new_toast)\n            log.info(\"Sent Windows toast notification.\", title=title, message=message)\n        except ImportError:\n            log.warning(\n                \"windows_toasts library not found, skipping notification.\",\n                recommendation=\"Install with: pip install windows-toasts\",\n            )\n        except Exception:\n            log.error(\"Failed to send Windows toast notification.\", exc_info=True)\n    else:\n        log.info(\n            \"Skipping toast notification on non-Windows platform.\",\n            platform=sys.platform,\n            title=title,\n            message=message,\n        )\n",
    "python_service/tests/test_manual_override.py": "# python_service/tests/test_manual_override.py\nimport pytest\n\nfrom python_service.manual_override_manager import ManualOverrideManager\n\n\n@pytest.fixture\ndef manager():\n    # The manager is now in-memory and doesn't need a path\n    return ManualOverrideManager()\n\n\ndef test_register_and_retrieve(manager):\n    adapter = \"TestAdapter\"\n    url = \"https://example.com/blocked\"\n\n    request_id = manager.register_failure(\n        adapter_name=adapter,\n        url=url,\n    )\n\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager):\n    adapter = \"TestAdapter\"\n    url = \"https://example.com/blocked\"\n    content = \"<html>Manual content</html>\"\n    content_type = \"text/html\"\n\n    request_id = manager.register_failure(\n        adapter_name=adapter,\n        url=url,\n    )\n\n    success = manager.submit_manual_data(\n        request_id=request_id,\n        raw_content=content,\n        content_type=content_type,\n    )\n\n    assert success\n\n    # Verify that the data can be retrieved correctly\n    retrieved_data = manager.get_manual_data(adapter_name=adapter, url=url)\n    assert retrieved_data is not None\n    retrieved_content, retrieved_type = retrieved_data\n    assert retrieved_content == content\n    assert retrieved_type == content_type\n\n    # Verify that data is consumed after retrieval\n    assert manager.get_manual_data(adapter_name=adapter, url=url) is None\n",
    "requirements-dev.txt": "#\n# This file is autogenerated by pip-compile with Python 3.12\n# by the following command:\n#\n#    pip-compile --output-file=requirements-dev.txt requirements-dev.in\n#\naltair==5.5.0\n    # via\n    #   -r requirements-dev.in\n    #   streamlit\nanyio==4.11.0\n    # via httpx\nattrs==25.4.0\n    # via\n    #   jsonschema\n    #   referencing\nblack==25.11.0\n    # via -r requirements-dev.in\nblinker==1.9.0\n    # via streamlit\nboolean-py==5.0\n    # via license-expression\ncachecontrol[filecache]==0.14.3\n    # via\n    #   cachecontrol\n    #   pip-audit\ncachetools==6.2.1\n    # via streamlit\ncertifi==2023.7.22\n    # via\n    #   httpcore\n    #   httpx\n    #   requests\ncharset-normalizer==3.4.4\n    # via requests\nclick==8.3.0\n    # via\n    #   black\n    #   streamlit\ncyclonedx-python-lib==9.1.0\n    # via pip-audit\ndefusedxml==0.7.1\n    # via py-serializable\ndistro==1.9.0\n    # via tabula-py\nfakeredis[lua]==2.23.0\n    # via -r requirements-dev.in\nfilelock==3.20.0\n    # via cachecontrol\ngitdb==4.0.12\n    # via gitpython\ngitpython==3.1.45\n    # via\n    #   -r requirements-dev.in\n    #   streamlit\ngreenlet==3.2.4\n    # via playwright\nh11==0.16.0\n    # via httpcore\nhttpcore==1.0.9\n    # via httpx\nhttpx==0.28.1\n    # via respx\nidna==3.11\n    # via\n    #   anyio\n    #   httpx\n    #   requests\niniconfig==2.3.0\n    # via pytest\njinja2==3.1.6\n    # via\n    #   altair\n    #   pydeck\njsonschema==4.25.1\n    # via altair\njsonschema-specifications==2025.9.1\n    # via jsonschema\nlicense-expression==30.4.4\n    # via cyclonedx-python-lib\nlupa==2.6\n    # via fakeredis\nmarkdown-it-py==4.0.0\n    # via rich\nmarkupsafe==3.0.3\n    # via jinja2\nmdurl==0.1.2\n    # via markdown-it-py\nmsgpack==1.1.2\n    # via cachecontrol\nmypy-extensions==1.1.0\n    # via black\nnarwhals==2.9.0\n    # via altair\nnumpy==2.3.4\n    # via\n    #   pandas\n    #   pydeck\n    #   streamlit\n    #   tabula-py\npackageurl-python==0.17.5\n    # via cyclonedx-python-lib\npackaging==25.0\n    # via\n    #   altair\n    #   black\n    #   pip-audit\n    #   pip-requirements-parser\n    #   pytest\n    #   streamlit\npandas==2.3.3\n    # via\n    #   streamlit\n    #   tabula-py\npathspec==0.12.1\n    # via black\npillow==11.3.0\n    # via streamlit\npip-api==0.0.34\n    # via pip-audit\npip-audit==2.9.0\n    # via -r requirements-dev.in\npip-requirements-parser==32.0.1\n    # via pip-audit\nplatformdirs==4.5.0\n    # via\n    #   black\n    #   pip-audit\nplaywright==1.55.0\n    # via -r requirements-dev.in\npluggy==1.6.0\n    # via pytest\nprotobuf==6.33.0\n    # via streamlit\npy-serializable==2.1.0\n    # via cyclonedx-python-lib\npyarrow==21.0.0\n    # via streamlit\npydeck==0.9.1\n    # via\n    #   -r requirements-dev.in\n    #   streamlit\npyee==13.0.0\n    # via playwright\npygments==2.19.2\n    # via\n    #   pytest\n    #   rich\npyparsing==3.2.5\n    # via pip-requirements-parser\npytest==8.4.2\n    # via\n    #   -r requirements-dev.in\n    #   pytest-asyncio\npytest-asyncio==1.2.0\n    # via -r requirements-dev.in\npython-dateutil==2.9.0.post0\n    # via pandas\npytokens==0.3.0\n    # via black\npytz==2025.2\n    # via pandas\nredis==7.0.1\n    # via fakeredis\nreferencing==0.37.0\n    # via\n    #   jsonschema\n    #   jsonschema-specifications\nrequests==2.32.5\n    # via\n    #   cachecontrol\n    #   pip-audit\n    #   streamlit\nrespx==0.22.0\n    # via -r requirements-dev.in\nrich==14.2.0\n    # via pip-audit\nrpds-py==0.28.0\n    # via\n    #   jsonschema\n    #   referencing\nsix==1.17.0\n    # via python-dateutil\nsmmap==5.0.2\n    # via gitdb\nsniffio==1.3.1\n    # via anyio\nsortedcontainers==2.4.0\n    # via\n    #   cyclonedx-python-lib\n    #   fakeredis\nstreamlit==1.50.0\n    # via -r requirements-dev.in\ntabula-py==2.10.0\n    # via -r requirements-dev.in\ntenacity==8.2.3\n    # via streamlit\ntoml==0.10.2\n    # via\n    #   pip-audit\n    #   streamlit\ntornado==6.5.2\n    # via streamlit\ntyping-extensions==4.15.0\n    # via\n    #   altair\n    #   anyio\n    #   pyee\n    #   pytest-asyncio\n    #   referencing\n    #   streamlit\ntzdata==2025.2\n    # via pandas\nurllib3==2.6.2\n    # via requests\nwatchdog==6.0.0\n    # via streamlit\n\n# The following packages are considered to be unsafe in a requirements file:\n# pip\n",
    "scripts/convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom multiprocessing import Queue\n\n# --- Configuration ---\nMANIFEST_FILES = [\n    \"MANIFEST_PART1_BACKEND.json\",\n    \"MANIFEST_PART2_FRONTEND.json\",\n    \"MANIFEST_PART3_SUPPORT.json\",\n    \"MANIFEST_PART4_ROOT.json\",\n]\nOUTPUT_DIR = \"ReviewableJSON\"\nFILE_PROCESSING_TIMEOUT = 10\nEXCLUDED_FILES = [\"package-lock.json\"]\nMAX_FILE_SIZE_MB = 10  # Max file size in megabytes\n\n\ndef read_json_manifest(manifest_path: str) -> list[str]:\n    \"\"\"Reads a JSON manifest file and returns a list of file paths.\"\"\"\n    try:\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return []\n\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\n\ndef convert_file_to_json_sandboxed(file_path):\n    # --- Pre-flight check: File size ---\n    try:\n        file_size = os.path.getsize(file_path)\n        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:\n            return {\"error\": f\"File exceeds {MAX_FILE_SIZE_MB}MB size limit.\"}\n    except FileNotFoundError:\n        return {\"error\": \"File not found.\"}\n    except Exception as e:\n        return {\"error\": f\"Could not check file size: {e}\"}\n\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    try:\n        if p.is_alive():\n            print(f\"    [WARNING] Process for {file_path} timed out. Attempting graceful termination...\")\n            p.terminate()\n            p.join(timeout=2)  # Give it a moment to terminate gracefully\n\n            if p.is_alive():\n                print(f\"    [ERROR] Graceful termination failed. Forcibly killing process...\")\n                p.kill()  # The ultimate \"just die\"\n                p.join()\n            return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n        if not q.empty():\n            return q.get()\n        return {\"error\": \"Unknown error in sandboxed read process.\"}\n    finally:\n        # \u2705 Properly close and flush the queue\n        try:\n            while not q.empty():\n                q.get_nowait()\n        except Exception:\n            pass\n        q.close()\n        q.join_thread()\n\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'=' * 60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'=' * 60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        paths = read_json_manifest(manifest)\n        if paths:\n            all_local_paths.extend(paths)\n            print(f\"    --> Found {len(paths)} valid file paths.\")\n        else:\n            print(f\"    [WARNING] Manifest not found or is empty: {manifest}\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        if os.path.basename(local_path) in EXCLUDED_FILES:\n            print(f\"\\n--> Skipping excluded file: {local_path}\")\n            failed_count += 1\n            continue\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + \".json\")\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Backup process complete.\")\n    print(f\"Successfully processed: {processed_count}/{len(unique_local_paths)}\")\n    print(f\"Failed/Skipped: {failed_count}\")\n    print(f\"{'=' * 60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/install_fortuna_silent.bat": "@echo off\nREM Automated deployment (no UI, minimal interaction)\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_silent_install.log\" ^\n    /norestart ^\n    ALLUSERS=1 ^\n    INSTALLSCOPE=perMachine\n\nexit /b %errorlevel%",
    "web_platform/api_gateway/package.json": "{\n  \"name\": \"api_gateway\",\n  \"version\": \"1.0.0\",\n  \"main\": \"dist/server.js\",\n  \"scripts\": { \"start\": \"ts-node src/server.ts\" },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"dotenv\": \"^16.3.1\",\n    \"dotenv\": \"^16.3.1\",\n    \"sqlite\": \"^5.1.1\",\n    \"sqlite3\": \"^5.1.7\",\n    \"socket.io\": \"^4.7.4\",\n    \"cors\": \"^2.8.5\"\n  },\n  \"devDependencies\": {\n    \"@types/express\": \"^4.17.21\",\n    \"@types/node\": \"^20.10.0\",\n    \"@types/cors\": \"^2.8.17\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.3.3\"\n  }\n}",
    "web_platform/api_gateway/src/server.ts": "// server.ts - Complete API Gateway with Database Integration and WebSocket\n\nimport express from 'express';\nimport { createServer } from 'http';\nimport { Server as SocketServer } from 'socket.io';\nimport cors from 'cors';\nimport sqlite3 from 'sqlite3';\nimport { open, Database } from 'sqlite';\nimport path from 'path';\n\n// Types\ninterface Race {\n  race_id: string;\n  track_name: string;\n  race_number: number | null;\n  post_time: string | null;\n  checkmate_score: number;\n  qualified: boolean;\n  trifecta_factors_json: string | null;\n  raw_data_json: string | null;\n  updated_at: string;\n}\n\ninterface AdapterStatus {\n  adapter_name: string;\n  status: string;\n  last_run: string;\n  races_found: number;\n  execution_time_ms: number;\n  error_message: string | null;\n}\n\n// Database Service\nclass DatabaseService {\n  private db: Database | null = null;\n  private dbPath: string;\n\n  constructor() {\n    this.dbPath = process.env.FORTUNA_DB_PATH || path.join(process.cwd(), '..', '..', 'shared_database', 'races.db');\n  }\n\n  async connect(): Promise<void> {\n    try {\n      this.db = await open({\n        filename: this.dbPath,\n        driver: sqlite3.Database\n      });\n      console.log(`[INFO] Connected to database: ${this.dbPath}`);\n    } catch (error) {\n      console.error('[ERROR] Failed to connect to database:', error);\n      throw error;\n    }\n  }\n\n  async getQualifiedRaces(): Promise<Race[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const races = await this.db.all<Race[]>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        WHERE qualified = 1\n        ORDER BY checkmate_score DESC, post_time ASC\n      `);\n      return races;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch qualified races:', error);\n      return [];\n    }\n  }\n\n  async getAllRaces(): Promise<Race[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const races = await this.db.all<Race[]>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        ORDER BY post_time ASC\n      `);\n      return races;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch all races:', error);\n      return [];\n    }\n  }\n\n  async getAdapterStatuses(): Promise<AdapterStatus[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const statuses = await this.db.all<AdapterStatus[]>(`\n        SELECT adapter_name, status, last_run, races_found,\n               execution_time_ms, error_message\n        FROM adapter_status\n        ORDER BY last_run DESC\n      `);\n      return statuses;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch adapter statuses:', error);\n      return [];\n    }\n  }\n\n  async getRaceById(raceId: string): Promise<Race | null> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const race = await this.db.get<Race>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        WHERE race_id = ?\n      `, raceId);\n      return race || null;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch race by ID:', error);\n      return null;\n    }\n  }\n}\n\n// Initialize Express and Socket.IO\nconst app = express();\nconst httpServer = createServer(app);\nconst io = new SocketServer(httpServer, {\n  cors: { origin: process.env.ALLOWED_ORIGINS || 'http://localhost:3000' }\n});\n\napp.use(cors());\napp.use(express.json());\n\nconst dbService = new DatabaseService();\n\n// API Endpoints\napp.get('/api/status', (req, res) => {\n  res.json({\n    status: 'online',\n    timestamp: new Date().toISOString(),\n    service: 'Checkmate API Gateway'\n  });\n});\n\napp.get('/api/races', async (req, res) => {\n  try {\n    const races = await dbService.getAllRaces();\n    res.json({ success: true, count: races.length, races });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch races' });\n  }\n});\n\napp.get('/api/races/qualified', async (req, res) => {\n  try {\n    const races = await dbService.getQualifiedRaces();\n    res.json({ success: true, count: races.length, races });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch qualified races' });\n  }\n});\n\napp.get('/api/races/:raceId', async (req, res) => {\n  try {\n    const race = await dbService.getRaceById(req.params.raceId);\n    if (race) {\n      res.json({ success: true, race });\n    } else {\n      res.status(404).json({ success: false, error: 'Race not found' });\n    }\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch race' });\n  }\n});\n\napp.get('/api/adapters/status', async (req, res) => {\n  try {\n    const statuses = await dbService.getAdapterStatuses();\n    res.json({ success: true, count: statuses.length, adapters: statuses });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch adapter statuses' });\n  }\n});\n\n// WebSocket Connection Handling\nio.on('connection', (socket) => {\n  console.log(`[WebSocket] Client connected: ${socket.id}`);\n\n  dbService.getQualifiedRaces().then(races => {\n    socket.emit('races_update', { races });\n  });\n\n  dbService.getAdapterStatuses().then(statuses => {\n    socket.emit('adapters_update', { adapters: statuses });\n  });\n\n  socket.on('disconnect', () => {\n    console.log(`[WebSocket] Client disconnected: ${socket.id}`);\n  });\n\n  socket.on('request_update', async () => {\n    const races = await dbService.getQualifiedRaces();\n    const statuses = await dbService.getAdapterStatuses();\n    socket.emit('races_update', { races });\n    socket.emit('adapters_update', { adapters: statuses });\n  });\n});\n\n// Broadcast updates to all clients periodically\nasync function broadcastUpdates() {\n  try {\n    const races = await dbService.getQualifiedRaces();\n    const statuses = await dbService.getAdapterStatuses();\n\n    io.emit('races_update', { races });\n    io.emit('adapters_update', { adapters: statuses });\n  } catch (error) {\n    console.error('[ERROR] Failed to broadcast updates:', error);\n  }\n}\n\n// Start Server\nconst PORT = process.env.PORT || 8080;\n\nasync function startServer() {\n  try {\n    await dbService.connect();\n\n    httpServer.listen(PORT, () => {\n      console.log('='.repeat(70));\n      console.log(`  Checkmate API Gateway`);\n      console.log(`  Running on port ${PORT}`);\n      console.log(`  Database: ${dbService['dbPath']}`);\n      console.log('='.repeat(70));\n    });\n\n    setInterval(broadcastUpdates, 15000);\n\n  } catch (error) {\n    console.error('[FATAL] Failed to start server:', error);\n    process.exit(1);\n  }\n}\n\n// Graceful shutdown\nprocess.on('SIGINT', async () => {\n  console.log('\\n[INFO] Shutting down gracefully...');\n  httpServer.close();\n  process.exit(0);\n});\n\nprocess.on('SIGTERM', async () => {\n  console.log('\\n[INFO] Shutting down gracefully...');\n  httpServer.close();\n  process.exit(0);\n});\n\nstartServer();",
    "web_platform/frontend/app/page.tsx": "'use client';\nimport dynamic from 'next/dynamic';\nimport React from 'react';\nimport { Tabs } from '../src/components/Tabs';\nimport { SettingsPage } from '../src/components/SettingsPage';\n\nconst LiveRaceDashboard = dynamic(\n  () => import('../src/components/LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  {\n    ssr: false,\n    loading: () => <p className=\"text-center text-xl mt-8\">Loading Dashboard...</p>\n  }\n);\n\nexport default function Home() {\n  const tabs = [\n    {\n      label: 'Dashboard',\n      content: <LiveRaceDashboard />,\n    },\n    {\n      label: 'Settings',\n      content: <SettingsPage />,\n    },\n  ];\n\n  return (\n    <main className=\"min-h-screen bg-gradient-to-br from-slate-900 via-purple-900 to-slate-900 p-8\">\n      <div className=\"max-w-7xl mx-auto space-y-8\">\n        <h1 className=\"text-4xl font-bold text-white\">Fortuna Faucet</h1>\n        <Tabs tabs={tabs} />\n      </div>\n    </main>\n  );\n}\n",
    "web_platform/frontend/src/components/EmptyState.tsx": "// web_platform/frontend/src/components/EmptyState.tsx\nimport React from 'react';\n\ninterface EmptyStateProps {\n  title: string;\n  message: string;\n  actionButton?: React.ReactNode;\n}\n\nexport const EmptyState: React.FC<EmptyStateProps> = ({ title, message, actionButton }) => {\n  return (\n    <div className=\"text-center p-8 bg-gray-800/50 border border-gray-700 rounded-lg mt-8\">\n      <svg\n        className=\"mx-auto h-12 w-12 text-gray-500\"\n        fill=\"none\"\n        viewBox=\"0 0 24 24\"\n        stroke=\"currentColor\"\n        aria-hidden=\"true\"\n      >\n        <path\n          vectorEffect=\"non-scaling-stroke\"\n          strokeLinecap=\"round\"\n          strokeLinejoin=\"round\"\n          strokeWidth={2}\n          d=\"M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z\"\n        />\n      </svg>\n      <h3 className=\"mt-2 text-xl font-semibold text-white\">{title}</h3>\n      <p className=\"mt-1 text-md text-gray-400\">\n        {message}\n      </p>\n      {actionButton && <div className=\"mt-6\">{actionButton}</div>}\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/RaceCard.tsx": "// web_platform/frontend/src/components/RaceCard.tsx\n'use client';\n\nimport React, { useState, useEffect } from 'react';\nimport type { Race, Runner } from '../types/racing';\n\n// Local types removed, now importing from '../types/racing'\n\ninterface RaceCardProps {\n  race: Race;\n}\n\nconst Countdown: React.FC<{ startTime: string }> = ({ startTime }) => {\n  const [currentTime, setCurrentTime] = useState(new Date());\n\n  useEffect(() => {\n    const timer = setInterval(() => setCurrentTime(new Date()), 1000);\n    return () => clearInterval(timer);\n  }, []);\n\n  const getCountdown = (startTimeStr: string) => {\n    const postTime = new Date(startTimeStr);\n    const diff = postTime.getTime() - currentTime.getTime();\n\n    if (diff <= 0) return { text: \"RACE COMPLETE\", color: \"text-gray-500\" };\n\n    const minutes = Math.floor(diff / 60000);\n    const seconds = Math.floor((diff % 60000) / 1000).toString().padStart(2, '0');\n\n    let color = \"text-green-400\";\n    if (minutes < 2) color = \"text-red-500 font-bold animate-pulse\";\n    else if (minutes < 10) color = \"text-yellow-400\";\n\n    return { text: `${minutes}:${seconds} to post`, color };\n  };\n\n  const countdown = getCountdown(startTime);\n\n  return (\n    <span className={`font-mono text-sm ${countdown.color}`}>{countdown.text}</span>\n  );\n};\n\nexport const RaceCard: React.FC<RaceCardProps> = ({ race }) => {\n  const activeRunners = race.runners.filter(r => !r.scratched);\n  activeRunners.sort((a, b) => a.number - b.number);\n\n  const getUniqueSourcesCount = (runners: Runner[]): number => {\n    const sources = new Set();\n    runners.forEach(runner => {\n      if (runner.odds) {\n        Object.keys(runner.odds).forEach(source => sources.add(source));\n      }\n    });\n    return sources.size;\n  };\n\n  const getBestOdds = (runner: Runner): { odds: number, source: string } | null => {\n    if (!runner.odds) return null;\n  const validOdds = Object.values(runner.odds).filter(o => o.win !== null && o.win !== undefined && o.win < 999);\n    if (validOdds.length === 0) return null;\n  const best = validOdds.reduce((min, o) => (o.win ?? 999) < (min.win ?? 999) ? o : min);\n    return { odds: best.win!, source: best.source };\n  };\n\n  return (\n    <div className={`race-card-enhanced border rounded-lg p-4 bg-gray-800 shadow-lg hover:border-purple-500 transition-all ${race.qualification_score && race.qualification_score >= 80 ? 'card-premium' : 'border-gray-700'}`}>\n      {/* Header with Smart Status Indicators */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <h2 className=\"text-2xl font-bold text-white\">{race.venue}</h2>\n            <div className=\"flex gap-2 text-sm text-gray-400\">\n              <span>Race {race.race_number}</span>\n              <span>\u2022</span>\n              <Countdown startTime={race.start_time} />\n            </div>\n            {race.favorite && (\n              <div className=\"flex items-center gap-2 mt-2 text-sm text-yellow-400\">\n                <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-4 w-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n                  <path d=\"M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z\" />\n                </svg>\n                <span className=\"font-semibold\">Favorite: {race.favorite.name}</span>\n              </div>\n            )}\n          </div>\n        </div>\n\n        {race.qualification_score && (\n          <div className={`px-4 py-2 rounded-full text-center ${\n            race.qualification_score >= 80 ? 'bg-red-500/20 text-red-400 border border-red-500/30' :\n            race.qualification_score >= 60 ? 'bg-yellow-500/20 text-yellow-400 border border-yellow-500/30' :\n            'bg-green-500/20 text-green-400 border border-green-500/30'\n          }`}>\n            <div className=\"font-bold text-lg\">{race.qualification_score.toFixed(0)}%</div>\n            <div className=\"text-xs\">Score</div>\n          </div>\n        )}\n      </div>\n\n      {/* Race Conditions Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Distance</div>\n          <div className=\"text-sm font-semibold text-white\">{race.distance || 'N/A'}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Surface</div>\n          <div className=\"text-sm font-semibold text-white\">{race.surface || 'Dirt'}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Field</div>\n          <div className=\"text-sm font-semibold text-white\">{activeRunners.length}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Sources</div>\n          <div className=\"text-sm font-semibold text-white\">{getUniqueSourcesCount(race.runners)}</div>\n        </div>\n      </div>\n\n      {/* Interactive Runner Rows */}\n      <div className=\"runners-table space-y-2\">\n        {activeRunners.map((runner, idx) => {\n          const bestOddsInfo = getBestOdds(runner);\n          return (\n            <div key={runner.number} className=\"runner-row group hover:bg-purple-500/10 transition-all rounded-md p-3\">\n              <div className=\"flex items-center justify-between\">\n                <div className=\"flex items-center gap-4 flex-1\">\n                  <div className={`w-10 h-10 rounded-full flex items-center justify-center font-bold transition-all group-hover:scale-110 text-gray-900 shadow-lg ${idx === 0 ? 'bg-gradient-to-br from-yellow-400 to-yellow-600 shadow-yellow-500/50' : idx === 1 ? 'bg-gradient-to-br from-gray-300 to-gray-500 shadow-gray-400/50' : idx === 2 ? 'bg-gradient-to-br from-orange-400 to-orange-600 shadow-orange-500/50' : 'bg-gray-700 text-gray-300'}`}>\n                    {runner.number}\n                  </div>\n                  <div className=\"flex flex-col\">\n                    <span className=\"font-bold text-white text-lg\">{runner.name}</span>\n                    <div className=\"flex gap-3 text-sm text-gray-400\">\n                      {runner.jockey && <span>J: {runner.jockey}</span>}\n                      {runner.trainer && <span>T: {runner.trainer}</span>}\n                    </div>\n                  </div>\n                </div>\n                {bestOddsInfo && (\n                  <div className=\"text-right\">\n                    <div className=\"text-2xl font-bold text-emerald-400\">{bestOddsInfo.odds.toFixed(2)}</div>\n                    <div className=\"text-xs text-gray-500\">via {bestOddsInfo.source}</div>\n                  </div>\n                )}\n              </div>\n            </div>\n          );\n        })}\n      </div>\n    </div>\n  );\n};",
    "web_platform/frontend/src/components/StatusDetailModal.tsx": "// web_platform/frontend/src/components/StatusDetailModal.tsx\nimport React from 'react';\n\ninterface StatusDetailModalProps {\n  isOpen: boolean;\n  onClose: () => void;\n  status: {\n      title: string;\n      details: string | Record<string, any>;\n  };\n}\n\nexport const StatusDetailModal: React.FC<StatusDetailModalProps> = ({ isOpen, onClose, status }) => {\n  if (!isOpen) {\n    return null;\n  }\n\n  const { title, details } = status;\n  const isDetailsString = typeof details === 'string';\n\n  // Determine status color only if details is an object with a status property\n  const statusColor = !isDetailsString && (details.status === 'SUCCESS' || details.status === 'OK')\n    ? 'text-green-400'\n    : 'text-gray-300'; // Default color\n\n  return (\n    <div className=\"fixed inset-0 bg-black/60 flex items-center justify-center z-50\" onClick={onClose}>\n      <div className=\"bg-gray-800 border border-gray-700 rounded-lg shadow-xl p-6 max-w-lg w-full\" onClick={e => e.stopPropagation()}>\n        <div className=\"flex justify-between items-start mb-4\">\n          <h3 className=\"text-xl font-bold text-white\">{title}</h3>\n          <button onClick={onClose} className=\"text-gray-400 hover:text-white\">&times;</button>\n        </div>\n        <div className=\"space-y-2 text-sm max-h-96 overflow-y-auto pr-2\">\n            {isDetailsString ? (\n                <div className=\"text-gray-300 whitespace-pre-wrap bg-gray-900/50 p-4 rounded-md\">{details}</div>\n            ) : (\n                Object.entries(details).map(([key, value]) => (\n                    <div key={key} className=\"grid grid-cols-3 gap-4 border-b border-gray-700/50 py-2\">\n                    <span className=\"font-semibold text-gray-400 capitalize\">{key.replace(/_/g, ' ')}</span>\n                    <span className={`col-span-2 break-words ${key === 'status' ? statusColor : 'text-gray-300'}`}>\n                        {typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value)}\n                    </span>\n                    </div>\n                ))\n            )}\n        </div>\n        <button\n          onClick={onClose}\n          className=\"bg-gray-600 hover:bg-gray-700 text-white font-bold py-2 px-4 rounded w-full mt-6\"\n        >\n          Close\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n",
    "web_platform/frontend/tsconfig.json": "{\n  \"compilerOptions\": {\n    \"lib\": [\n      \"dom\",\n      \"dom.iterable\",\n      \"esnext\"\n    ],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,\n    \"noEmit\": true,\n    \"incremental\": true,\n    \"esModuleInterop\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"preserve\",\n    \"plugins\": [\n      {\n        \"name\": \"next\"\n      }\n    ]\n  },\n  \"include\": [\n    \"next-env.d.ts\",\n    \".next/types/**/*.ts\",\n    \"**/*.ts\",\n    \"**/*.tsx\",\n    \"out/types/**/*.ts\"\n  ],\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\n",
    "web_service/__init__.py": "\"\"\"Web service package for Fortuna Faucet.\"\"\"\n__version__ = \"1.0.0\"\n__all__ = [\"backend\"]\n",
    "web_service/backend/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairExchange\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "web_service/backend/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass EquibaseAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        d = datetime.strptime(date, \"%Y-%m-%d\").date()\n        index_url = f\"/entries/Entries.cfm?ELEC_DATE={d.month}/{d.day}/{d.year}&STYLE=EQB\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Equibase index page\", url=index_url)\n            return None\n\n        parser = HTMLParser(index_response.text)\n        track_links = [\n            link.attributes[\"href\"]\n            for link in parser.css(\"div.track-information a\")\n            if \"race=\" not in link.attributes.get(\"href\", \"\")\n        ]\n\n        async def get_race_links_from_track(track_url: str):\n            response = await self.make_request(self.http_client, \"GET\", track_url, headers=self._get_headers())\n            if not response:\n                return []\n            parser = HTMLParser(response.text)\n            return [link.attributes[\"href\"] for link in parser.css(\"a.program-race-link\")]\n\n        tasks = [get_race_links_from_track(link) for link in track_links]\n        results = await asyncio.gather(*tasks)\n        race_links = [f\"{self.base_url}{link}\" for sublist in results for link in sublist]\n\n        async def fetch_single_html(race_url: str):\n            response = await self.make_request(self.http_client, \"GET\", race_url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first(\"div.track-information strong\")\n                if not venue_node:\n                    continue\n                venue = clean_text(venue_node.text())\n\n                race_number_node = parser.css_first(\"div.race-information strong\")\n                if not race_number_node:\n                    continue\n                race_number_text = race_number_node.text().replace(\"Race\", \"\").strip()\n                if not race_number_text.isdigit():\n                    continue\n                race_number = int(race_number_text)\n\n                post_time_node = parser.css_first(\"p.post-time span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    if runner := self._parse_runner(node):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first(\"td:nth-child(1)\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            name_node = node.css_first(\"td:nth-child(3)\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            odds_node = node.css_first(\"td:nth-child(10)\")\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError, IndexError):\n            self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n            return None\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        time_part = time_str.split(\" \")[-2] + \" \" + time_str.split(\" \")[-1]\n        dt_str = f\"{date_str} {time_part}\"\n        return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "web_service/backend/adapters/horseracingnation_adapter.py": "# python_service/adapters/horseracingnation_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass HorseRacingNationAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for horseracingnation.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"HorseRacingNation\"\n    BASE_URL = \"https://www.horseracingnation.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/racing_and_sports_adapter.py": "# python_service/adapters/racing_and_sports_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSports\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw meetings data from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            self.http_client,\n            \"GET\",\n            \"v1/racing/meetings\",\n            headers=headers,\n            params=params,\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        all_races = []\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in RacingAndSports response or invalid format.\")\n            return all_races\n\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse RacingAndSports race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str)\n        except (ValueError, TypeError):\n            self.logger.warning(\n                \"Invalid start time format for RacingAndSports race\",\n                start_time_str=start_time_str,\n                race_id=race_id,\n            )\n            return None\n\n        return Race(\n            id=f\"ras_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/tab_adapter.py": "# python_service/adapters/tab_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TabAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for tab.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/twinspires_adapter.py": "# python_service/adapters/twinspires_adapter.py\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for twinspires.com.\n    This is a placeholder for a full implementation using the discovered JSON API.\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        [MODIFIED FOR OFFLINE DEVELOPMENT]\n        Reads HTML content from a local fixture file instead of making a live API call.\n        This is a temporary measure to allow development while the live API is blocking requests.\n        \"\"\"\n        # Read the local HTML fixture\n        try:\n            with open(\"tests/fixtures/twinspires_sample.html\", \"r\") as f:\n                html_content = f.read()\n        except FileNotFoundError:\n            self.logger.error(\"TwinSpires test fixture not found.\")\n            return None\n\n        # To maintain the data structure the parser expects, we will create a mock\n        # raw_data object that resembles the original API response, but includes\n        # the HTML content.\n        return {\n            \"html_content\": html_content,\n            \"mock_track_data\": {\"trackId\": \"cd\", \"trackName\": \"Churchill Downs\", \"raceType\": \"Thoroughbred\"},\n            \"mock_race_card\": {\"raceNumber\": 5, \"postTime\": \"2025-10-26T16:30:00Z\"},\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        [MODIFIED FOR OFFLINE DEVELOPMENT]\n        Parses race and runner data from the mock raw_data object, which now\n        includes the HTML content from the local fixture.\n        \"\"\"\n        if not raw_data or \"html_content\" not in raw_data:\n            return []\n\n        self.logger.info(\"Parsing TwinSpires data from local fixture.\")\n\n        html_content = raw_data[\"html_content\"]\n        track = raw_data[\"mock_track_data\"]\n        race_card = raw_data[\"mock_race_card\"]\n\n        # Parse the runners from the HTML content\n        runners = self._parse_runners_from_html(html_content)\n\n        try:\n            start_time = datetime.fromisoformat(race_card.get(\"postTime\").replace(\"Z\", \"+00:00\"))\n\n            race = Race(\n                id=f\"ts_{track.get('trackId')}_{race_card.get('raceNumber')}\",\n                venue=track.get(\"trackName\"),\n                race_number=race_card.get(\"raceNumber\"),\n                start_time=start_time,\n                discipline=track.get(\"raceType\", \"Unknown\"),\n                runners=runners,\n                source=self.SOURCE_NAME,\n            )\n            return [race]\n        except Exception as e:\n            self.logger.warning(\n                \"Failed to parse race card from mock data.\",\n                error=e,\n                exc_info=True,\n            )\n            return []\n\n    def _parse_runners_from_html(self, html_content: str) -> List[Runner]:\n        \"\"\"Parses runner data from a race card's HTML content.\"\"\"\n        runners = []\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        runner_elements = soup.select(\"li.runner\")\n\n        for element in runner_elements:\n            try:\n                scratched = \"scratched\" in element.get(\"class\", [])\n\n                number_tag = element.select_one(\"span.runner-number\")\n                name_tag = element.select_one(\"span.runner-name\")\n                odds_tag = element.select_one(\"span.runner-odds\")\n\n                if not all([number_tag, name_tag, odds_tag]):\n                    continue\n\n                number = int(number_tag.text.strip())\n                name = name_tag.text.strip()\n                odds_str = odds_tag.text.strip()\n\n                odds = {}\n                if not scratched and odds_str not in [\"SCR\", \"\"]:\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    if win_odds:\n                        odds[self.SOURCE_NAME] = OddsData(\n                            win=win_odds,\n                            source=self.SOURCE_NAME,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=number,\n                        name=name,\n                        scratched=scratched,\n                        odds=odds,\n                    )\n                )\n            except (ValueError, TypeError) as e:\n                self.logger.warning(\"Failed to parse a runner, skipping.\", error=e, exc_info=True)\n                continue\n\n        return runners\n\n    async def _get_races_async(self, date: str) -> List[Race]:\n        raw_data = await self._fetch_data(date)\n        return self._parse_races(raw_data)\n\n    def get_races(self, date: str) -> List[Race]:\n        \"\"\"\n        Orchestrates the fetching and parsing of race data for a given date.\n        This method will be called by the FortunaEngine.\n        \"\"\"\n        self.logger.info(f\"Getting races for {date} from {self.SOURCE_NAME}\")\n        # This is a synchronous wrapper for the async orchestrator\n        # It's a temporary measure to allow me to see the API response.\n        import asyncio\n\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n        races = loop.run_until_complete(self._get_races_async(date))\n        return races\n",
    "web_service/backend/api.py": "# web_service/backend/api.py\n# Reconstructed by Jules to merge features from python_service with web_service structure.\n\nimport asyncio\nimport os\nimport sys\nfrom contextlib import asynccontextmanager\nfrom typing import List, Optional\n\nimport structlog\nfrom fastapi import Depends, FastAPI, HTTPException, Query, Request, WebSocket\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom starlette.websockets import WebSocketDisconnect\n\n# Corrected imports for web_service.backend\nfrom .config import get_settings\nfrom .engine import OddsEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .middleware.error_handler import UserFriendlyException, user_friendly_exception_handler, validation_exception_handler\nfrom .models import AggregatedResponse, QualifiedRacesResponse, Race\nfrom .security import verify_api_key\n\nlog = structlog.get_logger()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Manages application startup and shutdown events.\"\"\"\n    configure_logging()\n    log.info(\"Lifespan: Startup sequence initiated.\")\n\n    settings = get_settings()\n    engine = OddsEngine(config=settings)\n    app.state.engine = engine\n\n    log.info(\"Lifespan: Engine initialized successfully. Startup complete.\")\n    yield\n    log.info(\"Lifespan: Shutdown sequence initiated.\")\n    if hasattr(app.state, \"engine\") and app.state.engine:\n        await app.state.engine.close()\n    log.info(\"Lifespan: Shutdown sequence complete.\")\n\n# --- FastAPI App Initialization ---\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI(\n    title=\"Fortuna Faucet Web Service API\",\n    version=\"3.0\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\",\n)\n\napp.state.limiter = limiter\napp.add_middleware(SlowAPIMiddleware)\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\napp.add_exception_handler(UserFriendlyException, user_friendly_exception_handler)\napp.include_router(health_router)\n\n# Add CORS middleware for frontend development\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=get_settings().ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# --- Robust Pathing for Frozen Executables ---\ndef resource_path(relative_path: str) -> str:\n    \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n    if getattr(sys, 'frozen', False):\n        # PyInstaller creates a temp folder and stores path in _MEIPASS\n        base_path = sys._MEIPASS\n    else:\n        # In development, the base path is the project root.\n        # This assumes the script is run from the project root.\n        base_path = os.path.abspath(\".\")\n\n    return os.path.join(base_path, relative_path)\n\n\n# --- Static File Serving Logic (Corrected for PyInstaller) ---\nif os.getenv(\"FORTUNA_MODE\") == \"webservice\":\n    log.info(\"Application starting in 'webservice' mode, attempting to serve static files.\")\n\n    # Use the robust resource_path function to find the 'ui' directory.\n    # The spec file bundles 'web_service/frontend/out' into the 'ui' folder in the executable's root.\n    static_dir_key = \"ui\" if getattr(sys, 'frozen', False) else \"web_service/frontend/out\"\n    static_dir = resource_path(static_dir_key)\n    log.info(\"Resolved static files directory\", path=static_dir)\n\n    if not os.path.isdir(static_dir):\n        log.error(\"Static files directory not found! Frontend will not be served.\", path=static_dir)\n    else:\n        log.info(\"Mounting StaticFiles to serve the frontend.\")\n        app.mount(\"/\", StaticFiles(directory=static_dir, html=True), name=\"static\")\nelse:\n    log.info(\"FORTUNA_MODE is not 'webservice', static files will not be served by this API.\")\n\n\n# --- Dependency Injection ---\ndef get_engine(request: Request) -> OddsEngine:\n    if not hasattr(request.app.state, \"engine\") or request.app.state.engine is None:\n        raise HTTPException(status_code=503, detail=\"The OddsEngine is not available.\")\n    return request.app.state.engine\n\n# --- API Endpoints (Restored and Adapted) ---\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[str] = Query(None, description=\"Date in YYYY-MM-DD format.\"),\n    source: Optional[str] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    \"\"\"Fetches all race data for a given date from all or a specific source.\"\"\"\n    return await engine.fetch_all_odds(race_date, source)\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[str] = Query(None, description=\"Date in YYYY-MM-DD format.\"),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    # Example query parameters for an analyzer\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_odds: float = Query(2.0, ge=1.0),\n):\n    \"\"\"Fetches all race data and runs a specific analyzer to find qualified races.\"\"\"\n    # This is a simplified version; a real implementation would have a dynamic analyzer engine.\n    # For now, we'll just fetch and return all races as \"qualified\".\n    response = await engine.fetch_all_odds(race_date)\n    races = [Race(**r) for r in response.get(\"races\", [])]\n    return QualifiedRacesResponse(qualified_races=races, analysis_metadata={\"analyzer\": analyzer_name})\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    \"\"\"Gets the current status of all configured data adapters.\"\"\"\n    return engine.get_all_adapter_statuses()\n\n# Add other endpoints as needed, following the pattern above.\n",
    "web_service/backend/core/exceptions.py": "# python_service/core/exceptions.py\n\"\"\"\nCustom, application-specific exceptions for the Fortuna Faucet service.\n\nThis module defines a hierarchy of exception classes to provide standardized\nerror handling, particularly for the data adapter layer. Using these specific\nexceptions instead of generic ones allows for more precise error handling and\nclearer logging throughout the application.\n\"\"\"\n\n\nclass FortunaException(Exception):\n    \"\"\"Base class for all custom exceptions in this application.\"\"\"\n\n    pass\n\n\nclass AdapterError(FortunaException):\n    \"\"\"Base class for all adapter-related errors.\"\"\"\n\n    def __init__(self, adapter_name: str, message: str):\n        self.adapter_name = adapter_name\n        super().__init__(f\"[{adapter_name}] {message}\")\n\n\nclass AdapterRequestError(AdapterError):\n    \"\"\"Raised for general network or request-related issues.\"\"\"\n\n    pass\n\n\nclass AdapterHttpError(AdapterRequestError):\n    \"\"\"Raised for unsuccessful HTTP responses (e.g., 4xx or 5xx status codes).\"\"\"\n\n    def __init__(self, adapter_name: str, status_code: int, url: str):\n        self.status_code = status_code\n        self.url = url\n        message = f\"Received HTTP {status_code} from {url}\"\n        super().__init__(adapter_name, message)\n\n\nclass AdapterAuthError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 401/403 errors, indicating an auth failure.\"\"\"\n\n    pass\n\n\nclass AdapterRateLimitError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 429 errors, indicating a rate limit has been hit.\"\"\"\n\n    pass\n\n\nclass AdapterTimeoutError(AdapterRequestError):\n    \"\"\"Raised when a request to an external API times out.\"\"\"\n\n    pass\n\n\nclass AdapterConnectionError(AdapterRequestError):\n    \"\"\"Raised for DNS lookup failures or refused connections.\"\"\"\n\n    pass\n\n\nclass AdapterConfigError(AdapterError):\n    \"\"\"Raised when an adapter is missing necessary configuration (e.g., an API key).\"\"\"\n\n    pass\n\n\nclass AdapterParsingError(AdapterError):\n    \"\"\"Raised when an adapter fails to parse the response from an API.\"\"\"\n\n    pass\n",
    "web_service/backend/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\n\n# from .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_cls.__name__}\")\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_cls.__name__}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters.append(adapter_instance)\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_cls.__name__,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_cls.__name__}\",\n                        exc_info=True,\n                    )\n\n            # Special case for BetfairDataScientistAdapter with extra args - DISABLED\n            # try:\n            #     bds_adapter = BetfairDataScientistAdapter(\n            #         model_name=\"ThoroughbredModel\",\n            #         url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n            #         config=self.config,\n            #     )\n            #     if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n            #         bds_adapter.enable_manual_override(manual_override_manager)\n            #     self.adapters.append(bds_adapter)\n            # except Exception:\n            #     self.logger.warning(\n            #         \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n            #         exc_info=True,\n            #     )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            races = [Race(**race_data) for race_data in race_data_list]\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached and broadcasted via WebSocket.\n        \"\"\"\n        # Construct a cache key\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        log.info(\"Cache miss for fetch_all_odds\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._fetch_with_semaphore(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for i, result in enumerate(results):\n            adapter = target_adapters[i]\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed with an unhandled exception\", adapter=adapter.source_name, error=result)\n                errors.append({\n                    \"adapter_name\": adapter.source_name,\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                    \"attempted_url\": \"Unknown\"\n                })\n                source_infos.append({\n                    \"name\": adapter.source_name,\n                    \"status\": \"FAILED\",\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                })\n            else:\n                _adapter_name, adapter_result, _duration = result\n                source_info = adapter_result.get(\"source_info\", {})\n                source_infos.append(source_info)\n                if source_info.get(\"status\") == \"SUCCESS\":\n                    all_races.extend(adapter_result.get(\"races\", []))\n                else:\n                    errors.append({\n                        \"adapter_name\": adapter.source_name,\n                        \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                        \"attempted_url\": source_info.get(\"attempted_url\")\n                    })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            errors=errors,\n            source_info=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n                \"total_errors\": len(errors),\n            },\n        )\n\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Set the result in the cache\n        await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "web_service/backend/fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport structlog\n\nfrom .analyzer import AnalyzerEngine\nfrom .config import get_settings\nfrom .engine import OddsEngine\nfrom .etl import run_etl_for_yesterday\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = OddsEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n        try:\n            background_tasks = set()  # Create a dummy set for background tasks\n            aggregated_data = await self.odds_engine.fetch_all_odds(today_str, background_tasks)\n            all_races = aggregated_data.get(\"races\", [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer(\"trifecta\")\n            qualified_races_result = analyzer.qualify_races(all_races)\n            qualified_races_list = qualified_races_result.get(\"races\", [])\n            log.info(\n                \"Watchman: Initial target acquisition and ranking complete\",\n                target_count=len(qualified_races_list),\n            )\n\n            # Log the top targets for better observability\n            for race in qualified_races_list[:5]:\n                log.info(\n                    \"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat(),\n                )\n            return qualified_races_list\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        # active_targets = list(targets)\n\n        # from python_service.adapters.betfair_adapter import BetfairAdapter\n        # async with LiveOddsMonitor(betfair_adapter=BetfairAdapter(config=self.settings)) as live_monitor:\n        #     async with httpx.AsyncClient() as client:\n        #         while active_targets:\n        #             now = datetime.now(timezone.utc)\n\n        #             # Find races that are within the 5-minute monitoring window\n        #             races_to_monitor = [\n        #                 r\n        #                 for r in active_targets\n        #                 if r.start_time.replace(tzinfo=timezone.utc) > now\n        #                 and r.start_time.replace(tzinfo=timezone.utc)\n        #                 < now + timedelta(minutes=5)\n        #             ]\n\n        #             if races_to_monitor:\n        #                 for race in races_to_monitor:\n        #                     log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n        #                         race_id=race.id,\n        #                         venue=race.venue,\n        #                         score=race.qualification_score\n        #                     )\n        #                     updated_race = await live_monitor.monitor_race(race, client)\n        #                     log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n        #                     # Remove from target list to prevent re-monitoring\n        #                     active_targets = [t for t in active_targets if t.id != race.id]\n\n        #             if not active_targets:\n        #                 break # Exit loop if all targets are processed\n\n        #             await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        try:\n            initial_targets = await self.get_initial_targets()\n            if initial_targets:\n                await self.run_tactical_monitoring(initial_targets)\n            else:\n                log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n        finally:\n            await self.odds_engine.close()\n\n        # Run ETL for yesterday's data after all other operations are complete\n        try:\n            log.info(\"Starting daily ETL process for Scribe's Archives...\")\n            run_etl_for_yesterday()\n            log.info(\"Daily ETL process completed successfully.\")\n        except Exception:\n            log.error(\"Daily ETL process failed.\", exc_info=True)\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\n\nasync def main():\n    from .logging_config import configure_logging\n\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "web_service/backend/logging_config.py": "import logging\nimport sys\nimport structlog\n\ndef configure_logging():\n    \"\"\"\n    Configures structured logging for the application using structlog.\n    \"\"\"\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout, format=\"%(message)s\")\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.PositionalArgumentsFormatter(),\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "web_service/backend/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Annotated\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import Field\nfrom pydantic import WrapSerializer\n\n\ndef decimal_serializer(value: Decimal, handler: Callable[[Decimal], Any]) -> Any:\n    \"\"\"Custom serializer for Decimal to float conversion.\"\"\"\n    return float(value)\n\n\nJsonDecimal = Annotated[Decimal, WrapSerializer(decimal_serializer, when_used=\"json\")]\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[JsonDecimal] = None\n    place: Optional[JsonDecimal] = None\n    show: Optional[JsonDecimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n    is_error_placeholder: bool = Field(False, alias=\"isErrorPlaceholder\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AdapterError(FortunaBaseModel):\n    adapter_name: str = Field(..., alias=\"adapterName\")\n    error_message: str = Field(..., alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    errors: List[AdapterError]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n\n\nclass ManualParseRequest(FortunaBaseModel):\n    adapter_name: str\n    html_content: str = Field(..., max_length=5_000_000)  # ~5MB limit\n",
    "web_service/backend/requirements.in": "#\n# Fortuna Faucet - High-Level Backend Dependencies\n# This is the source of truth. Run 'pip-compile' to generate requirements.txt.\n#\n\n# --- Core Application Framework (Hard Pins) ---\nfastapi\nuvicorn==0.30.1\ncryptography\n\n# --- Core Application Dependencies (Flexible) ---\ntenacity\npydantic-settings\nhttpx[http2]\nselectolax==0.4.0\nbeautifulsoup4\nslowapi\nredis\npandas\nnumpy\nscipy\naiosqlite\nSQLAlchemy\ngreenlet==3.0.3\npsycopg2-binary\nstructlog\ncertifi\n\n# --- Desktop & OS Integration (Flexible) ---\npsutil\npywin32 ; sys_platform == 'win32'\nwindows-toasts ; sys_platform == 'win32'\nkeyring\npynput ; sys_platform == 'win32'\n\n# --- Development & Testing Dependencies ---\npytest\npytest-asyncio\nblack\n\n# --- Build Dependencies (Hard Pins) ---\n# Downgraded to 6.5.0 to resolve pywin32 bundling regression.\npyinstaller==6.5.0\nwheel\nsetuptools>=78.1.1,<81\npip-tools\nrequests>=2.32.4\nurllib3>=2.5.0\n",
    "web_service/backend/requirements.txt": "#\n# This file is autogenerated by pip-compile with Python 3.9\n# by the following command:\n#\n#    pip-compile --output-file=web_service/backend/requirements.txt web_service/backend/requirements.in\n#\naiosqlite==0.17.0\n    # via -r web_service/backend/requirements.in\naltgraph==0.17.4\n    # via pyinstaller\nannotated-types==0.7.0\n    # via pydantic\nanyio==3.7.1\n    # via\n    #   httpx\n    #   starlette\n    #   watchfiles\nasync-timeout==5.0.1\n    # via redis\nbeautifulsoup4==4.12.3\n    # via -r web_service/backend/requirements.in\nblack==24.4.2\n    # via -r web_service/backend/requirements.in\nbuild==1.2.1\n    # via pip-tools\ncertifi==2024.7.4\n    # via\n    #   -r web_service/backend/requirements.in\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==1.16.0\n    # via cryptography\ncharset-normalizer==3.3.2\n    # via requests\nclick==8.1.7\n    # via\n    #   black\n    #   pip-tools\n    #   rich-toolkit\n    #   typer\n    #   uvicorn\ncryptography==42.0.8\n    # via\n    #   -r web_service/backend/requirements.in\n    #   secretstorage\ndeprecated==1.2.14\n    # via limits\ndnspython==2.7.0\n    # via email-validator\nemail-validator==2.3.0\n    # via fastapi\nexceptiongroup==1.3.1\n    # via\n    #   anyio\n    #   pytest\nfastapi==0.111.0\n    # via -r web_service/backend/requirements.in\nfastapi-cli==0.0.20\n    # via fastapi\ngreenlet==3.0.3\n    # via\n    #   -r web_service/backend/requirements.in\n    #   sqlalchemy\nh11==0.14.0\n    # via\n    #   httpcore\n    #   uvicorn\nh2==4.1.0\n    # via httpx\nhpack==4.0.0\n    # via h2\nhttpcore==1.0.5\n    # via httpx\nhttptools==0.7.1\n    # via uvicorn\nhttpx[http2]==0.27.0\n    # via\n    #   -r web_service/backend/requirements.in\n    #   fastapi\nhyperframe==6.0.1\n    # via h2\nidna==3.7\n    # via\n    #   anyio\n    #   email-validator\n    #   httpx\n    #   requests\nimportlib-metadata==8.7.1\n    # via\n    #   build\n    #   keyring\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\niniconfig==2.0.0\n    # via pytest\njaraco-classes==3.4.0\n    # via keyring\njaraco-context==4.3.0\n    # via keyring\njaraco-functools==4.3.0\n    # via keyring\njeepney==0.8.0\n    # via\n    #   keyring\n    #   secretstorage\njinja2==3.1.6\n    # via fastapi\nkeyring==25.2.1\n    # via -r web_service/backend/requirements.in\nlimits==3.14.1\n    # via slowapi\nmarkdown-it-py==3.0.0\n    # via rich\nmarkupsafe==3.0.3\n    # via jinja2\nmdurl==0.1.2\n    # via markdown-it-py\nmore-itertools==10.3.0\n    # via\n    #   jaraco-classes\n    #   jaraco-functools\nmypy-extensions==1.0.0\n    # via black\nnumpy==1.23.5\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pandas\n    #   scipy\norjson==3.11.5\n    # via fastapi\npackaging==24.1\n    # via\n    #   black\n    #   build\n    #   limits\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\n    #   pytest\npandas==1.5.3\n    # via -r web_service/backend/requirements.in\npathspec==0.12.1\n    # via black\npip-tools==7.4.1\n    # via -r web_service/backend/requirements.in\nplatformdirs==4.2.2\n    # via black\npluggy==1.5.0\n    # via pytest\npsutil==5.9.8\n    # via -r web_service/backend/requirements.in\npsycopg2-binary==2.9.9\n    # via -r web_service/backend/requirements.in\npycparser==2.22\n    # via cffi\npydantic==2.8.2\n    # via\n    #   fastapi\n    #   pydantic-settings\npydantic-core==2.20.1\n    # via pydantic\npydantic-settings==2.3.4\n    # via -r web_service/backend/requirements.in\npygments==2.18.0\n    # via rich\npyinstaller==6.5.0\n    # via -r web_service/backend/requirements.in\npyinstaller-hooks-contrib==2024.6\n    # via pyinstaller\npyproject-hooks==1.1.0\n    # via\n    #   build\n    #   pip-tools\npytest==8.2.2\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pytest-asyncio\npytest-asyncio==0.23.7\n    # via -r web_service/backend/requirements.in\npython-dateutil==2.9.0.post0\n    # via pandas\npython-dotenv==1.0.1\n    # via\n    #   pydantic-settings\n    #   uvicorn\npython-multipart==0.0.20\n    # via fastapi\npytz==2024.1\n    # via pandas\npyyaml==6.0.3\n    # via uvicorn\nredis==5.0.6\n    # via -r web_service/backend/requirements.in\nrequests==2.32.5\n    # via -r web_service/backend/requirements.in\nrich==14.2.0\n    # via\n    #   rich-toolkit\n    #   typer\nrich-toolkit==0.17.1\n    # via fastapi-cli\nscipy==1.10.1\n    # via -r web_service/backend/requirements.in\nsecretstorage==3.3.3\n    # via keyring\nselectolax==0.4.0\n    # via -r web_service/backend/requirements.in\nshellingham==1.5.4\n    # via typer\nsix==1.16.0\n    # via python-dateutil\nslowapi==0.1.9\n    # via -r web_service/backend/requirements.in\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nsoupsieve==2.5\n    # via beautifulsoup4\nsqlalchemy==1.4.52\n    # via -r web_service/backend/requirements.in\nstarlette==0.37.2\n    # via fastapi\nstructlog==24.2.0\n    # via -r web_service/backend/requirements.in\ntenacity==8.2.3\n    # via -r web_service/backend/requirements.in\ntomli==2.3.0\n    # via\n    #   black\n    #   build\n    #   fastapi-cli\n    #   pip-tools\n    #   pytest\ntyper==0.21.0\n    # via fastapi-cli\ntyping-extensions==4.12.2\n    # via\n    #   aiosqlite\n    #   black\n    #   exceptiongroup\n    #   fastapi\n    #   limits\n    #   pydantic\n    #   pydantic-core\n    #   rich-toolkit\n    #   starlette\n    #   typer\n    #   uvicorn\nujson==5.11.0\n    # via fastapi\nurllib3==2.6.2\n    # via\n    #   -r web_service/backend/requirements.in\n    #   requests\nuvicorn==0.30.1\n    # via\n    #   -r web_service/backend/requirements.in\n    #   fastapi\n    #   fastapi-cli\nhttptools==0.7.1\n    # via uvicorn\nwebsockets==15.0.1\n    # via uvicorn\nwatchfiles==1.1.1\n    # via uvicorn\nwebsockets==15.0.1\n    # via uvicorn\nwheel==0.43.0\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pip-tools\nwrapt==1.16.0\n    # via deprecated\nzipp==3.23.0\n    # via importlib-metadata\n\n# The following packages are considered to be unsafe in a requirements file:\n# pip\n# setuptools\n",
    "web_service/backend/service_entry.py": "import win32serviceutil\nimport win32service\nimport win32event\nimport servicemanager\nimport socket\nimport sys\nimport os\nimport uvicorn\nimport multiprocessing\nimport threading\nfrom pathlib import Path\nimport asyncio\nimport logging\nfrom web_service.backend.windows_compat import setup_windows_event_loop\n\n# PATCH #2: This entire file is replaced to ensure correct service behavior.\n\n# --- UTF-8 Logging Configuration ---\n# Must be configured BEFORE any logging calls.\ndef configure_utf8_logging():\n    \"\"\"Configures stdout/stderr to use UTF-8 encoding.\"\"\"\n    if sys.stdout and sys.stdout.encoding != 'utf-8':\n        sys.stdout.reconfigure(encoding='utf-8')\n    if sys.stderr and sys.stderr.encoding != 'utf-8':\n        sys.stderr.reconfigure(encoding='utf-8')\n    # Basic logging config\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# --- Path Bootstrapping ---\ndef _bootstrap_path():\n    \"\"\"\n    Ensures the application's root directories are on the Python path.\n    This is critical for PyInstaller's frozen executables to find modules.\n    \"\"\"\n    if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n        # Running in a PyInstaller bundle.\n        sys.path.insert(0, sys._MEIPASS)\n    else:\n        # Running from source.\n        project_root = str(Path(__file__).parent.parent.parent)\n        sys.path.insert(0, project_root)\n\n# CRITICAL: Apply path and logging fixes at the earliest possible moment.\n_bootstrap_path()\nconfigure_utf8_logging()\nlog = logging.getLogger(__name__)\n\n# --- Resilient App Import ---\ntry:\n    log.info(\"Attempting to import 'app' from api...\")\n    from web_service.backend.api import app\n    log.info(\"Successfully imported 'app'.\")\nexcept (ImportError, ModuleNotFoundError) as e:\n    log.error(f\"FATAL: All import attempts failed: {e}. Cannot start service.\")\n    sys.exit(1)\n\nclass FortunaSvc(win32serviceutil.ServiceFramework):\n    _svc_name_ = 'FortunaWebService'\n    _svc_display_name_ = 'Fortuna Faucet Backend Service'\n    _svc_description_ = 'Data aggregation and analysis engine.'\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.server = None\n        self.server_thread = None\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        win32event.SetEvent(self.hWaitStop)\n        if self.server:\n            # Uvicorn's server has a 'should_exit' flag that can be set\n            # to signal a graceful shutdown.\n            self.server.should_exit = True\n            log.info(\"Server shutdown signaled.\")\n\n    def SvcDoRun(self):\n        # CRITICAL: Change CWD to the executable's directory.\n        # The default for a Windows Service is C:\\Windows\\System32,\n        # which will break all relative path logic.\n        if getattr(sys, 'frozen', False):\n            exe_path = os.path.dirname(sys.executable)\n            os.chdir(exe_path)\n            log.info(f\"Service running in frozen mode. CWD set to: {exe_path}\")\n\n        # CRITICAL: Set the asyncio event loop policy for Windows.\n        setup_windows_event_loop()\n\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, '')\n        )\n\n        log.info(f\"Starting {self._svc_display_name_}...\")\n\n        try:\n            # Configure Uvicorn to run the FastAPI app.\n            # Host '0.0.0.0' is often more reliable in containerized/CI environments.\n            config = uvicorn.Config(\n                app,\n                host='0.0.0.0',\n                port=8102,\n                log_config=None, # We are using our own logger\n                reload=False\n            )\n            self.server = uvicorn.Server(config)\n\n            # Run the server in a separate thread so we can listen for stop events.\n            self.server_thread = threading.Thread(target=self.server.run)\n            self.server_thread.start()\n            log.info(\"Uvicorn server thread started.\")\n\n            # Wait for the stop signal.\n            win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n            log.info(\"Stop signal received. Initiating shutdown...\")\n\n        except Exception as e:\n            # Log any exceptions that occur during service startup or execution.\n            log.error(f\"A critical error occurred in SvcDoRun: {e}\", exc_info=True)\n            self.SvcStop() # Attempt a graceful stop on error.\n        finally:\n            # Ensure the server thread is joined upon exit.\n            if self.server_thread and self.server_thread.is_alive():\n                self.server_thread.join()\n            log.info(f\"{self._svc_display_name_} has stopped.\")\n\n\ndef main():\n    \"\"\"Main entry point for command-line interaction.\"\"\"\n    # This support is critical for PyInstaller.\n    multiprocessing.freeze_support()\n\n    if len(sys.argv) == 1:\n        # Standard service startup logic\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaSvc)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        # Command-line arguments like 'install', 'start', 'stop', 'remove'.\n        win32serviceutil.HandleCommandLine(FortunaSvc)\n\nif __name__ == '__main__':\n    main()\n",
    "web_service/backend/utils/odds.py": "# Centralized odds parsing utility, created by Operation: The A+ Trifecta\nfrom decimal import Decimal\nfrom decimal import InvalidOperation\nfrom typing import Optional\nfrom typing import Union\n\n\ndef parse_odds_to_decimal(odds: Union[str, int, float, None]) -> Optional[Decimal]:\n    \"\"\"\n    Parse various odds formats to Decimal for precise financial calculations.\n    Handles fractional, decimal, and special cases ('EVS', 'SP', etc.).\n    Returns None for unparseable or invalid values.\n    \"\"\"\n    if odds is None:\n        return None\n\n    if isinstance(odds, (int, float)):\n        return Decimal(str(odds))\n\n    odds_str = str(odds).strip().upper()\n\n    SPECIAL_CASES = {\n        \"EVS\": Decimal(\"2.0\"),\n        \"EVENS\": Decimal(\"2.0\"),\n        \"SP\": None,\n        \"SCRATCHED\": None,\n        \"SCR\": None,\n        \"\": None,\n    }\n\n    if odds_str in SPECIAL_CASES:\n        return SPECIAL_CASES[odds_str]\n\n    if \"/\" in odds_str:\n        try:\n            parts = odds_str.split(\"/\")\n            if len(parts) != 2:\n                return None\n            num, den = map(Decimal, parts)\n            if den <= 0:\n                return None\n            return Decimal(\"1.0\") + (num / den)\n        except (ValueError, InvalidOperation):\n            return None\n\n    try:\n        return Decimal(odds_str)\n    except (ValueError, InvalidOperation):\n        return None\n",
    "web_service/frontend/app/components/AdapterStatusPanel.tsx": "// web_platform/frontend/src/components/AdapterStatusPanel.tsx\n'use client';\n\nimport React from 'react';\nimport { SourceInfo } from '../types/racing';\n\ninterface AdapterStatusPanelProps {\n  adapter: SourceInfo;\n  onFetchRaces: (sourceName: string) => void;\n}\n\nexport const AdapterStatusPanel: React.FC<AdapterStatusPanelProps> = ({ adapter, onFetchRaces }) => {\n  const isConfigured = adapter.status !== 'CONFIG_ERROR';\n\n  return (\n    <div className={`p-4 rounded-lg border ${isConfigured ? 'bg-slate-800 border-slate-700' : 'bg-yellow-900/20 border-yellow-700/50'}`}>\n      <div className=\"flex justify-between items-center\">\n        <h3 className=\"font-bold text-lg text-white\">{adapter.name}</h3>\n        <span className={`px-2 py-0.5 rounded-full text-xs font-medium ${isConfigured ? 'bg-green-500/20 text-green-300' : 'bg-yellow-500/20 text-yellow-300'}`}>\n          {isConfigured ? 'Ready' : 'Not Configured'}\n        </span>\n      </div>\n      <div className=\"mt-4 flex gap-2\">\n        <button\n          onClick={() => onFetchRaces(adapter.name)}\n          disabled={!isConfigured}\n          className=\"flex-1 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 disabled:bg-slate-700 disabled:text-slate-400 disabled:cursor-not-allowed\"\n        >\n          Automatic Load\n        </button>\n        <button\n          disabled\n          className=\"flex-1 px-4 py-2 bg-slate-700 text-slate-400 rounded cursor-not-allowed\"\n        >\n          Manual Entry (Coming Soon)\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/ManualOverridePanel.tsx": "// web_platform/frontend/src/components/ManualOverridePanel.tsx\nimport React, { useState } from 'react';\nimport { Race } from '../types/racing';\n\ninterface ManualOverridePanelProps {\n  adapterName: string;\n  attemptedUrl: string;\n  apiKey: string | null;\n  onParseSuccess: (adapterName: string, parsedRaces: Race[]) => void;\n}\n\nconst ManualOverridePanel: React.FC<ManualOverridePanelProps> = ({\n  adapterName,\n  attemptedUrl,\n  apiKey,\n  onParseSuccess,\n}) => {\n  const [showPanel, setShowPanel] = useState(true);\n  const [htmlContent, setHtmlContent] = useState('');\n  const [isSubmitting, setIsSubmitting] = useState(false);\n  const [error, setError] = useState<string | null>(null);\n\n  const handleSubmit = async () => {\n    if (!htmlContent.trim()) {\n      setError('HTML content cannot be empty.');\n      return;\n    }\n    if (!apiKey) {\n      setError('API key is not available. Cannot submit.');\n      return;\n    }\n\n    setIsSubmitting(true);\n    setError(null);\n\n    try {\n      const response = await fetch('/api/races/parse-manual', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'X-API-Key': apiKey,\n        },\n        body: JSON.stringify({\n          adapter_name: adapterName,\n          html_content: htmlContent,\n        }),\n      });\n\n      if (!response.ok) {\n        const errorData = await response.json();\n        throw new Error(errorData.detail || 'Failed to parse HTML.');\n      }\n\n      const parsedRaces: Race[] = await response.json();\n      onParseSuccess(adapterName, parsedRaces);\n      setShowPanel(false); // Hide panel on success\n\n    } catch (err) {\n      const errorMessage = err instanceof Error ? err.message : 'An unknown error occurred.';\n      setError(errorMessage);\n      console.error('Manual parse submission failed:', err);\n    } finally {\n      setIsSubmitting(false);\n    }\n  };\n\n\n  if (!showPanel) {\n    return null;\n  }\n\n  return (\n    <div className=\"bg-red-900 bg-opacity-50 border border-red-700 p-4 rounded-lg shadow-lg mb-4\">\n      <div className=\"flex justify-between items-center\">\n        <div>\n          <h3 className=\"font-bold text-red-300\">Data Fetch Failed: {adapterName}</h3>\n          <p className=\"text-sm text-red-400\">\n            The application failed to automatically retrieve data from:{' '}\n            <a href={attemptedUrl} target=\"_blank\" rel=\"noopener noreferrer\" className=\"underline hover:text-red-200\">\n              {attemptedUrl}\n            </a>\n          </p>\n        </div>\n        <button onClick={() => setShowPanel(false)} className=\"text-red-400 hover:text-red-200 text-2xl\">&times;</button>\n      </div>\n      <div className=\"mt-4\">\n        <p className=\"text-sm text-red-300 mb-2\">\n          <strong>To resolve this:</strong>\n          <ol className=\"list-decimal list-inside pl-4\">\n            <li>Click the link above to open the page in a new tab.</li>\n            <li>Right-click on the page and select \"View Page Source\".</li>\n            <li>Copy the entire HTML source code.</li>\n            <li>Paste the code into the text area below and click \"Submit Manual Data\".</li>\n          </ol>\n        </p>\n        <textarea\n          className=\"w-full h-24 p-2 bg-gray-900 border border-gray-700 rounded text-gray-300 font-mono text-xs\"\n          placeholder={`Paste HTML source for ${adapterName} here...`}\n          value={htmlContent}\n          onChange={(e) => setHtmlContent(e.target.value)}\n          disabled={isSubmitting}\n        />\n        {error && <p className=\"text-red-400 text-sm mt-2\">{error}</p>}\n        <div className=\"mt-2 flex gap-2\">\n          <button\n            onClick={handleSubmit}\n            className=\"px-3 py-1.5 bg-blue-600 text-white rounded hover:bg-blue-700 text-sm disabled:bg-blue-800 disabled:cursor-not-allowed\"\n            disabled={isSubmitting}\n          >\n            {isSubmitting ? 'Submitting...' : 'Submit Manual Data'}\n          </button>\n          <button\n            onClick={() => setShowPanel(false)}\n            className=\"px-3 py-1.5 bg-gray-700 text-white rounded hover:bg-gray-600 text-sm\"\n          >\n            Skip for Now\n          </button>\n        </div>\n      </div>\n    </div>\n  );\n};\n\nexport default ManualOverridePanel;\n",
    "web_service/frontend/app/components/SettingsPage.tsx": "// src/components/SettingsPage.tsx\n'use client';\n\nimport React, { useState, useEffect } from 'react';\n\nexport function SettingsPage() {\n  const [apiKey, setApiKey] = useState('');\n  const [betfairAppKey, setBetfairAppKey] = useState('');\n  const [betfairUsername, setBetfairUsername] = useState('');\n  const [betfairPassword, setBetfairPassword] = useState('');\n\n  useEffect(() => {\n    // Fetch the current API key when the component mounts\n    const fetchApiKey = async () => {\n      if (window.electronAPI?.getApiKey) {\n        const key = await window.electronAPI.getApiKey();\n        if (key) {\n          setApiKey(key);\n        }\n      }\n    };\n    fetchApiKey();\n  }, []);\n\n  const handleGenerateApiKey = async () => {\n    if (window.electronAPI?.generateApiKey) {\n      const newKey = await window.electronAPI.generateApiKey();\n      setApiKey(newKey);\n    }\n  };\n\n  const handleSaveSettings = async () => {\n    if (window.electronAPI?.saveApiKey && window.electronAPI?.saveBetfairCredentials) {\n      await window.electronAPI.saveApiKey(apiKey);\n      await window.electronAPI.saveBetfairCredentials({\n        appKey: betfairAppKey,\n        username: betfairUsername,\n        password: betfairPassword,\n      });\n      alert('Settings saved successfully!');\n    }\n  };\n\n  return (\n    <div className=\"bg-slate-800 p-8 rounded-lg border border-slate-700 text-white max-w-2xl mx-auto\">\n      <h2 className=\"text-3xl font-bold text-white mb-6\">Application Settings</h2>\n\n      <div className=\"space-y-8\">\n        <div>\n          <h3 className=\"text-xl font-semibold text-slate-300 mb-2\">API Key</h3>\n          <p className=\"text-sm text-slate-400 mb-3\">This key is required for the dashboard to communicate with the backend service.</p>\n          <div className=\"flex items-center space-x-2\">\n            <input\n              type=\"text\"\n              readOnly\n              value={apiKey}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 font-mono text-sm\"\n            />\n            <button\n              onClick={handleGenerateApiKey}\n              className=\"px-4 py-2 bg-blue-600 hover:bg-blue-700 rounded transition-colors font-semibold\"\n            >\n              Generate New Key\n            </button>\n          </div>\n        </div>\n\n        <div>\n          <h3 className=\"text-xl font-semibold text-slate-300 mb-2\">Betfair Credentials (Optional)</h3>\n           <p className=\"text-sm text-slate-400 mb-3\">Required for adapters that use the Betfair Exchange API.</p>\n          <div className=\"space-y-3\">\n            <input\n              type=\"password\"\n              placeholder=\"App Key\"\n              value={betfairAppKey}\n              onChange={(e) => setBetfairAppKey(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n            <input\n              type=\"text\"\n              placeholder=\"Username\"\n              value={betfairUsername}\n              onChange={(e) => setBetfairUsername(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n            <input\n              type=\"password\"\n              placeholder=\"Password\"\n              value={betfairPassword}\n              onChange={(e) => setBetfairPassword(e.target.value)}\n              className=\"w-full p-2 bg-slate-700 rounded border border-slate-600 placeholder-slate-500\"\n            />\n          </div>\n        </div>\n\n        <div className=\"flex justify-end pt-6 border-t border-slate-700\">\n          <button\n            onClick={handleSaveSettings}\n            className=\"px-8 py-3 bg-green-600 hover:bg-green-700 rounded font-bold text-lg transition-colors\"\n          >\n            Save All Settings\n          </button>\n        </div>\n      </div>\n    </div>\n  );\n}\n",
    "web_service/frontend/app/hooks/useRealTimeRaces.ts": "import { useState, useEffect } from 'react';\nimport { io, Socket } from 'socket.io-client';\nimport { Race } from '../types/racing';\n\nconst API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8080';\n\nexport function useRealTimeRaces() {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [isConnected, setIsConnected] = useState(false);\n\n  useEffect(() => {\n    const socket: Socket = io(API_URL);\n\n    socket.on('connect', () => setIsConnected(true));\n    socket.on('disconnect', () => setIsConnected(false));\n\n    socket.on('races_update', (data: { races: Race[] }) => {\n      if (data && Array.isArray(data.races)) {\n        setRaces(data.races);\n      }\n    });\n\n    // Cleanup on component unmount\n    return () => {\n      socket.disconnect();\n    };\n  }, []);\n\n  return { races, isConnected };\n}",
    "web_service/frontend/app/types/electron.d.ts": "// web_platform/frontend/src/types/electron.d.ts\n\n/**\n * This declaration file extends the global Window interface to include the\n * 'electronAPI' object exposed by the preload script. This provides\n * TypeScript with type information for the functions we're using for IPC.\n */\nexport {};\n\ndeclare global {\n  interface Window {\n    electronAPI?: {\n      /**\n       * Asynchronously fetches the secure API key from the main process.\n       * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n       */\n      getApiKey: () => Promise<string | null>;\n      /**\n       * Registers a callback for backend status updates from the main process.\n       * @param callback The function to execute. Receives an object with state and logs.\n       * @returns A function to unsubscribe the listener.\n       */\n      onBackendStatusUpdate: (callback: (status: { state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }) => void) => () => void;\n\n      /**\n       * Sends a command to the main process to restart the backend executable.\n       */\n      restartBackend: () => void;\n\n      /**\n       * Asynchronously fetches the current backend status from the main process.\n       * @returns {Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>}\n       */\n      getBackendStatus: () => Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>;\n      generateApiKey: () => Promise<string>;\n      saveApiKey: (apiKey: string) => Promise<{ success: boolean }>;\n      saveBetfairCredentials: (credentials: { appKey: string; username: string; password: string }) => Promise<{ success: boolean }>;\n      getApiPort: () => Promise<number | null>;\n    };\n  }\n}\n",
    "web_service/frontend/public/sw.js": "if(!self.define){let e,s={};const a=(a,n)=>(a=new URL(a+\".js\",n).href,s[a]||new Promise(s=>{if(\"document\"in self){const e=document.createElement(\"script\");e.src=a,e.onload=s,document.head.appendChild(e)}else e=a,importScripts(a),s()}).then(()=>{let e=s[a];if(!e)throw new Error(`Module ${a} didn\u2019t register its module`);return e}));self.define=(n,t)=>{const i=e||(\"document\"in self?document.currentScript.src:\"\")||location.href;if(s[i])return;let c={};const r=e=>a(e,i),o={module:{uri:i},exports:c,require:r};s[i]=Promise.all(n.map(e=>o[e]||r(e))).then(e=>(t(...e),c))}}define([\"./workbox-4754cb34\"],function(e){\"use strict\";importScripts(),self.skipWaiting(),e.clientsClaim(),e.precacheAndRoute([{url:\"/_next/app-build-manifest.json\",revision:\"b6130f23369e5df052a4061c412f24fa\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_buildManifest.js\",revision:\"c155cce658e53418dec34664328b51ac\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_ssgManifest.js\",revision:\"b6652df95db52feb4daf4eca35380933\"},{url:\"/_next/static/chunks/117-6326cd814d964913.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/816-7254031126ac0a96.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/928.d7f641058b89a54a.js\",revision:\"d7f641058b89a54a\"},{url:\"/_next/static/chunks/app/_not-found/page-e7dc36cd5a340c38.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/layout-605479d07717f01e.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/page-a2c385e93bfc2dac.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/fd9d1056-af804af0be509bea.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/framework-f66176bb897dc684.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-8563e00d234bd632.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-app-e0b3e4e952d25145.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_app-72b849fbd24ac258.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_error-7ba65e1336b92748.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/polyfills-42372ed130431b0a.js\",revision:\"846118c33b2c0e922d7b3a7676f81f6f\"},{url:\"/_next/static/chunks/webpack-d92cdde7bb2319ca.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/css/a55e4893d0564dbf.css\",revision:\"a55e4893d0564dbf\"},{url:\"/_next/static/media/19cfc7226ec3afaa-s.woff2\",revision:\"9dda5cfc9a46f256d0e131bb535e46f8\"},{url:\"/_next/static/media/21350d82a1f187e9-s.woff2\",revision:\"4e2553027f1d60eff32898367dd4d541\"},{url:\"/_next/static/media/8e9860b6e62d6359-s.woff2\",revision:\"01ba6c2a184b8cba08b0d57167664d75\"},{url:\"/_next/static/media/ba9851c3c22cd980-s.woff2\",revision:\"9e494903d6b0ffec1a1e14d34427d44d\"},{url:\"/_next/static/media/c5fe6dc8356a8c31-s.woff2\",revision:\"027a89e9ab733a145db70f09b8a18b42\"},{url:\"/_next/static/media/df0a9ae256c0569c-s.woff2\",revision:\"d54db44de5ccb18886ece2fda72bdfe0\"},{url:\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",revision:\"65850a373e258f1c897a2b3d75eb74de\"},{url:\"/manifest.json\",revision:\"23bffdb04aba9b85948642cffa772eae\"}],{ignoreURLParametersMatching:[]}),e.cleanupOutdatedCaches(),e.registerRoute(\"/\",new e.NetworkFirst({cacheName:\"start-url\",plugins:[{cacheWillUpdate:async({request:e,response:s,event:a,state:n})=>s&&\"opaqueredirect\"===s.type?new Response(s.body,{status:200,statusText:\"OK\",headers:s.headers}):s}]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:gstatic)\\.com\\/.*/i,new e.CacheFirst({cacheName:\"google-fonts-webfonts\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:31536e3})]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:googleapis)\\.com\\/.*/i,new e.StaleWhileRevalidate({cacheName:\"google-fonts-stylesheets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:eot|otf|ttc|ttf|woff|woff2|font.css)$/i,new e.StaleWhileRevalidate({cacheName:\"static-font-assets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:jpg|jpeg|gif|png|svg|ico|webp)$/i,new e.StaleWhileRevalidate({cacheName:\"static-image-assets\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/image\\?url=.+$/i,new e.StaleWhileRevalidate({cacheName:\"next-image\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp3|wav|ogg)$/i,new e.CacheFirst({cacheName:\"static-audio-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp4)$/i,new e.CacheFirst({cacheName:\"static-video-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:js)$/i,new e.StaleWhileRevalidate({cacheName:\"static-js-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:css|less)$/i,new e.StaleWhileRevalidate({cacheName:\"static-style-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/data\\/.+\\/.+\\.json$/i,new e.StaleWhileRevalidate({cacheName:\"next-data\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:json|xml|csv)$/i,new e.NetworkFirst({cacheName:\"static-data-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;const s=e.pathname;return!s.startsWith(\"/api/auth/\")&&!!s.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"apis\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:16,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;return!e.pathname.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"others\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>!(self.origin===e.origin),new e.NetworkFirst({cacheName:\"cross-origin\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:3600})]}),\"GET\")});\n",
    "wix/product_webservice.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\"\n     xmlns:fire=\"http://schemas.microsoft.com/wix/FirewallExtension\"\n     xmlns:util=\"http://schemas.microsoft.com/wix/UtilExtension\">\n\n  <Product Id=\"*\"\n           Name=\"Fortuna Web Service\"\n           Language=\"1033\"\n           Version=\"$(var.Version)\"\n           Manufacturer=\"Fortuna Development Team\"\n           UpgradeCode=\"A3A4A3B6-2313-4375-9A97-15206C81454A\">\n\n    <Package InstallerVersion=\"200\" Compressed=\"yes\" InstallScope=\"perMachine\" />\n    <MajorUpgrade DowngradeErrorMessage=\"A newer version of [ProductName] is already installed.\" />\n    <MediaTemplate EmbedCab=\"yes\" />\n\n    <Property Id=\"ARPNOREPAIR\" Value=\"no\" />\n    <Property Id=\"ARPNOMODIFY\" Value=\"yes\" />\n\n    <UI>\n      <UIRef Id=\"WixUI_Minimal\" />\n    </UI>\n\n    <WixVariable Id=\"WixUILicenseRtf\" Value=\"electron\\assets\\license.rtf\"/>\n    <WixVariable Id=\"WixUIBannerBmp\"  Value=\"electron\\assets\\banner.bmp\"/>\n    <WixVariable Id=\"WixUIDialogBmp\"  Value=\"electron\\assets\\dialog.bmp\"/>\n\n    <Feature Id=\"ProductFeature\" Title=\"Fortuna Web Service\" Level=\"1\">\n      <ComponentGroupRef Id=\"WebServiceComponents\" />\n      <ComponentRef Id=\"ApplicationShortcut\" />\n    </Feature>\n  </Product>\n\n  <Fragment>\n    <Directory Id=\"TARGETDIR\" Name=\"SourceDir\">\n      <Directory Id=\"ProgramFilesFolder\">\n        <Directory Id=\"INSTALLDIR\" Name=\"FortunaWebService\"/>\n      </Directory>\n      <Directory Id=\"ProgramMenuFolder\">\n        <Directory Id=\"ApplicationProgramsFolder\" Name=\"Fortuna Web Service\"/>\n      </Directory>\n      <Directory Id=\"CommonAppDataFolder\">\n        <Directory Id=\"FortunaData\" Name=\"FortunaWebService\"/>\n      </Directory>\n    </Directory>\n  </Fragment>\n\n  <Fragment>\n    <ComponentGroup Id=\"WebServiceComponents\" Directory=\"INSTALLDIR\">\n      <Component Id=\"WebServiceExecutable\" Guid=\"3F2A4A9C-4055-4D62-812E-B715A0123594\">\n        <File Id=\"WebServiceExe\" Source=\"staging/fortuna-webservice.exe\" KeyPath=\"yes\"/>\n        <ServiceInstall Id=\"FortunaWebService\"\n                        Name=\"FortunaWebService\"\n                        DisplayName=\"Fortuna Web Service\"\n                        Description=\"Provides live odds and race data via a web interface.\"\n                        Start=\"auto\"\n                        Type=\"ownProcess\"\n                        ErrorControl=\"normal\"\n                        Account=\"NetworkService\"/>\n        <ServiceControl Id=\"StartFortunaWebService\"\n                        Name=\"FortunaWebService\"\n                        Start=\"install\"\n                        Stop=\"both\"\n                        Remove=\"uninstall\"\n                        Wait=\"yes\"/>\n        <fire:FirewallException Id=\"FortunaFirewall\"\n                                Name=\"FortunaWebService\"\n                                Port=\"8088\"\n                                Protocol=\"tcp\"\n                                Scope=\"any\"/>\n      </Component>\n    </ComponentGroup>\n  </Fragment>\n\n  <Fragment>\n    <DirectoryRef Id=\"ApplicationProgramsFolder\">\n      <Component Id=\"ApplicationShortcut\" Guid=\"5E95E5B9-4F3D-4B9A-819B-9149C5E4700F\">\n        <util:InternetShortcut Id=\"DashboardShortcut\"\n                               Name=\"Fortuna Dashboard\"\n                               Target=\"http://localhost:8088\"/>\n        <Shortcut Id=\"UninstallProduct\"\n                  Name=\"Uninstall Fortuna Web Service\"\n                  Target=\"[SystemFolder]msiexec.exe\"\n                  Arguments=\"/x [ProductCode]\"\n                  Description=\"Uninstalls Fortuna Web Service\"/>\n        <RemoveFolder Id=\"ApplicationProgramsFolder\" On=\"uninstall\"/>\n        <RegistryValue Root=\"HKCU\"\n                       Key=\"Software\\FortunaWebService\"\n                       Name=\"installed\"\n                       Type=\"integer\"\n                       Value=\"1\"\n                       KeyPath=\"yes\"/>\n      </Component>\n    </DirectoryRef>\n  </Fragment>\n</Wix>\n"
}