{
    ".env.example": "# This file documents the environment variables needed to run Fortuna Faucet.\n# On first run, the application should copy this to '.env' if it doesn't exist.\n\n# --- Core Configuration ---\n# Required for API security in a deployed environment (though less critical for local-only)\nAPI_KEY=your_production_api_key_here\n\n# --- Betfair Credentials (Required for live odds) ---\nBETFAIR_USERNAME=\nBETFAIR_PASSWORD=\nBETFAIR_APP_KEY=\n\n# --- Optional API Keys for Other Data Sources ---\nTVG_API_KEY=\nTHE_RACING_API_KEY=\nRACING_AND_SPORTS_TOKEN=\n\n# --- Redis Configuration ---\n# The application will fall back to an in-memory cache if this is unreachable.\nREDIS_URL=redis://localhost:6379\n\n# --- CORS Configuration ---\n# Required for the Electron app to communicate with the backend.\nALLOWED_ORIGINS=http://localhost:3000\n",
    "STATUS.md": "# Project Status: Foundation Rebuilt, Hardening in Progress\n\n**Date:** 2025-10-03\n\n## Current State\n\n*   **Architecture:** The backend has been successfully rebuilt into a superior, asynchronous FastAPI application, as defined by 'Operation: Grand Synthesis'. The new foundation is stable, tested, and features a resilient `BaseAdapter` pattern.\n\n*   **Status:** The foundational refactoring is complete. The first two data adapters (`Betfair`, `TVG`) have been implemented on the new architecture. We are now in a new phase of development: **'Phase 2: Hardening & Expansion.'**\n\n*   **Documentation:** All core strategic documents and manifests have been synchronized with the new technical reality.\n\n*   **Next Steps:** Our immediate priority is to act on the verified intelligence from our Oracle (Jules1003). The next missions will focus on implementing critical API security features (rate limiting, authentication) and continuing the build-out of our adapter fleet.",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2)---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "assets/sounds/.gitkeep": "# This directory is for audio alert sound files (e.g., alert_premium.wav)",
    "check_port.py": "# check_port.py\nimport socket\nimport time\n\n\ndef check_server_status(host, port):\n    \"\"\"Checks if the server is accessible.\"\"\"\n    time.sleep(5)  # Give server time to start\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.connect((host, port))\n            print(\"SERVER CHECK: SUCCESS! Server is running and accessible.\")\n            return True\n        except ConnectionRefusedError:\n            print(\"SERVER CHECK: FAILED! Server is not accessible.\")\n            return False\n\n\nif __name__ == \"__main__\":\n    check_server_status(\"127.0.0.1\", 8000)\n",
    "configure_startup.py": "# configure_startup.py\nimport sys\nimport winreg\nfrom pathlib import Path\n\n\nclass StartupManager:\n    \"\"\"Manage Windows startup registry entries for the current user.\"\"\"\n\n    REGISTRY_PATH = r\"Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Run\"\n    APP_NAME = \"FortunaFaucetTray\"\n\n    @classmethod\n    def is_enabled(cls) -> bool:\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_READ)\n            winreg.QueryValueEx(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            return True\n        except FileNotFoundError:\n            return False\n\n    @classmethod\n    def enable(cls):\n        launcher_path = Path(__file__).parent / \"launcher.ps1\"\n        cmd = f'powershell.exe -WindowStyle Hidden -File \"{launcher_path}\"'\n\n        key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n        winreg.SetValueEx(key, cls.APP_NAME, 0, winreg.REG_SZ, cmd)\n        winreg.CloseKey(key)\n        print(\"Startup enabled.\")\n\n    @classmethod\n    def disable(cls):\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n            winreg.DeleteValue(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            print(\"Startup disabled.\")\n        except FileNotFoundError:\n            print(\"Already disabled.\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        if sys.argv[1] == \"enable\":\n            StartupManager.enable()\n        elif sys.argv[1] == \"disable\":\n            StartupManager.disable()\n        elif sys.argv[1] == \"status\":\n            print(f\"Startup is currently {'enabled' if StartupManager.is_enabled() else 'disabled'}\")\n    else:\n        print(\"Usage: python configure_startup.py [enable|disable|status]\")\n",
    "electron/install-dependencies.js": "const { execSync } = require('child_process');\nconst path = require('path');\nconst fs = require('fs');\n\n// Path to the bundled Python executable (fortuna-backend.exe)\nconst PYTHON_EXE = path.join(process.resourcesPath, 'fortuna-backend.exe');\n// Path to the Python service directory (where alembic.ini is)\nconst PYTHON_SERVICE_DIR = path.join(process.resourcesPath, 'python_service');\n\nfunction runCommand(command, cwd) {\n    console.log(`Executing: ${command} in ${cwd}`);\n    try {\n        const output = execSync(command, { cwd: cwd, encoding: 'utf-8' });\n        console.log(output);\n    } catch (error) {\n        console.error(`Command failed: ${command}`);\n        console.error(error.stderr || error.stdout || error.message);\n        throw new Error(`Post-install setup failed: ${command}`);\n    }\n}\n\nfunction setupDatabase() {\n    console.log('--- Starting Database Setup (Alembic Migrations) ---');\n    // NOTE: The bundled EXE must be able to run a command like 'alembic' or a custom script\n    // that executes the migrations. Assuming the bundled EXE can run a module.\n    // A more robust solution is to bundle a dedicated migration script.\n\n    // Assuming the bundled EXE can execute a module that runs Alembic\n    const migrationCommand = `${PYTHON_EXE} -m python_service.database.run_migrations`;\n\n    // The migration script needs access to the database URL from the config.\n    // This is a placeholder, as the config loading is complex in a frozen app.\n    // For now, we assume the bundled EXE handles config loading.\n\n    runCommand(migrationCommand, PYTHON_SERVICE_DIR);\n    console.log('--- Database Setup Complete ---');\n}\n\n// This function is called by the Electron Builder installer hook\nmodule.exports = async function() {\n    try {\n        setupDatabase();\n    } catch (e) {\n        console.error('FATAL: Post-install setup failed.', e);\n        // In a real installer, you might log this and continue, or show a user error.\n    }\n};\n",
    "electron/secure-settings-manager.js": "// electron/secure-settings-manager.js\nconst { app } = require('electron');\nconst fs = require('fs');\nconst path = require('path');\n\nconst SETTINGS_FILE = path.join(app.getPath('userData'), 'settings.json');\n\nclass SecureSettingsManager {\n constructor() {\n this.settings = this.loadSettings();\n }\n\n loadSettings() {\n try {\n if (fs.existsSync(SETTINGS_FILE)) {\n const data = fs.readFileSync(SETTINGS_FILE, 'utf-8');\n return JSON.parse(data);\n }\n } catch (error) {\n console.error('Error loading settings:', error);\n }\n return {};\n }\n\n saveSettings() {\n try {\n fs.writeFileSync(SETTINGS_FILE, JSON.stringify(this.settings, null, 2));\n } catch (error) {\n console.error('Error saving settings:', error);\n }\n }\n\n getApiKey() {\n return this.settings.apiKey || null;\n }\n\n saveApiKey(apiKey) {\n this.settings.apiKey = apiKey;\n this.saveSettings();\n return { success: true };\n }\n\n getBetfairCredentials() {\n return this.settings.betfair || null;\n }\n\n saveBetfairCredentials(credentials) {\n this.settings.betfair = credentials;\n this.saveSettings();\n return { success: true };\n }\n}\n\nmodule.exports = new SecureSettingsManager();\n",
    "jules-scratch/verification/verify_frontend.py": "from playwright.sync_api import sync_playwright\n\n\ndef run():\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        page = browser.new_page()\n        page.goto(\"http://localhost:3000\")\n        page.screenshot(path=\"jules-scratch/verification/verification.png\")\n        browser.close()\n\n\nif __name__ == \"__main__\":\n    run()\n",
    "jules-scratch/verification/verify_websockets.py": "from playwright.sync_api import sync_playwright, expect\nimport time\n\ndef run(playwright):\n    browser = playwright.chromium.launch(headless=True)\n    page = browser.new_page()\n\n    # Mock the Electron API and the WebSocket connection\n    page.add_init_script(\"\"\"\n        window.electronAPI = {\n            getApiKey: () => Promise.resolve('test-api-key'),\n            getBackendStatus: () => Promise.resolve({ state: 'running', logs: [] }),\n            onBackendStatusUpdate: (callback) => {\n                // Do nothing, assume it's always running\n                return () => {}; // Return an unsubscribe function\n            }\n        };\n\n        const mockSocket = {\n            listeners: {},\n            on(event, callback) {\n                this.listeners[event] = callback;\n            },\n            close() {},\n            readyState: 0, // Initially connecting\n            send() {},\n        };\n        window.mockSocket = mockSocket;\n        window.WebSocket = function(url) {\n            console.log('Mock WebSocket created for:', url);\n            setTimeout(() => {\n                mockSocket.readyState = 1; // OPEN\n                if(mockSocket.listeners.open) {\n                    mockSocket.listeners.open();\n                }\n            }, 100);\n            return mockSocket;\n        };\n    \"\"\")\n\n    # Intercept the API call and return a mock response\n    page.route(\"**/api/adapters/status\", lambda route: route.fulfill(\n        status=200,\n        json=[]\n    ))\n\n    page.goto(\"http://localhost:3000\")\n\n    # Wait for the status indicator to be visible\n    page.wait_for_selector('[data-testid=\"status-indicator\"]')\n\n    # Check that the status indicator eventually shows \"Live\"\n    expect(page.locator('[data-testid=\"status-indicator\"]')).to_have_text(\"Live\", timeout=5000)\n\n    # Simulate a WebSocket message with race data\n    page.evaluate(\"\"\"\n        window.mockSocket.listeners.message({\n            data: JSON.stringify({\n                races: [{\n                    id: 'test-race-1',\n                    venue: 'Test Park',\n                    raceNumber: 1,\n                    isErrorPlaceholder: false,\n                    startTime: new Date().toISOString(),\n                    runners: [{\n                        number: 1,\n                        name: 'Test Horse',\n                        odds: { 'TestSource': { win: 5.0, source: 'TestSource', last_updated: new Date().toISOString() } }\n                    }]\n                }],\n                source_info: [{\n                    name: 'TestSource',\n                    status: 'SUCCESS',\n                    races_fetched: 1,\n                    fetch_duration: 0.1\n                }]\n            })\n        });\n    \"\"\")\n\n    # Check that the UI has updated with the new data\n    expect(page.get_by_text(\"Test Park\")).to_be_visible()\n    expect(page.get_by_text(\"Test Horse\")).to_be_visible()\n\n    page.screenshot(path=\"jules-scratch/verification/websockets.png\")\n    browser.close()\n\nwith sync_playwright() as playwright:\n    run(playwright)\n",
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n# TEMPORARY FIX: Comment out the problematic adapter\n\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\n# from .betfair_datascientist_adapter import BetfairDataScientistAdapter  # DISABLED: PyInstaller NumPy issue\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\n\n__all__ = [\n    \"GbgbApiAdapter\",\n    \"TVGAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"AtTheRacesAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"SportingLifeAdapter\",\n    \"TimeformAdapter\",\n    \"HarnessAdapter\",\n    \"GreyhoundAdapter\",\n    \"TheRacingApiAdapter\",\n    # \"BetfairDataScientistAdapter\",  # DISABLED\n]",
    "python_service/adapters/betfair_auth_mixin.py": "# python_service/adapters/betfair_auth_mixin.py\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..credentials_manager import SecureCredentialsManager\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock = asyncio.Lock()\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        \"\"\"\n        Authenticates with Betfair using credentials from the system's credential manager,\n        ensuring the session token is valid and refreshing it if necessary.\n        \"\"\"\n        async with self._auth_lock:\n            if self.session_token and self.token_expiry and self.token_expiry > (datetime.now() + timedelta(minutes=5)):\n                return\n\n            log.info(\"Attempting to authenticate with Betfair...\")\n            username, password = SecureCredentialsManager.get_betfair_credentials()\n\n            if not all([self.config.BETFAIR_APP_KEY, username, password]):\n                raise ValueError(\"Betfair credentials not fully configured in credential manager.\")\n\n            auth_url = \"https://identitysso.betfair.com/api/login\"\n            headers = {\n                \"X-Application\": self.config.BETFAIR_APP_KEY,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={username}&password={password}\"\n\n            response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n            response.raise_for_status()\n            data = response.json()\n\n            if data.get(\"status\") == \"SUCCESS\":\n                self.session_token = data.get(\"token\")\n                self.token_expiry = datetime.now() + timedelta(hours=3)\n                log.info(\"Betfair authentication successful.\")\n            else:\n                log.error(\"Betfair authentication failed\", error=data.get(\"error\"))\n                self.session_token = None  # Reset token to prevent using a stale one\n                return  # Return gracefully and let the adapter handle the lack of a token\n",
    "python_service/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass EquibaseAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        d = datetime.strptime(date, \"%Y-%m-%d\").date()\n        index_url = f\"/entries/Entries.cfm?ELEC_DATE={d.month}/{d.day}/{d.year}&STYLE=EQB\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Equibase index page\", url=index_url)\n            return None\n\n        parser = HTMLParser(index_response.text)\n        track_links = [\n            link.attributes[\"href\"]\n            for link in parser.css(\"div.track-information a\")\n            if \"race=\" not in link.attributes.get(\"href\", \"\")\n        ]\n\n        async def get_race_links_from_track(track_url: str):\n            response = await self.make_request(self.http_client, \"GET\", track_url, headers=self._get_headers())\n            if not response:\n                return []\n            parser = HTMLParser(response.text)\n            return [link.attributes[\"href\"] for link in parser.css(\"a.program-race-link\")]\n\n        tasks = [get_race_links_from_track(link) for link in track_links]\n        results = await asyncio.gather(*tasks)\n        race_links = [f\"{self.base_url}{link}\" for sublist in results for link in sublist]\n\n        async def fetch_single_html(race_url: str):\n            response = await self.make_request(self.http_client, \"GET\", race_url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first(\"div.track-information strong\")\n                if not venue_node:\n                    continue\n                venue = clean_text(venue_node.text())\n\n                race_number_node = parser.css_first(\"div.race-information strong\")\n                if not race_number_node:\n                    continue\n                race_number_text = race_number_node.text().replace(\"Race\", \"\").strip()\n                if not race_number_text.isdigit():\n                    continue\n                race_number = int(race_number_text)\n\n                post_time_node = parser.css_first(\"p.post-time span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    if runner := self._parse_runner(node):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first(\"td:nth-child(1)\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            name_node = node.css_first(\"td:nth-child(3)\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            odds_node = node.css_first(\"td:nth-child(10)\")\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError, IndexError):\n            self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n            return None\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        time_part = time_str.split(\" \")[-2] + \" \" + time_str.split(\" \")[-1]\n        dt_str = f\"{date_str} {time_part}\"\n        return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/gbgb_api_adapter.py": "# python_service/adapters/gbgb_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GbgbApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Greyhound Board of Great Britain API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"GBGB\"\n    BASE_URL = \"https://api.gbgb.org.uk/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw meeting data from the GBGB API.\"\"\"\n        endpoint = f\"results/meeting/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, meetings_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw meeting data into a list of Race objects.\"\"\"\n        if not meetings_data:\n            return []\n\n        all_races = []\n        for meeting in meetings_data:\n            track_name = meeting.get(\"trackName\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name):\n                        all_races.append(race)\n                except (KeyError, TypeError):\n                    self.logger.error(\n                        \"Error parsing GBGB race\",\n                        race_id=race_data.get(\"raceId\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], track_name: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"raceId\")\n        race_number = race_data.get(\"raceNumber\")\n        race_time = race_data.get(\"raceTime\")\n\n        if not all([race_id, race_number, race_time]):\n            return None\n\n        return Race(\n            id=f\"gbgb_{race_id}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(race_time.replace(\"Z\", \"+00:00\")),\n            runners=self._parse_runners(race_data.get(\"traps\", [])),\n            source=self.source_name,\n            race_name=race_data.get(\"raceTitle\"),\n            distance=f\"{race_data.get('raceDistance')}m\",\n        )\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                trap_number = runner_data.get(\"trapNumber\")\n                dog_name = runner_data.get(\"dogName\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                sp = runner_data.get(\"sp\")\n                if sp:\n                    win_odds = parse_odds_to_decimal(sp)\n                    if win_odds and win_odds < 999:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Error parsing GBGB runner, skipping.\",\n                    runner_name=runner_data.get(\"dogName\"),\n                )\n                continue\n        return runners\n",
    "python_service/adapters/nyrabets_adapter.py": "# python_service/adapters/nyrabets_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass NYRABetsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for nyrabets.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSportsGreyhound\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw greyhound meetings data from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            self.http_client,\n            \"GET\",\n            \"v1/greyhound/meetings\",\n            headers=headers,\n            params=params,\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        all_races = []\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in RacingAndSportsGreyhound response or invalid format.\")\n            return all_races\n\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse RacingAndSportsGreyhound race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US racing data from the TVG API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v2/races/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(self.http_client, \"GET\", summary_url, headers=headers)\n        if not tracks_response:\n            return None\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get(\"tracks\", []):\n            track_id = track.get(\"id\")\n            for race in track.get(\"races\", []):\n                race_id = race.get(\"id\")\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(self.http_client, \"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        # Filter out exceptions and return only successful responses\n        return [resp.json() for resp in race_detail_responses if resp and not isinstance(resp, Exception)]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of detailed race JSON objects into Race models.\"\"\"\n        races = []\n        if not isinstance(raw_data, list):\n            self.logger.warning(\"raw_data is not a list, cannot parse TVG races.\")\n            return races\n\n        for race_detail in raw_data:\n            try:\n                if race := self._parse_race(race_detail):\n                    races.append(race)\n            except AdapterParsingError:\n                self.logger.warning(\n                    \"Failed to parse TVG race detail, skipping.\",\n                    race_detail=race_detail,\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_race(self, race_detail: dict) -> Optional[Race]:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get(\"track\")\n        race_info = race_detail.get(\"race\")\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get(\"runners\", []):\n            if runner_data.get(\"scratched\"):\n                continue\n\n            odds = runner_data.get(\"odds\", {})\n            current_odds = odds.get(\"currentPrice\", {})\n            odds_str = current_odds.get(\"fractional\") or odds.get(\"morningLinePrice\", {}).get(\"fractional\")\n\n            try:\n                number = int(runner_data.get(\"programNumber\", \"0\").replace(\"A\", \"\"))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            odds_data = {}\n            if odds_str:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n\n            runners.append(\n                Runner(\n                    number=number,\n                    name=clean_text(runner_data.get(\"name\")),\n                    odds=odds_data,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        post_time = race_info.get(\"postTime\")\n        if not post_time:\n            raise AdapterParsingError(self.source_name, \"Missing post time.\")\n\n        try:\n            start_time = datetime.fromisoformat(post_time.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name,\n                f\"Could not parse post time: {post_time}\",\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get(\"name\"),\n            race_number=race_info.get(\"number\"),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/api.py": "# python_service/api.py\n\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import List\nfrom typing import Optional\n\nimport aiosqlite\nimport structlog\nfrom fastapi import Depends\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import Request\nfrom fastapi import WebSocket\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom slowapi import Limiter\nfrom slowapi import _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom starlette.websockets import WebSocketDisconnect\n\n# --- PyInstaller Explicit Imports ---\nfrom .analyzer import AnalyzerEngine\nfrom .cache_manager import cache_manager\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .engine import OddsEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .manual_override_manager import ManualOverrideManager\nfrom .middleware.error_handler import UserFriendlyException\nfrom .middleware.error_handler import user_friendly_exception_handler\nfrom .middleware.error_handler import validation_exception_handler\nfrom .models import AggregatedResponse\nfrom .models import QualifiedRacesResponse\nfrom .models import Race\nfrom .models import TipsheetRace\nfrom .security import verify_api_key\n\n# ------------------------------------\n\nlog = structlog.get_logger()\n\n# Create a fixed thread pool for blocking calls at the module level\nexecutor = ThreadPoolExecutor(max_workers=1)\n\n\ndef _initialize_heavy_resources_sync(app: FastAPI):\n    \"\"\"\n    This synchronous function contains the blocking I/O and CPU-intensive\n    initialization of the OddsEngine and its ~25 adapters. By isolating it,\n    we can run it in a background thread without stalling the main Uvicorn event loop.\n    \"\"\"\n    log.info(\"Background initialization of heavy resources started.\")\n    try:\n        settings = get_settings()\n\n        # Initialize WebSocket connection manager\n        connection_manager = ConnectionManager()\n\n        # Initialize manual override manager\n        manual_override_manager = ManualOverrideManager()\n\n        # Initialize engine with manual override and WebSocket support\n        engine = OddsEngine(\n            config=settings,\n            manual_override_manager=manual_override_manager,\n            connection_manager=connection_manager,\n        )\n\n        # Store the initialized components on the app state\n        app.state.engine = engine\n        app.state.analyzer_engine = AnalyzerEngine()\n        app.state.manual_override_manager = manual_override_manager\n        app.state.connection_manager = connection_manager\n        log.info(\"Background initialization of heavy resources completed successfully.\")\n    except Exception:\n        log.critical(\"CRITICAL: Background initialization failed.\", exc_info=True)\n        # In a real-world scenario, you might want a more robust way\n        # to signal this failure to the main application.\n        app.state.engine = None\n\n\nclass ConnectionManager:\n    \"\"\"Manages active WebSocket connections.\"\"\"\n\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        log.info(\"WebSocket ConnectionManager initialized.\")\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        log.info(\"New WebSocket connection established.\")\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n        log.info(\"WebSocket connection closed.\")\n\n    async def broadcast(self, message: dict):\n        \"\"\"Broadcasts a message to all connected clients.\"\"\"\n        if not self.active_connections:\n            return\n\n        log.info(\n            \"Broadcasting message to connected clients\",\n            client_count=len(self.active_connections),\n        )\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(message)\n            except Exception:\n                log.error(\"Error sending message to a WebSocket client.\", exc_info=True)\n\n\n# Lifespan context manager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    configure_logging()\n    log.info(\"Uvicorn is online, starting lifespan hook.\")\n\n    # 1. Perform lightweight, non-blocking startup tasks\n    settings = get_settings()\n    await cache_manager.connect(settings.REDIS_URL)\n    log.info(\"Fast, non-blocking startup tasks complete (Redis connected).\")\n\n    # 2. Schedule the heavy, synchronous initialization to run in a background thread\n    loop = asyncio.get_event_loop()\n    loop.run_in_executor(executor, _initialize_heavy_resources_sync, app)\n    log.info(\"Heavy resource initialization has been scheduled in a background thread.\")\n\n    # 3. Yield control back to Uvicorn immediately. The server is now ready to accept requests\n    #    while the OddsEngine initializes in the background.\n    yield\n\n    # --- Shutdown Sequence ---\n    log.info(\"Server shutdown sequence initiated.\")\n    if hasattr(app.state, \"engine\") and app.state.engine:\n        log.info(\"Closing HTTP client resources.\")\n        await app.state.engine.close()\n\n    await cache_manager.disconnect()\n    executor.shutdown(wait=False)\n    log.info(\"Server shutdown sequence complete.\")\n\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI(\n    title=\"Fortuna Faucet API\",\n    version=\"2.1\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\",\n)\n\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\napp.add_exception_handler(UserFriendlyException, user_friendly_exception_handler)\napp.include_router(health_router)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"http://localhost:3001\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\"],\n    allow_headers=[\"*\"],\n)\n\n\ndef get_engine(request: Request) -> OddsEngine:\n    return request.app.state.engine\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        return engine.get_all_adapter_statuses()\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_favorite_odds: float = Query(2.5, ge=1.0, le=100.0),\n    min_second_favorite_odds: float = Query(4.0, ge=1.0, le=100.0),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        aggregated_data = await engine.fetch_all_odds(date_str)\n        races = [Race(**r) for r in aggregated_data.get(\"races\", [])]\n        analyzer_engine = request.app.state.analyzer_engine\n        custom_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds,\n        }\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(\"Error in /api/races/qualified\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\n@app.get(\"/api/races/filter-suggestions\")\nasync def get_filter_suggestions(engine: OddsEngine = Depends(get_engine)):\n    try:\n        date_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n        aggregated = await engine.fetch_all_odds(date_str)\n        if not aggregated or not aggregated.get(\"races\"):\n            return {\"suggestions\": {}}\n        field_sizes = [len(r[\"runners\"]) for r in aggregated[\"races\"]]\n        favorite_odds, second_favorite_odds = [], []\n        for race_data in aggregated[\"races\"]:\n            race = Race(**race_data)\n            runners = race.runners\n            if len(runners) >= 2:\n                odds_list = []\n                for runner in runners:\n                    if not runner.scratched and runner.odds:\n                        best_odd = min(\n                            (o.win for o in runner.odds.values() if o.win is not None),\n                            default=None,\n                        )\n                        if best_odd is not None:\n                            odds_list.append(float(best_odd))\n                if len(odds_list) >= 2:\n                    odds_list.sort()\n                    favorite_odds.append(odds_list[0])\n                    second_favorite_odds.append(odds_list[1])\n        return {\n            \"suggestions\": {\n                \"max_field_size\": {\"recommended\": (int(sum(field_sizes) / len(field_sizes)) if field_sizes else 10)},\n                \"min_favorite_odds\": {\"recommended\": 2.5},\n                \"min_second_favorite_odds\": {\"recommended\": 4.0},\n            }\n        }\n    except Exception:\n        log.error(\"Error generating filter suggestions\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Failed to generate suggestions\")\n\n\n@app.get(\"/api/races/source/{source_name}\", response_model=AggregatedResponse)\n@limiter.limit(\"60/minute\")\nasync def get_races_by_source(\n    source_name: str,\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(date_str, source=source_name)\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(f\"Error in /api/races/source/{source_name}\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    source: Optional[str] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(date_str, source)\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\nDB_PATH = \"fortuna.db\"\n\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = \"SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC\"\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    return results\n\n\n# API Models\nclass ManualDataSubmission(BaseModel):\n    request_id: str\n    content: str\n    content_type: str = \"html\"\n\n\n# New endpoints\n@app.get(\"/api/manual-overrides/pending\")\n@limiter.limit(\"60/minute\")\nasync def get_pending_overrides(\n    request: Request,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Get all pending manual override requests\"\"\"\n    pending = manager.get_pending_requests()\n    return {\"pending_requests\": [req.model_dump() for req in pending]}\n\n\n@app.post(\"/api/manual-overrides/submit\")\n@limiter.limit(\"30/minute\")\nasync def submit_manual_data(\n    request: Request,\n    submission: ManualDataSubmission,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Submit manually-provided data for a failed fetch\"\"\"\n    success = manager.submit_manual_data(\n        request_id=submission.request_id,\n        raw_content=submission.content,\n        content_type=submission.content_type,\n    )\n\n    if success:\n        return {\"status\": \"success\", \"message\": \"Manual data submitted\"}\n    else:\n        raise HTTPException(status_code=404, detail=\"Request not found\")\n\n\n@app.post(\"/api/manual-overrides/skip/{request_id}\")\n@limiter.limit(\"60/minute\")\nasync def skip_manual_override(\n    request: Request,\n    request_id: str,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Skip a manual override request\"\"\"\n    success = manager.skip_request(request_id)\n\n    if success:\n        return {\"status\": \"success\", \"message\": \"Request skipped\"}\n    else:\n        raise HTTPException(status_code=404, detail=\"Request not found\")\n\n\n@app.post(\"/api/manual-overrides/cleanup\")\n@limiter.limit(\"60/minute\")\nasync def cleanup_old_overrides(\n    request: Request,\n    max_age_hours: int = 24,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Clean up old manual override requests\"\"\"\n    manager.clear_old_requests(max_age_hours)\n    return {\"status\": \"success\", \"message\": \"Old requests cleaned\"}\n\n\n@app.websocket(\"/ws/live-updates\")\nasync def websocket_endpoint(websocket: WebSocket, api_key: str = Query(...)):\n    \"\"\"WebSocket endpoint for live race updates.\"\"\"\n    try:\n        # Use the existing API key verification logic\n        # This is a synchronous call, which is fine for auth at the start.\n        # In a real-world scenario with high connection rates, you might\n        # want to make this check asynchronous if it involved I/O.\n        verify_api_key(api_key)\n    except HTTPException as e:\n        log.warning(\"WebSocket connection rejected due to invalid API key.\")\n        await websocket.close(code=4001, reason=f\"Authentication failed: {e.detail}\")\n        return\n\n    manager = websocket.app.state.connection_manager\n    await manager.connect(websocket)\n    try:\n        # Keep the connection alive, listening for messages (if any)\n        while True:\n            # You could implement logic here to handle incoming messages if needed\n            # For now, it's just a broadcast-only connection\n            await websocket.receive_text()\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        log.info(\"Client disconnected from WebSocket.\")\n",
    "python_service/cache_manager.py": "# python_service/cache_manager.py\nimport asyncio\nimport hashlib\nimport json\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self):\n        self.redis_client = None\n        self.memory_cache = {}\n        self.is_configured = False\n        log.info(\"CacheManager initialized (not connected).\")\n\n    async def connect(self, redis_url: str):\n        if self.is_configured or not REDIS_AVAILABLE or not redis_url:\n            return\n\n        try:\n            log.info(\"Attempting to connect to Redis...\", url=redis_url)\n            # Use the async version of the client\n            self.redis_client = redis.asyncio.from_url(redis_url, decode_responses=True)\n            await self.redis_client.ping()  # Verify connection asynchronously\n            self.is_configured = True\n            log.info(\"Redis cache connected successfully.\")\n        except (redis.exceptions.ConnectionError, asyncio.TimeoutError) as e:\n            log.warning(\n                \"Failed to connect to Redis. Falling back to in-memory cache.\",\n                error=str(e),\n            )\n            self.redis_client = None\n            self.is_configured = False\n\n    async def disconnect(self):\n        if self.redis_client:\n            await self.redis_client.close()\n            log.info(\"Redis connection closed.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    async def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = await self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis GET failed, falling back to memory cache.\", error=e)\n\n        entry = self.memory_cache.get(key)\n        if entry and entry.get(\"expires_at\", datetime.min) > datetime.now():\n            return entry.get(\"value\")\n        return None\n\n    async def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        try:\n            serialized = json.dumps(value, default=str)\n        except (TypeError, ValueError) as e:\n            log.error(\"Failed to serialize value for caching.\", value=value, error=str(e))\n            return\n\n        if self.redis_client:\n            try:\n                await self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis SET failed, falling back to memory cache.\", error=e)\n\n        self.memory_cache[key] = {\n            \"value\": value,\n            \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds),\n        }\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager()\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = await cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n\n            try:\n                await cache_manager.set(cache_key, result, ttl_seconds)\n            except Exception as e:\n                log.error(\"Failed to store result in cache.\", error=str(e), key=cache_key)\n\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "python_service/fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport structlog\n\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.config import get_settings\nfrom python_service.engine import OddsEngine\nfrom python_service.etl import run_etl_for_yesterday\nfrom python_service.models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = OddsEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n        try:\n            background_tasks = set()  # Create a dummy set for background tasks\n            aggregated_data = await self.odds_engine.fetch_all_odds(today_str, background_tasks)\n            all_races = aggregated_data.get(\"races\", [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer(\"trifecta\")\n            qualified_races_result = analyzer.qualify_races(all_races)\n            qualified_races_list = qualified_races_result.get(\"races\", [])\n            log.info(\n                \"Watchman: Initial target acquisition and ranking complete\",\n                target_count=len(qualified_races_list),\n            )\n\n            # Log the top targets for better observability\n            for race in qualified_races_list[:5]:\n                log.info(\n                    \"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat(),\n                )\n            return qualified_races_list\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        # active_targets = list(targets)\n\n        # from python_service.adapters.betfair_adapter import BetfairAdapter\n        # async with LiveOddsMonitor(betfair_adapter=BetfairAdapter(config=self.settings)) as live_monitor:\n        #     async with httpx.AsyncClient() as client:\n        #         while active_targets:\n        #             now = datetime.now(timezone.utc)\n\n        #             # Find races that are within the 5-minute monitoring window\n        #             races_to_monitor = [\n        #                 r\n        #                 for r in active_targets\n        #                 if r.start_time.replace(tzinfo=timezone.utc) > now\n        #                 and r.start_time.replace(tzinfo=timezone.utc)\n        #                 < now + timedelta(minutes=5)\n        #             ]\n\n        #             if races_to_monitor:\n        #                 for race in races_to_monitor:\n        #                     log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n        #                         race_id=race.id,\n        #                         venue=race.venue,\n        #                         score=race.qualification_score\n        #                     )\n        #                     updated_race = await live_monitor.monitor_race(race, client)\n        #                     log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n        #                     # Remove from target list to prevent re-monitoring\n        #                     active_targets = [t for t in active_targets if t.id != race.id]\n\n        #             if not active_targets:\n        #                 break # Exit loop if all targets are processed\n\n        #             await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        try:\n            initial_targets = await self.get_initial_targets()\n            if initial_targets:\n                await self.run_tactical_monitoring(initial_targets)\n            else:\n                log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n        finally:\n            await self.odds_engine.close()\n\n        # Run ETL for yesterday's data after all other operations are complete\n        try:\n            log.info(\"Starting daily ETL process for Scribe's Archives...\")\n            run_etl_for_yesterday()\n            log.info(\"Daily ETL process completed successfully.\")\n        except Exception:\n            log.error(\"Daily ETL process failed.\", exc_info=True)\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\n\nasync def main():\n    from python_service.logging_config import configure_logging\n\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "python_service/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"127.0.0.1\", port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "python_service/initialize_db.py": "# python_service/initialize_db.py\nfrom db.init import initialize_database\n\n\ndef main():\n    \"\"\"\n    This script exists solely to initialize the database.\n    It should be called before the main server process is started.\n    \"\"\"\n    print(\"Initializing database...\", flush=True)\n    initialize_database()\n    print(\"Database initialization complete.\", flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    # Keep the processor chain simple for maximum reliability in bundled executables.\n    # More complex processors like StackInfoRenderer can cause issues in\n    # constrained environments.\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "python_service/main.py": "import uvicorn\nimport sys\nimport os\nfrom multiprocessing import freeze_support\n\n# Force UTF-8 encoding for stdout and stderr, crucial for PyInstaller on Windows\nos.environ['PYTHONUTF8'] = '1'\n\n# This is the definitive entry point for the Fortuna Faucet backend service.\n# It is designed to be compiled with PyInstaller.\n\ndef main():\n    \"\"\"\n    Primary entry point for the Fortuna Faucet backend application.\n    This function configures and runs the Uvicorn server.\n    It's crucial to launch the app this way to ensure PyInstaller's bootloader\n    can correctly resolve the package context.\n    \"\"\"\n    # When packaged, the executable's path needs to be added to sys.path\n    # to ensure that modules can be found.\n    if getattr(sys, 'frozen', False):\n        # CRITICAL: This is required for multiprocessing to work correctly when\n        # the application is frozen with PyInstaller on Windows.\n        freeze_support()\n\n        # If the application is run as a bundle, the PyInstaller bootloader\n        # extends the sys module by a flag frozen=True and sets the app\n        # path into variable _MEIPASS'.\n        application_path = os.path.dirname(sys.executable)\n        sys.path.append(application_path)\n        # Also add the parent directory to allow for relative imports.\n        sys.path.append(os.path.join(application_path, '..'))\n\n    # It's critical to import the app object *after* the path has been manipulated.\n    from python_service.api import app, HTTPException\n    from python_service.config import get_settings\n    from fastapi.staticfiles import StaticFiles\n    from fastapi.responses import FileResponse\n\n    settings = get_settings()\n\n    # --- Conditional UI Serving for Web Service Mode ---\n    # Only serve the UI if the FORTUNA_MODE environment variable is set to 'webservice'.\n    # This prevents the Electron-packaged backend from trying to serve files it doesn't have.\n    if os.environ.get(\"FORTUNA_MODE\") == \"webservice\":\n        # Define the path to the static UI files, accommodating PyInstaller's bundle.\n        if getattr(sys, 'frozen', False):\n            # In a bundled app, the UI files are in the '_MEIPASS/ui' directory.\n            STATIC_DIR = os.path.join(sys._MEIPASS, \"ui\")\n        else:\n            # In development, they are in the frontend's output directory.\n            STATIC_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"web_platform\", \"frontend\", \"out\"))\n\n        # Mount the static assets directory for CSS, JS, etc.\n        if os.path.exists(os.path.join(STATIC_DIR, \"_next\")):\n            app.mount(\"/_next\", StaticFiles(directory=os.path.join(STATIC_DIR, \"_next\")), name=\"next\")\n\n        # Serve the main index.html for any non-API path.\n        @app.get(\"/{full_path:path}\", include_in_schema=False)\n        async def serve_frontend(full_path: str):\n            if full_path.startswith(\"api/\") or full_path.startswith(\"docs\") or full_path == \"health\":\n                # This is an API route, let FastAPI handle it.\n                # A 404 will be raised naturally if no route matches.\n                return\n\n            index_path = os.path.join(STATIC_DIR, \"index.html\")\n            if os.path.exists(index_path):\n                return FileResponse(index_path)\n            else:\n                # This will only be hit if the frontend files are missing entirely.\n                raise HTTPException(status_code=404, detail=\"Frontend not found. Please build the frontend and ensure it's in the correct location.\")\n\n    # Check if the port is already in use before attempting to start the server.\n    # This prevents a common crash scenario when another instance is running.\n    import socket\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        if s.connect_ex((settings.UVICORN_HOST, settings.UVICORN_PORT)) == 0:\n            print(f\"FATAL: Port {settings.UVICORN_PORT} is already in use. Another process may be running.\")\n            sys.exit(1)\n\n    uvicorn.run(\n        app,\n        host=settings.UVICORN_HOST,\n        port=settings.UVICORN_PORT,\n        log_level=\"info\"\n    )\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/middleware/error_handler.py": "# python_service/middleware/error_handler.py\n\nfrom fastapi import Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\n\nfrom ..user_friendly_errors import ERROR_MAP\n\n\nclass UserFriendlyException(Exception):\n    def __init__(self, error_key: str, status_code: int = 500, details: str = None):\n        self.error_key = error_key\n        self.status_code = status_code\n        self.details = details\n        error_info = ERROR_MAP.get(error_key, ERROR_MAP[\"default\"])\n        self.message = error_info[\"message\"]\n        self.suggestion = error_info[\"suggestion\"]\n        super().__init__(self.message)\n\n\nasync def user_friendly_exception_handler(request: Request, exc: UserFriendlyException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\n            \"error\": {\n                \"message\": exc.message,\n                \"suggestion\": exc.suggestion,\n                \"details\": exc.details,\n            }\n        },\n    )\n\n\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"Convert Pydantic validation errors to user-friendly messages.\"\"\"\n    return JSONResponse(\n        status_code=422,\n        content={\n            \"detail\": \"Invalid request parameters\",\n            \"errors\": [\n                {\n                    \"field\": error[\"loc\"][-1] if error[\"loc\"] else \"unknown\",\n                    \"message\": error[\"msg\"],\n                    \"type\": error[\"type\"],\n                }\n                for error in exc.errors()\n            ],\n        },\n    )\n",
    "python_service/utils/odds.py": "# Centralized odds parsing utility, created by Operation: The A+ Trifecta\nfrom decimal import Decimal\nfrom decimal import InvalidOperation\nfrom typing import Optional\nfrom typing import Union\n\n\ndef parse_odds_to_decimal(odds: Union[str, int, float, None]) -> Optional[Decimal]:\n    \"\"\"\n    Parse various odds formats to Decimal for precise financial calculations.\n    Handles fractional, decimal, and special cases ('EVS', 'SP', etc.).\n    Returns None for unparseable or invalid values.\n    \"\"\"\n    if odds is None:\n        return None\n\n    if isinstance(odds, (int, float)):\n        return Decimal(str(odds))\n\n    odds_str = str(odds).strip().upper()\n\n    SPECIAL_CASES = {\n        \"EVS\": Decimal(\"2.0\"),\n        \"EVENS\": Decimal(\"2.0\"),\n        \"SP\": None,\n        \"SCRATCHED\": None,\n        \"SCR\": None,\n        \"\": None,\n    }\n\n    if odds_str in SPECIAL_CASES:\n        return SPECIAL_CASES[odds_str]\n\n    if \"/\" in odds_str:\n        try:\n            parts = odds_str.split(\"/\")\n            if len(parts) != 2:\n                return None\n            num, den = map(Decimal, parts)\n            if den <= 0:\n                return None\n            return Decimal(\"1.0\") + (num / den)\n        except (ValueError, InvalidOperation):\n            return None\n\n    try:\n        return Decimal(odds_str)\n    except (ValueError, InvalidOperation):\n        return None\n",
    "scripts/convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom multiprocessing import Queue\n\n# --- Configuration ---\nMANIFEST_FILES = [\n    \"MANIFEST_PART1_BACKEND.json\",\n    \"MANIFEST_PART2_FRONTEND.json\",\n    \"MANIFEST_PART3_SUPPORT.json\",\n    \"MANIFEST_PART4_ROOT.json\",\n]\nOUTPUT_DIR = \"ReviewableJSON\"\nFILE_PROCESSING_TIMEOUT = 10\nEXCLUDED_FILES = [\"package-lock.json\"]\nMAX_FILE_SIZE_MB = 10  # Max file size in megabytes\n\n\ndef read_json_manifest(manifest_path: str) -> list[str]:\n    \"\"\"Reads a JSON manifest file and returns a list of file paths.\"\"\"\n    try:\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return []\n\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\n\ndef convert_file_to_json_sandboxed(file_path):\n    # --- Pre-flight check: File size ---\n    try:\n        file_size = os.path.getsize(file_path)\n        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:\n            return {\"error\": f\"File exceeds {MAX_FILE_SIZE_MB}MB size limit.\"}\n    except FileNotFoundError:\n        return {\"error\": \"File not found.\"}\n    except Exception as e:\n        return {\"error\": f\"Could not check file size: {e}\"}\n\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    try:\n        if p.is_alive():\n            print(f\"    [WARNING] Process for {file_path} timed out. Attempting graceful termination...\")\n            p.terminate()\n            p.join(timeout=2)  # Give it a moment to terminate gracefully\n\n            if p.is_alive():\n                print(f\"    [ERROR] Graceful termination failed. Forcibly killing process...\")\n                p.kill()  # The ultimate \"just die\"\n                p.join()\n            return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n        if not q.empty():\n            return q.get()\n        return {\"error\": \"Unknown error in sandboxed read process.\"}\n    finally:\n        # \u2705 Properly close and flush the queue\n        try:\n            while not q.empty():\n                q.get_nowait()\n        except Exception:\n            pass\n        q.close()\n        q.join_thread()\n\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'=' * 60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'=' * 60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        paths = read_json_manifest(manifest)\n        if paths:\n            all_local_paths.extend(paths)\n            print(f\"    --> Found {len(paths)} valid file paths.\")\n        else:\n            print(f\"    [WARNING] Manifest not found or is empty: {manifest}\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        if os.path.basename(local_path) in EXCLUDED_FILES:\n            print(f\"\\n--> Skipping excluded file: {local_path}\")\n            failed_count += 1\n            continue\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + \".json\")\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Backup process complete.\")\n    print(f\"Successfully processed: {processed_count}/{len(unique_local_paths)}\")\n    print(f\"Failed/Skipped: {failed_count}\")\n    print(f\"{'=' * 60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/install_fortuna_gui.bat": "@echo off\nREM Interactive MSI installation with standard Windows UI\n\ntitle Fortuna Faucet Installation Wizard\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Administrator privileges required\n    echo Please right-click this file and select \"Run as Administrator\"\n    pause\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" /L*v \"%TEMP%\\fortuna_install.log\"\n\nif %errorlevel% equ 0 (\n    echo Installation completed successfully!\n    echo Access dashboard at: http://localhost:3000\n) else (\n    echo Installation failed. Log: %TEMP%\\fortuna_install.log\n)\npause",
    "tests/adapters/test_gbgb_api_adapter.py": "# tests/adapters/test_gbgb_api_adapter.py\n\nfrom datetime import date\nfrom decimal import Decimal\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef gbgb_adapter():\n    \"\"\"Returns a GbgbApiAdapter instance for testing.\"\"\"\n    return GbgbApiAdapter(config=get_test_settings())\n\n\n@pytest.mark.asyncio\nasync def test_get_gbgb_races_successfully(gbgb_adapter):\n    \"\"\"\n    SPEC: The GbgbApiAdapter should correctly parse a standard API response,\n    creating Race and Runner objects with the correct data, including fractional odds.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime(\"%Y-%m-%d\")\n    mock_api_response = [\n        {\n            \"trackName\": \"Towcester\",\n            \"races\": [\n                {\n                    \"raceId\": 12345,\n                    \"raceNumber\": 1,\n                    \"raceTime\": \"2025-10-09T18:00:00Z\",\n                    \"raceTitle\": \"The October Sprint\",\n                    \"raceDistance\": 500,\n                    \"traps\": [\n                        {\"trapNumber\": 1, \"dogName\": \"Rapid Rover\", \"sp\": \"5/2\"},\n                        {\"trapNumber\": 2, \"dogName\": \"Speedy Sue\", \"sp\": \"EVS\"},\n                        {\"trapNumber\": 3, \"dogName\": \"Lazy Larry\", \"sp\": \"10/1\"},\n                    ],\n                }\n            ],\n        }\n    ]\n    gbgb_adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in gbgb_adapter.get_races(mock_date)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.venue == \"Towcester\"\n    assert race.race_number == 1\n    assert race.race_name == \"The October Sprint\"\n    assert race.distance == \"500m\"\n    assert len(race.runners) == 3\n\n    runner1 = next(r for r in race.runners if r.number == 1)\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.odds[\"GBGB\"].win == Decimal(\"3.5\")\n\n    runner2 = next(r for r in race.runners if r.number == 2)\n    assert runner2.name == \"Speedy Sue\"\n    assert runner2.odds[\"GBGB\"].win == Decimal(\"2.0\")\n\n    runner3 = next(r for r in race.runners if r.number == 3)\n    assert runner3.name == \"Lazy Larry\"\n    assert runner3.odds[\"GBGB\"].win == Decimal(\"11.0\")\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(gbgb_adapter):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime(\"%Y-%m-%d\")\n    gbgb_adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = [race async for race in gbgb_adapter.get_races(mock_date)]\n\n    # ASSERT\n    assert races == []\n",
    "tests/adapters/test_greyhound_adapter.py": "from datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.greyhound_adapter import GreyhoundAdapter\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef test_settings():\n    \"\"\"Provides a valid Settings object for testing.\"\"\"\n    return get_test_settings()\n\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n\n    mock_api_response = {\n        \"cards\": [\n            {\n                \"track_name\": \"Test Track\",\n                \"races\": [\n                    {\n                        \"race_id\": \"test_race_123\",\n                        \"race_number\": 1,\n                        \"start_time\": int(datetime.now().timestamp()),\n                        \"runners\": [\n                            {\n                                \"dog_name\": \"Rapid Rover\",\n                                \"trap_number\": 1,\n                                \"odds\": {\"win\": \"2.5\"},\n                            },\n                            {\n                                \"dog_name\": \"Swift Sprint\",\n                                \"trap_number\": 2,\n                                \"scratched\": True,\n                            },\n                            {\n                                \"dog_name\": \"Lazy Larry\",\n                                \"trap_number\": 3,\n                                \"odds\": {\"win\": \"10.0\"},\n                            },\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == \"greyhound_test_race_123\"\n    assert race.venue == \"Test Track\"\n    assert len(race.runners) == 2  # One was scratched\n\n    runner1 = race.runners[0]\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.number == 1\n    assert runner1.odds[\"Greyhound Racing\"].win == 2.5\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter handles an empty API response gracefully.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value={\"cards\": []})\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(test_settings):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n",
    "tests/test_manual_override.py": "# tests/test_manual_override.py\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom python_service.api import app\nfrom python_service.api import get_settings\nfrom python_service.manual_override_manager import ManualOverrideManager\nfrom tests.conftest import get_test_settings\n\n# Override settings for tests\napp.dependency_overrides[get_settings] = get_test_settings\nAPI_KEY = get_test_settings().API_KEY\n\n\n@pytest.fixture\ndef manager() -> ManualOverrideManager:\n    \"\"\"Provides a clean ManualOverrideManager instance for each test.\"\"\"\n    return ManualOverrideManager()\n\n\ndef test_register_failure(manager: ManualOverrideManager):\n    adapter_name = \"TestAdapter\"\n    url = \"http://test.com/races\"\n    request_id = manager.register_failure(adapter_name, url)\n    assert request_id is not None\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter_name\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.submit_manual_data(request_id, \"<html></html>\", \"html\")\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is not None\n    assert data[0] == \"<html></html>\"\n    assert data[1] == \"html\"\n\n\ndef test_skip_request(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.skip_request(request_id)\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is None\n\n\ndef test_get_pending_overrides_endpoint(client):\n    # ARRANGE\n    # Access the manager *after* the TestClient has run the lifespan startup\n    manager = client.app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)  # Ensure a clean state by clearing all\n    manager.register_failure(\"EndpointAdapter\", \"http://endpoint.com/data\")\n\n    # ACT\n    response = client.get(\"/api/manual-overrides/pending\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    data = response.json()\n    assert \"pending_requests\" in data\n    assert len(data[\"pending_requests\"]) > 0\n    assert data[\"pending_requests\"][0][\"adapter_name\"] == \"EndpointAdapter\"\n\n\ndef test_submit_manual_data_endpoint(client):\n    # ARRANGE\n    manager = client.app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SubmitAdapter\", \"http://submit.com/data\")\n    submission = {\n        \"request_id\": request_id,\n        \"content\": \"<h1>Hello</h1>\",\n        \"content_type\": \"html\",\n    }\n    response = client.post(\n        \"/api/manual-overrides/submit\",\n        json=submission,\n        headers={\"X-API-Key\": API_KEY},\n    )\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    data = manager.get_manual_data(\"SubmitAdapter\", \"http://submit.com/data\")\n    assert data is not None\n    assert data[0] == \"<h1>Hello</h1>\"\n\n\ndef test_skip_manual_override_endpoint(client):\n    # ARRANGE\n    manager = client.app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SkipAdapter\", \"http://skip.com/data\")\n    response = client.post(f\"/api/manual-overrides/skip/{request_id}\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    # Verify the request is no longer pending\n    pending = manager.get_pending_requests()\n    assert not any(p.request_id == request_id for p in pending)\n",
    "web_platform/api_gateway/package.json": "{\n  \"name\": \"api_gateway\",\n  \"version\": \"1.0.0\",\n  \"main\": \"dist/server.js\",\n  \"scripts\": { \"start\": \"ts-node src/server.ts\" },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"dotenv\": \"^16.3.1\",\n    \"dotenv\": \"^16.3.1\",\n    \"sqlite\": \"^5.1.1\",\n    \"sqlite3\": \"^5.1.7\",\n    \"socket.io\": \"^4.7.4\",\n    \"cors\": \"^2.8.5\"\n  },\n  \"devDependencies\": {\n    \"@types/express\": \"^4.17.21\",\n    \"@types/node\": \"^20.10.0\",\n    \"@types/cors\": \"^2.8.17\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.3.3\"\n  }\n}",
    "web_platform/api_gateway/src/server.ts": "// server.ts - Complete API Gateway with Database Integration and WebSocket\n\nimport express from 'express';\nimport { createServer } from 'http';\nimport { Server as SocketServer } from 'socket.io';\nimport cors from 'cors';\nimport sqlite3 from 'sqlite3';\nimport { open, Database } from 'sqlite';\nimport path from 'path';\n\n// Types\ninterface Race {\n  race_id: string;\n  track_name: string;\n  race_number: number | null;\n  post_time: string | null;\n  checkmate_score: number;\n  qualified: boolean;\n  trifecta_factors_json: string | null;\n  raw_data_json: string | null;\n  updated_at: string;\n}\n\ninterface AdapterStatus {\n  adapter_name: string;\n  status: string;\n  last_run: string;\n  races_found: number;\n  execution_time_ms: number;\n  error_message: string | null;\n}\n\n// Database Service\nclass DatabaseService {\n  private db: Database | null = null;\n  private dbPath: string;\n\n  constructor() {\n    this.dbPath = process.env.FORTUNA_DB_PATH || path.join(process.cwd(), '..', '..', 'shared_database', 'races.db');\n  }\n\n  async connect(): Promise<void> {\n    try {\n      this.db = await open({\n        filename: this.dbPath,\n        driver: sqlite3.Database\n      });\n      console.log(`[INFO] Connected to database: ${this.dbPath}`);\n    } catch (error) {\n      console.error('[ERROR] Failed to connect to database:', error);\n      throw error;\n    }\n  }\n\n  async getQualifiedRaces(): Promise<Race[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const races = await this.db.all<Race[]>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        WHERE qualified = 1\n        ORDER BY checkmate_score DESC, post_time ASC\n      `);\n      return races;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch qualified races:', error);\n      return [];\n    }\n  }\n\n  async getAllRaces(): Promise<Race[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const races = await this.db.all<Race[]>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        ORDER BY post_time ASC\n      `);\n      return races;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch all races:', error);\n      return [];\n    }\n  }\n\n  async getAdapterStatuses(): Promise<AdapterStatus[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const statuses = await this.db.all<AdapterStatus[]>(`\n        SELECT adapter_name, status, last_run, races_found,\n               execution_time_ms, error_message\n        FROM adapter_status\n        ORDER BY last_run DESC\n      `);\n      return statuses;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch adapter statuses:', error);\n      return [];\n    }\n  }\n\n  async getRaceById(raceId: string): Promise<Race | null> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const race = await this.db.get<Race>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        WHERE race_id = ?\n      `, raceId);\n      return race || null;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch race by ID:', error);\n      return null;\n    }\n  }\n}\n\n// Initialize Express and Socket.IO\nconst app = express();\nconst httpServer = createServer(app);\nconst io = new SocketServer(httpServer, {\n  cors: { origin: process.env.ALLOWED_ORIGINS || 'http://localhost:3000' }\n});\n\napp.use(cors());\napp.use(express.json());\n\nconst dbService = new DatabaseService();\n\n// API Endpoints\napp.get('/api/status', (req, res) => {\n  res.json({\n    status: 'online',\n    timestamp: new Date().toISOString(),\n    service: 'Checkmate API Gateway'\n  });\n});\n\napp.get('/api/races', async (req, res) => {\n  try {\n    const races = await dbService.getAllRaces();\n    res.json({ success: true, count: races.length, races });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch races' });\n  }\n});\n\napp.get('/api/races/qualified', async (req, res) => {\n  try {\n    const races = await dbService.getQualifiedRaces();\n    res.json({ success: true, count: races.length, races });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch qualified races' });\n  }\n});\n\napp.get('/api/races/:raceId', async (req, res) => {\n  try {\n    const race = await dbService.getRaceById(req.params.raceId);\n    if (race) {\n      res.json({ success: true, race });\n    } else {\n      res.status(404).json({ success: false, error: 'Race not found' });\n    }\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch race' });\n  }\n});\n\napp.get('/api/adapters/status', async (req, res) => {\n  try {\n    const statuses = await dbService.getAdapterStatuses();\n    res.json({ success: true, count: statuses.length, adapters: statuses });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch adapter statuses' });\n  }\n});\n\n// WebSocket Connection Handling\nio.on('connection', (socket) => {\n  console.log(`[WebSocket] Client connected: ${socket.id}`);\n\n  dbService.getQualifiedRaces().then(races => {\n    socket.emit('races_update', { races });\n  });\n\n  dbService.getAdapterStatuses().then(statuses => {\n    socket.emit('adapters_update', { adapters: statuses });\n  });\n\n  socket.on('disconnect', () => {\n    console.log(`[WebSocket] Client disconnected: ${socket.id}`);\n  });\n\n  socket.on('request_update', async () => {\n    const races = await dbService.getQualifiedRaces();\n    const statuses = await dbService.getAdapterStatuses();\n    socket.emit('races_update', { races });\n    socket.emit('adapters_update', { adapters: statuses });\n  });\n});\n\n// Broadcast updates to all clients periodically\nasync function broadcastUpdates() {\n  try {\n    const races = await dbService.getQualifiedRaces();\n    const statuses = await dbService.getAdapterStatuses();\n\n    io.emit('races_update', { races });\n    io.emit('adapters_update', { adapters: statuses });\n  } catch (error) {\n    console.error('[ERROR] Failed to broadcast updates:', error);\n  }\n}\n\n// Start Server\nconst PORT = process.env.PORT || 8080;\n\nasync function startServer() {\n  try {\n    await dbService.connect();\n\n    httpServer.listen(PORT, () => {\n      console.log('='.repeat(70));\n      console.log(`  Checkmate API Gateway`);\n      console.log(`  Running on port ${PORT}`);\n      console.log(`  Database: ${dbService['dbPath']}`);\n      console.log('='.repeat(70));\n    });\n\n    setInterval(broadcastUpdates, 15000);\n\n  } catch (error) {\n    console.error('[FATAL] Failed to start server:', error);\n    process.exit(1);\n  }\n}\n\n// Graceful shutdown\nprocess.on('SIGINT', async () => {\n  console.log('\\n[INFO] Shutting down gracefully...');\n  httpServer.close();\n  process.exit(0);\n});\n\nprocess.on('SIGTERM', async () => {\n  console.log('\\n[INFO] Shutting down gracefully...');\n  httpServer.close();\n  process.exit(0);\n});\n\nstartServer();",
    "web_platform/api_gateway/tsconfig.json": "{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"commonjs\",\n    \"lib\": [\"ES2020\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}",
    "web_platform/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_platform/frontend/app/layout.tsx": "// web_platform/frontend/app/layout.tsx\nimport './globals.css';\nimport type { Metadata } from 'next';\nimport { Inter } from 'next/font/google';\nimport Providers from './Providers';\n\nconst inter = Inter({ subsets: ['latin'] });\n\nexport const metadata: Metadata = {\n  title: 'Fortuna',\n  description: 'Real-time horse racing analysis.',\n};\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  return (\n    <html lang=\"en\">\n      <body className={`${inter.className} bg-white text-gray-900 dark:bg-gray-900 dark:text-gray-100`}>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  );\n}",
    "web_platform/frontend/src/components/RaceFilters.tsx": "// web_platform/frontend/src/components/RaceFilters.tsx\n'use client';\n\nimport { useState, useCallback } from 'react';\nimport { Settings, RotateCcw } from 'lucide-react';\n\ninterface FilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nexport interface RaceFiltersProps {\n  onParamsChange: (params: FilterParams) => void;\n  isLoading: boolean;\n}\n\nconst DEFAULT_PARAMS: FilterParams = {\n  maxFieldSize: 10,\n  minFavoriteOdds: 2.5,\n  minSecondFavoriteOdds: 4.0,\n};\n\nexport function RaceFilters({ onParamsChange, isLoading }: RaceFiltersProps) {\n  const [params, setParams] = useState<FilterParams>(DEFAULT_PARAMS);\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  // Handle individual parameter changes\n  const handleChange = useCallback((key: keyof FilterParams, value: number) => {\n    setParams(prev => {\n      const updated = { ...prev, [key]: value };\n      onParamsChange(updated);\n      return updated;\n    });\n  }, [onParamsChange]);\n\n  // Reset to defaults\n  const handleReset = useCallback(() => {\n    setParams(DEFAULT_PARAMS);\n    onParamsChange(DEFAULT_PARAMS);\n  }, [onParamsChange]);\n\n  return (\n    <div className=\"bg-gradient-to-r from-slate-800 to-slate-900 rounded-lg p-4 mb-6 border border-slate-700\">\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-2\">\n          <Settings className=\"w-5 h-5 text-amber-500\" />\n          <h3 className=\"text-lg font-semibold text-white\">Race Filters</h3>\n        </div>\n        <button\n          onClick={() => setIsExpanded(!isExpanded)}\n          className=\"text-sm text-slate-400 hover:text-slate-200 transition\"\n        >\n          {isExpanded ? 'Hide' : 'Show'}\n        </button>\n      </div>\n\n      {isExpanded && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 pt-4 border-t border-slate-700\">\n          {/* Max Field Size */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Max Field Size\n              <span className=\"text-amber-500 ml-2\">{params.maxFieldSize}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2\"\n              max=\"20\"\n              value={params.maxFieldSize}\n              onChange={(e) => handleChange('maxFieldSize', parseInt(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Filters races with larger fields</p>\n          </div>\n\n          {/* Min Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"1.5\"\n              max=\"5\"\n              step=\"0.1\"\n              value={params.minFavoriteOdds}\n              onChange={(e) => handleChange('minFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = pickier favorites</p>\n          </div>\n\n          {/* Min Second Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min 2nd Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minSecondFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2.0\"\n              max=\"8\"\n              step=\"0.1\"\n              value={params.minSecondFavoriteOdds}\n              onChange={(e) => handleChange('minSecondFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = better odds separation</p>\n          </div>\n\n          {/* Reset Button */}\n          <div className=\"md:col-span-3 flex justify-end pt-4 border-t border-slate-700\">\n            <button\n              onClick={handleReset}\n              disabled={isLoading}\n              className=\"inline-flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 text-slate-200 rounded text-sm font-medium transition disabled:opacity-50\"\n            >\n              <RotateCcw className=\"w-4 h-4\" />\n              Reset to Defaults\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n}\n",
    "web_platform/frontend/src/components/StatusDetailModal.tsx": "// web_platform/frontend/src/components/StatusDetailModal.tsx\nimport React from 'react';\n\ninterface StatusDetailModalProps {\n  isOpen: boolean;\n  onClose: () => void;\n  status: {\n      title: string;\n      details: string | Record<string, any>;\n  };\n}\n\nexport const StatusDetailModal: React.FC<StatusDetailModalProps> = ({ isOpen, onClose, status }) => {\n  if (!isOpen) {\n    return null;\n  }\n\n  const { title, details } = status;\n  const isDetailsString = typeof details === 'string';\n\n  // Determine status color only if details is an object with a status property\n  const statusColor = !isDetailsString && (details.status === 'SUCCESS' || details.status === 'OK')\n    ? 'text-green-400'\n    : 'text-gray-300'; // Default color\n\n  return (\n    <div className=\"fixed inset-0 bg-black/60 flex items-center justify-center z-50\" onClick={onClose}>\n      <div className=\"bg-gray-800 border border-gray-700 rounded-lg shadow-xl p-6 max-w-lg w-full\" onClick={e => e.stopPropagation()}>\n        <div className=\"flex justify-between items-start mb-4\">\n          <h3 className=\"text-xl font-bold text-white\">{title}</h3>\n          <button onClick={onClose} className=\"text-gray-400 hover:text-white\">&times;</button>\n        </div>\n        <div className=\"space-y-2 text-sm max-h-96 overflow-y-auto pr-2\">\n            {isDetailsString ? (\n                <div className=\"text-gray-300 whitespace-pre-wrap bg-gray-900/50 p-4 rounded-md\">{details}</div>\n            ) : (\n                Object.entries(details).map(([key, value]) => (\n                    <div key={key} className=\"grid grid-cols-3 gap-4 border-b border-gray-700/50 py-2\">\n                    <span className=\"font-semibold text-gray-400 capitalize\">{key.replace(/_/g, ' ')}</span>\n                    <span className={`col-span-2 break-words ${key === 'status' ? statusColor : 'text-gray-300'}`}>\n                        {typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value)}\n                    </span>\n                    </div>\n                ))\n            )}\n        </div>\n        <button\n          onClick={onClose}\n          className=\"bg-gray-600 hover:bg-gray-700 text-white font-bold py-2 px-4 rounded w-full mt-6\"\n        >\n          Close\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n",
    "web_platform/frontend/src/utils/exportManager.ts": "// web_platform/frontend/src/utils/exportManager.ts\nimport { saveAs } from 'file-saver';\nimport * as XLSX from 'xlsx';\n\nexport class ExportManager {\n  static exportToExcel(races: any[], filename: string = 'fortuna_races') {\n    const workbook = XLSX.utils.book_new();\n\n    const summaryData = [\n      ['Total Qualified Races', races.length],\n      ['Generated At', new Date().toLocaleString()]\n    ];\n    const summarySheet = XLSX.utils.aoa_to_sheet(summaryData);\n    XLSX.utils.book_append_sheet(workbook, summarySheet, 'Summary');\n\n    const raceData = races.map(race => ({\n      'Venue': race.venue,\n      'Race Number': race.race_number,\n      'Post Time': new Date(race.start_time).toLocaleString(),\n      'Qualification Score': race.qualification_score || 0,\n      'Field Size': race.runners.filter(r => !r.scratched).length,\n      'Source': race.source\n    }));\n    const raceSheet = XLSX.utils.json_to_sheet(raceData);\n    XLSX.utils.book_append_sheet(workbook, raceSheet, 'Races');\n\n    XLSX.writeFile(workbook, `${filename}_${Date.now()}.xlsx`);\n  }\n}\n",
    "wix/WixUI_CustomInstallDir.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\"\n     xmlns:WixUI=\"http://schemas.microsoft.com/wix/WixUIExtension\">\n  <Fragment>\n    <UI Id=\"WixUI_CustomInstallDir\">\n        <DialogRef Id=\"BrowseDlg\" />\n        <DialogRef Id=\"DiskCostDlg\" />\n        <DialogRef Id=\"ErrorDlg\" />\n        <DialogRef Id=\"FatalError\" />\n        <DialogRef Id=\"FilesInUse\" />\n        <DialogRef Id=\"MsiRMFilesInUse\" />\n        <DialogRef Id=\"PrepareDlg\" />\n        <DialogRef Id=\"UserExit\" />\n        <DialogRef Id=\"WelcomeDlg\" />\n        <DialogRef Id=\"InstallDirDlg\" />\n        <DialogRef Id=\"VerifyReadyDlg\" />\n\n        <!-- Use our custom progress dialog instead of the default -->\n        <DialogRef Id=\"InstallProgressDlg\" />\n\n        <Publish Dialog=\"WelcomeDlg\" Control=\"Next\" Event=\"NewDialog\" Value=\"InstallDirDlg\">1</Publish>\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"WelcomeDlg\">1</Publish>\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Next\" Event=\"SetTargetPath\" Value=\"[WIXUI_INSTALLDIR]\" Order=\"1\" />\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Next\" Event=\"NewDialog\" Value=\"VerifyReadyDlg\" Order=\"2\">1</Publish>\n        <Publish Dialog=\"VerifyReadyDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"InstallDirDlg\" Order=\"1\">NOT Installed</Publish>\n        <Publish Dialog=\"VerifyReadyDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"MaintenanceTypeDlg\" Order=\"2\">Installed</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}