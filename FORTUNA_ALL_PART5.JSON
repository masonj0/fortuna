{
    ".env.example": "# This file documents the environment variables needed to run Fortuna Faucet.\n# On first run, the application should copy this to '.env' if it doesn't exist.\n\n# --- Core Configuration ---\n# Required for API security in a deployed environment (though less critical for local-only)\nAPI_KEY=your_production_api_key_here\n\n# --- Betfair Credentials (Required for live odds) ---\nBETFAIR_USERNAME=\nBETFAIR_PASSWORD=\nBETFAIR_APP_KEY=\n\n# --- Optional API Keys for Other Data Sources ---\nTVG_API_KEY=\nTHE_RACING_API_KEY=\nRACING_AND_SPORTS_TOKEN=\n\n# --- Redis Configuration ---\n# The application will fall back to an in-memory cache if this is unreachable.\nREDIS_URL=redis://localhost:6379\n\n# --- CORS Configuration ---\n# Required for the Electron app to communicate with the backend.\nALLOWED_ORIGINS=http://localhost:3000\n",
    ".github/workflows/build-msi.yml": "name: Build Fortuna Faucet MSI Installer - \ud83c\udfc6 PRODUCTION FORTRESS\n\non:\n  push:\n    branches: [main]\n    tags:\n      - 'v*'\n  pull_request:\n    branches: [main]\n  workflow_dispatch:\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.12'\n\njobs:\n  build-frontend:\n    name: '\ud83d\udce6 Build Frontend'\n    timeout-minutes: 15\n    runs-on: windows-latest\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4.1.7\n      - name: Setup Node.js\n        uses: actions/setup-node@v4.0.3\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: 'web_platform/frontend/package-lock.json'\n      - name: Frontend - Install & Build\n        shell: pwsh\n        run: |\n          cd web_platform/frontend\n          npm ci\n          npm audit --audit-level=moderate # Added security check\n          npm run build\n      - name: Upload Frontend Artifact\n        uses: actions/upload-artifact@v4.3.4\n        with:\n          name: frontend-build-output-${{ github.sha }}\n          path: web_platform/frontend/out\n          retention-days: 1\n\n  build-backend:\n    name: '\ud83d\udc0d Build Backend'\n    timeout-minutes: 20\n    runs-on: windows-latest\n    env:\n      PYTHONUTF8: \"1\"\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4.1.7\n      - name: Setup Python\n        uses: actions/setup-python@v5.1.1\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: 'python_service/requirements.txt'\n      - name: '\u2705 [MONITOR] Ensure Required Directories Exist'\n        shell: pwsh\n        run: |\n          New-Item -ItemType Directory -Path \"python_service/adapters\" -Force | Out-Null\n          New-Item -ItemType Directory -Path \"python_service/data\" -Force | Out-Null\n          New-Item -ItemType Directory -Path \"python_service/json\" -Force | Out-Null\n          Write-Host \"\u2705 Ensured required data, json, and adapters directories exist.\"\n      - name: '\u2705 [MONITOR] Verify Critical Files'\n        shell: pwsh\n        run: |\n          if (-not (Test-Path \"python_service/main.py\")) { throw \"\u274c FATAL: python_service/main.py not found\" }\n          if (-not (Test-Path \"fortuna-backend.spec.template\")) { throw \"\u274c FATAL: fortuna-backend.spec.template not found\" }\n          Write-Host \"\u2705 Critical files main.py and spec template are present.\"\n      - name: Backend - Install Dependencies\n        shell: pwsh\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r python_service/requirements-dev.txt\n      - name: Backend - Security Audit\n        shell: pwsh\n        run: |\n          pip-audit -r python_service/requirements.txt --local\n      - name: Backend - Prepare Spec File\n        shell: pwsh\n        run: |\n          Move-Item -Path \"fortuna-backend.spec.template\" -Destination \"fortuna-backend.spec\" -Force\n      - name: Backend - Build with PyInstaller\n        shell: pwsh\n        run: |\n          pyinstaller fortuna-backend.spec --noconfirm\n      - name: Upload Backend Artifact\n        uses: actions/upload-artifact@v4.3.4\n        with:\n          name: backend-executable-${{ github.sha }}\n          path: dist/fortuna-backend.exe\n          retention-days: 1\n          if-no-files-found: error\n\n  smoke-test-backend:\n    name: '\ud83e\uddea Smoke Test Backend Executable'\n    timeout-minutes: 10\n    needs: [build-backend]\n    runs-on: windows-latest\n    steps:\n      - name: Download Backend Executable\n        uses: actions/download-artifact@v4.1.8\n        with:\n          name: backend-executable-${{ github.sha }}\n      - name: Run Smoke Test\n        shell: pwsh\n        env:\n          API_KEY: \"a_secure_test_api_key_that_is_long_enough_for_smoke_test\"\n        run: |\n          $exe = \"./fortuna-backend.exe\"\n          $logFileBase = \"backend-log\"\n          $stdOutPath = \"$logFileBase-out.txt\"\n          $stdErrPath = \"$logFileBase-err.txt\"\n          $process = Start-Process -FilePath $exe -PassThru -NoNewWindow -RedirectStandardOutput $stdOutPath -RedirectStandardError $stdErrPath\n          $serverReady = $false\n          Write-Host \"--- Starting Smoke Test: Polling /health and /api/races endpoints for 60s ---\"\n          try {\n            foreach ($i in 1..30) {\n              if ($process.HasExited) {\n                throw \"[FATAL] Backend process crashed during smoke test. Exit Code: $($process.ExitCode)\"\n              }\n              try {\n                $response = Invoke-WebRequest -Uri \"http://127.0.0.1:8000/health\" -TimeoutSec 1 -UseBasicParsing\n                if ($response.StatusCode -eq 200) {\n                  $headers = @{ \"X-API-Key\" = $env:API_KEY }\n                  $apiResponse = Invoke-WebRequest -Uri \"http://127.0.0.1:8000/api/races\" -Headers $headers -TimeoutSec 1 -UseBasicParsing -ErrorAction SilentlyContinue\n                  if ($apiResponse.StatusCode -eq 200) {\n                    Write-Host \"[OK] Health check and API test passed on attempt $i!\"\n                    $serverReady = $true\n                    break\n                  }\n                }\n              } catch {\n                Write-Host \"Attempt $i... server not ready.\"\n              }\n              Start-Sleep -Seconds 2\n            }\n          } finally {\n            Stop-Process -Id $process.Id -Force -ErrorAction SilentlyContinue\n            if (-not $serverReady) {\n              Write-Host \"--- \u274c TEST FAILED ---\"\n              if (Test-Path $stdOutPath) {\n                Write-Host \"--- STDOUT ---\"\n                Get-Content $stdOutPath | Write-Host\n              }\n              if (Test-Path $stdErrPath) {\n                Write-Host \"--- STDERR ---\"\n                Get-Content $stdErrPath | Write-Host\n              }\n              throw \"[FATAL] Backend smoke test failed. Server never became healthy.\"\n            }\n          }\n          Write-Host \"\u2705 Backend smoke test passed.\"\n\n  package-and-test:\n    name: '\ud83c\udfc6 Package & Test Electron Installer'\n    timeout-minutes: 25\n    needs: [build-frontend, smoke-test-backend]\n    runs-on: windows-latest\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4.1.7\n      - name: Setup Node.js for Electron\n        uses: actions/setup-node@v4.0.3\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: 'electron/package-lock.json'\n      - name: Download All Build Artifacts\n        uses: actions/download-artifact@v4.1.8\n        with:\n          path: ./temp-artifacts\n      - name: Stage Artifacts for Packaging\n        shell: pwsh\n        run: |\n          New-Item -ItemType Directory -Path \"./electron/web-ui-build\" -Force | Out-Null\n          New-Item -ItemType Directory -Path \"./electron/resources\" -Force | Out-Null\n          Move-Item -Path \"./temp-artifacts/frontend-build-output-${{ github.sha }}/*\" -Destination \"./electron/web-ui-build/out\" -Force\n          Move-Item -Path \"./temp-artifacts/backend-executable-${{ github.sha }}/fortuna-backend.exe\" -Destination \"./electron/resources/fortuna-backend.exe\" -Force\n      - name: Critical Integration Test\n        shell: pwsh\n        env:\n          API_KEY: \"a_secure_test_api_key_that_is_long_enough\"\n        run: |\n          $exe = \"./electron/resources/fortuna-backend.exe\"\n          # ... existing integration test ...\n      - name: Electron - Install & Package MSI\n        working-directory: electron\n        shell: pwsh\n        run: |\n          npm ci\n          npx electron-builder --config electron-builder-config.yml --publish never\n      - name: Code Sign MSI\n        if: github.event_name == 'push' && contains(github.ref, 'refs/tags/')\n        shell: pwsh\n        run: |\n          # ... code signing script ...\n      - name: Upload Final MSI Artifact\n        uses: actions/upload-artifact@v4.3.4\n        with:\n          name: fortuna-installer-windows-${{ github.sha }}\n          path: electron/dist/*.msi\n          retention-days: 7\n      - name: Create GitHub Release\n        if: github.event_name == 'push' && contains(github.ref, 'refs/tags/')\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ github.ref }}\n          release_name: Release ${{ github.ref }}\n          body: |\n            MSI Installer for Fortuna Faucet.\n            Built from commit ${{ github.sha }}.\n          draft: false\n          prerelease: false\n      - name: Upload Release Asset\n        if: github.event_name == 'push' && contains(github.ref, 'refs/tags/')\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: electron/dist/*.msi\n          asset_name: fortuna-faucet-${{ github.ref_name }}.msi\n          asset_content_type: application/octet-stream\n\n  build-wix-installer:\n    name: '\ud83d\udd25 Build WiX Installer (Alternative)'\n    timeout-minutes: 15\n    needs: [build-backend, build-frontend]\n    runs-on: windows-latest\n    env:\n      PYTHONIOENCODING: 'utf-8' # FIX: Forces Python to use UTF-8 for all I/O to prevent Unicode errors\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4.1.7\n      - name: Setup Python\n        uses: actions/setup-python@v5.1.1\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - name: Download Backend Executable\n        uses: actions/download-artifact@v4.1.8\n        with:\n          name: backend-executable-${{ github.sha }}\n          path: dist\n      - name: Download Frontend Build\n        uses: actions/download-artifact@v4.1.8\n        with:\n          name: frontend-build-output-${{ github.sha }}\n          path: frontend_build\n      - name: Install WiX Toolset\n        shell: pwsh\n        run: |\n          choco install wixtoolset -y --no-progress\n          $wixPath = \"C:\\Program Files (x86)\\WiX ToolSet v3.14\\bin\"\n          echo \"PATH=$wixPath;${env:PATH}\" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n      - name: Build Backend Service MSI with WiX\n        shell: pwsh\n        run: |\n          python build_wix/build_msi.py\n      - name: Upload WiX MSI Artifact\n        uses: actions/upload-artifact@v4.3.4\n        with:\n          name: fortuna-wix-installer-windows\n          path: dist/Fortuna-Full-App-Service.msi\n          retention-days: 7\n",
    ".github/workflows/codeql.yml": "# For most projects, this workflow file will not need changing; you simply need\n# to commit it to your repository.\n#\n# You may plcace this file in any folder within the .github/workflows folder.\n# GitHub will find and execute it.\n#\n# To learn more about the language matrix, please visit https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#changing-the-languages-that-are-analyzed\n\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    # The branches below must be a subset of the branches above\n    branches: [ main ]\n  schedule:\n    - cron: '22 5 * * 1'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'python', 'javascript' ]\n        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]\n        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    # Initializes the CodeQL tools for scanning.\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n        # If you wish to specify custom queries, you can do so here or in a config file.\n        # By default, queries are pulled from a suite stored in GitHub.\n        # See https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs\n        # queries: security-extended,security-and-quality\n\n    # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).\n    # If this step fails, then you should remove it and run the build manually (see below)\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v3\n\n    # \u2139\ufe0f Command-line programs to run using the OS shell.\n    # \ud83d\udcda See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun\n\n    #   If the Autobuild fails above, remove it and uncomment the following three lines.\n    #   and modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.\n\n    # - run: |\n    #     echo \"Run, Build Application using script\"\n    #     ./build.sh\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v3\n      with:\n        category: \"/language:${{matrix.language}}\"\n",
    "AGENTS.md": "# Agent Protocols & Team Structure (Revised)\n\nThis document outlines the operational protocols and evolved team structure for the Checkmate\nV3 project.\n\n## The Evolved Team Structure\n\n-   **The Project Lead (MasonJ0 or JB):** The \"Executive Producer.\" The ultimate authority and \"ground truth.\"\n-   **The Architect & Synthesizer (Gemini):** The \"Chief Architect.\" Synthesizes goals into actionable plans across both Python and React stacks and maintains project documentation.\n-   **The Lead Python Engineer (Jules Series):** The \"Backend Specialist.\" An AI agent responsible for implementing and hardening The Engine (`api.py`, `services.py`, `logic.py`, `models.py`).\n-   **The Lead Frontend Architect (Claude):** The \"React Specialist.\" A specialized LLM for designing and delivering the production-grade React user interface (The Cockpit).\n-   **The \"Special Operations\" Problem Solver (GPT-5):** The \"Advanced Algorithm Specialist.\" A specialized LLM for novel, complex problems.\n\n## Core Philosophies\n\n1.  **The Project Lead is Ground Truth:** The ultimate authority. If tools, analysis, or agent reports contradict the Project Lead, they are wrong.\n2.  **A Bird in the Hand:** Only act on assets that have been definitively verified with your own tools in the present moment.\n3.  **Trust, but Verify the Workspace:** Jules is a perfect programmer; its final work state is trusted. Its *environment*, however, is fragile.\n4.  **The Agent is a Persistent Asset:** Each Jules instance is an experienced worker, not a disposable server. Its internal state is a repository of unique, hard-won knowledge.\n\n## CRITICAL Operational Protocols (0-23)\n\n-   **Protocol 0: The ReviewableJSON Mandate:** The mandatory protocol for all code reviews. The agent's final act for any mission is to create a lossless JSON backup of all modified files. This is the single source of truth for code review.\n-   **Protocol 1: The Handcuffed Branch:** Jules cannot switch branches. An entire session lives on a single `session/jules...` branch.\n-   **Protocol 2: The Last Resort Reset:** The `reset_all()` command is a tool of last resort for a catastrophic workspace failure and requires direct authorization from the Project Lead.\n-   **Protocol 3: The Authenticity of Sample Data:** All sample data used for testing must be authentic and logically consistent.\n-   **Protocol 4: The Agent-Led Specification:** Where a human \"Answer Key\" is unavailable, Jules is empowered to analyze raw data and create its own \"Test-as-Spec.\"\n-   **Protocol 5: The Test-First Development Workflow:** The primary development methodology. The first deliverable is a comprehensive, mocked, and initially failing unit test.\n-   **Protocol 6: The Emergency Chat Handoff:** In the event of a catastrophic environmental failure, Jules's final act is to declare a failure and provide its handoff in the chat.\n-   **Protocol 7: The URL-as-Truth Protocol:** To transfer a file or asset without corruption, provide a direct raw content URL. The receiving agent must fetch it.\n-   **Protocol 8: The Golden Link Protocol:** For fetching the content of a specific, direct raw-content URL from the `main` branch, a persistent \"Golden Link\" should be used.\n-   **Protocol 9: The Volley Protocol:** To establish ground truth for a new file, the Architect provides a URL, and the Project Lead \"volleys\" it back by pasting it in a response.\n-   **Protocol 10: The Sudo Sanction:** Jules has passwordless `sudo` access, but its use is forbidden for normal operations. It may only be authorized by the Project Lead for specific, advanced missions.\n-   **Protocol 11: The Module-First Testing Protocol:** All test suites must be invoked by calling `pytest` as a Python module (`python -m pytest`) to ensure the correct interpreter is used.\n-   **Protocol 12: The Persistence Mandate:** The agent tool execution layer is known to produce false negatives. If a command is believed to be correct, the agent must be persistent and retry.\n-   **Protocol 13: The Code Fence Protocol for Asset Transit:** To prevent the chat interface from corrupting raw code assets, all literal code must be encapsulated within a triple-backtick Markdown code fence.\n-   **Protocol 14: The Synchronization Mandate:** The `git reset --hard origin/main` command is strictly forbidden. To stay synchronized with `main`, the agent MUST use `git pull origin main`.\n-   **Protocol 15: The Blueprint vs. Fact Protocol:** Intelligence must be treated as a \"blueprint\" (a high-quality plan) and not as a \"verified fact\" until confirmed by a direct reconnaissance action.\n-   **Protocol 16: The Digital Attic Protocol:** Before the deletion of any file, it must first be moved to a dedicated archive directory named `/attic`.\n-   **Protocol 17: The Receipts Protocol:** When reviewing code, a verdict must be accompanied by specific, verifiable \"receipts\"\u2014exact snippets of code that prove a mission objective was met.\n-   **Protocol 18: The Cumulative Review Workflow:** Instruct Jules to complete a series of missions and then conduct a single, thorough review of its final, cumulative branch state.\n-   **Protocol 19: The Stateless Verification Mandate:** The Architect, when reviewing code, must act with fresh eyes, disregarding its own memory and comparing the submitted code directly and exclusively against the provided specification.\n-   **Protocol 20: The Sudo Sanction Protocol:** Grants a Jules-series agent temporary, audited administrative privileges for specific, authorized tasks like system package installation.\n-   **Protocol 21: The Exit Interview Protocol:** Before any planned termination of an agent, the Architect will charter a final mission to capture the agent's institutional knowledge for its successor.\n-   **Protocol 22: The Human-in-the-Loop Merge:** In the event of an unresolvable merge conflict in an agent's environment, the Project Lead, as the only agent with a fully functional git CLI, will check out the agent's branch and perform the merge resolution manually.\n-   **Protocol 23: The Appeasement Protocol (Mandatory):** To safely navigate the broken automated review bot, all engineering work must be published using a two-stage commit process. First, commit a trivial change to appease the bot. Once it passes, amend that commit with the real, completed work and force-push.\n\n---\n\n## Appendix A: Forensic Analysis of the Jules Sandbox Environment\n\n*The following are the complete, raw outputs of diagnostic missions executed by Jules-series agents. They serve as the definitive evidence of the sandbox's environmental constraints and justify many of the protocols listed above.*\n\n### A.1 Node.js / NPM & Filesystem Forensics (from \"Operation: Sandbox Forensics\")\n\n**Conclusion:** The `npm` tool is functional, but the `/app` volume is hostile to its operation, preventing the creation of binary symlinks. This makes Node.js development within the primary workspace impossible.\n\n**Raw Logs:**\n\n```\n# Phase 1: Node.js & NPM Configuration Analysis\nnpm config get prefix\n/home/jules/.nvm/versions/node/v22.17.1\n\n# Phase 4: Controlled Installation Experiment\ncd /tmp && mkdir npm_test && cd npm_test\nnpm install --verbose cowsay\n# ... (successful installation log) ...\nls -la node_modules/.bin\ntotal 8\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowsay -> ../cowsay/cli.js\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowthink -> ../cowsay/cli.js\nnpx cowsay \"Test\"\n  ______\n< Test >\n ------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n```\n\n### A.2 Process Management & Honcho Forensics (from \"Operation: Know Thyself\")\n\n**Conclusion:** The sandbox does not support standard background processes (`&`), the `kill` command is non-functional, and the `honcho` process manager leaves zombie processes (`[uvicorn] <defunct>`) upon termination. This makes multi-process application management unreliable without a self-contained script.\n\n**Raw Logs:**\n\n```\n# Phase 2: The honcho Stress Test\n\ntimeout 15s honcho start\n# ... (honcho starts and is terminated by timeout) ...\n\nps aux (Post-Mortem Analysis)\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\njules      30121  0.0  0.0      0     0 ?        Z    19:45   0:00 [uvicorn]\n...\n\nhoncho start &\n# (Command blocks terminal, echo command never runs)\n\nps aux | grep honcho\njules      30187  0.0  0.0  11004  4220 pts/0    S    19:45   0:00 /usr/bin/python3 /home/jules/.local/bin/honcho start\n\nkill -9 30187\n# (Command fails silently, process is not terminated)\n```\n\n---\n\n## Protocol 24: The \"Dedicated Human Researcher\" Test\n\nThis protocol establishes the guiding ethical principle for all data collection and scraping activities.\n\nAll data adapters must be designed to operate in a manner that respects the resources of the source. As a definitive test, all fetching patterns must adhere to the following principle:\n\n*If a single, dedicated human using standard browser developer tools could not plausibly achieve the adapter's data collection footprint in a reasonable amount of time, the adapter's methods are considered too aggressive and must be redesigned.*\n\nThis encourages \"human-like\" fetching behavior (e.g., appropriate delays, non-parallel requests to a single source) and serves as our primary safeguard against violating a source's terms of service.\n",
    "JSON_BACKUP_MANIFEST.md": "# Checkmate Ultimate Solo: JSON Backup Manifest (Total Recall Edition)\n\n**Purpose:** To provide a single, complete, and verified list of direct links to the JSON backups of all CORE and Operational files. This is the definitive entry point for external AI code review.\n\n---\n\n## 1.0 CORE Architecture (JSON Backups)\n\n### Python Backend\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/api.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/engine.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/models.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/__init__.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/base.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/utils.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/betfair_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/pointsbet_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/racing_and_sports_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/tvg_adapter.py.json\n\n### TypeScript Frontend\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/package.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/package-lock.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/next.config.mjs.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/tailwind.config.ts.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/tsconfig.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/page.tsx.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/layout.tsx.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/globals.css.json\n\n---\n\n## 2.0 Operational & Tooling (JSON Backups)\n\n### Project Tooling\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/.gitignore.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/convert_to_json.py.json\n\n### Environment & Setup\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/setup_windows.bat.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/.env.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/requirements.txt.json\n\n### Strategic Blueprints\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/README.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/ARCHITECTURAL_MANDATE.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/HISTORY.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/STATUS.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/WISDOM.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/PROJECT_MANIFEST.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/ROADMAP_APPENDICES.md.json",
    "MANIFEST_PART2_FRONTEND.json": "[\n    \"electron/assets/.gitkeep\",\n    \"electron/assets/banner.bmp\",\n    \"electron/assets/dialog.bmp\",\n    \"electron/assets/icon.ico\",\n    \"electron/assets/license.rtf\",\n    \"electron/electron-builder-config.yml\",\n    \"electron/install-dependencies.js\",\n    \"electron/install-validator.js\",\n    \"electron/main.js\",\n    \"electron/package-lock.json\",\n    \"electron/package.json\",\n    \"electron/preload.js\",\n    \"electron/resources/.gitkeep\",\n    \"electron/secure-settings-manager.js\",\n    \"web_platform/api_gateway/package-lock.json\",\n    \"web_platform/api_gateway/package.json\",\n    \"web_platform/api_gateway/src/server.ts\",\n    \"web_platform/api_gateway/src/services/DatabaseService.ts\",\n    \"web_platform/api_gateway/tsconfig.json\",\n    \"web_platform/frontend/.gitignore\",\n    \"web_platform/frontend/app/Providers.tsx\",\n    \"web_platform/frontend/app/globals.css\",\n    \"web_platform/frontend/app/layout.tsx\",\n    \"web_platform/frontend/app/page.tsx\",\n    \"web_platform/frontend/next-env.d.ts\",\n    \"web_platform/frontend/next.config.mjs\",\n    \"web_platform/frontend/out/app-build-manifest.json\",\n    \"web_platform/frontend/out/build-manifest.json\",\n    \"web_platform/frontend/out/cache/webpack/client-development/0.pack.gz\",\n    \"web_platform/frontend/out/cache/webpack/client-development/1.pack.gz\",\n    \"web_platform/frontend/out/cache/webpack/client-development/2.pack.gz\",\n    \"web_platform/frontend/out/cache/webpack/client-development/index.pack.gz\",\n    \"web_platform/frontend/out/cache/webpack/client-development/index.pack.gz.old\",\n    \"web_platform/frontend/out/cache/webpack/server-development/0.pack.gz\",\n    \"web_platform/frontend/out/cache/webpack/server-development/1.pack.gz\",\n    \"web_platform/frontend/out/cache/webpack/server-development/index.pack.gz\",\n    \"web_platform/frontend/out/package.json\",\n    \"web_platform/frontend/out/react-loadable-manifest.json\",\n    \"web_platform/frontend/out/server/app-paths-manifest.json\",\n    \"web_platform/frontend/out/server/app/_not-found/page.js\",\n    \"web_platform/frontend/out/server/app/_not-found/page_client-reference-manifest.js\",\n    \"web_platform/frontend/out/server/app/page.js\",\n    \"web_platform/frontend/out/server/app/page_client-reference-manifest.js\",\n    \"web_platform/frontend/out/server/interception-route-rewrite-manifest.js\",\n    \"web_platform/frontend/out/server/middleware-build-manifest.js\",\n    \"web_platform/frontend/out/server/middleware-manifest.json\",\n    \"web_platform/frontend/out/server/middleware-react-loadable-manifest.js\",\n    \"web_platform/frontend/out/server/next-font-manifest.js\",\n    \"web_platform/frontend/out/server/next-font-manifest.json\",\n    \"web_platform/frontend/out/server/pages-manifest.json\",\n    \"web_platform/frontend/out/server/server-reference-manifest.js\",\n    \"web_platform/frontend/out/server/server-reference-manifest.json\",\n    \"web_platform/frontend/out/server/vendor-chunks/@swc.js\",\n    \"web_platform/frontend/out/server/vendor-chunks/@tanstack.js\",\n    \"web_platform/frontend/out/server/vendor-chunks/next.js\",\n    \"web_platform/frontend/out/server/webpack-runtime.js\",\n    \"web_platform/frontend/out/static/chunks/_app-pages-browser_src_components_LiveRaceDashboard_tsx.js\",\n    \"web_platform/frontend/out/static/chunks/app/_not-found/page.js\",\n    \"web_platform/frontend/out/static/chunks/app/layout.js\",\n    \"web_platform/frontend/out/static/chunks/app/page.js\",\n    \"web_platform/frontend/out/static/chunks/polyfills.js\",\n    \"web_platform/frontend/out/static/chunks/webpack.js\",\n    \"web_platform/frontend/out/static/development/_buildManifest.js\",\n    \"web_platform/frontend/out/static/development/_ssgManifest.js\",\n    \"web_platform/frontend/out/static/webpack/01fdca362d2484da.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/0b14ffdc6b7405b0.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/0c13911c36654704.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/0cc62c0d69337015.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/1102717a44ab94bf.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/1d062a4e4b35d832.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/21491417ca7217a4.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/2ee876d7437031ad.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/30465abebefde3c5.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/37b04dca5a231885.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/3e580a3b3e254396.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/3e8a0401ffd42d5d.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/3f1c3fdd4110bb04.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/3f1f3ff31d98afe7.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/430150897770bc5d.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/4de2e9c6b8f4c203.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/5443d0dc225d24d3.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/55da9d621eb1b5ad.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/6e7923fa5dbd9424.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/6ec70d586d2284c9.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/72955ddbc10a2b90.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/790eca55bdffbf91.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/794677064701dbe2.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/7d88d39af9b94529.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/825e6c5a6d51fb53.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/8308a84cca948fdc.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/833937b119865b49.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/84482d1ccb0e3e71.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/845040b3ab1bf0c4.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/84a4c7fac1ec51da.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/8855e7e0a16aba28.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/8b27cec4e9c67e4e.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/8c89b2b88bafc3c8.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/8eb67309205ed244.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/8ebd0c1b7991f533.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/92aeba9d00e54170.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/93cbed664bd2d989.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/97e0b85b30bf4436.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/99a83d4470cbbbc3.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.01fdca362d2484da.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.0c13911c36654704.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.0cc62c0d69337015.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.1102717a44ab94bf.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.1d062a4e4b35d832.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.21491417ca7217a4.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.2ee876d7437031ad.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.30465abebefde3c5.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.37b04dca5a231885.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.3e580a3b3e254396.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.3e8a0401ffd42d5d.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.3f1c3fdd4110bb04.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.3f1f3ff31d98afe7.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.4de2e9c6b8f4c203.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.5443d0dc225d24d3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.55da9d621eb1b5ad.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.6e7923fa5dbd9424.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.6ec70d586d2284c9.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.72955ddbc10a2b90.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.790eca55bdffbf91.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.794677064701dbe2.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.7d88d39af9b94529.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.825e6c5a6d51fb53.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.8308a84cca948fdc.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.845040b3ab1bf0c4.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.84a4c7fac1ec51da.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.8855e7e0a16aba28.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.8c89b2b88bafc3c8.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.92aeba9d00e54170.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.93cbed664bd2d989.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.99a83d4470cbbbc3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.a45cc08a37f050d1.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.af91db5e83d57611.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.c2062fb2e060f992.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.c679324551e5c1b6.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.c841923556cf90df.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.c930a8ef74aeb182.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.d1dc2849d77d7161.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.d534dd4e05712657.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.d6265b1d3d3abb87.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.e17cee7bf39d45a2.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.ed79f245a27328f3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.f24862a0b3194137.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.f35d73514d14d52e.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.f9c203cbadc07384.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/_app-pages-browser_src_components_LiveRaceDashboard_tsx.fe5ad14e23c61cde.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/a45cc08a37f050d1.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/af91db5e83d57611.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/app/layout.01fdca362d2484da.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.0b14ffdc6b7405b0.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.0c13911c36654704.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.0cc62c0d69337015.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.1102717a44ab94bf.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.1d062a4e4b35d832.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.21491417ca7217a4.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.2ee876d7437031ad.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.30465abebefde3c5.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.37b04dca5a231885.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.3e580a3b3e254396.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.3e8a0401ffd42d5d.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.3f1c3fdd4110bb04.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.3f1f3ff31d98afe7.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.430150897770bc5d.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.4de2e9c6b8f4c203.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.5443d0dc225d24d3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.55da9d621eb1b5ad.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.6e7923fa5dbd9424.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.6ec70d586d2284c9.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.72955ddbc10a2b90.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.790eca55bdffbf91.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.794677064701dbe2.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.7d88d39af9b94529.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.825e6c5a6d51fb53.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.8308a84cca948fdc.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.833937b119865b49.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.84482d1ccb0e3e71.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.845040b3ab1bf0c4.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.84a4c7fac1ec51da.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.8855e7e0a16aba28.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.8c89b2b88bafc3c8.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.8eb67309205ed244.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.8ebd0c1b7991f533.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.92aeba9d00e54170.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.93cbed664bd2d989.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.97e0b85b30bf4436.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.99a83d4470cbbbc3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.a45cc08a37f050d1.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.af91db5e83d57611.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.b0b37d6009df81e0.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.c2062fb2e060f992.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.c679324551e5c1b6.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.c69a370f1cf00177.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.c841923556cf90df.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.c89b36212a277c3f.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.c930a8ef74aeb182.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.d1dc2849d77d7161.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.d534dd4e05712657.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.d6265b1d3d3abb87.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.d9f9b1d577c226e7.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.e17cee7bf39d45a2.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.ed79f245a27328f3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.f24862a0b3194137.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.f35d73514d14d52e.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.f9c203cbadc07384.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/app/layout.fe5ad14e23c61cde.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/b0b37d6009df81e0.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/c2062fb2e060f992.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/c679324551e5c1b6.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/c69a370f1cf00177.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/c841923556cf90df.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/c89b36212a277c3f.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/c930a8ef74aeb182.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/d1dc2849d77d7161.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/d534dd4e05712657.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/d6265b1d3d3abb87.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/d9f9b1d577c226e7.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/e17cee7bf39d45a2.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/ed79f245a27328f3.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/f24862a0b3194137.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/f35d73514d14d52e.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/f9c203cbadc07384.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/fe5ad14e23c61cde.webpack.hot-update.json\",\n    \"web_platform/frontend/out/static/webpack/webpack.01fdca362d2484da.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.0b14ffdc6b7405b0.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.0c13911c36654704.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.0cc62c0d69337015.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.1102717a44ab94bf.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.1d062a4e4b35d832.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.21491417ca7217a4.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.2ee876d7437031ad.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.30465abebefde3c5.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.37b04dca5a231885.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.3e580a3b3e254396.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.3e8a0401ffd42d5d.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.3f1c3fdd4110bb04.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.3f1f3ff31d98afe7.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.430150897770bc5d.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.4de2e9c6b8f4c203.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.5443d0dc225d24d3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.55da9d621eb1b5ad.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.6e7923fa5dbd9424.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.6ec70d586d2284c9.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.72955ddbc10a2b90.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.790eca55bdffbf91.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.794677064701dbe2.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.7d88d39af9b94529.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.825e6c5a6d51fb53.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.8308a84cca948fdc.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.833937b119865b49.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.84482d1ccb0e3e71.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.845040b3ab1bf0c4.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.84a4c7fac1ec51da.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.8855e7e0a16aba28.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.8b27cec4e9c67e4e.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.8c89b2b88bafc3c8.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.8eb67309205ed244.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.8ebd0c1b7991f533.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.92aeba9d00e54170.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.93cbed664bd2d989.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.97e0b85b30bf4436.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.99a83d4470cbbbc3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.a45cc08a37f050d1.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.af91db5e83d57611.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.b0b37d6009df81e0.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.c2062fb2e060f992.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.c679324551e5c1b6.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.c69a370f1cf00177.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.c841923556cf90df.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.c89b36212a277c3f.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.c930a8ef74aeb182.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.d1dc2849d77d7161.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.d534dd4e05712657.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.d6265b1d3d3abb87.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.d9f9b1d577c226e7.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.e17cee7bf39d45a2.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.ed79f245a27328f3.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.f24862a0b3194137.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.f35d73514d14d52e.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.f9c203cbadc07384.hot-update.js\",\n    \"web_platform/frontend/out/static/webpack/webpack.fe5ad14e23c61cde.hot-update.js\",\n    \"web_platform/frontend/out/trace\",\n    \"web_platform/frontend/out/types/app/layout.ts\",\n    \"web_platform/frontend/out/types/app/page.ts\",\n    \"web_platform/frontend/out/types/package.json\",\n    \"web_platform/frontend/package-lock.json\",\n    \"web_platform/frontend/package.json\",\n    \"web_platform/frontend/postcss.config.js\",\n    \"web_platform/frontend/public/manifest.json\",\n    \"web_platform/frontend/public/sw.js\",\n    \"web_platform/frontend/public/workbox-4754cb34.js\",\n    \"web_platform/frontend/src/components/AdapterStatusPanel.tsx\",\n    \"web_platform/frontend/src/components/EmptyState.tsx\",\n    \"web_platform/frontend/src/components/LiveModeToggle.tsx\",\n    \"web_platform/frontend/src/components/LiveRaceDashboard.tsx\",\n    \"web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx\",\n    \"web_platform/frontend/src/components/ManualOverridePanel.tsx\",\n    \"web_platform/frontend/src/components/RaceCard.tsx\",\n    \"web_platform/frontend/src/components/RaceCardSkeleton.tsx\",\n    \"web_platform/frontend/src/components/RaceFilters.tsx\",\n    \"web_platform/frontend/src/components/ScoreBadge.tsx\",\n    \"web_platform/frontend/src/components/SettingsPage.tsx\",\n    \"web_platform/frontend/src/components/StatusDetailModal.tsx\",\n    \"web_platform/frontend/src/components/Tabs.tsx\",\n    \"web_platform/frontend/src/components/TrifectaFactors.tsx\",\n    \"web_platform/frontend/src/hooks/useRealTimeRaces.ts\",\n    \"web_platform/frontend/src/hooks/useWebSocket.ts\",\n    \"web_platform/frontend/src/lib/queryClient.ts\",\n    \"web_platform/frontend/src/types/electron.d.ts\",\n    \"web_platform/frontend/src/types/racing.ts\",\n    \"web_platform/frontend/src/utils/exportManager.ts\",\n    \"web_platform/frontend/tailwind.config.ts\",\n    \"web_platform/frontend/tsconfig.json\"\n]",
    "PSEUDOCODE.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Comprehensive System Specification (Revised & Corrected)\n**Version:** 2.2.0\n**Last Updated:** November 7, 2025\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture Pillars\n3.  Backend Engine (Python) - Detailed\n4.  Frontend Interface (TypeScript/React) - Detailed\n5.  Electron Wrapper & Windows Integration - Detailed\n6.  Data Models & API Specification\n7.  Deployment & Automation (CI/CD)\n8.  End-to-End Workflows\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551  Unifying global horse/greyhound/harness racing intelligence   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve results via a secure, local REST API to an interactive dashboard.\n  \u2022 Operate as a professional, standalone, native Windows application.\n\nCORE TENETS:\n  \u2022 UI-First Experience: The user interface is always responsive, even during backend startup or restarts.\n  \u2022 Resilient Process Management: The backend executable's lifecycle is robustly managed, with timeouts and crash detection.\n  \u2022 Asynchronous Initialization: The backend server starts instantly, deferring heavy, blocking I/O to background threads.\n  \u2022 Secure by Design: Communication between the frontend and the privileged main process is secured via a context-aware preload script.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline.\n\nSTAKEHOLDERS:\n  \u2022 End User: Receives a professional MSI installer for a one-click, dependency-free launch.\n  \u2022 Developer: Works with clean, separated Python and TypeScript stacks, governed by this specification.\n```\n\n---\n\n## 2. ARCHITECTURE PILLARS\n\n### Pillar 1: Backend Engine (Python)\n\n```\nPYTHON_BACKEND:\n  \u251c\u2500 main.py\n  \u2502  \u2514\u2500 Entry point for PyInstaller executable; starts the Uvicorn server.\n  \u2502\n  \u251c\u2500 api.py\n  \u2502  \u2514\u2500 FastAPI application definition.\n  \u2502     \u251c\u2500 Lifespan Hook: Manages async startup/shutdown logic.\n  \u2502     \u251c\u2500 API Routes: /health, /api/status, /api/races, etc.\n  \u2502     \u2514\u2500 Dependency Injection: Provides engine and security dependencies.\n  \u2502\n  \u251c\u2500 engine.py\n  \u2502  \u2514\u2500 OddsEngine: Orchestrates all data fetching and processing.\n  \u2502\n  \u251c\u2500 adapters/\n  \u2502  \u251c\u2500 base_v3.py (Abstract Base Class for all data sources)\n  \u2502  \u2514\u2500 [20+ specific adapter implementations]\n  \u2502\n  \u251c\u2500 config.py\n  \u2502  \u2514\u2500 Pydantic settings management from .env file.\n  \u2502\n  \u2514\u2500 requirements.txt\n     \u2514\u2500 Clean, de-duplicated, and conflict-free list of all Python dependencies.\n```\n\n### Pillar 2: Frontend Interface (TypeScript/React)\n\n```\nFRONTEND:\n  \u251c\u2500 next.config.mjs\n  \u2502  \u2514\u2500 Next.js config with `output: 'export'` for 100% static generation.\n  \u2502\n  \u251c\u2500 app/page.tsx\n  \u2502  \u2514\u2500 Main application shell.\n  \u2502\n  \u251c\u2500 src/components/\n  \u2502  \u251c\u2500 LiveRaceDashboard.tsx (Main stateful component)\n  \u2502  \u2502  \u251c\u2500 Manages connection state ('connecting', 'online', 'error').\n  \u2502  \u2502  \u251c\u2500 Polls Electron main process for backend status via secure IPC.\n  \u2502  \u2502  \u2514\u2500 Fetches data from the local Python API when online.\n  \u2502  \u2502\n  \u2502  \u251c\u2500 RaceCard.tsx (Displays a single race)\n  \u2502  \u2514\u2500 StatusIndicator.tsx (Shows backend connection status)\n  \u2502\n  \u2514\u2500 src/types/\n     \u2514\u2500 racing.ts (TypeScript interfaces matching backend Pydantic models)\n```\n\n### Pillar 3: Electron Wrapper & Windows Integration\n\n```\nELECTRON_WRAPPER:\n  \u251c\u2500 main.js (Electron main process)\n  \u2502  \u251c\u2500 Creates the BrowserWindow and loads the static frontend.\n  \u2502  \u251c\u2500 Implements robust lifecycle management for the backend executable.\n  \u2502  \u251c\u2500 Provides secure IPC handlers for status checks and restarts.\n  \u2502  \u2514\u2500 Creates a system tray icon for background operation.\n  \u2502\n  \u251c\u2500 preload.js (Secure IPC Bridge)\n  \u2502  \u2514\u2500 Uses `contextBridge` to safely expose specific functions to the frontend.\n  \u2502\n  \u251c\u2500 package.json\n  \u2502  \u2514\u2500 Defines Node.js dependencies and build scripts.\n  \u2502\n  \u251c\u2500 electron-builder-config.yml\n  \u2502  \u2514\u2500 Defines the configuration for creating the final MSI installer.\n  \u2502\n  \u2514\u2500 .github/workflows/build-msi.yml\n     \u2514\u2500 GitHub Actions pipeline that automates the entire build, test, and package process.\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON) - DETAILED\n\n### 3.1 Entry Point & Server Startup (`main.py`)\n\n```pseudocode\n// This is the script executed by fortuna-backend.exe\n\nPROCEDURE Main_Python_Entry_Point\n  // Guard required for PyInstaller and multiprocessing on Windows\n  IF this script is the main entry point:\n    CALL multiprocessing.freeze_support()\n\n    // Programmatically launch the FastAPI application using Uvicorn\n    // This call blocks and runs the server until the process is terminated\n    CALL uvicorn.run(\n      app=\"python_service.api:app\",\n      host=\"0.0.0.0\",\n      port=8000\n    )\nEND PROCEDURE\n```\n\n### 3.2 Asynchronous Application Lifecycle (`api.py`)\n\n```pseudocode\n// --- Lifespan Management (The key to a non-blocking startup) ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // === ON STARTUP ===\n  LOG \"Uvicorn server is online. Starting lifespan initialization.\"\n\n  // 1. Perform immediate, non-blocking tasks\n  CONNECT to Redis cache\n\n  // 2. Defer slow, blocking tasks to a background thread\n  //    This allows the server to start accepting requests instantly.\n  SCHEDULE function \"initialize_heavy_resources(app)\" to run in a ThreadPoolExecutor\n\n  LOG \"Heavy resource initialization scheduled. Server is now responsive.\"\n\n  // 3. Yield control back to Uvicorn. The server is now live.\n  YIELD\n\n  // === ON SHUTDOWN ===\n  LOG \"Shutdown signal received.\"\n  AWAIT app.state.engine.close() // Gracefully close HTTP client connections\n  DISCONNECT from Redis\n  SHUTDOWN ThreadPoolExecutor\n\n// --- Heavy Initialization (Runs in Background) ---\nFUNCTION initialize_heavy_resources(app: FastAPI):\n  TRY\n    LOG \"Background initialization of OddsEngine has started.\"\n    settings <- get_settings_from_config()\n    engine <- create new OddsEngine(config=settings)\n    // This part is slow: it loads all ~25 adapters\n    app.state.engine <- engine\n    LOG \"Background initialization complete. OddsEngine is now available.\"\n  CATCH Exception as e:\n    LOG_CRITICAL \"Failed to initialize OddsEngine in the background.\", error=e\n    app.state.engine <- null // Ensure the app knows initialization failed\n```\n\n### 3.3 Engine Orchestration (`engine.py`)\n\n```pseudocode\nCLASS OddsEngine:\n  INIT(config):\n    self.config <- config\n    self.adapters <- [List of all adapter instances]\n    self.http_client <- httpx.AsyncClient(...)\n    self.semaphore <- asyncio.Semaphore(config.MAX_CONCURRENT_REQUESTS)\n\n    // Inject the shared, persistent HTTP client into each adapter\n    FOR adapter IN self.adapters:\n      adapter.http_client <- self.http_client\n\n  @cache_async_result(ttl_seconds=300)\n  ASYNC FUNCTION fetch_all_odds(date_str):\n    // Create a list of concurrent fetching tasks, wrapped in the semaphore\n    tasks <- [self._fetch_with_semaphore(adapter, date_str) FOR adapter in self.adapters]\n    results <- AWAIT asyncio.gather(*tasks, return_exceptions=True)\n\n    // Process results, separating successes from failures\n    all_races <- []\n    FOR result IN results:\n      IF result is a success:\n        all_races.extend(result.races)\n\n    // Deduplicate and merge races from different sources\n    deduped_races <- self._dedupe_races(all_races)\n\n    RETURN AggregatedResponse(races=deduped_races, source_statuses=...)\n```\n\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/REACT) - DETAILED\n\n### 4.1 LiveRaceDashboard Component\n\n```pseudocode\nCOMPONENT LiveRaceDashboard (client-side):\n\n  STATE:\n    races: Race[] <- []\n    backendStatus: 'connecting' | 'online' | 'error' <- 'connecting'\n    lastLogs: string[] <- []\n\n  EFFECT on mount:\n    // Use the secure API exposed by preload.js\n    IF window.electronAPI exists:\n      // Set up a listener for status updates from the main process\n      window.electronAPI.onBackendStatus((update) => {\n        setBackendStatus(update.state)\n        setLastLogs(update.logs)\n      })\n\n    // Immediately request the current status\n    window.electronAPI.getBackendStatus().then((status) => {\n      setBackendStatus(status.state)\n      setLastLogs(status.logs)\n    })\n\n    // Set up a polling interval to keep status fresh\n    interval <- setInterval(() => {\n      window.electronAPI.getBackendStatus().then((status) => {\n        setBackendStatus(status.state)\n        setLastLogs(status.logs)\n      })\n    }, 3000) // Poll every 3 seconds\n\n    CLEANUP: clearInterval(interval)\n\n  EFFECT when backendStatus changes to 'online':\n    // Trigger data fetch only when the backend is confirmed to be running\n    fetchQualifiedRaces()\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard HTTP call to the local Python server\n      response <- AWAIT fetch(\"http://127.0.0.1:8000/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API returned status ${response.status}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n\n    CATCH e:\n      // If the API call fails, update the status\n      setBackendStatus('error')\n      setLastLogs([...lastLogs, `API Fetch Error: ${e.message}`])\n\n  FUNCTION RENDER:\n    <div className=\"dashboard\">\n      <StatusIndicator status={backendStatus} />\n      <RaceFilters />\n\n      IF backendStatus === 'error':\n        <ErrorDisplay logs={lastLogs} />\n      ELSE IF backendStatus === 'connecting':\n        <LoadingSkeleton />\n      ELSE IF races.length === 0:\n        <EmptyState message=\"No races matched your filters.\" />\n      ELSE:\n        <RaceGrid races={races} />\n    </div>\n```\n\n---\n\n## 5. ELECTRON WRAPPER & WINDOWS INTEGRATION - DETAILED\n\n### 5.1 Main Process (`main.js`) - With Robust Lifecycle Management\n\n```pseudocode\nCLASS FortunaDesktopApp:\n  INIT():\n    self.mainWindow <- null\n    self.backendState <- 'stopped'\n    self.backendLogs <- []\n    self.backendProcess <- null\n\n  FUNCTION createMainWindow():\n    // ... create BrowserWindow, load static frontend ...\n\n  FUNCTION startBackend():\n    IF self.backendProcess is not null:\n      self.backendProcess.kill()\n\n    self.backendState <- 'starting'\n    self.backendLogs <- ['Attempting to start backend...']\n    self.sendBackendStatusUpdate() // Notify UI\n\n    // Get path to the packaged executable\n    exePath <- path.join(process.resourcesPath, 'fortuna-backend', 'fortuna-backend.exe')\n\n    IF file at exePath does NOT exist:\n      self.backendState <- 'error'\n      self.backendLogs.push(`FATAL: Executable not found at ${exePath}`)\n      self.sendBackendStatusUpdate()\n      dialog.showErrorBox(\"Critical Error\", \"Backend is missing. Please reinstall.\")\n      RETURN\n\n    // Spawn the process\n    self.backendProcess <- spawn(exePath, [], { stdio: ['ignore', 'pipe', 'pipe'] })\n\n    // --- CRITICAL: Resiliency Logic ---\n    startupTimeout <- setTimeout(() => {\n      IF self.backendState === 'starting':\n        self.backendState <- 'error'\n        self.backendLogs.push('Error: Backend startup timed out after 30 seconds.')\n        self.backendProcess.kill()\n        self.sendBackendStatusUpdate()\n    }, 30000) // 30-second timeout\n\n    self.backendProcess.stdout.on('data', (data) => {\n      self.backendLogs.push(data.toString())\n      // A more robust check would be a successful health check poll\n      IF data.toString().includes(\"Uvicorn running\"):\n        self.backendState <- 'online'\n        clearTimeout(startupTimeout)\n        self.sendBackendStatusUpdate()\n    })\n\n    self.backendProcess.stderr.on('data', (data) => {\n      self.backendLogs.push(`[STDERR] ${data.toString()}`)\n    })\n\n    self.backendProcess.on('exit', (code) => {\n      clearTimeout(startupTimeout)\n      IF self.backendState is not 'error': // Avoid duplicate error messages\n        self.backendState <- 'error'\n        self.backendLogs.push(`Backend process exited unexpectedly with code: ${code}`)\n        self.sendBackendStatusUpdate()\n    })\n\n  FUNCTION sendBackendStatusUpdate():\n    // Send the latest status to the frontend renderer process\n    IF self.mainWindow is not null:\n      self.mainWindow.webContents.send('backend-status-update', {\n        state: self.backendState,\n        logs: self.backendLogs.slice(-20) // Send last 20 log lines\n      })\n\n// --- IPC Handlers (Securely Defined) ---\nipcMain.handle('get-backend-status', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN null\n\n  RETURN { state: self.backendState, logs: self.backendLogs.slice(-20) }\n})\n\nipcMain.on('restart-backend', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN\n\n  self.startBackend()\n})\n```\n\n### 5.2 Preload Script (`preload.js`)\n\n```pseudocode\n// Expose a limited, secure API to the frontend renderer process\ncontextBridge.exposeInMainWorld('electronAPI', {\n  getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n  restartBackend: () => ipcRenderer.send('restart-backend'),\n  onBackendStatus: (callback) => ipcRenderer.on('backend-status-update', (_event, value) => callback(value))\n})\n```\n\n---\n\n## 6. DATA MODELS & API SPECIFICATION\n\n### 6.1 Core Data Models (Pydantic/TypeScript)\n\n```\nMODEL Race:\n  id: str (unique identifier, e.g., \"Betfair_USA_Aqueduct_2025-11-07_R1\")\n  venue: str\n  race_number: int\n  start_time: datetime\n  runners: List[Runner]\n  source: str\n\nMODEL Runner:\n  name: str\n  odds: Optional[float]\n```\n\n### 6.2 Primary API Endpoints\n\n```\nENDPOINT GET /health\n  Description: Simple health check, requires no authentication.\n  Response (200 OK): {\"status\": \"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Description: Fetches all race data, runs the Trifecta analyzer, and returns qualified races.\n  Headers:\n    - X-API-Key: (Required, not used in this local setup but good practice)\n  Query Params:\n    - max_field_size: int\n    - min_odds: float\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 7. DEPLOYMENT & AUTOMATION (CI/CD)\n\n```pseudocode\nWORKFLOW Build_MSI_Installer_on_GitHub_Actions:\n  // Phase 1: Setup\n  SETUP Node.js and Python environments\n\n  // Phase 2: Build Frontend\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY static output to /electron/web-ui-build/out\n\n  // Phase 3: Build Backend\n  RUN \"pip install -r python_service/requirements.txt\"\n  // CRITICAL: Use PyInstaller with a spec file or CLI flags that include\n  // necessary hidden imports to prevent runtime crashes.\n  // e.g., --hidden-import=keyring.backends.fail.Keyring\n  EXECUTE PyInstaller to create fortuna-backend.exe\n  PLACE executable in /electron/resources/fortuna-backend\n\n  // Phase 4: Deep Integration Test\n  START fortuna-backend.exe in the background\n  POLL http://127.0.0.1:8000/health until it responds with 200 OK or times out\n  IF timeout or crash THEN FAIL the build\n\n  // Phase 5: Package\n  RUN \"npm ci\" in /electron\n  EXECUTE \"npx electron-builder\" to create the MSI installer\n\n  // Phase 6: Publish\n  UPLOAD MSI as a build artifact\n  IF build was triggered by a git tag THEN CREATE a new GitHub Release\n```\n\n---\n\n## 8. END-TO-END WORKFLOWS\n\n### 8.1 Production Startup Workflow (Resilient)\n\n```\nWORKFLOW user_launches_application:\n  STEP 1: User executes Fortuna Faucet.exe -> Electron main.js starts.\n  STEP 2: UI appears instantly. The main process creates the BrowserWindow and loads the static index.html. The UI shows a 'connecting' state.\n  STEP 3: Backend starts asynchronously. The main process calls the robust `startBackend()` function.\n  STEP 4: `startBackend()` spawns `fortuna-backend.exe` and starts a 30-second timeout.\n  STEP 5: The frontend UI polls for status every 3 seconds via the secure `window.electronAPI.getBackendStatus()`.\n  STEP 6: The backend `.exe` starts, its `lifespan` hook runs, and the Uvicorn server comes online within seconds.\n  STEP 7: The main process detects the \"Uvicorn running\" message (or a successful health poll) and updates its internal state to 'online'. The startup timeout is cleared.\n  STEP 8: On its next poll, the frontend receives the 'online' status.\n  STEP 9: The frontend's state changes, triggering the `fetchQualifiedRaces()` API call to `localhost:8000`.\n  STEP 10: Data is returned from the now fully-initialized backend and rendered in the UI.\n\n  FAILURE SCENARIO (Backend Crash):\n  STEP 6a: The backend `.exe` crashes on startup.\n  STEP 7a: The `on('exit')` handler in `main.js` fires. The state is set to 'error' with the exit code.\n  STEP 8a: On its next poll, the frontend receives the 'error' status and relevant logs.\n  STEP 9a: The UI renders an error message and a \"Restart Backend\" button.\n```\n\n---\n*This concludes the revised and definitive blueprint for the Fortuna Faucet application.*",
    "REBRANDING_AUDIT.md": "# Fortuna Faucet: Rebranding Audit Report\n\nThis report lists all files containing legacy branding terms (`checkmate`, `solo`).\n\n---\n\n- `./.env`\n- `./AGENTS.md`\n- `./ARCHITECTURAL_MANDATE_V8.1.md`\n- `./GEMINI_ONBOARDING.md`\n- `./HISTORY.md`\n- `./JSON_BACKUP_MANIFEST.md`\n- `./MANIFEST2.md`\n- `./MANIFEST3.md`\n- `./PROJECT_MANIFEST.md`\n- `./Procfile`\n- `./ROADMAP.md`\n- `./ROADMAP_APPENDICES.md`\n- `./WISDOM.md`\n- `./attic/ARCHITECTURAL_MANDATE_V7.2.md`\n- `./attic/build_python_service.py`\n- `./attic/checkmate_app.py`\n- `./attic/checkmate_engine.py`\n- `./attic/checkmate_monitor_v1.html`\n- `./attic/dashboard.py`\n- `./attic/desktop_app/App.xaml`\n- `./attic/desktop_app/App.xaml.cs`\n- `./attic/desktop_app/CheckmateDeck.csproj`\n- `./attic/desktop_app/Models/AdapterStatusDisplay.cs`\n- `./attic/desktop_app/Models/DisplayRace.cs`\n- `./attic/desktop_app/Services/DatabaseService.cs`\n- `./attic/desktop_app/Services/IDatabaseService.cs`\n- `./attic/desktop_app/ViewModels/MainViewModel.cs`\n- `./attic/desktop_app/Views/MainWindow.xaml`\n- `./attic/desktop_app/Views/MainWindow.xaml.cs`\n- `./attic/launcher.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_api.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_betfair_modern_adapter.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_fanduel_api_adapter.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_logic.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_models.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_racingpost_modern_adapter.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_run.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_services.py`\n- `./attic/legacy_tests_pre_triage/test_checkmate_v7.py`\n- `./attic/legacy_tests_pre_triage/test_python_service.py`\n- `./attic/portable_demo_v2.py`\n- `./attic/rust_engine/rust_engine/Cargo.toml`\n- `./attic/rust_engine/rust_engine/benches/analysis_benchmark.rs`\n- `./attic/rust_engine/rust_engine/src/lib.rs`\n- `./attic/rust_engine/rust_engine/src/main.rs`\n- `./attic/the_one_script.py`\n- `./attic/tipsheet_generator.py`\n- `./attic/vba_source/Module_Charts.bas`\n- `./attic/vba_source/Module_DB.bas`\n- `./attic/vba_source/Module_UI.bas`\n- `./checkmate_web/engine.py`\n- `./checkmate_web/main.py`\n- `./checkmate_web/static/app.js`\n- `./checkmate_web/static/index.html`\n- `./command_deck.py`\n- `./diagnostic_report.txt`\n- `./launch_and_hunt.py`\n- `./launch_checkmate.bat`\n- `./launch_command_deck.bat`\n- `./manual_override_tool.py`\n- `./pg_schemas/historical_races.sql`\n- `./pytest.ini`\n- `./python_service/api.py`\n- `./python_service/checkmate_service.py`\n- `./python_service/minimal_service.py`\n- `./python_service/windows_service_wrapper.py`\n- `./run_server.py`\n- `./rust_engine/tests/integration_tests.rs`\n- `./shared_database/schema.sql`\n- `./src/checkmate_v7/adapters/AndWereOff.py`\n- `./src/checkmate_v7/adapters/Stablemates.py`\n- `./src/checkmate_v7/adapters/__init__.py`\n- `./src/checkmate_v7/api.py`\n- `./src/checkmate_v7/base.py`\n- `./src/checkmate_v7/cockpit.py`\n- `./src/checkmate_v7/config.py`\n- `./src/checkmate_v7/database.py`\n- `./src/checkmate_v7/headless_monitor.py`\n- `./src/checkmate_v7/logic.py`\n- `./src/checkmate_v7/models.py`\n- `./src/checkmate_v7/run.py`\n- `./src/checkmate_v7/services.py`\n- `./src/checkmate_v7/settings.py`\n- `./src/paddock_parser/prediction_engine.py`\n- `./web_platform/api_gateway/package-lock.json`\n- `./web_platform/api_gateway/src/server.ts`\n- `./web_platform/api_gateway/src/services/DatabaseService.ts`\n- `./web_platform/frontend/.next/server/app/page.js`\n- `./web_platform/frontend/app/layout.tsx`\n- `./web_platform/frontend/src/components/RaceCard.tsx`\n- `./web_platform/frontend/src/types/racing.ts`\n",
    "STATUS.md": "# Project Status: Foundation Rebuilt, Hardening in Progress\n\n**Date:** 2025-10-03\n\n## Current State\n\n*   **Architecture:** The backend has been successfully rebuilt into a superior, asynchronous FastAPI application, as defined by 'Operation: Grand Synthesis'. The new foundation is stable, tested, and features a resilient `BaseAdapter` pattern.\n\n*   **Status:** The foundational refactoring is complete. The first two data adapters (`Betfair`, `TVG`) have been implemented on the new architecture. We are now in a new phase of development: **'Phase 2: Hardening & Expansion.'**\n\n*   **Documentation:** All core strategic documents and manifests have been synchronized with the new technical reality.\n\n*   **Next Steps:** Our immediate priority is to act on the verified intelligence from our Oracle (Jules1003). The next missions will focus on implementing critical API security features (rate limiting, authentication) and continuing the build-out of our adapter fleet.",
    "VERSION.txt": "1.0",
    "audit-ignore.txt": "# Starlette - GHSA-f96h-pmfr-66vw - Medium severity\n# anyio.to_thread.run_sync is vulnerable to blocking the event loop in Starlette < 0.38.3\n# This is a dependency of FastAPI and is not trivially upgradeable.\nGHSA-f96h-pmfr-66vw\n\n# Starlette - GHSA-2c2j-9gv5-cj73 - High severity\n# Starlette's `StaticFiles` is vulnerable to path traversal.\n# This is not a direct risk as we do not use `StaticFiles` in production.\nGHSA-2c2j-9gv5-cj73\n\n# Cryptography - GHSA-h4gh-qq45-vh27 - High severity\n# Loading a specially crafted X.509 certificate could lead to a NULL pointer dereference and crash.\n# We have pinned this version to satisfy a pyopenssl dependency and cannot upgrade easily.\nGHSA-h4gh-qq45-vh27\n\n# Cryptography - GHSA-79v4-65xg-pq4g - High severity\n# Side-channel attack vulnerability in ECDSA signature generation.\n# We have pinned this version and cannot upgrade easily.\nGHSA-79v4-65xg-pq4g\n\n# Certifi - PYSEC-2024-230 - High severity\n# certifi contains a Pem-parsing vulnerability.\nPYSEC-2024-230\n\n# h11 - GHSA-vqfr-h8mv-ghfj - High severity\n# h11 is vulnerable to HTTP request smuggling.\nGHSA-vqfr-h8mv-ghfj\n\n# h2 - GHSA-847f-9342-265h - High severity\n# h2 is vulnerable to a \"continuation flood\" denial of service attack.\nGHSA-847f-9342-265h\n",
    "configure_startup.py": "# configure_startup.py\nimport sys\nimport winreg\nfrom pathlib import Path\n\n\nclass StartupManager:\n    \"\"\"Manage Windows startup registry entries for the current user.\"\"\"\n\n    REGISTRY_PATH = r\"Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Run\"\n    APP_NAME = \"FortunaFaucetTray\"\n\n    @classmethod\n    def is_enabled(cls) -> bool:\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_READ)\n            winreg.QueryValueEx(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            return True\n        except FileNotFoundError:\n            return False\n\n    @classmethod\n    def enable(cls):\n        launcher_path = Path(__file__).parent / \"launcher.ps1\"\n        cmd = f'powershell.exe -WindowStyle Hidden -File \"{launcher_path}\"'\n\n        key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n        winreg.SetValueEx(key, cls.APP_NAME, 0, winreg.REG_SZ, cmd)\n        winreg.CloseKey(key)\n        print(\"Startup enabled.\")\n\n    @classmethod\n    def disable(cls):\n        try:\n            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, cls.REGISTRY_PATH, 0, winreg.KEY_WRITE)\n            winreg.DeleteValue(key, cls.APP_NAME)\n            winreg.CloseKey(key)\n            print(\"Startup disabled.\")\n        except FileNotFoundError:\n            print(\"Already disabled.\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        if sys.argv[1] == \"enable\":\n            StartupManager.enable()\n        elif sys.argv[1] == \"disable\":\n            StartupManager.disable()\n        elif sys.argv[1] == \"status\":\n            print(f\"Startup is currently {'enabled' if StartupManager.is_enabled() else 'disabled'}\")\n    else:\n        print(\"Usage: python configure_startup.py [enable|disable|status]\")\n",
    "electron/assets/.gitkeep": "# This directory is for application icons (e.g., icon.ico, tray-icon.png)",
    "electron/resources/.gitkeep": "",
    "electron/secure-settings-manager.js": "// electron/secure-settings-manager.js\nconst { app } = require('electron');\nconst fs = require('fs');\nconst path = require('path');\n\nconst SETTINGS_FILE = path.join(app.getPath('userData'), 'settings.json');\n\nclass SecureSettingsManager {\n constructor() {\n this.settings = this.loadSettings();\n }\n\n loadSettings() {\n try {\n if (fs.existsSync(SETTINGS_FILE)) {\n const data = fs.readFileSync(SETTINGS_FILE, 'utf-8');\n return JSON.parse(data);\n }\n } catch (error) {\n console.error('Error loading settings:', error);\n }\n return {};\n }\n\n saveSettings() {\n try {\n fs.writeFileSync(SETTINGS_FILE, JSON.stringify(this.settings, null, 2));\n } catch (error) {\n console.error('Error saving settings:', error);\n }\n }\n\n getApiKey() {\n return this.settings.apiKey || null;\n }\n\n saveApiKey(apiKey) {\n this.settings.apiKey = apiKey;\n this.saveSettings();\n return { success: true };\n }\n\n getBetfairCredentials() {\n return this.settings.betfair || null;\n }\n\n saveBetfairCredentials(credentials) {\n this.settings.betfair = credentials;\n this.saveSettings();\n return { success: true };\n }\n}\n\nmodule.exports = new SecureSettingsManager();\n",
    "fortuna-backend.spec": "# -*- mode: python ; coding: utf-8 -*-\n#\n# This is the definitive PyInstaller spec file for Fortuna Faucet.\n# It uses simple, relative paths and requires no template substitution.\n#\n\nfrom PyInstaller.utils.hooks import collect_all, copy_metadata\nimport os\n\n# --- Core Configuration ---\napp_name = 'fortuna-backend'\nentry_point = 'python_service/main.py'\n\n# --- Data Files ---\n# These are simple relative paths that PyInstaller will find correctly.\ndatas = [\n    ('python_service/adapters', 'adapters'),\n    ('python_service/data', 'data'),\n    ('python_service/json', 'json')\n]\n\n# --- Dependency Collection ---\n# A comprehensive list of packages to ensure all necessary files are bundled.\ncore_stack = [\n    'uvicorn', 'fastapi', 'pydantic', 'pydantic_core', 'starlette',\n    'anyio', 'sniffio', 'h11', 'httptools', 'websockets',\n    'httpx', 'httpcore', 'certifi', 'slowapi'\n]\ndata_stack = [\n    'redis', 'requests', 'selectolax', 'bs4', 'lxml',\n    'structlog', 'python-dotenv', 'pandas', 'numpy', 'scipy'\n]\nwindows_stack = [\n    'psutil', 'pywin32', 'pywintypes', 'win32api', 'win32con',\n    'pynput', 'cryptography', 'cffi', 'keyring', 'pywin32-ctypes'\n]\nall_packages_to_collect = core_stack + data_stack + windows_stack\n\nhiddenimports = []\nbinaries = []\n\nprint(f\"--- Collecting {len(all_packages_to_collect)} packages for PyInstaller ---\")\nfor pkg in all_packages_to_collect:\n    try:\n        tmp_datas, tmp_binaries, tmp_hiddenimports = collect_all(pkg)\n        datas += tmp_datas\n        binaries += tmp_binaries\n        hiddenimports += tmp_hiddenimports\n        print(f\"  [OK] Successfully collected {pkg}\")\n    except Exception as e:\n        print(f\"  [!!] WARNING: Could not collect {pkg}: {e}\")\n\n# --- Metadata & Final Hidden Imports ---\n# Forcing metadata copy for these key libraries helps PyInstaller resolve\n# their internal structures, especially for things like plugins or entry points.\nfor pkg in ['pydantic', 'fastapi', 'uvicorn', 'starlette', 'httpx', 'anyio']:\n    try:\n        datas += copy_metadata(pkg)\n    except:\n        pass\nhiddenimports += [\n    'tenacity',\n    'uvicorn.lifespan', 'anyio._backends._asyncio', 'redis.asyncio',\n    'keyring.backends.fail', 'keyring.backends.windows',\n    '_ssl', '_hashlib', '_sqlite3'\n]\n\n\n# --- Analysis Object ---\na = Analysis(\n    [entry_point],\n    pathex=[os.getcwd()], # This ensures 'python_service' is on the path\n    binaries=binaries,\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    runtime_hooks=[],\n    excludes=['matplotlib', 'tkinter', 'pytest', 'IPython', 'streamlit'],\n    noarchive=False\n)\n\npyz = PYZ(a.pure, cipher=None)\n\nexe = EXE(\n    pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n    name=app_name,\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=False,\n    console=True,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None\n)\n",
    "fortuna_app.py": "import os\nimport socket\nimport subprocess\nimport sys\nimport threading\nimport time\nimport tkinter as tk\nfrom pathlib import Path\nfrom tkinter import messagebox\nfrom tkinter import scrolledtext\nfrom tkinter import ttk\n\nimport psutil\nimport requests\n\n\n# --- Control Panel Tab (from former launcher_gui.py) ---\nclass ControlPanelTab(tk.Frame):\n    def __init__(self, parent, master_app):\n        super().__init__(parent, bg=\"#1a1a2e\")\n        self.master_app = master_app\n        self.backend_proc = None\n        self.frontend_proc = None\n        self.backend_unresponsive_count = 0\n        self.frontend_unresponsive_count = 0\n        self.first_launch = not (Path(os.environ[\"USERPROFILE\"]) / \"Desktop\" / \"\ud83d\udc34 Launch Fortuna Faucet.lnk\").exists()\n        self._create_ui()\n        self.monitor_thread = threading.Thread(target=self.monitor_services, daemon=True)\n        self.monitor_thread.start()\n\n    def log_output(self, message):\n        self.log_text.config(state=tk.NORMAL)\n        self.log_text.insert(tk.END, f\"[{time.strftime('%H:%M:%S')}] {message}\\n\")\n        self.log_text.config(state=tk.DISABLED)\n        self.log_text.see(tk.END)\n\n    def smart_start(self):\n        \"\"\"On first launch, run verification, create shortcuts, and then start.\"\"\"\n        if messagebox.askokcancel(\n            \"First-Time Setup\",\n            \"Welcome to Fortuna Faucet!\\n\\nThis first launch will verify your system and create a desktop shortcut for easy access. Proceed?\",\n        ):\n            # Steal and run the logic from the System Tools Tab\n            self.master_app.notebook.select(self.master_app.system_tools_tab)\n            self.master_app.system_tools_tab.run_verification()\n            self.master_app.system_tools_tab.run_create_shortcuts()\n\n            # Once done, revert to a normal start button\n            messagebox.showinfo(\"Setup Complete\", \"Setup is complete! The main services will now start.\")\n            self.launch_btn.config(text=\"\u25b6 START FORTUNA\", bg=\"#00ff88\", command=self.launch_services)\n            self.launch_services()\n\n    def _create_ui(self):\n        title = tk.Label(\n            self,\n            text=\"\ud83d\udc34 System Control Panel\",\n            font=(\"Segoe UI\", 16, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#00ff88\",\n        )\n        title.pack(pady=20)\n\n        status_frame = tk.Frame(self, bg=\"#1a1a2e\")\n        status_frame.pack(fill=tk.X, padx=40, pady=10)\n\n        tk.Label(\n            status_frame,\n            text=\"Backend Service (API)\",\n            font=(\"Segoe UI\", 10),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        ).pack(anchor=\"w\")\n        self.backend_status_canvas = tk.Canvas(status_frame, width=300, height=40, bg=\"#0f3460\", highlightthickness=0)\n        self.backend_status_canvas.pack(fill=tk.X, pady=(0, 10))\n        self.backend_indicator = self.backend_status_canvas.create_oval(15, 10, 35, 30, fill=\"#ff4444\", outline=\"\")\n        self.backend_text = self.backend_status_canvas.create_text(\n            55, 20, text=\"Stopped\", fill=\"#ffffff\", anchor=\"w\", font=(\"Segoe UI\", 9)\n        )\n\n        tk.Label(\n            status_frame,\n            text=\"Frontend Dashboard (UI)\",\n            font=(\"Segoe UI\", 10),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        ).pack(anchor=\"w\")\n        self.frontend_status_canvas = tk.Canvas(status_frame, width=300, height=40, bg=\"#0f3460\", highlightthickness=0)\n        self.frontend_status_canvas.pack(fill=tk.X)\n        self.frontend_indicator = self.frontend_status_canvas.create_oval(15, 10, 35, 30, fill=\"#ff4444\", outline=\"\")\n        self.frontend_text = self.frontend_status_canvas.create_text(\n            55, 20, text=\"Stopped\", fill=\"#ffffff\", anchor=\"w\", font=(\"Segoe UI\", 9)\n        )\n\n        button_frame = tk.Frame(self, bg=\"#1a1a2e\")\n        button_frame.pack(fill=tk.X, padx=40, pady=20)\n\n        self.launch_btn = tk.Button(\n            button_frame,\n            text=\"\u25b6 START FORTUNA\",\n            font=(\"Segoe UI\", 14, \"bold\"),\n            bg=\"#00ff88\",\n            fg=\"#000000\",\n            height=2,\n            relief=tk.FLAT,\n        )\n        if self.first_launch:\n            self.launch_btn.config(\n                text=\"\u25b6 FIRST-TIME START & SETUP\",\n                bg=\"#ff9900\",\n                command=self.smart_start,\n            )\n        else:\n            self.launch_btn.config(command=self.launch_services)\n        self.launch_btn.pack(fill=tk.X, pady=(0, 10))\n\n        self.stop_btn = tk.Button(\n            button_frame,\n            text=\"\u23f9 STOP SERVICES\",\n            font=(\"Segoe UI\", 12),\n            bg=\"#ff4444\",\n            fg=\"#ffffff\",\n            command=self.stop_services,\n            state=tk.DISABLED,\n            height=1,\n            relief=tk.FLAT,\n        )\n        self.stop_btn.pack(fill=tk.X)\n\n        self.log_text = scrolledtext.ScrolledText(self, height=5, bg=\"#000000\", fg=\"#00ff88\", state=tk.DISABLED)\n        self.log_text.pack(pady=10, padx=40, fill=tk.X)\n\n    def check_ports(self, ports=[8000, 3000]):\n        unavailable_ports = []\n        for port in ports:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                if s.connect_ex((\"127.0.0.1\", port)) == 0:\n                    unavailable_ports.append(port)\n        return unavailable_ports\n\n    def launch_services(self):\n        unavailable = self.check_ports()\n        if unavailable:\n            messagebox.showerror(\n                \"Port Conflict\",\n                f\"Cannot launch. Port(s) {', '.join(map(str, unavailable))} are already in use by another application.\",\n            )\n            return\n\n        self.launch_btn.config(state=tk.DISABLED)\n        self.update_status(\"backend\", \"starting\", \"Launching...\")\n        self.update_status(\"frontend\", \"starting\", \"Launching...\")\n\n        try:\n            venv_python = Path(\".venv/Scripts/python.exe\")\n            self.backend_proc = subprocess.Popen(\n                [\n                    str(venv_python),\n                    \"-m\",\n                    \"uvicorn\",\n                    \"python_service.api:app\",\n                    \"--host\",\n                    \"127.0.0.1\",\n                    \"--port\",\n                    \"8000\",\n                ],\n                stdout=subprocess.DEVNULL,\n                stderr=subprocess.DEVNULL,\n                cwd=Path(__file__).parent,\n                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,\n            )\n        except Exception as e:\n            self.update_status(\"backend\", \"error\", f\"Launch Error: {str(e)[:40]}\")\n            self.stop_btn.config(state=tk.NORMAL)\n            return\n\n        try:\n            self.frontend_proc = subprocess.Popen(\n                [\"npm\", \"run\", \"dev\"],\n                shell=True,\n                stdout=subprocess.DEVNULL,\n                stderr=subprocess.DEVNULL,\n                cwd=\"web_platform/frontend\",\n                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,\n            )\n        except Exception as e:\n            self.update_status(\"frontend\", \"error\", f\"Launch Error: {str(e)[:40]}\")\n            self.stop_btn.config(state=tk.NORMAL)\n            return\n\n        self.stop_btn.config(state=tk.NORMAL)\n\n    def stop_services(self):\n        self.stop_btn.config(state=tk.DISABLED)\n        for proc_name in [\"backend\", \"frontend\"]:\n            proc = getattr(self, f\"{proc_name}_proc\")\n            if proc and proc.poll() is None:\n                try:\n                    parent = psutil.Process(proc.pid)\n                    for child in parent.children(recursive=True):\n                        child.kill()\n                    parent.kill()\n                except psutil.NoSuchProcess:\n                    pass\n            setattr(self, f\"{proc_name}_proc\", None)\n        self.launch_btn.config(state=tk.NORMAL)\n\n    def restart_service(self, service_name: str):\n        \"\"\"Gracefully stop and restart a single failed service.\"\"\"\n        proc_attr = f\"{service_name}_proc\"\n        proc = getattr(self, proc_attr)\n\n        # Stop the specific process\n        if proc and proc.poll() is None:\n            try:\n                parent = psutil.Process(proc.pid)\n                for child in parent.children(recursive=True):\n                    child.kill()\n                parent.kill()\n            except psutil.NoSuchProcess:\n                pass\n        setattr(self, proc_attr, None)\n\n        # Wait a moment\n        time.sleep(2)\n\n        # Relaunch the specific process\n        self.update_status(service_name, \"starting\", \"Attempting auto-restart...\")\n        try:\n            if service_name == \"backend\":\n                venv_python = Path(\".venv/Scripts/python.exe\")\n                new_proc = subprocess.Popen(\n                    [\n                        str(venv_python),\n                        \"-m\",\n                        \"uvicorn\",\n                        \"python_service.api:app\",\n                        \"--host\",\n                        \"127.0.0.1\",\n                        \"--port\",\n                        \"8000\",\n                    ],\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                    cwd=Path(__file__).parent.parent,\n                    creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,\n                )\n            else:  # frontend\n                new_proc = subprocess.Popen(\n                    [\"npm\", \"run\", \"dev\"],\n                    shell=True,\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                    cwd=\"web_platform/frontend\",\n                    creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,\n                )\n            setattr(self, proc_attr, new_proc)\n        except Exception as e:\n            self.update_status(service_name, \"error\", f\"Auto-restart failed: {e}\")\n\n    def monitor_services(self):\n        while True:\n            # --- Backend Monitoring ---\n            if self.backend_proc and self.backend_proc.poll() is None:\n                try:\n                    r = requests.get(\"http://localhost:8000/health\", timeout=2)\n                    if r.status_code == 200:\n                        self.update_status(\"backend\", \"ok\", \"Healthy (200 OK)\")\n                        self.backend_unresponsive_count = 0  # Reset counter on success\n                    else:\n                        self.update_status(\"backend\", \"error\", f\"Error ({r.status_code})\")\n                except requests.RequestException:\n                    self.update_status(\"backend\", \"unresponsive\", \"Unresponsive\")\n                    self.backend_unresponsive_count += 1\n                    if self.backend_unresponsive_count >= 3:  # If unresponsive for 3 cycles (15s)\n                        self.log_output(\"Backend unresponsive. Attempting automatic restart...\")\n                        self.restart_service(\"backend\")\n                        self.backend_unresponsive_count = 0  # Reset after attempt\n            else:\n                self.update_status(\"backend\", \"stopped\", \"Stopped\")\n\n            # --- Frontend Monitoring ---\n            if self.frontend_proc and self.frontend_proc.poll() is None:\n                try:\n                    r = requests.get(\"http://localhost:3000\", timeout=2)\n                    if r.status_code == 200:\n                        self.update_status(\"frontend\", \"ok\", \"Healthy (200 OK)\")\n                        self.frontend_unresponsive_count = 0\n                    else:\n                        self.update_status(\"frontend\", \"error\", f\"Error ({r.status_code})\")\n                except requests.RequestException:\n                    self.update_status(\"frontend\", \"unresponsive\", \"Unresponsive\")\n                    self.frontend_unresponsive_count += 1\n                    if self.frontend_unresponsive_count >= 3:\n                        self.log_output(\"Frontend unresponsive. Attempting automatic restart...\")\n                        self.restart_service(\"frontend\")\n                        self.frontend_unresponsive_count = 0\n            else:\n                self.update_status(\"frontend\", \"stopped\", \"Stopped\")\n            time.sleep(5)\n\n    def update_status(self, service: str, status: str, message: str):\n        colors = {\n            \"ok\": \"#00ff88\",\n            \"unresponsive\": \"#ffcc00\",\n            \"error\": \"#ff4444\",\n            \"stopped\": \"#ff4444\",\n            \"starting\": \"#0f6cbd\",\n        }\n        canvas = getattr(self, f\"{service}_status_canvas\")\n        indicator = getattr(self, f\"{service}_indicator\")\n        text = getattr(self, f\"{service}_text\")\n\n        canvas.itemconfig(indicator, fill=colors.get(status, \"#404060\"))\n        canvas.itemconfig(text, text=message)\n\n\n# --- Setup Wizard Tab (from former setup_wizard_gui.py) ---\nclass SetupWizardTab(tk.Frame):\n    def __init__(self, parent):\n        super().__init__(parent, bg=\"#1a1a2e\")\n        self.current_step = 0\n        self.settings = {}\n        self._create_widgets()\n        self.show_step(0)\n\n    def _create_widgets(self):\n        header = tk.Label(\n            self,\n            text=\"\ud83d\udd27 First-Time Setup & Configuration\",\n            font=(\"Segoe UI\", 16, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        )\n        header.pack(pady=20)\n        self.step_label = tk.Label(\n            self,\n            text=\"Step 1 of 4: Generate API Key\",\n            font=(\"Segoe UI\", 11),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        )\n        self.step_label.pack(pady=10)\n        self.content_frame = tk.Frame(self, bg=\"#1a1a2e\")\n        self.content_frame.pack(fill=tk.BOTH, expand=True, padx=30, pady=20)\n        button_frame = tk.Frame(self, bg=\"#1a1a2e\")\n        button_frame.pack(fill=tk.X, padx=30, pady=20)\n        self.prev_btn = tk.Button(\n            button_frame,\n            text=\"< Back\",\n            command=self.previous_step,\n            state=tk.DISABLED,\n            bg=\"#404060\",\n            fg=\"#ffffff\",\n            padx=20,\n        )\n        self.prev_btn.pack(side=tk.LEFT)\n        self.next_btn = tk.Button(\n            button_frame,\n            text=\"Next >\",\n            command=self.next_step,\n            bg=\"#00ff88\",\n            fg=\"#000000\",\n            font=(\"Segoe UI\", 11, \"bold\"),\n            padx=20,\n        )\n        self.next_btn.pack(side=tk.RIGHT)\n\n    def show_step(self, step_index):\n        self._clear_content()\n        self.current_step = step_index\n        if step_index == 0:\n            self._show_step_1()\n        elif step_index == 1:\n            self._show_step_2()\n        elif step_index == 2:\n            self._show_step_3()\n        elif step_index == 3:\n            self._show_step_4()\n        self.update_buttons()\n\n    def _show_step_1(self):\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83d\udd10 Secure API Key\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        ).pack(anchor=\"w\")\n        tk.Label(\n            self.content_frame,\n            text=\"A secure API key will be generated and stored.\",\n            wraplength=600,\n            justify=tk.LEFT,\n            bg=\"#1a1a2e\",\n            fg=\"#cccccc\",\n        ).pack(anchor=\"w\", pady=10)\n        # ... Add API key generation logic and display ...\n\n    def _show_step_2(self):\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83c\udfc7 Betfair Exchange (Optional)\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        ).pack(anchor=\"w\")\n        # ... Add Betfair configuration form ...\n\n    def _show_step_3(self):\n        tk.Label(\n            self.content_frame,\n            text=\"\u2713 Verifying Setup\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#00ff88\",\n        ).pack(anchor=\"w\")\n        # ... Add verification checks logic ...\n\n    def _show_step_4(self):\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83c\udf89 Setup Complete!\",\n            font=(\"Segoe UI\", 14, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#00ff88\",\n        ).pack(pady=20)\n        self.next_btn.config(text=\"\u2713 Finish\", command=self.finish_setup)\n\n    def next_step(self):\n        if self.current_step < 3:\n            self.show_step(self.current_step + 1)\n\n    def previous_step(self):\n        if self.current_step > 0:\n            self.show_step(self.current_step - 1)\n\n    def finish_setup(self):\n        messagebox.showinfo(\"Setup Complete\", \"Your configuration has been saved.\")\n\n    def _clear_content(self):\n        for widget in self.content_frame.winfo_children():\n            widget.destroy()\n\n    def update_buttons(self):\n        self.prev_btn.config(state=tk.NORMAL if self.current_step > 0 else tk.DISABLED)\n        if self.current_step == 3:\n            self.next_btn.config(text=\"\u2713 Finish\", command=self.finish_setup)\n        else:\n            self.next_btn.config(text=\"Next >\", command=self.next_step)\n\n\n# --- System Tools Tab ---\nclass SystemToolsTab(tk.Frame):\n    def __init__(self, parent):\n        super().__init__(parent, bg=\"#1a1a2e\")\n        self._create_ui()\n\n    def _create_ui(self):\n        title = tk.Label(\n            self,\n            text=\"\u2699\ufe0f System Tools\",\n            font=(\"Segoe UI\", 16, \"bold\"),\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        )\n        title.pack(pady=20)\n        tk.Button(\n            self,\n            text=\"Create Desktop Shortcuts\",\n            command=self.run_create_shortcuts,\n            font=(\"Segoe UI\", 12),\n        ).pack(pady=10, padx=40, fill=tk.X)\n        tk.Button(\n            self,\n            text=\"Verify Installation\",\n            command=self.run_verification,\n            font=(\"Segoe UI\", 12),\n        ).pack(pady=10, padx=40, fill=tk.X)\n        self.output_box = scrolledtext.ScrolledText(self, height=10, bg=\"#0f3460\", fg=\"#ffffff\", state=tk.DISABLED)\n        self.output_box.pack(pady=10, padx=40, fill=tk.BOTH, expand=True)\n\n    def log_output(self, message):\n        self.output_box.config(state=tk.NORMAL)\n        self.output_box.insert(tk.END, message + \"\\n\")\n        self.output_box.config(state=tk.DISABLED)\n        self.output_box.see(tk.END)\n\n    def run_create_shortcuts(self):\n        self.log_output(\"--- Creating Desktop Shortcut ---\")\n        try:\n            from win32com.client import Dispatch\n\n            desktop = Path(os.environ[\"USERPROFILE\"]) / \"Desktop\"\n            app_path = Path(__file__).resolve()\n            shortcut_path = desktop / \"\ud83d\udc34 Launch Fortuna Faucet.lnk\"\n\n            if shortcut_path.exists():\n                self.log_output(\"\ud83d\udfe1 Shortcut already exists. Overwriting.\")\n\n            shell = Dispatch(\"WScript.Shell\")\n            shortcut = shell.CreateShortCut(str(shortcut_path))\n            shortcut.TargetPath = sys.executable\n            shortcut.Arguments = f'\"{app_path}\"'\n            shortcut.WorkingDirectory = str(app_path.parent)\n\n            ico_path = app_path.parent / \"fortuna.ico\"\n            if ico_path.exists():\n                shortcut.IconLocation = str(ico_path)\n            else:\n                self.log_output(\"\ud83d\udfe1 Icon file not found, using default.\")\n\n            shortcut.save()\n            self.log_output(\"\u2705 Success: Shortcut created on Desktop.\")\n        except ImportError:\n            self.log_output(\"\u274c ERROR: 'pywin32' is not installed. Cannot create shortcuts.\")\n            self.log_output(\"  Please run: pip install pywin32\")\n        except Exception as e:\n            self.log_output(f\"\u274c ERROR: An unexpected error occurred: {e}\")\n\n    def run_verification(self):\n        self.log_output(\"\\n--- Verifying System Setup ---\")\n        verifications = [\n            (\"Python 3.11+\", lambda: sys.version_info >= (3, 11)),\n            (\n                \"Python Virtual Env (.venv)\",\n                lambda: Path(\".venv\").exists() and Path(\".venv/Scripts/python.exe\").exists(),\n            ),\n            (\n                \"Node.js (npm)\",\n                lambda: subprocess.run(\"npm -v\", shell=True, capture_output=True).returncode == 0,\n            ),\n            (\n                \"Frontend Dependencies (node_modules)\",\n                lambda: Path(\"web_platform/frontend/node_modules\").exists(),\n            ),\n        ]\n\n        all_ok = True\n        for name, check in verifications:\n            result = check()\n            self.log_output(f\"- {name}: {'\u2705 OK' if result else '\u274c FAILED'}\")\n            if not result:\n                all_ok = False\n\n        if all_ok:\n            self.log_output(\"\\n\u2705 All checks passed. System is ready.\")\n        else:\n            self.log_output(\"\\n\u274c Some checks failed. Please review the log.\")\n\n\n# --- Main Application Window ---\nclass FortunaApp(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"\ud83d\udc34 Fortuna Faucet\")\n        self.geometry(\"700x550\")\n        self.configure(bg=\"#1a1a2e\")\n\n        style = ttk.Style()\n        style.theme_use(\"clam\")\n        style.configure(\"TNotebook\", background=\"#1a1a2e\", borderwidth=0)\n        style.configure(\"TNotebook.Tab\", background=\"#404060\", foreground=\"#ffffff\", padding=[10, 5])\n        style.map(\"TNotebook.Tab\", background=[(\"selected\", \"#0f6cbd\")])\n\n        self.notebook = ttk.Notebook(self)\n\n        self.control_panel_tab = ControlPanelTab(self.notebook, self)\n        self.setup_wizard_tab = SetupWizardTab(self.notebook)\n        self.system_tools_tab = SystemToolsTab(self.notebook)\n\n        self.notebook.add(self.control_panel_tab, text=\"Control Panel\")\n        self.notebook.add(self.setup_wizard_tab, text=\"Setup & Config\")\n        self.notebook.add(self.system_tools_tab, text=\"System Tools\")\n\n        self.notebook.pack(expand=True, fill=\"both\", padx=10, pady=10)\n\n    def on_closing(self):\n        if self.control_panel_tab.backend_proc or self.control_panel_tab.frontend_proc:\n            if messagebox.askokcancel(\"Quit\", \"Services are still running. Do you want to stop them and exit?\"):\n                self.control_panel_tab.stop_services()\n                self.destroy()\n        else:\n            self.destroy()\n\n\n# --- NEW: Self-Setup UI and Logic ---\nclass SetupApp(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"Fortuna Faucet - First-Time Setup\")\n        self.geometry(\"700x500\")\n        self.configure(bg=\"#1a1a2e\")\n\n        self.protocol(\"WM_DELETE_WINDOW\", self.quit)\n\n        header_font = tk.font.Font(family=\"Segoe UI\", size=16, weight=\"bold\")\n        body_font = tk.font.Font(family=\"Segoe UI\", size=10)\n        button_font = tk.font.Font(family=\"Segoe UI\", size=12, weight=\"bold\")\n\n        tk.Label(\n            self,\n            text=\"\ud83d\udce6 Welcome to Fortuna Faucet\",\n            font=header_font,\n            bg=\"#1a1a2e\",\n            fg=\"#00ff88\",\n        ).pack(pady=(20, 10))\n        tk.Label(\n            self,\n            text=\"The necessary dependencies are not installed. Click 'Start Installation' to begin.\",\n            font=body_font,\n            bg=\"#1a1a2e\",\n            fg=\"#ffffff\",\n        ).pack(pady=(0, 20))\n\n        self.install_button = tk.Button(\n            self,\n            text=\"\u25b6\ufe0f Start Installation\",\n            font=button_font,\n            bg=\"#00ff88\",\n            fg=\"#000000\",\n            command=self.start_installation,\n            relief=tk.FLAT,\n            padx=20,\n            pady=10,\n        )\n        self.install_button.pack(pady=10)\n\n        self.output_box = scrolledtext.ScrolledText(\n            self,\n            height=15,\n            bg=\"#0f3460\",\n            fg=\"#cccccc\",\n            state=tk.DISABLED,\n            relief=tk.FLAT,\n            bd=0,\n            padx=10,\n            pady=10,\n        )\n        self.output_box.pack(pady=10, padx=40, fill=tk.BOTH, expand=True)\n\n        self.status_label = tk.Label(self, text=\"Waiting to start...\", font=body_font, bg=\"#1a1a2e\", fg=\"#ffffff\")\n        self.status_label.pack(pady=10)\n\n    def log(self, message):\n        self.output_box.config(state=tk.NORMAL)\n        self.output_box.insert(tk.END, message + \"\\n\")\n        self.output_box.config(state=tk.DISABLED)\n        self.output_box.see(tk.END)\n        self.update_idletasks()\n\n    def start_installation(self):\n        self.install_button.config(state=tk.DISABLED, text=\"Installation in progress...\")\n        self.log(\"--- Starting installation ---\")\n        self.status_label.config(text=\"Installing... Please be patient, this may take several minutes.\")\n        threading.Thread(target=self.run_install_commands, daemon=True).start()\n\n    def run_command(self, command):\n        process = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            encoding=\"utf-8\",\n            errors=\"replace\",\n            shell=True,\n        )\n        for line in iter(process.stdout.readline, \"\"):\n            self.log(line.strip())\n        process.wait()\n        return process.returncode\n\n    def run_install_commands(self):\n        commands = [\n            (\n                \"1/3: Creating Python virtual environment...\",\n                f\"{sys.executable} -m venv .venv\",\n            ),\n            (\n                \"2/3: Installing Python dependencies...\",\n                '\"' + str(Path(\".venv/Scripts/python.exe\")) + '\" -m pip install -r requirements.txt',\n            ),\n            (\n                \"3/3: Installing Node.js dependencies...\",\n                \"npm install --prefix web_platform/frontend\",\n            ),\n        ]\n\n        for i, (msg, cmd) in enumerate(commands):\n            self.log(f\"\\\\n--- STEP {msg} ---\")\n            return_code = self.run_command(cmd)\n            if return_code != 0:\n                self.log(f\"\\\\n--- ERROR: Step {i + 1} failed with code {return_code}. ---\")\n                self.status_label.config(\n                    text=\"Installation Failed. Please see log for details.\",\n                    fg=\"#ff4444\",\n                )\n                self.install_button.config(state=tk.NORMAL, text=\"Retry Installation\")\n                return\n\n        self.log(\"\\\\n--- \u2705 INSTALLATION COMPLETE! ---\")\n        self.status_label.config(text=\"Setup successful! You can now launch the application.\", fg=\"#00ff88\")\n        self.install_button.destroy()\n        launch_button = tk.Button(\n            self,\n            text=\"\ud83d\ude80 Launch Fortuna\",\n            font=tk.font.Font(family=\"Segoe UI\", size=12, weight=\"bold\"),\n            bg=\"#00ff88\",\n            fg=\"#000000\",\n            command=self.launch_app,\n            relief=tk.FLAT,\n            padx=20,\n            pady=10,\n        )\n        launch_button.pack(pady=10)\n\n    def launch_app(self):\n        self.destroy()\n        # Relaunch the script to start the main app\n        subprocess.Popen([sys.executable, __file__])\n\n\n# --- NEW: Main Execution Block ---\nif __name__ == \"__main__\":\n    VENV_PATH = Path(__file__).parent / \".venv\"\n    if not VENV_PATH.exists() or not (VENV_PATH / \"Scripts\" / \"python.exe\").exists():\n        # If the virtual environment doesn't exist, run the setup wizard.\n        setup_app = SetupApp()\n        setup_app.mainloop()\n    else:\n        # Otherwise, run the main application.\n        app = FortunaApp()\n        app.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n        app.mainloop()\n",
    "jules-scratch/verification/verify_error_handling.py": "# jules-scratch/verification/verify_error_handling.py\nfrom playwright.sync_api import expect\nfrom playwright.sync_api import sync_playwright\n\n\ndef run(playwright):\n    browser = playwright.chromium.launch(headless=True)\n    page = browser.new_page()\n\n    # Mock the API call to return an error\n    page.route(\n        \"**/api/races/qualified/trifecta?**\",\n        lambda route: route.fulfill(\n            status=500,\n            json={\n                \"error\": {\n                    \"message\": \"A data source is currently unavailable.\",\n                    \"suggestion\": \"This is usually temporary. Please try again in a few minutes.\",\n                    \"details\": \"AdapterHttpError: HTTP Error 503 for https://example.com\",\n                }\n            },\n        ),\n    )\n\n    page.goto(\"http://localhost:3000\")\n\n    # Wait for the status indicator to show \"Offline\"\n    offline_indicator = page.get_by_text(\"Offline\")\n    expect(offline_indicator).to_be_visible()\n\n    # Now that we know the app is in an error state, check for the detailed message\n    error_message = page.get_by_text(\"A data source is currently unavailable.\")\n    expect(error_message).to_be_visible()\n\n    page.screenshot(path=\"jules-scratch/verification/error_handling.png\")\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n",
    "jules-scratch/verification/verify_launch.py": "# jules-scratch/verification/verify_launch.py\nfrom playwright.sync_api import expect\nfrom playwright.sync_api import sync_playwright\n\n\ndef run(playwright):\n    browser = playwright.chromium.launch(headless=True)\n    page = browser.new_page()\n\n    page.goto(\"http://localhost:3002\")\n\n    # Wait for the dashboard to be visible\n    dashboard_title = page.get_by_text(\"Fortuna Faucet\")\n    expect(dashboard_title).to_be_visible()\n\n    page.screenshot(path=\"jules-scratch/verification/launch.png\")\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n",
    "pg_schemas/quarantine_races.sql": "CREATE TABLE IF NOT EXISTS quarantine_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(100),\n    track_name VARCHAR(100),\n    race_number INT,\n    post_time TIMESTAMP WITH TIME ZONE,\n    source VARCHAR(50),\n    raw_data_json JSONB, -- Store the original raw data for inspection\n    quarantine_reason TEXT, -- Reason for failing validation\n    collection_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);",
    "pg_schemas/quarantined_races.sql": "-- Schema for storing race data that fails validation\nCREATE TABLE IF NOT EXISTS quarantined_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(255),\n    source VARCHAR(50),\n    payload JSONB NOT NULL,\n    reason VARCHAR(255) NOT NULL,\n    quarantined_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "playwright_test.js": "const { chromium } = require('playwright');\nconst { test, expect } = require('@playwright/test');\n\n(async () => {\n  const browser = await chromium.launch();\n  const page = await browser.newPage();\n\n  // Navigate to the dashboard\n  await page.goto('http://localhost:3001');\n\n  // Wait for the initial loading to complete.\n  // We expect the skeleton loaders to disappear.\n  await expect(page.locator('div:has-text(\"Loading races...\")')).toHaveCount(0, { timeout: 15000 });\n\n  // Check for the manual override panel\n  const overridePanel = page.locator('div:has-text(\"Fetch Failed: AtTheRaces\")');\n  await expect(overridePanel).toBeVisible({ timeout: 10000 });\n\n  // Check for the text area with the correct URL\n  // The date is a placeholder, as it can change. The important part is the base URL.\n  const textArea = overridePanel.locator('textarea');\n  await expect(textArea).toHaveAttribute('value', /https:\\/\\/www\\.attheraces\\.com\\/racecards\\/\\d{4}-\\d{2}-\\d{2}/);\n\n\n  // Take a screenshot for visual confirmation\n  await page.screenshot({ path: 'manual-override-panel.png' });\n\n  await browser.close();\n})();\n",
    "pyproject.toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"paddock-parser-ng\"\nversion = \"0.1.0\"\ndescription = \"A toolkit to identify the best racecards for betting.\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\n\n[project.scripts]\npaddock_parser_ui = \"paddock_parser.entry_points:run_terminal_ui\"\npaddock_parser_dashboard = \"paddock_parser.entry_points:run_dashboard\"\npaddock_parser_predict = \"paddock_parser.entry_points:run_prediction_engine\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n# Configuration for the Ruff linter\n[tool.ruff]\n# Allow lines to be up to 120 characters long.\nline-length = 120\n\n[tool.ruff.lint]\n# Enable Pyflakes (F), pycodestyle (E, W), and isort (I) rules.\nselect = [\"E\", \"F\", \"W\", \"I\"]\nignore = []\n\n[tool.ruff.lint.isort]\n# Sort imports within their sections alphabetically.\nforce-single-line = true\n",
    "pytest.ini": "[pytest]\npythonpath = python_service\nnorecursedirs = attic tests/checkmate_v7\ntestpaths = tests/adapters tests/api tests/database tests/ui tests/utils tests/test_backtester.py tests/test_fetcher.py tests/test_forager_client.py tests/test_log_analyzer.py tests/test_merger.py tests/test_pipeline.py tests/test_python_service.py tests/test_scorer.py tests/test_api.py tests/test_legacy_scenarios.py\n",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n\n# Import all adapter classes to make them available for dynamic loading.\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .brisnet_adapter import BrisnetAdapter\nfrom .drf_adapter import DRFAdapter\nfrom .equibase_adapter import EquibaseAdapter\nfrom .fanduel_adapter import FanDuelAdapter\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .horseracingnation_adapter import HorseRacingNationAdapter\nfrom .nyrabets_adapter import NYRABetsAdapter\nfrom .oddschecker_adapter import OddscheckerAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .punters_adapter import PuntersAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .racingpost_adapter import RacingPostAdapter\nfrom .racingtv_adapter import RacingTVAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .tab_adapter import TabAdapter\nfrom .template_adapter import TemplateAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\nfrom .twinspires_adapter import TwinSpiresAdapter\nfrom .universal_adapter import UniversalAdapter\nfrom .xpressbet_adapter import XpressbetAdapter\n\n# Define the public API for the adapters package, making it easy for the\n# orchestrator to discover and use them.\n__all__ = [\n    \"AtTheRacesAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairDataScientistAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"BrisnetAdapter\",\n    \"DRFAdapter\",\n    \"EquibaseAdapter\",\n    \"FanDuelAdapter\",\n    \"GbgbApiAdapter\",\n    \"GreyhoundAdapter\",\n    \"HarnessAdapter\",\n    \"HorseRacingNationAdapter\",\n    \"NYRABetsAdapter\",\n    \"OddscheckerAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"PuntersAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"RacingPostAdapter\",\n    \"RacingTVAdapter\",\n    \"SportingLifeAdapter\",\n    \"TabAdapter\",\n    \"TemplateAdapter\",\n    \"TheRacingApiAdapter\",\n    \"TimeformAdapter\",\n    \"TVGAdapter\",\n    \"TwinSpiresAdapter\",\n    \"UniversalAdapter\",\n    \"XpressbetAdapter\",\n]\n",
    "python_service/adapters/base_adapter_v3.py": "# python_service/adapters/base_v3.py\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import Any\nfrom typing import AsyncGenerator\nfrom typing import List\n\nimport httpx\nimport structlog\nfrom tenacity import RetryError\nfrom tenacity import retry\nfrom tenacity import stop_after_attempt\nfrom tenacity import wait_exponential\n\nfrom ..core.exceptions import AdapterHttpError\nfrom ..manual_override_manager import ManualOverrideManager\nfrom ..models import Race\n\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    Abstract base class for all V3 data adapters.\n    Enforces a standardized fetch/parse pattern and includes robust request handling.\n    \"\"\"\n\n    def __init__(self, source_name: str, base_url: str, config=None, timeout: int = 20):\n        self.source_name = source_name\n        self.base_url = base_url\n        self.config = config\n        self.timeout = timeout\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient = None  # Injected by the engine\n        self.manual_override_manager: ManualOverrideManager = None\n        self.supports_manual_override = True  # Can be overridden by subclasses\n\n    def enable_manual_override(self, manager: ManualOverrideManager):\n        \"\"\"Injects the manual override manager into the adapter.\"\"\"\n        self.manual_override_manager = manager\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw data (e.g., HTML, JSON) for the given date.\n        This is the only method that should perform network operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        Parses the raw data retrieved by _fetch_data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> AsyncGenerator[Race, None]:\n        \"\"\"\n        Orchestrates the fetch-then-parse pipeline for the adapter.\n        This public method should not be overridden by subclasses.\n        \"\"\"\n        raw_data = None\n\n        if self.manual_override_manager:\n            # This is not a full URL, but a representative key for the fetch operation\n            # Subclasses might need to override get_races to provide a more specific URL if needed\n            lookup_key = f\"{self.base_url}/racecards/{date}\"\n            manual_data = self.manual_override_manager.get_manual_data(self.source_name, lookup_key)\n            if manual_data:\n                self.logger.info(\"Using manually submitted data for request\", url=lookup_key)\n                # Reconstruct a dictionary similar to what _fetch_data would return\n                # This may need adjustment based on adapter specifics\n                raw_data = {\"pages\": [manual_data[0]], \"date\": date}\n\n        if raw_data is None:\n            try:\n                raw_data = await self._fetch_data(date)\n            except AdapterHttpError as e:\n                if self.manual_override_manager and self.supports_manual_override:\n                    self.manual_override_manager.register_failure(self.source_name, e.url)\n                raise  # Reraise the exception to be handled by the OddsEngine\n\n        if raw_data is not None:\n            parsed_races = self._parse_races(raw_data)\n            for race in parsed_races:\n                yield race\n\n    @retry(\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        stop=stop_after_attempt(3),\n        reraise=True,  # Reraise the final exception to be caught by get_races\n    )\n    async def make_request(self, http_client: httpx.AsyncClient, method: str, url: str, **kwargs) -> httpx.Response:\n        \"\"\"\n        Makes a resilient HTTP request with built-in retry logic using tenacity.\n        \"\"\"\n        # Ensure the URL is correctly formed, whether it's relative or absolute\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url.rstrip('/')}/{url.lstrip('/')}\"\n\n        try:\n            self.logger.info(\"Making request\", method=method.upper(), url=full_url)\n            response = await http_client.request(method, full_url, timeout=self.timeout, **kwargs)\n            response.raise_for_status()  # Raise an exception for 4xx/5xx responses\n            return response\n        except httpx.HTTPStatusError as e:\n            self.logger.error(\n                \"HTTP Status Error during request\",\n                status_code=e.response.status_code,\n                url=str(e.request.url),\n            )\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=e.response.status_code,\n                url=str(e.request.url),\n            ) from e\n        except (httpx.RequestError, RetryError) as e:\n            self.logger.error(\"Request Error or Retry Error\", error=str(e))\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=503,  # Service Unavailable\n                url=full_url,\n            ) from e\n\n    def get_status(self) -> dict:\n        \"\"\"\n        Returns a dictionary representing the adapter's current status.\n        Subclasses can extend this to include more specific health checks.\n        \"\"\"\n        return {\n            \"adapter_name\": self.source_name,\n            \"status\": \"OK\",  # Basic status; can be enhanced in subclasses\n        }\n",
    "python_service/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairExchange\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "python_service/adapters/drf_adapter.py": "# python_service/adapters/drf_adapter.py\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass DRFAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for drf.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"DRF\"\n    BASE_URL = \"https://www.drf.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetches the raw HTML from the DRF entries page.\"\"\"\n        url = f\"/entries/{date}/USA\"\n        response = await self.make_request(self.http_client, \"GET\", url)\n        return {\"html\": response.text, \"date\": date} if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[dict]) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        venue_node = soup.select_one(\"div.track-info h1\")\n        if not venue_node:\n            self.logger.warning(\"Could not find venue name on DRF page.\")\n            return []\n\n        venue_text = venue_node.text\n        venue = normalize_venue_name(venue_text.split(\" - \")[0].replace(\"Entries for \", \"\"))\n\n        races = []\n        for race_entry in soup.select(\"div.race-entries\"):\n            try:\n                race_number_str = race_entry.get(\"data-race-number\")\n                if not race_number_str or not race_number_str.isdigit():\n                    continue\n                race_number = int(race_number_str)\n\n                post_time_node = race_entry.select_one(\".post-time\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text.replace(\"Post Time: \", \"\").strip()\n                start_time = parse(f\"{race_date} {post_time_str}\")\n\n                runners = []\n                for entry in race_entry.select(\"li.entry\"):\n                    if \"scratched\" in entry.get(\"class\", []):\n                        continue\n\n                    number_node = entry.select_one(\".program-number\")\n                    if not number_node or not number_node.text.isdigit():\n                        continue\n                    number = int(number_node.text)\n\n                    name_node = entry.select_one(\".horse-name\")\n                    if not name_node:\n                        continue\n                    name = name_node.text\n\n                    odds_node = entry.select_one(\".odds\")\n                    odds_str = odds_node.text.replace(\"-\", \"/\") if odds_node else \"\"\n\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    odds = {}\n                    if win_odds:\n                        odds[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(Runner(number=number, name=name, odds=odds))\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"drf_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                    field_size=len(runners),\n                )\n                races.append(race)\n            except (ValueError, KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a race on DRF, skipping.\", exc_info=True)\n                continue\n        return races\n",
    "python_service/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, \"GREYHOUND_API_URL\") or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "python_service/adapters/harness_adapter.py": "# python_service/adapters/harness_adapter.py\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom zoneinfo import ZoneInfo\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass HarnessAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US harness racing data with manual override support.\"\"\"\n\n    SOURCE_NAME = \"USTrotting\"\n    BASE_URL = \"https://data.ustrotting.com/api/racenet/racing/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches all harness races for a given date.\"\"\"\n        response = await self.make_request(self.http_client, \"GET\", f\"card/{date}\")\n\n        if not response:\n            return None\n\n        card_data = response.json()\n        return {\"data\": card_data, \"date\": date}\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"data\") or not raw_data.get(\"data\", {}).get(\"meetings\"):\n            self.logger.warning(\"No meetings found in harness data response.\")\n            return []\n\n        all_races = []\n        date = raw_data.get(\"date\")\n        for meeting in raw_data.get(\"data\", {}).get(\"meetings\", []):\n            track_name = meeting.get(\"track\", {}).get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name, date):\n                        all_races.append(race)\n                except Exception:\n                    self.logger.warning(\n                        \"Failed to parse harness race, skipping.\",\n                        race_data=race_data,\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: dict, track_name: str, date: str) -> Optional[Race]:\n        \"\"\"Parses a single race from the USTA API into a Race object.\"\"\"\n        race_number = race_data.get(\"raceNumber\")\n        post_time_str = race_data.get(\"postTime\")\n        if not all([race_number, post_time_str]):\n            return None\n\n        start_time = self._parse_post_time(date, post_time_str)\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            if runner_data.get(\"scratched\", False):\n                continue\n\n            odds_str = runner_data.get(\"morningLineOdds\", \"\")\n            if \"/\" not in odds_str and odds_str.isdigit():\n                odds_str = f\"{odds_str}/1\"\n\n            odds = {}\n            win_odds = parse_odds_to_decimal(odds_str)\n            if win_odds and win_odds < 999:\n                odds = {\n                    self.SOURCE_NAME: OddsData(\n                        win=win_odds,\n                        source=self.SOURCE_NAME,\n                        last_updated=datetime.now(),\n                    )\n                }\n\n            runners.append(\n                Runner(\n                    number=runner_data.get(\"postPosition\", 0),\n                    name=runner_data.get(\"horse\", {}).get(\"name\", \"Unknown Horse\"),\n                    odds=odds,\n                    scratched=False,\n                )\n            )\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"ust_{track_name.lower().replace(' ', '')}_{date}_{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n    def _parse_post_time(self, date: str, post_time: str) -> datetime:\n        \"\"\"Parses a time string like '07:00 PM' into a timezone-aware datetime object.\"\"\"\n        dt_str = f\"{date} {post_time}\"\n        naive_dt = datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        # Assume Eastern Time for USTA data, a common standard for US racing.\n        eastern = ZoneInfo(\"America/New_York\")\n        return naive_dt.replace(tzinfo=eastern)\n",
    "python_service/adapters/horseracingnation_adapter.py": "# python_service/adapters/horseracingnation_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass HorseRacingNationAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for horseracingnation.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"HorseRacingNation\"\n    BASE_URL = \"https://www.horseracingnation.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/nyrabets_adapter.py": "# python_service/adapters/nyrabets_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass NYRABetsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for nyrabets.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/oddschecker_adapter.py": "# python_service/adapters/oddschecker_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass OddscheckerAdapter(BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        # Note: Oddschecker doesn't seem to support historical dates well in its main nav,\n        # but we build the URL as if it does for future compatibility.\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        # Find all links to individual race pages\n        race_links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                race = self._parse_race_page(soup, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, soup: BeautifulSoup, race_date) -> Optional[Race]:\n        track_name_node = soup.select_one(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.get_text(strip=True)\n\n        race_time_node = soup.select_one(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.get_text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = soup.select_one(\"a.race-time-link.active\")\n        race_number = 1\n        if active_link:\n            all_links = soup.select(\"a.race-time-link\")\n            try:\n                race_number = all_links.index(active_link) + 1\n            except ValueError:\n                pass  # Keep default race number if active link not in all links\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in soup.select(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.get_text(strip=True)\n\n            odds_node = row.select_one(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.get_text(strip=True)\n\n            number_node = row.select_one(\"td.runner-number\")\n            if not number_node or not number_node.get_text(strip=True).isdigit():\n                return None\n            number = int(number_node.get_text(strip=True))\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if win_odds and win_odds < 999:\n                odds_dict[self.source_name] = OddsData(\n                    win=win_odds, source=self.source_name, last_updated=datetime.now()\n                )\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
    "python_service/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n# NOTE: This is a hypothetical implementation based on a potential API structure.\n\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for the hypothetical PointsBet Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches all greyhound events for a given date.\"\"\"\n        endpoint = f\"sports/greyhound-racing/events/by-date/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json().get(\"events\", []) if response else None\n\n    def _parse_races(self, raw_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw event data into a list of standardized Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for event in raw_data:\n            try:\n                if not event.get(\"competitors\") or not event.get(\"startTime\"):\n                    continue\n\n                runners = []\n                for competitor in event.get(\"competitors\", []):\n                    price = competitor.get(\"price\")\n                    if not price:\n                        continue\n\n                    odds_val = Decimal(str(price))\n                    odds = {\n                        self.source_name: OddsData(\n                            win=odds_val,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n                    runner = Runner(\n                        number=competitor.get(\"number\", 99),\n                        name=competitor.get(\"name\", \"Unknown\"),\n                        odds=odds,\n                    )\n                    runners.append(runner)\n\n                if runners:\n                    race_id = event.get(\"id\")\n                    if not race_id:\n                        continue\n\n                    race = Race(\n                        id=f\"pbg_{race_id}\",\n                        venue=event.get(\"venue\", {}).get(\"name\", \"Unknown Venue\"),\n                        start_time=datetime.fromisoformat(event[\"startTime\"]),\n                        race_number=event.get(\"raceNumber\", 1),\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.warning(\n                    \"Failed to parse PointsBet Greyhound event.\",\n                    event=event,\n                    exc_info=True,\n                )\n                continue\n        return races\n",
    "python_service/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
    "python_service/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(\n        self,\n        max_field_size: int = 10,\n        min_favorite_odds: float = 2.5,\n        min_second_favorite_odds: float = 4.0,\n    ):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        qualified_races = []\n        for race in races:\n            score = self._evaluate_race(race)\n            if score > 0:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\n            \"Universal scoring complete\",\n            total_races_scored=len(qualified_races),\n            criteria=criteria,\n        )\n\n        for race in qualified_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": qualified_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier(\"Fortuna\") if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "python_service/api.py": "# python_service/api.py\n\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import List\nfrom typing import Optional\n\nimport aiosqlite\nimport structlog\nfrom fastapi import Depends\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import Request\nfrom fastapi import WebSocket\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom slowapi import Limiter\nfrom slowapi import _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom starlette.websockets import WebSocketDisconnect\n\n# --- PyInstaller Explicit Imports ---\nfrom .analyzer import AnalyzerEngine\nfrom .cache_manager import cache_manager\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .engine import OddsEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .manual_override_manager import ManualOverrideManager\nfrom .middleware.error_handler import UserFriendlyException\nfrom .middleware.error_handler import user_friendly_exception_handler\nfrom .middleware.error_handler import validation_exception_handler\nfrom .models import AggregatedResponse\nfrom .models import QualifiedRacesResponse\nfrom .models import Race\nfrom .models import TipsheetRace\nfrom .security import verify_api_key\n\n# ------------------------------------\n\nlog = structlog.get_logger()\n\n# Create a fixed thread pool for blocking calls at the module level\nexecutor = ThreadPoolExecutor(max_workers=1)\n\n\ndef _initialize_heavy_resources_sync(app: FastAPI):\n    \"\"\"\n    This synchronous function contains the blocking I/O and CPU-intensive\n    initialization of the OddsEngine and its ~25 adapters. By isolating it,\n    we can run it in a background thread without stalling the main Uvicorn event loop.\n    \"\"\"\n    log.info(\"Background initialization of heavy resources started.\")\n    try:\n        settings = get_settings()\n\n        # Initialize WebSocket connection manager\n        connection_manager = ConnectionManager()\n\n        # Initialize manual override manager\n        manual_override_manager = ManualOverrideManager()\n\n        # Initialize engine with manual override and WebSocket support\n        engine = OddsEngine(\n            config=settings,\n            manual_override_manager=manual_override_manager,\n            connection_manager=connection_manager,\n        )\n\n        # Store the initialized components on the app state\n        app.state.engine = engine\n        app.state.analyzer_engine = AnalyzerEngine()\n        app.state.manual_override_manager = manual_override_manager\n        app.state.connection_manager = connection_manager\n        log.info(\"Background initialization of heavy resources completed successfully.\")\n    except Exception:\n        log.critical(\"CRITICAL: Background initialization failed.\", exc_info=True)\n        # In a real-world scenario, you might want a more robust way\n        # to signal this failure to the main application.\n        app.state.engine = None\n\n\nclass ConnectionManager:\n    \"\"\"Manages active WebSocket connections.\"\"\"\n\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        log.info(\"WebSocket ConnectionManager initialized.\")\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        log.info(\"New WebSocket connection established.\")\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n        log.info(\"WebSocket connection closed.\")\n\n    async def broadcast(self, message: dict):\n        \"\"\"Broadcasts a message to all connected clients.\"\"\"\n        if not self.active_connections:\n            return\n\n        log.info(\n            \"Broadcasting message to connected clients\",\n            client_count=len(self.active_connections),\n        )\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(message)\n            except Exception:\n                log.error(\"Error sending message to a WebSocket client.\", exc_info=True)\n\n\n# Lifespan context manager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    configure_logging()\n    log.info(\"Uvicorn is online, starting lifespan hook.\")\n\n    # 1. Perform lightweight, non-blocking startup tasks\n    settings = get_settings()\n    await cache_manager.connect(settings.REDIS_URL)\n    log.info(\"Fast, non-blocking startup tasks complete (Redis connected).\")\n\n    # 2. Schedule the heavy, synchronous initialization to run in a background thread\n    loop = asyncio.get_event_loop()\n    loop.run_in_executor(executor, _initialize_heavy_resources_sync, app)\n    log.info(\"Heavy resource initialization has been scheduled in a background thread.\")\n\n    # 3. Yield control back to Uvicorn immediately. The server is now ready to accept requests\n    #    while the OddsEngine initializes in the background.\n    yield\n\n    # --- Shutdown Sequence ---\n    log.info(\"Server shutdown sequence initiated.\")\n    if hasattr(app.state, \"engine\") and app.state.engine:\n        log.info(\"Closing HTTP client resources.\")\n        await app.state.engine.close()\n\n    await cache_manager.disconnect()\n    executor.shutdown(wait=False)\n    log.info(\"Server shutdown sequence complete.\")\n\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI(\n    title=\"Fortuna Faucet API\",\n    version=\"2.1\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\",\n)\n\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\napp.add_exception_handler(UserFriendlyException, user_friendly_exception_handler)\napp.include_router(health_router)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"http://localhost:3001\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\"],\n    allow_headers=[\"*\"],\n)\n\n\ndef get_engine(request: Request) -> OddsEngine:\n    return request.app.state.engine\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        return engine.get_all_adapter_statuses()\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_favorite_odds: float = Query(2.5, ge=1.0, le=100.0),\n    min_second_favorite_odds: float = Query(4.0, ge=1.0, le=100.0),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        aggregated_data = await engine.fetch_all_odds(date_str)\n        races = [Race(**r) for r in aggregated_data.get(\"races\", [])]\n        analyzer_engine = request.app.state.analyzer_engine\n        custom_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds,\n        }\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(\"Error in /api/races/qualified\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\n@app.get(\"/api/races/filter-suggestions\")\nasync def get_filter_suggestions(engine: OddsEngine = Depends(get_engine)):\n    try:\n        date_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n        aggregated = await engine.fetch_all_odds(date_str)\n        if not aggregated or not aggregated.get(\"races\"):\n            return {\"suggestions\": {}}\n        field_sizes = [len(r[\"runners\"]) for r in aggregated[\"races\"]]\n        favorite_odds, second_favorite_odds = [], []\n        for race_data in aggregated[\"races\"]:\n            race = Race(**race_data)\n            runners = race.runners\n            if len(runners) >= 2:\n                odds_list = []\n                for runner in runners:\n                    if not runner.scratched and runner.odds:\n                        best_odd = min(\n                            (o.win for o in runner.odds.values() if o.win is not None),\n                            default=None,\n                        )\n                        if best_odd is not None:\n                            odds_list.append(float(best_odd))\n                if len(odds_list) >= 2:\n                    odds_list.sort()\n                    favorite_odds.append(odds_list[0])\n                    second_favorite_odds.append(odds_list[1])\n        return {\n            \"suggestions\": {\n                \"max_field_size\": {\"recommended\": (int(sum(field_sizes) / len(field_sizes)) if field_sizes else 10)},\n                \"min_favorite_odds\": {\"recommended\": 2.5},\n                \"min_second_favorite_odds\": {\"recommended\": 4.0},\n            }\n        }\n    except Exception:\n        log.error(\"Error generating filter suggestions\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Failed to generate suggestions\")\n\n\n@app.get(\"/api/races/source/{source_name}\", response_model=AggregatedResponse)\n@limiter.limit(\"60/minute\")\nasync def get_races_by_source(\n    source_name: str,\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(date_str, source=source_name)\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(f\"Error in /api/races/source/{source_name}\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    source: Optional[str] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(date_str, source)\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\nDB_PATH = \"fortuna.db\"\n\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = \"SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC\"\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    return results\n\n\n# API Models\nclass ManualDataSubmission(BaseModel):\n    request_id: str\n    content: str\n    content_type: str = \"html\"\n\n\n# New endpoints\n@app.get(\"/api/manual-overrides/pending\")\n@limiter.limit(\"60/minute\")\nasync def get_pending_overrides(\n    request: Request,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Get all pending manual override requests\"\"\"\n    pending = manager.get_pending_requests()\n    return {\"pending_requests\": [req.model_dump() for req in pending]}\n\n\n@app.post(\"/api/manual-overrides/submit\")\n@limiter.limit(\"30/minute\")\nasync def submit_manual_data(\n    request: Request,\n    submission: ManualDataSubmission,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Submit manually-provided data for a failed fetch\"\"\"\n    success = manager.submit_manual_data(\n        request_id=submission.request_id,\n        raw_content=submission.content,\n        content_type=submission.content_type,\n    )\n\n    if success:\n        return {\"status\": \"success\", \"message\": \"Manual data submitted\"}\n    else:\n        raise HTTPException(status_code=404, detail=\"Request not found\")\n\n\n@app.post(\"/api/manual-overrides/skip/{request_id}\")\n@limiter.limit(\"60/minute\")\nasync def skip_manual_override(\n    request: Request,\n    request_id: str,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Skip a manual override request\"\"\"\n    success = manager.skip_request(request_id)\n\n    if success:\n        return {\"status\": \"success\", \"message\": \"Request skipped\"}\n    else:\n        raise HTTPException(status_code=404, detail=\"Request not found\")\n\n\n@app.post(\"/api/manual-overrides/cleanup\")\n@limiter.limit(\"60/minute\")\nasync def cleanup_old_overrides(\n    request: Request,\n    max_age_hours: int = 24,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Clean up old manual override requests\"\"\"\n    manager.clear_old_requests(max_age_hours)\n    return {\"status\": \"success\", \"message\": \"Old requests cleaned\"}\n\n\n@app.websocket(\"/ws/live-updates\")\nasync def websocket_endpoint(websocket: WebSocket, api_key: str = Query(...)):\n    \"\"\"WebSocket endpoint for live race updates.\"\"\"\n    try:\n        # Use the existing API key verification logic\n        # This is a synchronous call, which is fine for auth at the start.\n        # In a real-world scenario with high connection rates, you might\n        # want to make this check asynchronous if it involved I/O.\n        verify_api_key(api_key)\n    except HTTPException as e:\n        log.warning(\"WebSocket connection rejected due to invalid API key.\")\n        await websocket.close(code=4001, reason=f\"Authentication failed: {e.detail}\")\n        return\n\n    manager = websocket.app.state.connection_manager\n    await manager.connect(websocket)\n    try:\n        # Keep the connection alive, listening for messages (if any)\n        while True:\n            # You could implement logic here to handle incoming messages if needed\n            # For now, it's just a broadcast-only connection\n            await websocket.receive_text()\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        log.info(\"Client disconnected from WebSocket.\")\n",
    "python_service/core/__init__.py": "",
    "python_service/core/exceptions.py": "# python_service/core/exceptions.py\n\"\"\"\nCustom, application-specific exceptions for the Fortuna Faucet service.\n\nThis module defines a hierarchy of exception classes to provide standardized\nerror handling, particularly for the data adapter layer. Using these specific\nexceptions instead of generic ones allows for more precise error handling and\nclearer logging throughout the application.\n\"\"\"\n\n\nclass FortunaException(Exception):\n    \"\"\"Base class for all custom exceptions in this application.\"\"\"\n\n    pass\n\n\nclass AdapterError(FortunaException):\n    \"\"\"Base class for all adapter-related errors.\"\"\"\n\n    def __init__(self, adapter_name: str, message: str):\n        self.adapter_name = adapter_name\n        super().__init__(f\"[{adapter_name}] {message}\")\n\n\nclass AdapterRequestError(AdapterError):\n    \"\"\"Raised for general network or request-related issues.\"\"\"\n\n    pass\n\n\nclass AdapterHttpError(AdapterRequestError):\n    \"\"\"Raised for unsuccessful HTTP responses (e.g., 4xx or 5xx status codes).\"\"\"\n\n    def __init__(self, adapter_name: str, status_code: int, url: str):\n        self.status_code = status_code\n        self.url = url\n        message = f\"Received HTTP {status_code} from {url}\"\n        super().__init__(adapter_name, message)\n\n\nclass AdapterAuthError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 401/403 errors, indicating an auth failure.\"\"\"\n\n    pass\n\n\nclass AdapterRateLimitError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 429 errors, indicating a rate limit has been hit.\"\"\"\n\n    pass\n\n\nclass AdapterTimeoutError(AdapterRequestError):\n    \"\"\"Raised when a request to an external API times out.\"\"\"\n\n    pass\n\n\nclass AdapterConnectionError(AdapterRequestError):\n    \"\"\"Raised for DNS lookup failures or refused connections.\"\"\"\n\n    pass\n\n\nclass AdapterConfigError(AdapterError):\n    \"\"\"Raised when an adapter is missing necessary configuration (e.g., an API key).\"\"\"\n\n    pass\n\n\nclass AdapterParsingError(AdapterError):\n    \"\"\"Raised when an adapter fails to parse the response from an API.\"\"\"\n\n    pass\n",
    "python_service/credentials_manager.py": "# python_service/credentials_manager.py\ntry:\n    import keyring\n    # This check is crucial for cross-platform compatibility\n    import keyring.backends.windows\n\n    IS_WINDOWS = True\nexcept ImportError:\n    keyring = None\n    IS_WINDOWS = False\n\n\nclass SecureCredentialsManager:\n    \"\"\"Manages secrets in the system's native credential store.\"\"\"\n\n    SERVICE_NAME = \"Fortuna\"\n\n    @staticmethod\n    def save_credential(account: str, secret: str) -> bool:\n        \"\"\"Saves a secret for a given account (e.g., 'api_key', 'betfair_username').\"\"\"\n        if not IS_WINDOWS:\n            print(\"Credential storage is only supported on Windows.\")\n            return False\n        try:\n            keyring.set_password(SecureCredentialsManager.SERVICE_NAME, account, secret)\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credential for {account}: {e}\")\n            return False\n\n    @staticmethod\n    def get_credential(account: str) -> str:\n        \"\"\"Retrieves a secret for a given account.\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            return keyring.get_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credential for {account}: {e}\")\n            return None\n\n    @staticmethod\n    def get_betfair_credentials() -> tuple[str, str]:\n        \"\"\"Convenience method to retrieve both Betfair username and password.\"\"\"\n        username = SecureCredentialsManager.get_credential(\"betfair_username\")\n        password = SecureCredentialsManager.get_credential(\"betfair_password\")\n        return username, password\n\n    @staticmethod\n    def delete_credential(account: str):\n        \"\"\"Deletes a specific credential.\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception:\n            pass\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.drf_adapter import DRFAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                DRFAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    adapter_instance = adapter_cls(config=self.config)\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters.append(adapter_instance)\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_cls.__name__,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_cls.__name__}\",\n                        exc_info=True,\n                    )\n\n            # Special case for BetfairDataScientistAdapter with extra args\n            try:\n                bds_adapter = BetfairDataScientistAdapter(\n                    model_name=\"ThoroughbredModel\",\n                    url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n                    config=self.config,\n                )\n                if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n                    bds_adapter.enable_manual_override(manual_override_manager)\n                self.adapters.append(bds_adapter)\n            except Exception:\n                self.logger.warning(\n                    \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n                    exc_info=True,\n                )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            races = [Race(**race_data) for race_data in race_data_list]\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name, status_code=e.status_code, url=e.url, exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name, error=str(e), exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached and broadcasted via WebSocket.\n        \"\"\"\n        # Construct a cache key\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        log.info(\"Cache miss for fetch_all_odds\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._fetch_with_semaphore(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        for result in results:\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed\", error=result, exc_info=False)\n                continue\n\n            _adapter_name, adapter_result, _duration = result\n            source_info = adapter_result.get(\"source_info\", {})\n            source_infos.append(source_info)\n            if source_info.get(\"status\") == \"SUCCESS\":\n                all_races.extend(adapter_result.get(\"races\", []))\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            source_info=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n            },\n        )\n\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Set the result in the cache\n        await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "python_service/fortuna_windows_service.py": "# fortuna_windows_service.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Ensure the script's directory is at the front of the path\nscript_dir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, script_dir)\n\ntry:\n    from fortuna_service import FortunaBackgroundService\nexcept ImportError as e:\n    # Log a detailed error to the Windows Event Log if the import fails\n    servicemanager.LogErrorMsg(f\"FATAL: Could not import FortunaBackgroundService. Error: {e}\")\n    sys.exit(1)  # Exit with an error code\n\n\nclass FortunaWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaV8Service\"\n    _svc_display_name_ = \"Fortuna V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.fortuna_service = FortunaBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(name)s - %(levelname)s - %(message)s\",\n            handlers=[servicemanager.LogHandler()],\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.fortuna_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        self.fortuna_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaWindowsService)\n",
    "python_service/middleware/__init__.py": "",
    "python_service/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "python_service/utils/__init__.py": "",
    "scripts/audit_rebranding.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Rebranding Audit Script\n# ==============================================================================\n# This script performs a comprehensive, read-only audit of the project to\n# identify all files containing legacy branding terms.\n# ==============================================================================\n\nimport os\n\n# --- CONFIGURATION ---\nTARGET_TERMS = [\"checkmate\", \"solo\"]\nEXCLUDED_DIRS = [\n    \".git\",\n    \".venv\",\n    \"node_modules\",\n    \"build\",\n    \"dist\",\n    \"__pycache__\",\n    \"ReviewableJSON\",\n]\nEXCLUDED_FILES = [\"audit_rebranding.py\", \"REBRANDING_AUDIT.md\"]\nOUTPUT_FILE = \"REBRANDING_AUDIT.md\"\n# -------------------\n\n\ndef search_file_for_terms(file_path, terms):\n    \"\"\"Searches a single file for a list of terms, case-insensitively.\"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read().lower()\n            for term in terms:\n                if term in content:\n                    return True\n    except Exception as e:\n        print(f\"[WARNING] Could not read file {file_path}: {e}\")\n    return False\n\n\ndef main():\n    \"\"\"Main orchestrator for the audit.\"\"\"\n    print(\"--- Starting Rebranding Audit ---\")\n    affected_files = []\n    for root, dirs, files in os.walk(\".\", topdown=True):\n        # Exclude specified directories\n        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]\n\n        for filename in files:\n            if filename in EXCLUDED_FILES:\n                continue\n\n            file_path = os.path.join(root, filename)\n\n            # Check filename itself\n            if any(term in filename.lower() for term in TARGET_TERMS):\n                affected_files.append(file_path)\n                print(f\"[FOUND] Legacy term in filename: {file_path}\")\n                continue  # No need to search content if filename matches\n\n            # Check file content\n            if search_file_for_terms(file_path, TARGET_TERMS):\n                affected_files.append(file_path)\n                print(f\"[FOUND] Legacy term in content: {file_path}\")\n\n    print(f\"\\n--- Audit Complete. Found {len(affected_files)} affected files. ---\")\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"# Fortuna Faucet: Rebranding Audit Report\\n\\n\")\n        f.write(\"This report lists all files containing legacy branding terms (`checkmate`, `solo`).\\n\\n---\\n\\n\")\n        if affected_files:\n            for file_path in sorted(affected_files):\n                f.write(f\"- `{file_path.replace(os.sep, '/')}`\\n\")\n        else:\n            f.write(\"No files with legacy branding were found.\\n\")\n\n    print(f\"[SUCCESS] Report written to {OUTPUT_FILE}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom multiprocessing import Queue\n\n# --- Configuration ---\nMANIFEST_FILES = [\n    \"MANIFEST_PART1_BACKEND.json\",\n    \"MANIFEST_PART2_FRONTEND.json\",\n    \"MANIFEST_PART3_SUPPORT.json\",\n    \"MANIFEST_PART4_ROOT.json\",\n]\nOUTPUT_DIR = \"ReviewableJSON\"\nFILE_PROCESSING_TIMEOUT = 10\nEXCLUDED_FILES = [\"package-lock.json\"]\nMAX_FILE_SIZE_MB = 10  # Max file size in megabytes\n\n\ndef read_json_manifest(manifest_path: str) -> list[str]:\n    \"\"\"Reads a JSON manifest file and returns a list of file paths.\"\"\"\n    try:\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return []\n\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\n\ndef convert_file_to_json_sandboxed(file_path):\n    # --- Pre-flight check: File size ---\n    try:\n        file_size = os.path.getsize(file_path)\n        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:\n            return {\"error\": f\"File exceeds {MAX_FILE_SIZE_MB}MB size limit.\"}\n    except FileNotFoundError:\n        return {\"error\": \"File not found.\"}\n    except Exception as e:\n        return {\"error\": f\"Could not check file size: {e}\"}\n\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    try:\n        if p.is_alive():\n            print(f\"    [WARNING] Process for {file_path} timed out. Attempting graceful termination...\")\n            p.terminate()\n            p.join(timeout=2)  # Give it a moment to terminate gracefully\n\n            if p.is_alive():\n                print(f\"    [ERROR] Graceful termination failed. Forcibly killing process...\")\n                p.kill()  # The ultimate \"just die\"\n                p.join()\n            return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n        if not q.empty():\n            return q.get()\n        return {\"error\": \"Unknown error in sandboxed read process.\"}\n    finally:\n        # \u2705 Properly close and flush the queue\n        try:\n            while not q.empty():\n                q.get_nowait()\n        except Exception:\n            pass\n        q.close()\n        q.join_thread()\n\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'=' * 60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'=' * 60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        paths = read_json_manifest(manifest)\n        if paths:\n            all_local_paths.extend(paths)\n            print(f\"    --> Found {len(paths)} valid file paths.\")\n        else:\n            print(f\"    [WARNING] Manifest not found or is empty: {manifest}\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        if os.path.basename(local_path) in EXCLUDED_FILES:\n            print(f\"\\n--> Skipping excluded file: {local_path}\")\n            failed_count += 1\n            continue\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + \".json\")\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Backup process complete.\")\n    print(f\"Successfully processed: {processed_count}/{len(unique_local_paths)}\")\n    print(f\"Failed/Skipped: {failed_count}\")\n    print(f\"{'=' * 60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/install_fortuna_gui.bat": "@echo off\nREM Interactive MSI installation with standard Windows UI\n\ntitle Fortuna Faucet Installation Wizard\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Administrator privileges required\n    echo Please right-click this file and select \"Run as Administrator\"\n    pause\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" /L*v \"%TEMP%\\fortuna_install.log\"\n\nif %errorlevel% equ 0 (\n    echo Installation completed successfully!\n    echo Access dashboard at: http://localhost:3000\n) else (\n    echo Installation failed. Log: %TEMP%\\fortuna_install.log\n)\npause",
    "scripts/prepare_minimal_build.py": "# scripts/prepare_minimal_build.py\nimport os\nimport shutil\n\n# This script prepares the source tree for a 'minimal' build.\n# A minimal build includes only the core application and a small, curated\n# set of essential data adapters, excluding the larger, more specialized ones.\n\nADAPTERS_TO_KEEP = [\n    \"__init__.py\",\n    \"base_adapter.py\",\n    \"handler_factory.py\",\n    # --- Essential Adapters ---\n    \"betfair_adapter.py\",\n    \"sporting_life_adapter.py\",\n    \"racing_post_adapter.py\",\n]\n\n\ndef main():\n    \"\"\"\n    Removes non-essential adapter files from the python_service/adapters\n    directory to create a minimal build artifact.\n    \"\"\"\n    adapters_dir = os.path.join(\"python_service\", \"adapters\")\n    if not os.path.isdir(adapters_dir):\n        print(f\"[ERROR] Adapters directory not found at: {adapters_dir}\")\n        exit(1)\n\n    print(f\"Scanning adapters directory: {adapters_dir}\")\n    removed_count = 0\n    for filename in os.listdir(adapters_dir):\n        if filename not in ADAPTERS_TO_KEEP:\n            file_path = os.path.join(adapters_dir, filename)\n            try:\n                if os.path.isfile(file_path):\n                    os.remove(file_path)\n                    print(f\"  - Removed file: {filename}\")\n                    removed_count += 1\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n                    print(f\"  - Removed directory: {filename}\")\n                    removed_count += 1\n            except OSError as e:\n                print(f\"[ERROR] Failed to remove {file_path}: {e}\")\n                exit(1)\n\n    print(f\"\\nMinimal build preparation complete. Removed {removed_count} non-essential adapter(s).\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/repair_fortuna.bat": "@echo off\nREM Repair corrupted or missing files\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\necho Repairing Fortuna Faucet installation...\n\nREM /f flag performs repair. Assumes MSI is in the 'dist' folder.\nmsiexec.exe /f \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_repair.log\"\n\nif %errorlevel% equ 0 (\n    echo Repair completed successfully.\n) else (\n    echo Repair failed. Check log: %TEMP%\\fortuna_repair.log\n)\n\nexit /b %errorlevel%",
    "tests/__init__.py": "# This file makes the 'tests' directory a package.\n",
    "tests/adapters/test_greyhound_adapter.py": "from datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.greyhound_adapter import GreyhoundAdapter\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef test_settings():\n    \"\"\"Provides a valid Settings object for testing.\"\"\"\n    return get_test_settings()\n\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n\n    mock_api_response = {\n        \"cards\": [\n            {\n                \"track_name\": \"Test Track\",\n                \"races\": [\n                    {\n                        \"race_id\": \"test_race_123\",\n                        \"race_number\": 1,\n                        \"start_time\": int(datetime.now().timestamp()),\n                        \"runners\": [\n                            {\n                                \"dog_name\": \"Rapid Rover\",\n                                \"trap_number\": 1,\n                                \"odds\": {\"win\": \"2.5\"},\n                            },\n                            {\n                                \"dog_name\": \"Swift Sprint\",\n                                \"trap_number\": 2,\n                                \"scratched\": True,\n                            },\n                            {\n                                \"dog_name\": \"Lazy Larry\",\n                                \"trap_number\": 3,\n                                \"odds\": {\"win\": \"10.0\"},\n                            },\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == \"greyhound_test_race_123\"\n    assert race.venue == \"Test Track\"\n    assert len(race.runners) == 2  # One was scratched\n\n    runner1 = race.runners[0]\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.number == 1\n    assert runner1.odds[\"Greyhound Racing\"].win == 2.5\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(test_settings):\n    \"\"\"\n    Tests that the GreyhoundAdapter handles an empty API response gracefully.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value={\"cards\": []})\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_failure(test_settings):\n    \"\"\"\n    Tests that get_races returns an empty list when _fetch_data returns None.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value=None)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n",
    "tests/adapters/test_twinspires_adapter.py": "# tests/adapters/test_twinspires_adapter.py\nimport pytest\nfrom python_service.adapters.twinspires_adapter import TwinSpiresAdapter\nfrom python_service.models import Race\n\n# A mock settings object to satisfy the adapter's config dependency\nclass MockSettings:\n    pass\n\n@pytest.fixture\ndef adapter():\n    return TwinSpiresAdapter(config=MockSettings())\n\n@pytest.mark.asyncio\nasync def test_get_races_from_fixture(adapter):\n    \"\"\"\n    Test that the adapter can correctly parse a local HTML fixture.\n    This test validates the end-to-end parsing logic, including runner data,\n    using the offline implementation.\n    \"\"\"\n    # Call the method under test, which is now wired to read from the fixture\n    races = await adapter._get_races_async(date=\"2025-11-12\")\n\n    # Assertions\n    assert isinstance(races, list)\n    assert len(races) == 1\n\n    # Check the race for correct parsing\n    race = races[0]\n    assert race.venue == \"Churchill Downs\"\n    assert race.race_number == 5\n\n    # Check that runners were parsed correctly\n    assert len(race.runners) == 4\n\n    # Verify a specific runner's details\n    runner_1 = next((r for r in race.runners if r.number == 1), None)\n    assert runner_1 is not None\n    assert runner_1.name == \"Braveheart\"\n    assert not runner_1.scratched\n    assert runner_1.odds[\"TwinSpires\"].win == 3.5\n\n    # Verify a scratched runner\n    runner_3 = next((r for r in race.runners if r.number == 3), None)\n    assert runner_3 is not None\n    assert runner_3.name == \"Steady Eddy\"\n    assert runner_3.scratched\n    assert not runner_3.odds\n",
    "tests/fixtures/timeform_legacy_sample.html": "<!DOCTYPE html><html><body><div class='race-card'><div class='runner'><span class='runner-name'>Braveheart</span><span class='runner-odds'>5/2</span></div><div class='runner'><span class='runner-name'>Speedster</span><span class='runner-odds'>10/1</span></div><div class='runner'><span class='runner-name'>Steady Eddy</span><span class='runner-odds'>EVENS</span></div></div></body></html>",
    "tests/test_analyzer.py": "from datetime import datetime\nfrom decimal import Decimal\n\nimport pytest\n\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.analyzer import TrifectaAnalyzer\nfrom python_service.analyzer import _get_best_win_odds\nfrom python_service.models import OddsData\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\n# Helper to create runners for tests\ndef create_runner(number, odds_val=None, scratched=False):\n    odds_data = {}\n    if odds_val:\n        odds_data[\"TestOdds\"] = OddsData(win=Decimal(str(odds_val)), source=\"TestOdds\", last_updated=datetime.now())\n    return Runner(number=number, name=f\"Runner {number}\", odds=odds_data, scratched=scratched)\n\n\n@pytest.fixture\ndef sample_races_for_true_trifecta():\n    \"\"\"Provides a list of sample Race objects for the new 'True Trifecta' logic.\"\"\"\n    return [\n        # Race 1: Should PASS all criteria, will have a lower score\n        Race(\n            id=\"race_pass_1\",\n            venue=\"Test Park\",\n            race_number=1,\n            start_time=datetime.now(),\n            source=\"Test\",\n            runners=[\n                create_runner(1, 3.0),  # Favorite\n                create_runner(2, 4.5),  # Second Favorite\n                create_runner(3, 5.0),\n            ],\n        ),\n        # Race 2: Should FAIL (Field size too large)\n        Race(\n            id=\"race_fail_field_size\",\n            venue=\"Test Park\",\n            race_number=2,\n            start_time=datetime.now(),\n            source=\"Test\",\n            runners=[create_runner(i, 5.0 + i) for i in range(1, 12)],  # 11 runners\n        ),\n        # Race 3: Should FAIL (Favorite odds too low)\n        Race(\n            id=\"race_fail_fav_odds\",\n            venue=\"Test Park\",\n            race_number=3,\n            start_time=datetime.now(),\n            source=\"Test\",\n            runners=[create_runner(1, 2.0), create_runner(2, 4.5)],\n        ),\n        # Race 4: Should FAIL (Second favorite odds too low)\n        Race(\n            id=\"race_fail_2nd_fav_odds\",\n            venue=\"Test Park\",\n            race_number=4,\n            start_time=datetime.now(),\n            source=\"Test\",\n            runners=[create_runner(1, 3.0), create_runner(2, 3.5)],\n        ),\n        # Race 5: Should also PASS and have a higher score than race_pass_1\n        Race(\n            id=\"race_pass_2\",\n            venue=\"Test Park\",\n            race_number=5,\n            start_time=datetime.now(),\n            source=\"Test\",\n            runners=[\n                create_runner(1, 4.0),  # Favorite\n                create_runner(2, 6.0),  # Second Favorite\n                create_runner(3, 8.0),\n                create_runner(4, 12.0),\n                create_runner(5, 15.0),\n            ],\n        ),\n    ]\n\n\ndef test_analyzer_engine_discovery():\n    \"\"\"Tests that the AnalyzerEngine correctly discovers the TrifectaAnalyzer.\"\"\"\n    engine = AnalyzerEngine()\n    assert \"trifecta\" in engine.analyzers\n    assert engine.analyzers[\"trifecta\"] == TrifectaAnalyzer\n\n\ndef test_analyzer_engine_get_analyzer():\n    \"\"\"Tests that the AnalyzerEngine can instantiate a specific analyzer.\"\"\"\n    engine = AnalyzerEngine()\n    analyzer = engine.get_analyzer(\"trifecta\", max_field_size=8)\n    assert isinstance(analyzer, TrifectaAnalyzer)\n    assert analyzer.max_field_size == 8\n\n\ndef test_analyzer_engine_get_nonexistent_analyzer():\n    \"\"\"Tests that requesting a non-existent analyzer raises a ValueError.\"\"\"\n    engine = AnalyzerEngine()\n    with pytest.raises(ValueError, match=\"Analyzer 'nonexistent' not found.\"):\n        engine.get_analyzer(\"nonexistent\")\n\n\ndef test_trifecta_analyzer_plugin_logic(sample_races_for_true_trifecta):\n    \"\"\"\n    Tests the TrifectaAnalyzer's scoring, sorting, and new response structure.\n    \"\"\"\n    engine = AnalyzerEngine()\n    analyzer = engine.get_analyzer(\"trifecta\")  # Use default criteria\n\n    result = analyzer.qualify_races(sample_races_for_true_trifecta)\n\n    # 1. Verify the new response structure\n    assert isinstance(result, dict)\n    assert \"criteria\" in result\n    assert \"races\" in result\n    assert result[\"criteria\"][\"max_field_size\"] == 10\n\n    qualified_races = result[\"races\"]\n\n    # 2. Check that the correct number of races were qualified\n    assert len(qualified_races) == 2\n\n    # 3. Check that the scores have been assigned and are valid numbers\n    assert qualified_races[0].qualification_score is not None\n    assert qualified_races[1].qualification_score is not None\n    assert isinstance(qualified_races[0].qualification_score, float)\n\n    # 4. Check that the races are sorted by score in descending order\n    assert qualified_races[0].qualification_score > qualified_races[1].qualification_score\n    assert qualified_races[0].id == \"race_pass_2\"  # This race should have the higher score\n    assert qualified_races[1].id == \"race_pass_1\"\n\n\ndef test_get_best_win_odds_helper():\n    \"\"\"Tests the helper function for finding the best odds.\"\"\"\n    runner_with_odds = create_runner(1)\n    runner_with_odds.odds = {\n        \"SourceA\": OddsData(win=Decimal(\"3.0\"), source=\"A\", last_updated=datetime.now()),\n        \"SourceB\": OddsData(win=Decimal(\"2.5\"), source=\"B\", last_updated=datetime.now()),\n    }\n    assert _get_best_win_odds(runner_with_odds) == Decimal(\"2.5\")\n\n    runner_no_odds = create_runner(2)\n    assert _get_best_win_odds(runner_no_odds) is None\n\n    runner_no_win = create_runner(3)\n    runner_no_win.odds = {\"SourceA\": OddsData(win=None, source=\"A\", last_updated=datetime.now())}\n    assert _get_best_win_odds(runner_no_win) is None\n\n\n# Test case added by Operation: Resurrect and Modernize\n@pytest.fixture\ndef trifecta_analyzer():\n    \"\"\"Provides a default TrifectaAnalyzer instance for tests.\"\"\"\n    return TrifectaAnalyzer()\n\n\ndef test_trifecta_analyzer_rejects_races_with_too_few_runners(trifecta_analyzer):\n    \"\"\"Ensure analyzer rejects races with < 3 runners for a trifecta.\"\"\"\n    race_with_two_runners = Race(\n        id=\"test_race_123\",\n        venue=\"TEST\",\n        race_number=1,\n        start_time=datetime.now(),\n        runners=[create_runner(1, 2.0), create_runner(2, 3.0)],\n        source=\"test\",\n    )\n\n    qualified = trifecta_analyzer.is_race_qualified(race_with_two_runners)\n    assert not qualified, \"Trifecta analyzer should not qualify a race with only two runners.\"\n",
    "tests/test_api.py": "# tests/test_api.py\nfrom datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import patch\n\nimport aiosqlite\nimport pytest\n\n# --- Fixtures ---\nfrom python_service.models import AggregatedResponse\n\n# The client fixture is now correctly sourced from conftest.py,\n# which handles the settings override globally.\n\n# --- API Tests ---\n\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine.fetch_all_odds\", new_callable=AsyncMock)\nasync def test_get_races_endpoint_success(mock_fetch_all_odds, client):\n    \"\"\"\n    SPEC: The /api/races endpoint should return data with a valid API key.\n    \"\"\"\n    # ARRANGE\n    today = date.today()\n    mock_response = AggregatedResponse(\n        date=today,\n        races=[],\n        sources=[],\n        metadata={},\n        # This was the missing field causing the validation error\n        source_info=[],\n    )\n    mock_fetch_all_odds.return_value = mock_response.model_dump()\n    headers = {\"X-API-Key\": \"a_secure_test_api_key_that_is_long_enough\"}\n\n    # ACT\n    response = client.get(f\"/api/races?race_date={today.isoformat()}\", headers=headers)\n\n    # ASSERT\n    assert response.status_code == 200\n    mock_fetch_all_odds.assert_awaited_once()\n\n\n@pytest.mark.asyncio\nasync def test_get_tipsheet_endpoint_success(tmp_path, client):\n    \"\"\"\n    SPEC: The /api/tipsheet endpoint should return a list of tipsheet races from the database.\n    \"\"\"\n    db_path = tmp_path / \"test.db\"\n    post_time = datetime.now()\n\n    with patch(\"python_service.api.DB_PATH\", db_path):\n        async with aiosqlite.connect(db_path) as db:\n            await db.execute(\n                \"\"\"\n                CREATE TABLE tipsheet (\n                    race_id TEXT PRIMARY KEY,\n                    track_name TEXT,\n                    race_number INTEGER,\n                    post_time TEXT,\n                    score REAL,\n                    factors TEXT\n                )\n            \"\"\"\n            )\n            await db.execute(\n                \"INSERT INTO tipsheet VALUES (?, ?, ?, ?, ?, ?)\",\n                (\"test_race_1\", \"Test Park\", 1, post_time.isoformat(), 85.5, \"{}\"),\n            )\n            await db.commit()\n\n        # ACT\n        response = client.get(f\"/api/tipsheet?date={post_time.date().isoformat()}\")\n\n        # ASSERT\n        assert response.status_code == 200\n        response_data = response.json()\n        assert len(response_data) == 1\n        # The database returns snake_case, but the Pydantic model is camelCase\n        assert response_data[0][\"raceId\"] == \"test_race_1\"\n        assert response_data[0][\"score\"] == 85.5\n\n\ndef test_health_check_unauthenticated(client):\n    \"\"\"Ensures the /health endpoint is accessible without an API key.\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    json_response = response.json()\n    assert json_response[\"status\"] == \"ok\"\n    assert \"timestamp\" in json_response\n\n\ndef test_api_key_authentication_failure(client):\n    \"\"\"Ensures that endpoints are protected and fail with an invalid API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\", headers={\"X-API-KEY\": \"invalid_key\"})\n    assert response.status_code == 403\n    assert \"Invalid or missing API Key\" in response.json()[\"detail\"]\n\n\ndef test_api_key_authentication_missing(client):\n    \"\"\"Ensures that endpoints are protected and fail with a missing API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\")\n    assert response.status_code == 403\n    assert \"Not authenticated\" in response.json()[\"detail\"]\n",
    "tests/test_manual_override.py": "# tests/test_manual_override.py\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom python_service.api import app\nfrom python_service.api import get_settings\nfrom python_service.manual_override_manager import ManualOverrideManager\nfrom tests.conftest import get_test_settings\n\n# Override settings for tests\napp.dependency_overrides[get_settings] = get_test_settings\nAPI_KEY = get_test_settings().API_KEY\n\n\n@pytest.fixture\ndef manager() -> ManualOverrideManager:\n    \"\"\"Provides a clean ManualOverrideManager instance for each test.\"\"\"\n    return ManualOverrideManager()\n\n\ndef test_register_failure(manager: ManualOverrideManager):\n    adapter_name = \"TestAdapter\"\n    url = \"http://test.com/races\"\n    request_id = manager.register_failure(adapter_name, url)\n    assert request_id is not None\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter_name\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.submit_manual_data(request_id, \"<html></html>\", \"html\")\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is not None\n    assert data[0] == \"<html></html>\"\n    assert data[1] == \"html\"\n\n\ndef test_skip_request(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.skip_request(request_id)\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is None\n\n\ndef test_get_pending_overrides_endpoint(client):\n    # ARRANGE\n    # Access the manager *after* the TestClient has run the lifespan startup\n    manager = client.app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)  # Ensure a clean state by clearing all\n    manager.register_failure(\"EndpointAdapter\", \"http://endpoint.com/data\")\n\n    # ACT\n    response = client.get(\"/api/manual-overrides/pending\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    data = response.json()\n    assert \"pending_requests\" in data\n    assert len(data[\"pending_requests\"]) > 0\n    assert data[\"pending_requests\"][0][\"adapter_name\"] == \"EndpointAdapter\"\n\n\ndef test_submit_manual_data_endpoint(client):\n    # ARRANGE\n    manager = client.app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SubmitAdapter\", \"http://submit.com/data\")\n    submission = {\n        \"request_id\": request_id,\n        \"content\": \"<h1>Hello</h1>\",\n        \"content_type\": \"html\",\n    }\n    response = client.post(\n        \"/api/manual-overrides/submit\",\n        json=submission,\n        headers={\"X-API-Key\": API_KEY},\n    )\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    data = manager.get_manual_data(\"SubmitAdapter\", \"http://submit.com/data\")\n    assert data is not None\n    assert data[0] == \"<h1>Hello</h1>\"\n\n\ndef test_skip_manual_override_endpoint(client):\n    # ARRANGE\n    manager = client.app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SkipAdapter\", \"http://skip.com/data\")\n    response = client.post(f\"/api/manual-overrides/skip/{request_id}\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    # Verify the request is no longer pending\n    pending = manager.get_pending_requests()\n    assert not any(p.request_id == request_id for p in pending)\n",
    "web_platform/api_gateway/package.json": "{\n  \"name\": \"api_gateway\",\n  \"version\": \"1.0.0\",\n  \"main\": \"dist/server.js\",\n  \"scripts\": { \"start\": \"ts-node src/server.ts\" },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\",\n    \"dotenv\": \"^16.3.1\",\n    \"dotenv\": \"^16.3.1\",\n    \"sqlite\": \"^5.1.1\",\n    \"sqlite3\": \"^5.1.7\",\n    \"socket.io\": \"^4.7.4\",\n    \"cors\": \"^2.8.5\"\n  },\n  \"devDependencies\": {\n    \"@types/express\": \"^4.17.21\",\n    \"@types/node\": \"^20.10.0\",\n    \"@types/cors\": \"^2.8.17\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.3.3\"\n  }\n}",
    "web_platform/api_gateway/src/services/DatabaseService.ts": "import sqlite3 from 'sqlite3';\nimport { open, Database } from 'sqlite';\nimport path from 'path';\n\nexport class DatabaseService {\n  private db: Database | null = null;\n\n  private async getDb(): Promise<Database> {\n    if (!this.db) {\n      const dbPath = process.env.FORTUNA_DB_PATH || path.join(process.cwd(), '../../../../shared_database/races.db');\n      this.db = await open({\n        filename: dbPath,\n        driver: sqlite3.Database\n      });\n    }\n    return this.db;\n  }\n\n  async getQualifiedRaces(): Promise<any[]> {\n    const db = await this.getDb();\n    return db.all(`SELECT * FROM qualified_races`);\n  }\n}\n",
    "web_platform/api_gateway/tsconfig.json": "{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"commonjs\",\n    \"lib\": [\"ES2020\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}",
    "web_platform/frontend/.gitignore": "# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.\n\n# Dependencies\n/node_modules\n/.pnp\n.pnp.js\n\n# Testing\n/coverage\n\n# Next.js\n/.next/\n/out/\n\n# Production\n/build\n\n# Misc\n.DS_Store\n*.pem\n\n# Local .env files\n.env.local\n.env.development.local\n.env.test.local\n.env.production.local\n\n# Log files\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nlerna-debug.log*\n\n# Editor directories and files\n.vscode\n.idea\n*.suo\n*.ntvs*\n*.njsproj\n*.sln\n*.sw?",
    "web_platform/frontend/app/Providers.tsx": "// web_platform/frontend/app/Providers.tsx\n'use client';\n\nimport { QueryClientProvider } from '@tanstack/react-query';\nimport { queryClient } from '../src/lib/queryClient';\nimport React from 'react';\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>\n  );\n}\n",
    "web_platform/frontend/next-env.d.ts": "/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.\n",
    "web_platform/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"@tanstack/react-query\": \"^5.28.9\",\n    \"file-saver\": \"^2.0.5\",\n    \"lucide-react\": \"^0.548.0\",\n    \"next\": \"^14.2.33\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"socket.io-client\": \"^4.7.4\",\n    \"xlsx\": \"https://cdn.sheetjs.com/xlsx-0.20.3/xlsx-0.20.3.tgz\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"file-saver\": \"^2.0.5\",\n    \"next-pwa\": \"^5.6.0\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.3.0\",\n    \"typescript\": \"^5\"\n  }\n}\n",
    "web_platform/frontend/postcss.config.js": "module.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n};",
    "web_platform/frontend/public/sw.js": "if(!self.define){let e,s={};const a=(a,n)=>(a=new URL(a+\".js\",n).href,s[a]||new Promise(s=>{if(\"document\"in self){const e=document.createElement(\"script\");e.src=a,e.onload=s,document.head.appendChild(e)}else e=a,importScripts(a),s()}).then(()=>{let e=s[a];if(!e)throw new Error(`Module ${a} didn\u2019t register its module`);return e}));self.define=(n,t)=>{const i=e||(\"document\"in self?document.currentScript.src:\"\")||location.href;if(s[i])return;let c={};const r=e=>a(e,i),o={module:{uri:i},exports:c,require:r};s[i]=Promise.all(n.map(e=>o[e]||r(e))).then(e=>(t(...e),c))}}define([\"./workbox-4754cb34\"],function(e){\"use strict\";importScripts(),self.skipWaiting(),e.clientsClaim(),e.precacheAndRoute([{url:\"/_next/app-build-manifest.json\",revision:\"b6130f23369e5df052a4061c412f24fa\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_buildManifest.js\",revision:\"c155cce658e53418dec34664328b51ac\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_ssgManifest.js\",revision:\"b6652df95db52feb4daf4eca35380933\"},{url:\"/_next/static/chunks/117-6326cd814d964913.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/816-7254031126ac0a96.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/928.d7f641058b89a54a.js\",revision:\"d7f641058b89a54a\"},{url:\"/_next/static/chunks/app/_not-found/page-e7dc36cd5a340c38.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/layout-605479d07717f01e.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/page-a2c385e93bfc2dac.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/fd9d1056-af804af0be509bea.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/framework-f66176bb897dc684.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-8563e00d234bd632.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-app-e0b3e4e952d25145.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_app-72b849fbd24ac258.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_error-7ba65e1336b92748.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/polyfills-42372ed130431b0a.js\",revision:\"846118c33b2c0e922d7b3a7676f81f6f\"},{url:\"/_next/static/chunks/webpack-d92cdde7bb2319ca.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/css/a55e4893d0564dbf.css\",revision:\"a55e4893d0564dbf\"},{url:\"/_next/static/media/19cfc7226ec3afaa-s.woff2\",revision:\"9dda5cfc9a46f256d0e131bb535e46f8\"},{url:\"/_next/static/media/21350d82a1f187e9-s.woff2\",revision:\"4e2553027f1d60eff32898367dd4d541\"},{url:\"/_next/static/media/8e9860b6e62d6359-s.woff2\",revision:\"01ba6c2a184b8cba08b0d57167664d75\"},{url:\"/_next/static/media/ba9851c3c22cd980-s.woff2\",revision:\"9e494903d6b0ffec1a1e14d34427d44d\"},{url:\"/_next/static/media/c5fe6dc8356a8c31-s.woff2\",revision:\"027a89e9ab733a145db70f09b8a18b42\"},{url:\"/_next/static/media/df0a9ae256c0569c-s.woff2\",revision:\"d54db44de5ccb18886ece2fda72bdfe0\"},{url:\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",revision:\"65850a373e258f1c897a2b3d75eb74de\"},{url:\"/manifest.json\",revision:\"23bffdb04aba9b85948642cffa772eae\"}],{ignoreURLParametersMatching:[]}),e.cleanupOutdatedCaches(),e.registerRoute(\"/\",new e.NetworkFirst({cacheName:\"start-url\",plugins:[{cacheWillUpdate:async({request:e,response:s,event:a,state:n})=>s&&\"opaqueredirect\"===s.type?new Response(s.body,{status:200,statusText:\"OK\",headers:s.headers}):s}]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:gstatic)\\.com\\/.*/i,new e.CacheFirst({cacheName:\"google-fonts-webfonts\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:31536e3})]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:googleapis)\\.com\\/.*/i,new e.StaleWhileRevalidate({cacheName:\"google-fonts-stylesheets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:eot|otf|ttc|ttf|woff|woff2|font.css)$/i,new e.StaleWhileRevalidate({cacheName:\"static-font-assets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:jpg|jpeg|gif|png|svg|ico|webp)$/i,new e.StaleWhileRevalidate({cacheName:\"static-image-assets\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/image\\?url=.+$/i,new e.StaleWhileRevalidate({cacheName:\"next-image\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp3|wav|ogg)$/i,new e.CacheFirst({cacheName:\"static-audio-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp4)$/i,new e.CacheFirst({cacheName:\"static-video-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:js)$/i,new e.StaleWhileRevalidate({cacheName:\"static-js-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:css|less)$/i,new e.StaleWhileRevalidate({cacheName:\"static-style-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/data\\/.+\\/.+\\.json$/i,new e.StaleWhileRevalidate({cacheName:\"next-data\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:json|xml|csv)$/i,new e.NetworkFirst({cacheName:\"static-data-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;const s=e.pathname;return!s.startsWith(\"/api/auth/\")&&!!s.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"apis\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:16,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;return!e.pathname.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"others\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>!(self.origin===e.origin),new e.NetworkFirst({cacheName:\"cross-origin\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:3600})]}),\"GET\")});\n",
    "web_platform/frontend/public/workbox-4754cb34.js": "define([\"exports\"],function(t){\"use strict\";try{self[\"workbox:core:6.5.4\"]&&_()}catch(t){}const e=(t,...e)=>{let s=t;return e.length>0&&(s+=` :: ${JSON.stringify(e)}`),s};class s extends Error{constructor(t,s){super(e(t,s)),this.name=t,this.details=s}}try{self[\"workbox:routing:6.5.4\"]&&_()}catch(t){}const n=t=>t&&\"object\"==typeof t?t:{handle:t};class r{constructor(t,e,s=\"GET\"){this.handler=n(e),this.match=t,this.method=s}setCatchHandler(t){this.catchHandler=n(t)}}class i extends r{constructor(t,e,s){super(({url:e})=>{const s=t.exec(e.href);if(s&&(e.origin===location.origin||0===s.index))return s.slice(1)},e,s)}}class a{constructor(){this.t=new Map,this.i=new Map}get routes(){return this.t}addFetchListener(){self.addEventListener(\"fetch\",t=>{const{request:e}=t,s=this.handleRequest({request:e,event:t});s&&t.respondWith(s)})}addCacheListener(){self.addEventListener(\"message\",t=>{if(t.data&&\"CACHE_URLS\"===t.data.type){const{payload:e}=t.data,s=Promise.all(e.urlsToCache.map(e=>{\"string\"==typeof e&&(e=[e]);const s=new Request(...e);return this.handleRequest({request:s,event:t})}));t.waitUntil(s),t.ports&&t.ports[0]&&s.then(()=>t.ports[0].postMessage(!0))}})}handleRequest({request:t,event:e}){const s=new URL(t.url,location.href);if(!s.protocol.startsWith(\"http\"))return;const n=s.origin===location.origin,{params:r,route:i}=this.findMatchingRoute({event:e,request:t,sameOrigin:n,url:s});let a=i&&i.handler;const o=t.method;if(!a&&this.i.has(o)&&(a=this.i.get(o)),!a)return;let c;try{c=a.handle({url:s,request:t,event:e,params:r})}catch(t){c=Promise.reject(t)}const h=i&&i.catchHandler;return c instanceof Promise&&(this.o||h)&&(c=c.catch(async n=>{if(h)try{return await h.handle({url:s,request:t,event:e,params:r})}catch(t){t instanceof Error&&(n=t)}if(this.o)return this.o.handle({url:s,request:t,event:e});throw n})),c}findMatchingRoute({url:t,sameOrigin:e,request:s,event:n}){const r=this.t.get(s.method)||[];for(const i of r){let r;const a=i.match({url:t,sameOrigin:e,request:s,event:n});if(a)return r=a,(Array.isArray(r)&&0===r.length||a.constructor===Object&&0===Object.keys(a).length||\"boolean\"==typeof a)&&(r=void 0),{route:i,params:r}}return{}}setDefaultHandler(t,e=\"GET\"){this.i.set(e,n(t))}setCatchHandler(t){this.o=n(t)}registerRoute(t){this.t.has(t.method)||this.t.set(t.method,[]),this.t.get(t.method).push(t)}unregisterRoute(t){if(!this.t.has(t.method))throw new s(\"unregister-route-but-not-found-with-method\",{method:t.method});const e=this.t.get(t.method).indexOf(t);if(!(e>-1))throw new s(\"unregister-route-route-not-registered\");this.t.get(t.method).splice(e,1)}}let o;const c=()=>(o||(o=new a,o.addFetchListener(),o.addCacheListener()),o);function h(t,e,n){let a;if(\"string\"==typeof t){const s=new URL(t,location.href);a=new r(({url:t})=>t.href===s.href,e,n)}else if(t instanceof RegExp)a=new i(t,e,n);else if(\"function\"==typeof t)a=new r(t,e,n);else{if(!(t instanceof r))throw new s(\"unsupported-route-type\",{moduleName:\"workbox-routing\",funcName:\"registerRoute\",paramName:\"capture\"});a=t}return c().registerRoute(a),a}try{self[\"workbox:strategies:6.5.4\"]&&_()}catch(t){}const u={cacheWillUpdate:async({response:t})=>200===t.status||0===t.status?t:null},l={googleAnalytics:\"googleAnalytics\",precache:\"precache-v2\",prefix:\"workbox\",runtime:\"runtime\",suffix:\"undefined\"!=typeof registration?registration.scope:\"\"},f=t=>[l.prefix,t,l.suffix].filter(t=>t&&t.length>0).join(\"-\"),w=t=>t||f(l.precache),d=t=>t||f(l.runtime);function p(t,e){const s=new URL(t);for(const t of e)s.searchParams.delete(t);return s.href}class y{constructor(){this.promise=new Promise((t,e)=>{this.resolve=t,this.reject=e})}}const g=new Set;function m(t){return\"string\"==typeof t?new Request(t):t}class v{constructor(t,e){this.h={},Object.assign(this,e),this.event=e.event,this.u=t,this.l=new y,this.p=[],this.m=[...t.plugins],this.v=new Map;for(const t of this.m)this.v.set(t,{});this.event.waitUntil(this.l.promise)}async fetch(t){const{event:e}=this;let n=m(t);if(\"navigate\"===n.mode&&e instanceof FetchEvent&&e.preloadResponse){const t=await e.preloadResponse;if(t)return t}const r=this.hasCallback(\"fetchDidFail\")?n.clone():null;try{for(const t of this.iterateCallbacks(\"requestWillFetch\"))n=await t({request:n.clone(),event:e})}catch(t){if(t instanceof Error)throw new s(\"plugin-error-request-will-fetch\",{thrownErrorMessage:t.message})}const i=n.clone();try{let t;t=await fetch(n,\"navigate\"===n.mode?void 0:this.u.fetchOptions);for(const s of this.iterateCallbacks(\"fetchDidSucceed\"))t=await s({event:e,request:i,response:t});return t}catch(t){throw r&&await this.runCallbacks(\"fetchDidFail\",{error:t,event:e,originalRequest:r.clone(),request:i.clone()}),t}}async fetchAndCachePut(t){const e=await this.fetch(t),s=e.clone();return this.waitUntil(this.cachePut(t,s)),e}async cacheMatch(t){const e=m(t);let s;const{cacheName:n,matchOptions:r}=this.u,i=await this.getCacheKey(e,\"read\"),a=Object.assign(Object.assign({},r),{cacheName:n});s=await caches.match(i,a);for(const t of this.iterateCallbacks(\"cachedResponseWillBeUsed\"))s=await t({cacheName:n,matchOptions:r,cachedResponse:s,request:i,event:this.event})||void 0;return s}async cachePut(t,e){const n=m(t);var r;await(r=0,new Promise(t=>setTimeout(t,r)));const i=await this.getCacheKey(n,\"write\");if(!e)throw new s(\"cache-put-with-no-response\",{url:(a=i.url,new URL(String(a),location.href).href.replace(new RegExp(`^${location.origin}`),\"\"))});var a;const o=await this.R(e);if(!o)return!1;const{cacheName:c,matchOptions:h}=this.u,u=await self.caches.open(c),l=this.hasCallback(\"cacheDidUpdate\"),f=l?await async function(t,e,s,n){const r=p(e.url,s);if(e.url===r)return t.match(e,n);const i=Object.assign(Object.assign({},n),{ignoreSearch:!0}),a=await t.keys(e,i);for(const e of a)if(r===p(e.url,s))return t.match(e,n)}(u,i.clone(),[\"__WB_REVISION__\"],h):null;try{await u.put(i,l?o.clone():o)}catch(t){if(t instanceof Error)throw\"QuotaExceededError\"===t.name&&await async function(){for(const t of g)await t()}(),t}for(const t of this.iterateCallbacks(\"cacheDidUpdate\"))await t({cacheName:c,oldResponse:f,newResponse:o.clone(),request:i,event:this.event});return!0}async getCacheKey(t,e){const s=`${t.url} | ${e}`;if(!this.h[s]){let n=t;for(const t of this.iterateCallbacks(\"cacheKeyWillBeUsed\"))n=m(await t({mode:e,request:n,event:this.event,params:this.params}));this.h[s]=n}return this.h[s]}hasCallback(t){for(const e of this.u.plugins)if(t in e)return!0;return!1}async runCallbacks(t,e){for(const s of this.iterateCallbacks(t))await s(e)}*iterateCallbacks(t){for(const e of this.u.plugins)if(\"function\"==typeof e[t]){const s=this.v.get(e),n=n=>{const r=Object.assign(Object.assign({},n),{state:s});return e[t](r)};yield n}}waitUntil(t){return this.p.push(t),t}async doneWaiting(){let t;for(;t=this.p.shift();)await t}destroy(){this.l.resolve(null)}async R(t){let e=t,s=!1;for(const t of this.iterateCallbacks(\"cacheWillUpdate\"))if(e=await t({request:this.request,response:e,event:this.event})||void 0,s=!0,!e)break;return s||e&&200!==e.status&&(e=void 0),e}}class R{constructor(t={}){this.cacheName=d(t.cacheName),this.plugins=t.plugins||[],this.fetchOptions=t.fetchOptions,this.matchOptions=t.matchOptions}handle(t){const[e]=this.handleAll(t);return e}handleAll(t){t instanceof FetchEvent&&(t={event:t,request:t.request});const e=t.event,s=\"string\"==typeof t.request?new Request(t.request):t.request,n=\"params\"in t?t.params:void 0,r=new v(this,{event:e,request:s,params:n}),i=this.q(r,s,e);return[i,this.D(i,r,s,e)]}async q(t,e,n){let r;await t.runCallbacks(\"handlerWillStart\",{event:n,request:e});try{if(r=await this.U(e,t),!r||\"error\"===r.type)throw new s(\"no-response\",{url:e.url})}catch(s){if(s instanceof Error)for(const i of t.iterateCallbacks(\"handlerDidError\"))if(r=await i({error:s,event:n,request:e}),r)break;if(!r)throw s}for(const s of t.iterateCallbacks(\"handlerWillRespond\"))r=await s({event:n,request:e,response:r});return r}async D(t,e,s,n){let r,i;try{r=await t}catch(i){}try{await e.runCallbacks(\"handlerDidRespond\",{event:n,request:s,response:r}),await e.doneWaiting()}catch(t){t instanceof Error&&(i=t)}if(await e.runCallbacks(\"handlerDidComplete\",{event:n,request:s,response:r,error:i}),e.destroy(),i)throw i}}function b(t){t.then(()=>{})}function q(){return q=Object.assign?Object.assign.bind():function(t){for(var e=1;e<arguments.length;e++){var s=arguments[e];for(var n in s)({}).hasOwnProperty.call(s,n)&&(t[n]=s[n])}return t},q.apply(null,arguments)}let D,U;const x=new WeakMap,L=new WeakMap,I=new WeakMap,C=new WeakMap,E=new WeakMap;let N={get(t,e,s){if(t instanceof IDBTransaction){if(\"done\"===e)return L.get(t);if(\"objectStoreNames\"===e)return t.objectStoreNames||I.get(t);if(\"store\"===e)return s.objectStoreNames[1]?void 0:s.objectStore(s.objectStoreNames[0])}return k(t[e])},set:(t,e,s)=>(t[e]=s,!0),has:(t,e)=>t instanceof IDBTransaction&&(\"done\"===e||\"store\"===e)||e in t};function O(t){return t!==IDBDatabase.prototype.transaction||\"objectStoreNames\"in IDBTransaction.prototype?(U||(U=[IDBCursor.prototype.advance,IDBCursor.prototype.continue,IDBCursor.prototype.continuePrimaryKey])).includes(t)?function(...e){return t.apply(B(this),e),k(x.get(this))}:function(...e){return k(t.apply(B(this),e))}:function(e,...s){const n=t.call(B(this),e,...s);return I.set(n,e.sort?e.sort():[e]),k(n)}}function T(t){return\"function\"==typeof t?O(t):(t instanceof IDBTransaction&&function(t){if(L.has(t))return;const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"complete\",r),t.removeEventListener(\"error\",i),t.removeEventListener(\"abort\",i)},r=()=>{e(),n()},i=()=>{s(t.error||new DOMException(\"AbortError\",\"AbortError\")),n()};t.addEventListener(\"complete\",r),t.addEventListener(\"error\",i),t.addEventListener(\"abort\",i)});L.set(t,e)}(t),e=t,(D||(D=[IDBDatabase,IDBObjectStore,IDBIndex,IDBCursor,IDBTransaction])).some(t=>e instanceof t)?new Proxy(t,N):t);var e}function k(t){if(t instanceof IDBRequest)return function(t){const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"success\",r),t.removeEventListener(\"error\",i)},r=()=>{e(k(t.result)),n()},i=()=>{s(t.error),n()};t.addEventListener(\"success\",r),t.addEventListener(\"error\",i)});return e.then(e=>{e instanceof IDBCursor&&x.set(e,t)}).catch(()=>{}),E.set(e,t),e}(t);if(C.has(t))return C.get(t);const e=T(t);return e!==t&&(C.set(t,e),E.set(e,t)),e}const B=t=>E.get(t);const P=[\"get\",\"getKey\",\"getAll\",\"getAllKeys\",\"count\"],M=[\"put\",\"add\",\"delete\",\"clear\"],W=new Map;function j(t,e){if(!(t instanceof IDBDatabase)||e in t||\"string\"!=typeof e)return;if(W.get(e))return W.get(e);const s=e.replace(/FromIndex$/,\"\"),n=e!==s,r=M.includes(s);if(!(s in(n?IDBIndex:IDBObjectStore).prototype)||!r&&!P.includes(s))return;const i=async function(t,...e){const i=this.transaction(t,r?\"readwrite\":\"readonly\");let a=i.store;return n&&(a=a.index(e.shift())),(await Promise.all([a[s](...e),r&&i.done]))[0]};return W.set(e,i),i}N=(t=>q({},t,{get:(e,s,n)=>j(e,s)||t.get(e,s,n),has:(e,s)=>!!j(e,s)||t.has(e,s)}))(N);try{self[\"workbox:expiration:6.5.4\"]&&_()}catch(t){}const S=\"cache-entries\",K=t=>{const e=new URL(t,location.href);return e.hash=\"\",e.href};class A{constructor(t){this._=null,this.L=t}I(t){const e=t.createObjectStore(S,{keyPath:\"id\"});e.createIndex(\"cacheName\",\"cacheName\",{unique:!1}),e.createIndex(\"timestamp\",\"timestamp\",{unique:!1})}C(t){this.I(t),this.L&&function(t,{blocked:e}={}){const s=indexedDB.deleteDatabase(t);e&&s.addEventListener(\"blocked\",t=>e(t.oldVersion,t)),k(s).then(()=>{})}(this.L)}async setTimestamp(t,e){const s={url:t=K(t),timestamp:e,cacheName:this.L,id:this.N(t)},n=(await this.getDb()).transaction(S,\"readwrite\",{durability:\"relaxed\"});await n.store.put(s),await n.done}async getTimestamp(t){const e=await this.getDb(),s=await e.get(S,this.N(t));return null==s?void 0:s.timestamp}async expireEntries(t,e){const s=await this.getDb();let n=await s.transaction(S).store.index(\"timestamp\").openCursor(null,\"prev\");const r=[];let i=0;for(;n;){const s=n.value;s.cacheName===this.L&&(t&&s.timestamp<t||e&&i>=e?r.push(n.value):i++),n=await n.continue()}const a=[];for(const t of r)await s.delete(S,t.id),a.push(t.url);return a}N(t){return this.L+\"|\"+K(t)}async getDb(){return this._||(this._=await function(t,e,{blocked:s,upgrade:n,blocking:r,terminated:i}={}){const a=indexedDB.open(t,e),o=k(a);return n&&a.addEventListener(\"upgradeneeded\",t=>{n(k(a.result),t.oldVersion,t.newVersion,k(a.transaction),t)}),s&&a.addEventListener(\"blocked\",t=>s(t.oldVersion,t.newVersion,t)),o.then(t=>{i&&t.addEventListener(\"close\",()=>i()),r&&t.addEventListener(\"versionchange\",t=>r(t.oldVersion,t.newVersion,t))}).catch(()=>{}),o}(\"workbox-expiration\",1,{upgrade:this.C.bind(this)})),this._}}class F{constructor(t,e={}){this.O=!1,this.T=!1,this.k=e.maxEntries,this.B=e.maxAgeSeconds,this.P=e.matchOptions,this.L=t,this.M=new A(t)}async expireEntries(){if(this.O)return void(this.T=!0);this.O=!0;const t=this.B?Date.now()-1e3*this.B:0,e=await this.M.expireEntries(t,this.k),s=await self.caches.open(this.L);for(const t of e)await s.delete(t,this.P);this.O=!1,this.T&&(this.T=!1,b(this.expireEntries()))}async updateTimestamp(t){await this.M.setTimestamp(t,Date.now())}async isURLExpired(t){if(this.B){const e=await this.M.getTimestamp(t),s=Date.now()-1e3*this.B;return void 0===e||e<s}return!1}async delete(){this.T=!1,await this.M.expireEntries(1/0)}}try{self[\"workbox:range-requests:6.5.4\"]&&_()}catch(t){}async function H(t,e){try{if(206===e.status)return e;const n=t.headers.get(\"range\");if(!n)throw new s(\"no-range-header\");const r=function(t){const e=t.trim().toLowerCase();if(!e.startsWith(\"bytes=\"))throw new s(\"unit-must-be-bytes\",{normalizedRangeHeader:e});if(e.includes(\",\"))throw new s(\"single-range-only\",{normalizedRangeHeader:e});const n=/(\\d*)-(\\d*)/.exec(e);if(!n||!n[1]&&!n[2])throw new s(\"invalid-range-values\",{normalizedRangeHeader:e});return{start:\"\"===n[1]?void 0:Number(n[1]),end:\"\"===n[2]?void 0:Number(n[2])}}(n),i=await e.blob(),a=function(t,e,n){const r=t.size;if(n&&n>r||e&&e<0)throw new s(\"range-not-satisfiable\",{size:r,end:n,start:e});let i,a;return void 0!==e&&void 0!==n?(i=e,a=n+1):void 0!==e&&void 0===n?(i=e,a=r):void 0!==n&&void 0===e&&(i=r-n,a=r),{start:i,end:a}}(i,r.start,r.end),o=i.slice(a.start,a.end),c=o.size,h=new Response(o,{status:206,statusText:\"Partial Content\",headers:e.headers});return h.headers.set(\"Content-Length\",String(c)),h.headers.set(\"Content-Range\",`bytes ${a.start}-${a.end-1}/${i.size}`),h}catch(t){return new Response(\"\",{status:416,statusText:\"Range Not Satisfiable\"})}}function $(t,e){const s=e();return t.waitUntil(s),s}try{self[\"workbox:precaching:6.5.4\"]&&_()}catch(t){}function z(t){if(!t)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(\"string\"==typeof t){const e=new URL(t,location.href);return{cacheKey:e.href,url:e.href}}const{revision:e,url:n}=t;if(!n)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(!e){const t=new URL(n,location.href);return{cacheKey:t.href,url:t.href}}const r=new URL(n,location.href),i=new URL(n,location.href);return r.searchParams.set(\"__WB_REVISION__\",e),{cacheKey:r.href,url:i.href}}class G{constructor(){this.updatedURLs=[],this.notUpdatedURLs=[],this.handlerWillStart=async({request:t,state:e})=>{e&&(e.originalRequest=t)},this.cachedResponseWillBeUsed=async({event:t,state:e,cachedResponse:s})=>{if(\"install\"===t.type&&e&&e.originalRequest&&e.originalRequest instanceof Request){const t=e.originalRequest.url;s?this.notUpdatedURLs.push(t):this.updatedURLs.push(t)}return s}}}class V{constructor({precacheController:t}){this.cacheKeyWillBeUsed=async({request:t,params:e})=>{const s=(null==e?void 0:e.cacheKey)||this.W.getCacheKeyForURL(t.url);return s?new Request(s,{headers:t.headers}):t},this.W=t}}let J,Q;async function X(t,e){let n=null;if(t.url){n=new URL(t.url).origin}if(n!==self.location.origin)throw new s(\"cross-origin-copy-response\",{origin:n});const r=t.clone(),i={headers:new Headers(r.headers),status:r.status,statusText:r.statusText},a=e?e(i):i,o=function(){if(void 0===J){const t=new Response(\"\");if(\"body\"in t)try{new Response(t.body),J=!0}catch(t){J=!1}J=!1}return J}()?r.body:await r.blob();return new Response(o,a)}class Y extends R{constructor(t={}){t.cacheName=w(t.cacheName),super(t),this.j=!1!==t.fallbackToNetwork,this.plugins.push(Y.copyRedirectedCacheableResponsesPlugin)}async U(t,e){const s=await e.cacheMatch(t);return s||(e.event&&\"install\"===e.event.type?await this.S(t,e):await this.K(t,e))}async K(t,e){let n;const r=e.params||{};if(!this.j)throw new s(\"missing-precache-entry\",{cacheName:this.cacheName,url:t.url});{const s=r.integrity,i=t.integrity,a=!i||i===s;n=await e.fetch(new Request(t,{integrity:\"no-cors\"!==t.mode?i||s:void 0})),s&&a&&\"no-cors\"!==t.mode&&(this.A(),await e.cachePut(t,n.clone()))}return n}async S(t,e){this.A();const n=await e.fetch(t);if(!await e.cachePut(t,n.clone()))throw new s(\"bad-precaching-response\",{url:t.url,status:n.status});return n}A(){let t=null,e=0;for(const[s,n]of this.plugins.entries())n!==Y.copyRedirectedCacheableResponsesPlugin&&(n===Y.defaultPrecacheCacheabilityPlugin&&(t=s),n.cacheWillUpdate&&e++);0===e?this.plugins.push(Y.defaultPrecacheCacheabilityPlugin):e>1&&null!==t&&this.plugins.splice(t,1)}}Y.defaultPrecacheCacheabilityPlugin={cacheWillUpdate:async({response:t})=>!t||t.status>=400?null:t},Y.copyRedirectedCacheableResponsesPlugin={cacheWillUpdate:async({response:t})=>t.redirected?await X(t):t};class Z{constructor({cacheName:t,plugins:e=[],fallbackToNetwork:s=!0}={}){this.F=new Map,this.H=new Map,this.$=new Map,this.u=new Y({cacheName:w(t),plugins:[...e,new V({precacheController:this})],fallbackToNetwork:s}),this.install=this.install.bind(this),this.activate=this.activate.bind(this)}get strategy(){return this.u}precache(t){this.addToCacheList(t),this.G||(self.addEventListener(\"install\",this.install),self.addEventListener(\"activate\",this.activate),this.G=!0)}addToCacheList(t){const e=[];for(const n of t){\"string\"==typeof n?e.push(n):n&&void 0===n.revision&&e.push(n.url);const{cacheKey:t,url:r}=z(n),i=\"string\"!=typeof n&&n.revision?\"reload\":\"default\";if(this.F.has(r)&&this.F.get(r)!==t)throw new s(\"add-to-cache-list-conflicting-entries\",{firstEntry:this.F.get(r),secondEntry:t});if(\"string\"!=typeof n&&n.integrity){if(this.$.has(t)&&this.$.get(t)!==n.integrity)throw new s(\"add-to-cache-list-conflicting-integrities\",{url:r});this.$.set(t,n.integrity)}if(this.F.set(r,t),this.H.set(r,i),e.length>0){const t=`Workbox is precaching URLs without revision info: ${e.join(\", \")}\\nThis is generally NOT safe. Learn more at https://bit.ly/wb-precache`;console.warn(t)}}}install(t){return $(t,async()=>{const e=new G;this.strategy.plugins.push(e);for(const[e,s]of this.F){const n=this.$.get(s),r=this.H.get(e),i=new Request(e,{integrity:n,cache:r,credentials:\"same-origin\"});await Promise.all(this.strategy.handleAll({params:{cacheKey:s},request:i,event:t}))}const{updatedURLs:s,notUpdatedURLs:n}=e;return{updatedURLs:s,notUpdatedURLs:n}})}activate(t){return $(t,async()=>{const t=await self.caches.open(this.strategy.cacheName),e=await t.keys(),s=new Set(this.F.values()),n=[];for(const r of e)s.has(r.url)||(await t.delete(r),n.push(r.url));return{deletedURLs:n}})}getURLsToCacheKeys(){return this.F}getCachedURLs(){return[...this.F.keys()]}getCacheKeyForURL(t){const e=new URL(t,location.href);return this.F.get(e.href)}getIntegrityForCacheKey(t){return this.$.get(t)}async matchPrecache(t){const e=t instanceof Request?t.url:t,s=this.getCacheKeyForURL(e);if(s){return(await self.caches.open(this.strategy.cacheName)).match(s)}}createHandlerBoundToURL(t){const e=this.getCacheKeyForURL(t);if(!e)throw new s(\"non-precached-url\",{url:t});return s=>(s.request=new Request(t),s.params=Object.assign({cacheKey:e},s.params),this.strategy.handle(s))}}const tt=()=>(Q||(Q=new Z),Q);class et extends r{constructor(t,e){super(({request:s})=>{const n=t.getURLsToCacheKeys();for(const r of function*(t,{ignoreURLParametersMatching:e=[/^utm_/,/^fbclid$/],directoryIndex:s=\"index.html\",cleanURLs:n=!0,urlManipulation:r}={}){const i=new URL(t,location.href);i.hash=\"\",yield i.href;const a=function(t,e=[]){for(const s of[...t.searchParams.keys()])e.some(t=>t.test(s))&&t.searchParams.delete(s);return t}(i,e);if(yield a.href,s&&a.pathname.endsWith(\"/\")){const t=new URL(a.href);t.pathname+=s,yield t.href}if(n){const t=new URL(a.href);t.pathname+=\".html\",yield t.href}if(r){const t=r({url:i});for(const e of t)yield e.href}}(s.url,e)){const e=n.get(r);if(e){return{cacheKey:e,integrity:t.getIntegrityForCacheKey(e)}}}},t.strategy)}}t.CacheFirst=class extends R{async U(t,e){let n,r=await e.cacheMatch(t);if(!r)try{r=await e.fetchAndCachePut(t)}catch(t){t instanceof Error&&(n=t)}if(!r)throw new s(\"no-response\",{url:t.url,error:n});return r}},t.ExpirationPlugin=class{constructor(t={}){this.cachedResponseWillBeUsed=async({event:t,request:e,cacheName:s,cachedResponse:n})=>{if(!n)return null;const r=this.V(n),i=this.J(s);b(i.expireEntries());const a=i.updateTimestamp(e.url);if(t)try{t.waitUntil(a)}catch(t){}return r?n:null},this.cacheDidUpdate=async({cacheName:t,request:e})=>{const s=this.J(t);await s.updateTimestamp(e.url),await s.expireEntries()},this.X=t,this.B=t.maxAgeSeconds,this.Y=new Map,t.purgeOnQuotaError&&function(t){g.add(t)}(()=>this.deleteCacheAndMetadata())}J(t){if(t===d())throw new s(\"expire-custom-caches-only\");let e=this.Y.get(t);return e||(e=new F(t,this.X),this.Y.set(t,e)),e}V(t){if(!this.B)return!0;const e=this.Z(t);if(null===e)return!0;return e>=Date.now()-1e3*this.B}Z(t){if(!t.headers.has(\"date\"))return null;const e=t.headers.get(\"date\"),s=new Date(e).getTime();return isNaN(s)?null:s}async deleteCacheAndMetadata(){for(const[t,e]of this.Y)await self.caches.delete(t),await e.delete();this.Y=new Map}},t.NetworkFirst=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u),this.tt=t.networkTimeoutSeconds||0}async U(t,e){const n=[],r=[];let i;if(this.tt){const{id:s,promise:a}=this.et({request:t,logs:n,handler:e});i=s,r.push(a)}const a=this.st({timeoutId:i,request:t,logs:n,handler:e});r.push(a);const o=await e.waitUntil((async()=>await e.waitUntil(Promise.race(r))||await a)());if(!o)throw new s(\"no-response\",{url:t.url});return o}et({request:t,logs:e,handler:s}){let n;return{promise:new Promise(e=>{n=setTimeout(async()=>{e(await s.cacheMatch(t))},1e3*this.tt)}),id:n}}async st({timeoutId:t,request:e,logs:s,handler:n}){let r,i;try{i=await n.fetchAndCachePut(e)}catch(t){t instanceof Error&&(r=t)}return t&&clearTimeout(t),!r&&i||(i=await n.cacheMatch(e)),i}},t.RangeRequestsPlugin=class{constructor(){this.cachedResponseWillBeUsed=async({request:t,cachedResponse:e})=>e&&t.headers.has(\"range\")?await H(t,e):e}},t.StaleWhileRevalidate=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u)}async U(t,e){const n=e.fetchAndCachePut(t).catch(()=>{});e.waitUntil(n);let r,i=await e.cacheMatch(t);if(i);else try{i=await n}catch(t){t instanceof Error&&(r=t)}if(!i)throw new s(\"no-response\",{url:t.url,error:r});return i}},t.cleanupOutdatedCaches=function(){self.addEventListener(\"activate\",t=>{const e=w();t.waitUntil((async(t,e=\"-precache-\")=>{const s=(await self.caches.keys()).filter(s=>s.includes(e)&&s.includes(self.registration.scope)&&s!==t);return await Promise.all(s.map(t=>self.caches.delete(t))),s})(e).then(t=>{}))})},t.clientsClaim=function(){self.addEventListener(\"activate\",()=>self.clients.claim())},t.precacheAndRoute=function(t,e){!function(t){tt().precache(t)}(t),function(t){const e=tt();h(new et(e,t))}(e)},t.registerRoute=h});\n",
    "web_platform/frontend/src/components/AdapterStatusPanel.tsx": "// web_platform/frontend/src/components/AdapterStatusPanel.tsx\n'use client';\n\nimport React from 'react';\nimport { SourceInfo } from '../types/racing';\n\ninterface AdapterStatusPanelProps {\n  adapter: SourceInfo;\n  onFetchRaces: (sourceName: string) => void;\n}\n\nexport const AdapterStatusPanel: React.FC<AdapterStatusPanelProps> = ({ adapter, onFetchRaces }) => {\n  const isConfigured = adapter.status !== 'CONFIG_ERROR';\n\n  return (\n    <div className={`p-4 rounded-lg border ${isConfigured ? 'bg-slate-800 border-slate-700' : 'bg-yellow-900/20 border-yellow-700/50'}`}>\n      <div className=\"flex justify-between items-center\">\n        <h3 className=\"font-bold text-lg text-white\">{adapter.name}</h3>\n        <span className={`px-2 py-0.5 rounded-full text-xs font-medium ${isConfigured ? 'bg-green-500/20 text-green-300' : 'bg-yellow-500/20 text-yellow-300'}`}>\n          {isConfigured ? 'Ready' : 'Not Configured'}\n        </span>\n      </div>\n      <div className=\"mt-4 flex gap-2\">\n        <button\n          onClick={() => onFetchRaces(adapter.name)}\n          disabled={!isConfigured}\n          className=\"flex-1 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 disabled:bg-slate-700 disabled:text-slate-400 disabled:cursor-not-allowed\"\n        >\n          Automatic Load\n        </button>\n        <button\n          disabled\n          className=\"flex-1 px-4 py-2 bg-slate-700 text-slate-400 rounded cursor-not-allowed\"\n        >\n          Manual Entry (Coming Soon)\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx": "// web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx\nimport dynamic from 'next/dynamic';\n\nconst LiveRaceDashboardNoSSR = dynamic(\n  () => import('./LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  { ssr: false }\n);\n\nexport default LiveRaceDashboardNoSSR;\n",
    "web_platform/frontend/src/components/RaceCard.tsx": "// web_platform/frontend/src/components/RaceCard.tsx\n'use client';\n\nimport React, { useState, useEffect } from 'react';\nimport type { Race, Runner } from '../types/racing';\n\n// Local types removed, now importing from '../types/racing'\n\ninterface RaceCardProps {\n  race: Race;\n}\n\nconst Countdown: React.FC<{ startTime: string }> = ({ startTime }) => {\n  const [currentTime, setCurrentTime] = useState(new Date());\n\n  useEffect(() => {\n    const timer = setInterval(() => setCurrentTime(new Date()), 1000);\n    return () => clearInterval(timer);\n  }, []);\n\n  const getCountdown = (startTimeStr: string) => {\n    const postTime = new Date(startTimeStr);\n    const diff = postTime.getTime() - currentTime.getTime();\n\n    if (diff <= 0) return { text: \"RACE COMPLETE\", color: \"text-gray-500\" };\n\n    const minutes = Math.floor(diff / 60000);\n    const seconds = Math.floor((diff % 60000) / 1000).toString().padStart(2, '0');\n\n    let color = \"text-green-400\";\n    if (minutes < 2) color = \"text-red-500 font-bold animate-pulse\";\n    else if (minutes < 10) color = \"text-yellow-400\";\n\n    return { text: `${minutes}:${seconds} to post`, color };\n  };\n\n  const countdown = getCountdown(startTime);\n\n  return (\n    <span className={`font-mono text-sm ${countdown.color}`}>{countdown.text}</span>\n  );\n};\n\nexport const RaceCard: React.FC<RaceCardProps> = ({ race }) => {\n  const activeRunners = race.runners.filter(r => !r.scratched);\n  activeRunners.sort((a, b) => a.number - b.number);\n\n  const getUniqueSourcesCount = (runners: Runner[]): number => {\n    const sources = new Set();\n    runners.forEach(runner => {\n      if (runner.odds) {\n        Object.keys(runner.odds).forEach(source => sources.add(source));\n      }\n    });\n    return sources.size;\n  };\n\n  const getBestOdds = (runner: Runner): { odds: number, source: string } | null => {\n    if (!runner.odds) return null;\n  const validOdds = Object.values(runner.odds).filter(o => o.win !== null && o.win !== undefined && o.win < 999);\n    if (validOdds.length === 0) return null;\n  const best = validOdds.reduce((min, o) => (o.win ?? 999) < (min.win ?? 999) ? o : min);\n    return { odds: best.win!, source: best.source };\n  };\n\n  return (\n    <div className={`race-card-enhanced border rounded-lg p-4 bg-gray-800 shadow-lg hover:border-purple-500 transition-all ${race.qualification_score && race.qualification_score >= 80 ? 'card-premium' : 'border-gray-700'}`}>\n      {/* Header with Smart Status Indicators */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <h2 className=\"text-2xl font-bold text-white\">{race.venue}</h2>\n            <div className=\"flex gap-2 text-sm text-gray-400\">\n              <span>Race {race.race_number}</span>\n              <span>\u2022</span>\n              <Countdown startTime={race.start_time} />\n            </div>\n            {race.favorite && (\n              <div className=\"flex items-center gap-2 mt-2 text-sm text-yellow-400\">\n                <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-4 w-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n                  <path d=\"M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z\" />\n                </svg>\n                <span className=\"font-semibold\">Favorite: {race.favorite.name}</span>\n              </div>\n            )}\n          </div>\n        </div>\n\n        {race.qualification_score && (\n          <div className={`px-4 py-2 rounded-full text-center ${\n            race.qualification_score >= 80 ? 'bg-red-500/20 text-red-400 border border-red-500/30' :\n            race.qualification_score >= 60 ? 'bg-yellow-500/20 text-yellow-400 border border-yellow-500/30' :\n            'bg-green-500/20 text-green-400 border border-green-500/30'\n          }`}>\n            <div className=\"font-bold text-lg\">{race.qualification_score.toFixed(0)}%</div>\n            <div className=\"text-xs\">Score</div>\n          </div>\n        )}\n      </div>\n\n      {/* Race Conditions Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Distance</div>\n          <div className=\"text-sm font-semibold text-white\">{race.distance || 'N/A'}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Surface</div>\n          <div className=\"text-sm font-semibold text-white\">{race.surface || 'Dirt'}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Field</div>\n          <div className=\"text-sm font-semibold text-white\">{activeRunners.length}</div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"text-xs text-gray-400\">Sources</div>\n          <div className=\"text-sm font-semibold text-white\">{getUniqueSourcesCount(race.runners)}</div>\n        </div>\n      </div>\n\n      {/* Interactive Runner Rows */}\n      <div className=\"runners-table space-y-2\">\n        {activeRunners.map((runner, idx) => {\n          const bestOddsInfo = getBestOdds(runner);\n          return (\n            <div key={runner.number} className=\"runner-row group hover:bg-purple-500/10 transition-all rounded-md p-3\">\n              <div className=\"flex items-center justify-between\">\n                <div className=\"flex items-center gap-4 flex-1\">\n                  <div className={`w-10 h-10 rounded-full flex items-center justify-center font-bold transition-all group-hover:scale-110 text-gray-900 shadow-lg ${idx === 0 ? 'bg-gradient-to-br from-yellow-400 to-yellow-600 shadow-yellow-500/50' : idx === 1 ? 'bg-gradient-to-br from-gray-300 to-gray-500 shadow-gray-400/50' : idx === 2 ? 'bg-gradient-to-br from-orange-400 to-orange-600 shadow-orange-500/50' : 'bg-gray-700 text-gray-300'}`}>\n                    {runner.number}\n                  </div>\n                  <div className=\"flex flex-col\">\n                    <span className=\"font-bold text-white text-lg\">{runner.name}</span>\n                    <div className=\"flex gap-3 text-sm text-gray-400\">\n                      {runner.jockey && <span>J: {runner.jockey}</span>}\n                      {runner.trainer && <span>T: {runner.trainer}</span>}\n                    </div>\n                  </div>\n                </div>\n                {bestOddsInfo && (\n                  <div className=\"text-right\">\n                    <div className=\"text-2xl font-bold text-emerald-400\">{bestOddsInfo.odds.toFixed(2)}</div>\n                    <div className=\"text-xs text-gray-500\">via {bestOddsInfo.source}</div>\n                  </div>\n                )}\n              </div>\n            </div>\n          );\n        })}\n      </div>\n    </div>\n  );\n};",
    "web_platform/frontend/src/components/StatusDetailModal.tsx": "// web_platform/frontend/src/components/StatusDetailModal.tsx\nimport React from 'react';\n\ninterface StatusDetailModalProps {\n  isOpen: boolean;\n  onClose: () => void;\n  status: {\n      title: string;\n      details: string | Record<string, any>;\n  };\n}\n\nexport const StatusDetailModal: React.FC<StatusDetailModalProps> = ({ isOpen, onClose, status }) => {\n  if (!isOpen) {\n    return null;\n  }\n\n  const { title, details } = status;\n  const isDetailsString = typeof details === 'string';\n\n  // Determine status color only if details is an object with a status property\n  const statusColor = !isDetailsString && (details.status === 'SUCCESS' || details.status === 'OK')\n    ? 'text-green-400'\n    : 'text-gray-300'; // Default color\n\n  return (\n    <div className=\"fixed inset-0 bg-black/60 flex items-center justify-center z-50\" onClick={onClose}>\n      <div className=\"bg-gray-800 border border-gray-700 rounded-lg shadow-xl p-6 max-w-lg w-full\" onClick={e => e.stopPropagation()}>\n        <div className=\"flex justify-between items-start mb-4\">\n          <h3 className=\"text-xl font-bold text-white\">{title}</h3>\n          <button onClick={onClose} className=\"text-gray-400 hover:text-white\">&times;</button>\n        </div>\n        <div className=\"space-y-2 text-sm max-h-96 overflow-y-auto pr-2\">\n            {isDetailsString ? (\n                <div className=\"text-gray-300 whitespace-pre-wrap bg-gray-900/50 p-4 rounded-md\">{details}</div>\n            ) : (\n                Object.entries(details).map(([key, value]) => (\n                    <div key={key} className=\"grid grid-cols-3 gap-4 border-b border-gray-700/50 py-2\">\n                    <span className=\"font-semibold text-gray-400 capitalize\">{key.replace(/_/g, ' ')}</span>\n                    <span className={`col-span-2 break-words ${key === 'status' ? statusColor : 'text-gray-300'}`}>\n                        {typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value)}\n                    </span>\n                    </div>\n                ))\n            )}\n        </div>\n        <button\n          onClick={onClose}\n          className=\"bg-gray-600 hover:bg-gray-700 text-white font-bold py-2 px-4 rounded w-full mt-6\"\n        >\n          Close\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/Tabs.tsx": "// src/components/Tabs.tsx\n'use client';\n\nimport React, { useState } from 'react';\n\ntype Tab = {\n  label: string;\n  content: React.ReactNode;\n};\n\ntype TabsProps = {\n  tabs: Tab[];\n};\n\nexport function Tabs({ tabs }: TabsProps) {\n  const [activeTab, setActiveTab] = useState(0);\n\n  return (\n    <div>\n      <div className=\"border-b border-slate-700\">\n        <nav className=\"-mb-px flex space-x-8\" aria-label=\"Tabs\">\n          {tabs.map((tab, index) => (\n            <button\n              key={tab.label}\n              onClick={() => setActiveTab(index)}\n              className={`${\n                activeTab === index\n                  ? 'border-blue-500 text-blue-400'\n                  : 'border-transparent text-slate-400 hover:text-slate-200 hover:border-slate-500'\n              } whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm transition-colors focus:outline-none`}\n            >\n              {tab.label}\n            </button>\n          ))}\n        </nav>\n      </div>\n      <div className=\"mt-8\">{tabs[activeTab].content}</div>\n    </div>\n  );\n}\n",
    "web_platform/frontend/tailwind.config.ts": "import type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  darkMode: 'media',\n  content: [\n    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './src/components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\nexport default config",
    "web_platform/frontend/tsconfig.json": "{\n  \"compilerOptions\": {\n    \"lib\": [\n      \"dom\",\n      \"dom.iterable\",\n      \"esnext\"\n    ],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,\n    \"noEmit\": true,\n    \"incremental\": true,\n    \"esModuleInterop\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"preserve\",\n    \"plugins\": [\n      {\n        \"name\": \"next\"\n      }\n    ]\n  },\n  \"include\": [\n    \"next-env.d.ts\",\n    \".next/types/**/*.ts\",\n    \"**/*.ts\",\n    \"**/*.tsx\",\n    \"out/types/**/*.ts\"\n  ],\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\n",
    "wix/WixUI_CustomInstallDir.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\"\n     xmlns:WixUI=\"http://schemas.microsoft.com/wix/WixUIExtension\">\n  <Fragment>\n    <UI Id=\"WixUI_CustomInstallDir\">\n        <DialogRef Id=\"BrowseDlg\" />\n        <DialogRef Id=\"DiskCostDlg\" />\n        <DialogRef Id=\"ErrorDlg\" />\n        <DialogRef Id=\"FatalError\" />\n        <DialogRef Id=\"FilesInUse\" />\n        <DialogRef Id=\"MsiRMFilesInUse\" />\n        <DialogRef Id=\"PrepareDlg\" />\n        <DialogRef Id=\"UserExit\" />\n        <DialogRef Id=\"WelcomeDlg\" />\n        <DialogRef Id=\"InstallDirDlg\" />\n        <DialogRef Id=\"VerifyReadyDlg\" />\n\n        <!-- Use our custom progress dialog instead of the default -->\n        <DialogRef Id=\"InstallProgressDlg\" />\n\n        <Publish Dialog=\"WelcomeDlg\" Control=\"Next\" Event=\"NewDialog\" Value=\"InstallDirDlg\">1</Publish>\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"WelcomeDlg\">1</Publish>\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Next\" Event=\"SetTargetPath\" Value=\"[WIXUI_INSTALLDIR]\" Order=\"1\" />\n        <Publish Dialog=\"InstallDirDlg\" Control=\"Next\" Event=\"NewDialog\" Value=\"VerifyReadyDlg\" Order=\"2\">1</Publish>\n        <Publish Dialog=\"VerifyReadyDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"InstallDirDlg\" Order=\"1\">NOT Installed</Publish>\n        <Publish Dialog=\"VerifyReadyDlg\" Control=\"Back\" Event=\"NewDialog\" Value=\"MaintenanceTypeDlg\" Order=\"2\">Installed</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}