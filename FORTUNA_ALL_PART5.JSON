{
    ".github/workflows/codeql.yml": "# System Timestamp: 2025-11-29 13:19:26.933797\n# System Timestamp: 2025-12-03 22:44:34.655552917\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ \"main\" ]\n  workflow_dispatch:\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'python', 'javascript' ]\n        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]\n        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    # Initializes the CodeQL tools for scanning.\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n        # If you wish to specify custom queries, you can do so here or in a config file.\n        # By default, queries are pulled from a suite stored in GitHub.\n        # See https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs\n        # queries: security-extended,security-and-quality\n\n    - name: Install Python dependencies\n      if: matrix.language == 'python'\n      run: |\n        python -m pip install --upgrade pip\n        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi\n        if [ -f python_service/requirements-dev.txt ]; then pip install -r python_service/requirements-dev.txt; fi\n        if [ -f web_service/backend/requirements-dev.txt ]; then pip install -r web_service/backend/requirements-dev.txt; fi\n\n    - name: Install Javascript dependencies and build\n      if: matrix.language == 'javascript'\n      run: |\n        if [ -f web_service/frontend/package.json ]; then (cd web_service/frontend && npm install && npm run build); fi\n        if [ -f web_platform/frontend/package.json ]; then (cd web_platform/frontend && npm install && npm run build); fi\n        if [ -f electron/package.json ]; then (cd electron && npm install); fi\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v3\n      with:\n        category: \"/language:${{matrix.language}}\"\n",
    "PSEUDOCODE.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Comprehensive System Specification (Revised & Corrected)\n**Version:** 2.2.0\n**Last Updated:** November 7, 2025\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture Pillars\n3.  Backend Engine (Python) - Detailed\n4.  Frontend Interface (TypeScript/React) - Detailed\n5.  Electron Wrapper & Windows Integration - Detailed\n6.  Data Models & API Specification\n7.  Deployment & Automation (CI/CD)\n8.  End-to-End Workflows\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551  Unifying global horse/greyhound/harness racing intelligence   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve results via a secure, local REST API to an interactive dashboard.\n  \u2022 Operate as a professional, standalone, native Windows application.\n\nCORE TENETS:\n  \u2022 UI-First Experience: The user interface is always responsive, even during backend startup or restarts.\n  \u2022 Resilient Process Management: The backend executable's lifecycle is robustly managed, with timeouts and crash detection.\n  \u2022 Asynchronous Initialization: The backend server starts instantly, deferring heavy, blocking I/O to background threads.\n  \u2022 Secure by Design: Communication between the frontend and the privileged main process is secured via a context-aware preload script.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline.\n\nSTAKEHOLDERS:\n  \u2022 End User: Receives a professional MSI installer for a one-click, dependency-free launch.\n  \u2022 Developer: Works with clean, separated Python and TypeScript stacks, governed by this specification.\n```\n\n---\n\n## 2. ARCHITECTURE PILLARS\n\n### Pillar 1: Backend Engine (Python)\n\n```\nPYTHON_BACKEND:\n  \u251c\u2500 main.py\n  \u2502  \u2514\u2500 Entry point for PyInstaller executable; starts the Uvicorn server.\n  \u2502\n  \u251c\u2500 api.py\n  \u2502  \u2514\u2500 FastAPI application definition.\n  \u2502     \u251c\u2500 Lifespan Hook: Manages async startup/shutdown logic.\n  \u2502     \u251c\u2500 API Routes: /health, /api/status, /api/races, etc.\n  \u2502     \u2514\u2500 Dependency Injection: Provides engine and security dependencies.\n  \u2502\n  \u251c\u2500 engine.py\n  \u2502  \u2514\u2500 OddsEngine: Orchestrates all data fetching and processing.\n  \u2502\n  \u251c\u2500 adapters/\n  \u2502  \u251c\u2500 base_v3.py (Abstract Base Class for all data sources)\n  \u2502  \u2514\u2500 [20+ specific adapter implementations]\n  \u2502\n  \u251c\u2500 config.py\n  \u2502  \u2514\u2500 Pydantic settings management from .env file.\n  \u2502\n  \u2514\u2500 requirements.txt\n     \u2514\u2500 Clean, de-duplicated, and conflict-free list of all Python dependencies.\n```\n\n### Pillar 2: Frontend Interface (TypeScript/React)\n\n```\nFRONTEND:\n  \u251c\u2500 next.config.mjs\n  \u2502  \u2514\u2500 Next.js config with `output: 'export'` for 100% static generation.\n  \u2502\n  \u251c\u2500 app/page.tsx\n  \u2502  \u2514\u2500 Main application shell.\n  \u2502\n  \u251c\u2500 src/components/\n  \u2502  \u251c\u2500 LiveRaceDashboard.tsx (Main stateful component)\n  \u2502  \u2502  \u251c\u2500 Manages connection state ('connecting', 'online', 'error').\n  \u2502  \u2502  \u251c\u2500 Polls Electron main process for backend status via secure IPC.\n  \u2502  \u2502  \u2514\u2500 Fetches data from the local Python API when online.\n  \u2502  \u2502\n  \u2502  \u251c\u2500 RaceCard.tsx (Displays a single race)\n  \u2502  \u2514\u2500 StatusIndicator.tsx (Shows backend connection status)\n  \u2502\n  \u2514\u2500 src/types/\n     \u2514\u2500 racing.ts (TypeScript interfaces matching backend Pydantic models)\n```\n\n### Pillar 3: Electron Wrapper & Windows Integration\n\n```\nELECTRON_WRAPPER:\n  \u251c\u2500 main.js (Electron main process)\n  \u2502  \u251c\u2500 Creates the BrowserWindow and loads the static frontend.\n  \u2502  \u251c\u2500 Implements robust lifecycle management for the backend executable.\n  \u2502  \u251c\u2500 Provides secure IPC handlers for status checks and restarts.\n  \u2502  \u2514\u2500 Creates a system tray icon for background operation.\n  \u2502\n  \u251c\u2500 preload.js (Secure IPC Bridge)\n  \u2502  \u2514\u2500 Uses `contextBridge` to safely expose specific functions to the frontend.\n  \u2502\n  \u251c\u2500 package.json\n  \u2502  \u2514\u2500 Defines Node.js dependencies and build scripts.\n  \u2502\n  \u251c\u2500 electron-builder-config.yml\n  \u2502  \u2514\u2500 Defines the configuration for creating the final MSI installer.\n  \u2502\n  \u2514\u2500 .github/workflows/build-msi.yml\n     \u2514\u2500 GitHub Actions pipeline that automates the entire build, test, and package process.\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON) - DETAILED\n\n### 3.1 Entry Point & Server Startup (`main.py`)\n\n```pseudocode\n// This is the script executed by fortuna-backend.exe\n\nPROCEDURE Main_Python_Entry_Point\n  // Guard required for PyInstaller and multiprocessing on Windows\n  IF this script is the main entry point:\n    CALL multiprocessing.freeze_support()\n\n    // Programmatically launch the FastAPI application using Uvicorn\n    // This call blocks and runs the server until the process is terminated\n    CALL uvicorn.run(\n      app=\"python_service.api:app\",\n      host=\"0.0.0.0\",\n      port=8000\n    )\nEND PROCEDURE\n```\n\n### 3.2 Asynchronous Application Lifecycle (`api.py`)\n\n```pseudocode\n// --- Lifespan Management (The key to a non-blocking startup) ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // === ON STARTUP ===\n  LOG \"Uvicorn server is online. Starting lifespan initialization.\"\n\n  // 1. Perform immediate, non-blocking tasks\n  CONNECT to Redis cache\n\n  // 2. Defer slow, blocking tasks to a background thread\n  //    This allows the server to start accepting requests instantly.\n  SCHEDULE function \"initialize_heavy_resources(app)\" to run in a ThreadPoolExecutor\n\n  LOG \"Heavy resource initialization scheduled. Server is now responsive.\"\n\n  // 3. Yield control back to Uvicorn. The server is now live.\n  YIELD\n\n  // === ON SHUTDOWN ===\n  LOG \"Shutdown signal received.\"\n  AWAIT app.state.engine.close() // Gracefully close HTTP client connections\n  DISCONNECT from Redis\n  SHUTDOWN ThreadPoolExecutor\n\n// --- Heavy Initialization (Runs in Background) ---\nFUNCTION initialize_heavy_resources(app: FastAPI):\n  TRY\n    LOG \"Background initialization of OddsEngine has started.\"\n    settings <- get_settings_from_config()\n    engine <- create new OddsEngine(config=settings)\n    // This part is slow: it loads all ~25 adapters\n    app.state.engine <- engine\n    LOG \"Background initialization complete. OddsEngine is now available.\"\n  CATCH Exception as e:\n    LOG_CRITICAL \"Failed to initialize OddsEngine in the background.\", error=e\n    app.state.engine <- null // Ensure the app knows initialization failed\n```\n\n### 3.3 Engine Orchestration (`engine.py`)\n\n```pseudocode\nCLASS OddsEngine:\n  INIT(config):\n    self.config <- config\n    self.adapters <- [List of all adapter instances]\n    self.http_client <- httpx.AsyncClient(...)\n    self.semaphore <- asyncio.Semaphore(config.MAX_CONCURRENT_REQUESTS)\n\n    // Inject the shared, persistent HTTP client into each adapter\n    FOR adapter IN self.adapters:\n      adapter.http_client <- self.http_client\n\n  @cache_async_result(ttl_seconds=300)\n  ASYNC FUNCTION fetch_all_odds(date_str):\n    // Create a list of concurrent fetching tasks, wrapped in the semaphore\n    tasks <- [self._fetch_with_semaphore(adapter, date_str) FOR adapter in self.adapters]\n    results <- AWAIT asyncio.gather(*tasks, return_exceptions=True)\n\n    // Process results, separating successes from failures\n    all_races <- []\n    FOR result IN results:\n      IF result is a success:\n        all_races.extend(result.races)\n\n    // Deduplicate and merge races from different sources\n    deduped_races <- self._dedupe_races(all_races)\n\n    RETURN AggregatedResponse(races=deduped_races, source_statuses=...)\n```\n\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/REACT) - DETAILED\n\n### 4.1 LiveRaceDashboard Component\n\n```pseudocode\nCOMPONENT LiveRaceDashboard (client-side):\n\n  STATE:\n    races: Race[] <- []\n    backendStatus: 'connecting' | 'online' | 'error' <- 'connecting'\n    lastLogs: string[] <- []\n\n  EFFECT on mount:\n    // Use the secure API exposed by preload.js\n    IF window.electronAPI exists:\n      // Set up a listener for status updates from the main process\n      window.electronAPI.onBackendStatus((update) => {\n        setBackendStatus(update.state)\n        setLastLogs(update.logs)\n      })\n\n    // Immediately request the current status\n    window.electronAPI.getBackendStatus().then((status) => {\n      setBackendStatus(status.state)\n      setLastLogs(status.logs)\n    })\n\n    // Set up a polling interval to keep status fresh\n    interval <- setInterval(() => {\n      window.electronAPI.getBackendStatus().then((status) => {\n        setBackendStatus(status.state)\n        setLastLogs(status.logs)\n      })\n    }, 3000) // Poll every 3 seconds\n\n    CLEANUP: clearInterval(interval)\n\n  EFFECT when backendStatus changes to 'online':\n    // Trigger data fetch only when the backend is confirmed to be running\n    fetchQualifiedRaces()\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard HTTP call to the local Python server\n      response <- AWAIT fetch(\"http://127.0.0.1:8000/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API returned status ${response.status}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n\n    CATCH e:\n      // If the API call fails, update the status\n      setBackendStatus('error')\n      setLastLogs([...lastLogs, `API Fetch Error: ${e.message}`])\n\n  FUNCTION RENDER:\n    <div className=\"dashboard\">\n      <StatusIndicator status={backendStatus} />\n      <RaceFilters />\n\n      IF backendStatus === 'error':\n        <ErrorDisplay logs={lastLogs} />\n      ELSE IF backendStatus === 'connecting':\n        <LoadingSkeleton />\n      ELSE IF races.length === 0:\n        <EmptyState message=\"No races matched your filters.\" />\n      ELSE:\n        <RaceGrid races={races} />\n    </div>\n```\n\n---\n\n## 5. ELECTRON WRAPPER & WINDOWS INTEGRATION - DETAILED\n\n### 5.1 Main Process (`main.js`) - With Robust Lifecycle Management\n\n```pseudocode\nCLASS FortunaDesktopApp:\n  INIT():\n    self.mainWindow <- null\n    self.backendState <- 'stopped'\n    self.backendLogs <- []\n    self.backendProcess <- null\n\n  FUNCTION createMainWindow():\n    // ... create BrowserWindow, load static frontend ...\n\n  FUNCTION startBackend():\n    IF self.backendProcess is not null:\n      self.backendProcess.kill()\n\n    self.backendState <- 'starting'\n    self.backendLogs <- ['Attempting to start backend...']\n    self.sendBackendStatusUpdate() // Notify UI\n\n    // Get path to the packaged executable\n    exePath <- path.join(process.resourcesPath, 'fortuna-backend', 'fortuna-backend.exe')\n\n    IF file at exePath does NOT exist:\n      self.backendState <- 'error'\n      self.backendLogs.push(`FATAL: Executable not found at ${exePath}`)\n      self.sendBackendStatusUpdate()\n      dialog.showErrorBox(\"Critical Error\", \"Backend is missing. Please reinstall.\")\n      RETURN\n\n    // Spawn the process\n    self.backendProcess <- spawn(exePath, [], { stdio: ['ignore', 'pipe', 'pipe'] })\n\n    // --- CRITICAL: Resiliency Logic ---\n    startupTimeout <- setTimeout(() => {\n      IF self.backendState === 'starting':\n        self.backendState <- 'error'\n        self.backendLogs.push('Error: Backend startup timed out after 30 seconds.')\n        self.backendProcess.kill()\n        self.sendBackendStatusUpdate()\n    }, 30000) // 30-second timeout\n\n    self.backendProcess.stdout.on('data', (data) => {\n      self.backendLogs.push(data.toString())\n      // A more robust check would be a successful health check poll\n      IF data.toString().includes(\"Uvicorn running\"):\n        self.backendState <- 'online'\n        clearTimeout(startupTimeout)\n        self.sendBackendStatusUpdate()\n    })\n\n    self.backendProcess.stderr.on('data', (data) => {\n      self.backendLogs.push(`[STDERR] ${data.toString()}`)\n    })\n\n    self.backendProcess.on('exit', (code) => {\n      clearTimeout(startupTimeout)\n      IF self.backendState is not 'error': // Avoid duplicate error messages\n        self.backendState <- 'error'\n        self.backendLogs.push(`Backend process exited unexpectedly with code: ${code}`)\n        self.sendBackendStatusUpdate()\n    })\n\n  FUNCTION sendBackendStatusUpdate():\n    // Send the latest status to the frontend renderer process\n    IF self.mainWindow is not null:\n      self.mainWindow.webContents.send('backend-status-update', {\n        state: self.backendState,\n        logs: self.backendLogs.slice(-20) // Send last 20 log lines\n      })\n\n// --- IPC Handlers (Securely Defined) ---\nipcMain.handle('get-backend-status', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN null\n\n  RETURN { state: self.backendState, logs: self.backendLogs.slice(-20) }\n})\n\nipcMain.on('restart-backend', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN\n\n  self.startBackend()\n})\n```\n\n### 5.2 Preload Script (`preload.js`)\n\n```pseudocode\n// Expose a limited, secure API to the frontend renderer process\ncontextBridge.exposeInMainWorld('electronAPI', {\n  getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n  restartBackend: () => ipcRenderer.send('restart-backend'),\n  onBackendStatus: (callback) => ipcRenderer.on('backend-status-update', (_event, value) => callback(value))\n})\n```\n\n---\n\n## 6. DATA MODELS & API SPECIFICATION\n\n### 6.1 Core Data Models (Pydantic/TypeScript)\n\n```\nMODEL Race:\n  id: str (unique identifier, e.g., \"Betfair_USA_Aqueduct_2025-11-07_R1\")\n  venue: str\n  race_number: int\n  start_time: datetime\n  runners: List[Runner]\n  source: str\n\nMODEL Runner:\n  name: str\n  odds: Optional[float]\n```\n\n### 6.2 Primary API Endpoints\n\n```\nENDPOINT GET /health\n  Description: Simple health check, requires no authentication.\n  Response (200 OK): {\"status\": \"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Description: Fetches all race data, runs the Trifecta analyzer, and returns qualified races.\n  Headers:\n    - X-API-Key: (Required, not used in this local setup but good practice)\n  Query Params:\n    - max_field_size: int\n    - min_odds: float\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 7. DEPLOYMENT & AUTOMATION (CI/CD)\n\n```pseudocode\nWORKFLOW Build_MSI_Installer_on_GitHub_Actions:\n  // Phase 1: Setup\n  SETUP Node.js and Python environments\n\n  // Phase 2: Build Frontend\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY static output to /electron/web-ui-build/out\n\n  // Phase 3: Build Backend\n  RUN \"pip install -r python_service/requirements.txt\"\n  // CRITICAL: Use PyInstaller with a spec file or CLI flags that include\n  // necessary hidden imports to prevent runtime crashes.\n  // e.g., --hidden-import=keyring.backends.fail.Keyring\n  EXECUTE PyInstaller to create fortuna-backend.exe\n  PLACE executable in /electron/resources/fortuna-backend\n\n  // Phase 4: Deep Integration Test\n  START fortuna-backend.exe in the background\n  POLL http://127.0.0.1:8000/health until it responds with 200 OK or times out\n  IF timeout or crash THEN FAIL the build\n\n  // Phase 5: Package\n  RUN \"npm ci\" in /electron\n  EXECUTE \"npx electron-builder\" to create the MSI installer\n\n  // Phase 6: Publish\n  UPLOAD MSI as a build artifact\n  IF build was triggered by a git tag THEN CREATE a new GitHub Release\n```\n\n---\n\n## 8. END-TO-END WORKFLOWS\n\n### 8.1 Production Startup Workflow (Resilient)\n\n```\nWORKFLOW user_launches_application:\n  STEP 1: User executes Fortuna Faucet.exe -> Electron main.js starts.\n  STEP 2: UI appears instantly. The main process creates the BrowserWindow and loads the static index.html. The UI shows a 'connecting' state.\n  STEP 3: Backend starts asynchronously. The main process calls the robust `startBackend()` function.\n  STEP 4: `startBackend()` spawns `fortuna-backend.exe` and starts a 30-second timeout.\n  STEP 5: The frontend UI polls for status every 3 seconds via the secure `window.electronAPI.getBackendStatus()`.\n  STEP 6: The backend `.exe` starts, its `lifespan` hook runs, and the Uvicorn server comes online within seconds.\n  STEP 7: The main process detects the \"Uvicorn running\" message (or a successful health poll) and updates its internal state to 'online'. The startup timeout is cleared.\n  STEP 8: On its next poll, the frontend receives the 'online' status.\n  STEP 9: The frontend's state changes, triggering the `fetchQualifiedRaces()` API call to `localhost:8000`.\n  STEP 10: Data is returned from the now fully-initialized backend and rendered in the UI.\n\n  FAILURE SCENARIO (Backend Crash):\n  STEP 6a: The backend `.exe` crashes on startup.\n  STEP 7a: The `on('exit')` handler in `main.js` fires. The state is set to 'error' with the exit code.\n  STEP 8a: On its next poll, the frontend receives the 'error' status and relevant logs.\n  STEP 9a: The UI renders an error message and a \"Restart Backend\" button.\n```\n\n---\n*This concludes the revised and definitive blueprint for the Fortuna Faucet application.*",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2)---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "electron/assets/.gitkeep": "# This directory is for application icons (e.g., icon.ico, tray-icon.png)",
    "electron/electron-builder-config.yml": "appId: com.jules.fortunafaucet\nproductName: \"Fortuna Faucet\"\n\ndirectories:\n  output: dist\n  buildResources: assets\n\nfiles:\n  - filter:\n      - \"**/*\"\n\nextraResources:\n  - from: \"../python-service-bin\"\n    to: \"resources/python-service-bin\"\n    filter:\n      - \"**/*\"\n  - from: \"../web_platform/frontend/out\"\n    to: \"resources/frontend\"\n    filter:\n      - \"**/*\"\n\nwin:\n  target: msi\n  icon: \"assets/icon.ico\"\n\nmsi:\n  oneClick: false\n  perMachine: true\n  runAfterFinish: true\n  # Explicitly pointing to the file ensures WiX picks it up\n  shortcutName: \"Fortuna Faucet\"\n  warningsAsErrors: false",
    "electron/main.js": "// electron/main.js - CORRECTED VERSION\nconst { app, BrowserWindow, Tray, Menu, nativeImage, ipcMain, dialog } = require('electron');\nconst { autoUpdater } = require('electron-updater');\nconst { spawn } = require('child_process');\nconst net = require('net');\nconst path = require('path');\nconst fs = require('fs');\nconst SecureSettingsManager = require('./secure-settings-manager');\n\nclass FortunaDesktopApp {\n constructor() {\n this.backendProcess = null;\n this.mainWindow = null;\n this.tray = null;\n this.backendState = 'stopped'; // \"stopped\", \"starting\", \"running\", \"error\"\n this.backendLogs = [];\n this.isBackendStarting = false;\n }\n\n sendBackendStatusUpdate() {\n if (this.mainWindow) {\n this.mainWindow.webContents.send('backend-status-update', {\n state: this.backendState,\n logs: this.backendLogs.slice(-20) // Send last 20 log entries\n });\n }\n }\n\n stopBackend() {\n if (this.backendProcess && !this.backendProcess.killed) {\n console.log('Stopping backend process...');\n this.backendProcess.kill();\n this.backendState = 'stopped';\n this.isBackendStarting = false; // Ensure lock is released on stop\n this.backendLogs.push('Backend process stopped by user.');\n this.sendBackendStatusUpdate();\n }\n }\n\n  checkPortInUse(port) {\n    return new Promise((resolve, reject) => {\n      const server = net.createServer();\n      server.once('error', (err) => {\n        if (err.code === 'EADDRINUSE') {\n          resolve(true); // Port is in use\n        } else {\n          reject(err);\n        }\n      });\n      server.once('listening', () => {\n        server.close(() => {\n          resolve(false); // Port is free\n        });\n      });\n      server.listen(port, '127.0.0.1');\n    });\n  }\n\n  async startBackend() {\n    if (this.isBackendStarting) {\n      console.log('Backend start already in progress. Ignoring request.');\n      return;\n    }\n    this.isBackendStarting = true;\n    this.backendState = 'starting';\n    this.backendLogs = ['Attempting to start backend...'];\n    this.sendBackendStatusUpdate();\n\n    const port = process.env.FORTUNA_PORT || 8000;\n\n    try {\n      const isPortInUse = await this.checkPortInUse(port);\n      if (isPortInUse) {\n        console.log(`Port ${port} is already in use. Assuming backend is running.`);\n        this.backendState = 'running';\n        this.backendLogs.push(`Port ${port} is already in use. Assuming backend is running.`);\n        this.isBackendStarting = false;\n        this.sendBackendStatusUpdate();\n        return; // The most important change: we just stop here.\n      }\n    } catch (error) {\n        const errorMsg = `Error checking port ${port}: ${error.message}`;\n        console.error(errorMsg);\n        this.backendState = 'error';\n        this.backendLogs.push(errorMsg);\n        this.isBackendStarting = false;\n        this.sendBackendStatusUpdate();\n        dialog.showErrorBox('Network Error', `Could not check port ${port}. Please check your network configuration.`);\n        return;\n    }\n\n    if (this.backendProcess && !this.backendProcess.killed) {\n      console.log('An old backend process was found. Terminating it before starting a new one.');\n      this.backendProcess.kill();\n    }\n\n    const isDev = !app.isPackaged;\n    let backendCommand;\n    const backendArgs = [];\n    let backendCwd = process.cwd();\n\n    if (isDev) {\n      // This logic remains the same for development\n      console.log('[DEV MODE] Configuring backend to run from Python venv...');\n      backendCommand = path.join(__dirname, '..', '.venv', 'Scripts', 'python.exe');\n      backendArgs.push('-m', 'uvicorn', 'api:app', '--host', '127.0.0.1', '--port', port);\n      backendCwd = path.join(__dirname, '..', 'python_service');\n    } else {\n      console.log('[PROD MODE] Configuring backend to run from packaged executable...');\n      backendCommand = path.join(process.resourcesPath, 'fortuna-backend.exe');\n    }\n\n    if (!fs.existsSync(backendCommand)) {\n      const errorMsg = `FATAL: Backend executable not found at ${backendCommand}`;\n      console.error(errorMsg);\n      this.backendState = 'error';\n      this.backendLogs.push(errorMsg);\n      this.isBackendStarting = false;\n      this.sendBackendStatusUpdate();\n      dialog.showErrorBox('Backend Missing', 'The backend service executable is missing. Please try reinstalling Fortuna Faucet.');\n      return;\n    }\n\n    console.log(`Spawning backend: ${backendCommand} ${backendArgs.join(' ')}`);\n    this.backendProcess = spawn(backendCommand, backendArgs, {\n      cwd: backendCwd,\n      env: { ...process.env, FORTUNA_PORT: port.toString() },\n      stdio: ['ignore', 'pipe', 'pipe'],\n    });\n\n    this.backendProcess.stdout.on('data', (data) => {\n      const output = data.toString().trim();\n      console.log(`[Backend] ${output}`);\n      this.backendLogs.push(output);\n      if (this.backendState !== 'running' && output.includes('Application startup complete')) {\n        console.log('\u2705 Backend is ready!');\n        this.backendState = 'running';\n        this.isBackendStarting = false;\n      }\n      this.sendBackendStatusUpdate();\n    });\n\n this.backendProcess.stderr.on('data', (data) => {\n const errorOutput = data.toString().trim();\n console.error(`[Backend ERROR] ${errorOutput}`);\n this.backendLogs.push(`ERROR: ${errorOutput}`);\n this.backendState = 'error';\n this.isBackendStarting = false;\n this.sendBackendStatusUpdate();\n });\n\n this.backendProcess.on('error', (err) => {\n const errorMsg = `FATAL: Failed to start backend process: ${err.message}`;\n console.error(errorMsg);\n this.backendLogs.push(errorMsg);\n this.backendState = 'error';\n this.isBackendStarting = false;\n this.sendBackendStatusUpdate();\n });\n\n this.backendProcess.on('exit', (code) => {\n if (code !== 0 && this.backendState !== 'stopped') {\n const errorMsg = `Backend process exited unexpectedly with code ${code}`;\n console.error(errorMsg);\n this.backendLogs.push(errorMsg);\n this.backendState = 'error';\n this.isBackendStarting = false;\n this.sendBackendStatusUpdate();\n }\n });\n }\n\n getFrontendPath() {\n const isDev = !app.isPackaged;\n\n if (isDev) {\n return 'http://localhost:3000';\n }\n\n const indexPath = path.join(app.getAppPath(), 'web-ui-build', 'out', 'index.html');\n const { pathToFileURL } = require('url');\n return pathToFileURL(indexPath).toString();\n }\n\n createMainWindow() {\n this.mainWindow = new BrowserWindow({\n width: 1600,\n height: 1000,\n title: 'Fortuna Faucet - Racing Analysis',\n icon: path.join(__dirname, 'assets', 'icon.ico'),\n webPreferences: {\n nodeIntegration: false,\n contextIsolation: true,\n preload: path.join(__dirname, 'preload.js')\n },\n autoHideMenuBar: true,\n backgroundColor: '#1a1a2e'\n });\n\n const frontendUrl = this.getFrontendPath();\n this.mainWindow.loadURL(frontendUrl);\n\n if (!app.isPackaged) {\n this.mainWindow.webContents.openDevTools();\n }\n\n this.mainWindow.on('close', (event) => {\n if (!app.isQuitting) {\n event.preventDefault();\n this.mainWindow.hide();\n }\n });\n }\n\n createSystemTray() {\n // ... (rest of the file is unchanged)\n }\n\n initialize() {\n  ipcMain.handle('get-api-port', () => {\n    return process.env.FORTUNA_PORT || 8000;\n  });\n this.createMainWindow();\n this.createSystemTray();\n this.startBackend();\n\n // Check for updates\n autoUpdater.checkForUpdatesAndNotify();\n\n autoUpdater.on('update-downloaded', (info) => {\n const dialogOpts = {\n type: 'info',\n buttons: ['Restart', 'Later'],\n title: 'Application Update',\n message: process.platform === 'win32' ? info.releaseName : info.releaseName,\n detail: 'A new version has been downloaded. Restart the application to apply the updates.'\n };\n\n dialog.showMessageBox(dialogOpts).then((returnValue) => {\n if (returnValue.response === 0) autoUpdater.quitAndInstall();\n });\n });\n\n ipcMain.on('restart-backend', () => this.startBackend());\n ipcMain.on('stop-backend', () => this.stopBackend());\n ipcMain.handle('get-backend-status', async () => ({\n state: this.backendState,\n logs: this.backendLogs.slice(-20)\n }));\n\n ipcMain.handle('get-api-key', async () => {\n return SecureSettingsManager.getApiKey();\n });\n\n ipcMain.handle('generate-api-key', async () => {\n const crypto = require('node:crypto');\n const newKey = crypto.randomBytes(16).toString('hex');\n SecureSettingsManager.saveApiKey(newKey);\n return newKey;\n });\n\n ipcMain.handle('save-api-key', async (event, apiKey) => {\n return SecureSettingsManager.saveApiKey(apiKey);\n });\n\n ipcMain.handle('save-betfair-credentials', async (event, credentials) => {\n return SecureSettingsManager.saveBetfairCredentials(credentials);\n });\n }\n\n cleanup() {\n if (this.backendProcess && !this.backendProcess.killed) {\n this.backendProcess.kill();\n }\n }\n}\n\nlet fortunaApp;\n\napp.whenReady().then(() => {\n fortunaApp = new FortunaDesktopApp();\n fortunaApp.initialize();\n});\n\napp.on('window-all-closed', () => {\n if (process.platform !== 'darwin') {\n // Do nothing, keep app running in tray\n }\n});\n\napp.on('activate', () => {\n if (BrowserWindow.getAllWindows().length === 0) {\n fortunaApp.createMainWindow();\n } else {\n fortunaApp.mainWindow.show();\n }\n});\n\napp.on('before-quit', () => {\n app.isQuitting = true;\n if (fortunaApp) {\n fortunaApp.cleanup();\n }\n});\n",
    "electron/preload.js": "// electron/preload.js\n// This script runs in a privileged environment with access to Node.js APIs.\n// It's used to securely expose specific functionality to the renderer process (the web UI).\n\nconst { contextBridge, ipcRenderer } = require('electron');\n\n// Expose a safe, limited API to the frontend.\ncontextBridge.exposeInMainWorld('electronAPI', {\n /**\n * Asynchronously fetches the secure API key from the main process.\n * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n */\n getApiKey: () => ipcRenderer.invoke('get-api-key'),\n\n /**\n * Asynchronously generates and saves a new secure API key.\n * @returns {Promise<string>} A promise that resolves with the newly generated API key.\n */\n generateApiKey: () => ipcRenderer.invoke('generate-api-key'),\n\n /**\n * Asynchronously saves a provided API key.\n * @param {string} apiKey - The API key to save.\n * @returns {Promise<{success: boolean}>} A promise that resolves with the result of the save operation.\n */\n saveApiKey: (apiKey) => ipcRenderer.invoke('save-api-key', apiKey),\n\n /**\n * Asynchronously saves Betfair credentials.\n * @param {{username: string, apiKey: string}} credentials - The credentials to save.\n * @returns {Promise<{success: boolean}>} A promise that resolves with the result of the save operation.\n */\n saveBetfairCredentials: (credentials) => ipcRenderer.invoke('save-betfair-credentials', credentials),\n\n /**\n  * Restarts the backend service.\n  */\n restartBackend: () => ipcRenderer.send('restart-backend'),\n\n /**\n  * Stops the backend service.\n  */\n stopBackend: () => ipcRenderer.send('stop-backend'),\n\n /**\n  * Fetches the current status of the backend service.\n  * @returns {Promise<{state: string, logs: string[]}>} A promise that resolves with the backend status.\n  */\n getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n\n /**\n  * Subscribes to backend status updates.\n  * @param {function(event, {state: string, logs: string[]})} callback - The function to call with status updates.\n  */\n onBackendStatusUpdate: (callback) => {\n    // Deliberately strip event sender from callback to avoid security risks\n    const subscription = (event, ...args) => callback(...args);\n    ipcRenderer.on('backend-status-update', subscription);\n\n    // Return a function to unsubscribe\n    return () => {\n      ipcRenderer.removeListener('backend-status-update', subscription);\n    };\n  },\n\n  /**\n   * Gets the port the backend API is running on.\n   * @returns {Promise<number>} A promise that resolves with the port number.\n   */\n  getApiPort: () => ipcRenderer.invoke('get-api-port'),\n});\n",
    "fortuna-backend-webservice.spec": "# -*- mode: python ; coding: utf-8 -*-\n\nimport os\nfrom pathlib import Path\nfrom textwrap import dedent\n\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\nproject_root = Path(SPECPATH)\nversion_string = os.environ.get(\"FORTUNA_VERSION\", \"0.0.0\")\n\n\ndef ensure_path(rel_path: str) -> Path:\n    target = project_root / rel_path\n    if not target.exists():\n        raise SystemExit(f\"[spec] Required path missing: {target}\")\n    return target\n\n\ndef include_tree(rel_path: str, target: str, store: list):\n    absolute = ensure_path(rel_path)\n    store.append((str(absolute), target))\n    print(f\"[spec] Including {absolute} -> {target}\")\n\n\ndef build_version_file(version: str) -> str:\n    parts = [int(p) for p in version.split(\".\") if p.isdigit()]\n    while len(parts) < 4:\n        parts.append(0)\n    parts = parts[:4]\n    file_content = dedent(\n        f\"\"\"\n        VSVersionInfo(\n          ffi=FixedFileInfo(\n            filevers={tuple(parts)},\n            prodvers={tuple(parts)},\n            mask=0x3f,\n            flags=0x0,\n            OS=0x40004,\n            fileType=0x1,\n            subtype=0x0,\n            date=(0, 0)\n          ),\n          kids=[\n            StringFileInfo([\n              StringTable(\n                '040904B0',\n                [\n                  StringStruct('CompanyName', 'Fortuna Development Team'),\n                  StringStruct('FileDescription', 'Fortuna Backend Web Service'),\n                  StringStruct('FileVersion', '{version}'),\n                  StringStruct('InternalName', 'fortuna-backend'),\n                  StringStruct('ProductName', 'Fortuna Web Service'),\n                  StringStruct('ProductVersion', '{version}')\n                ]\n              )\n            ]),\n            VarFileInfo([VarStruct('Translation', [1033, 1200])])\n          ]\n        )\n        \"\"\"\n    ).strip()\n    version_dir = project_root / \"build\" / \"pyinstaller\"\n    version_dir.mkdir(parents=True, exist_ok=True)\n    version_file = version_dir / \"version-info.txt\"\n    version_file.write_text(file_content, encoding=\"utf-8\")\n    return str(version_file)\n\n\ndatas = []\nhiddenimports = set()\n\ninclude_tree(\"staging/ui\", \"ui\", datas)\ninclude_tree(\"python_service/adapters\", \"adapters\", datas)\ninclude_tree(\"python_service/data\", \"data\", datas)\ninclude_tree(\"python_service/json\", \"json\", datas)\n\ndatas += collect_data_files(\"uvicorn\", includes=[\"*.html\", \"*.json\"])\ndatas += collect_data_files(\"slowapi\", includes=[\"*.json\", \"*.yaml\"])\ndatas += collect_data_files(\"structlog\", includes=[\"*.json\"])\ndatas += collect_data_files(\"certifi\")\n\nhiddenimports.update(collect_submodules(\"python_service\"))\nhiddenimports.update(\n    [\n        \"uvicorn.logging\",\n        \"uvicorn.loops.auto\",\n        \"uvicorn.lifespan.on\",\n        \"uvicorn.protocols.http.h11_impl\",\n        \"uvicorn.protocols.http.httptools_impl\",\n        \"uvicorn.protocols.websockets.wsproto_impl\",\n        \"uvicorn.protocols.websockets.websockets_impl\",\n        \"fastapi.routing\",\n        \"fastapi.middleware.cors\",\n        \"fastapi.middleware.gzip\",\n        \"starlette.staticfiles\",\n        \"starlette.middleware.cors\",\n        \"anyio._backends._asyncio\",\n        \"httpcore\",\n        \"httpx\",\n        \"python_multipart\",\n        \"slowapi\",\n        \"structlog\",\n        \"tenacity\",\n        \"aiosqlite\",\n        \"selectolax\",\n        \"pydantic_core\",\n        \"pydantic_settings.sources\",\n    ]\n)\n\nanalysis = Analysis(\n    [\"python_service/main.py\"],\n    pathex=[str(project_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=sorted(hiddenimports),\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[\"tests\", \"pytest\", \"web_service\"],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\npyz = PYZ(analysis.pure, analysis.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    analysis.scripts,\n    analysis.binaries,\n    analysis.zipfiles,\n    analysis.datas,\n    [],\n    name=\"fortuna-backend\",\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=False,\n    upx_exclude=[],\n    runtime_tmpdir=None,\n    console=False,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon=None,\n    version=build_version_file(version_string),\n)\n",
    "package-lock.json": "{\n  \"name\": \"app\",\n  \"lockfileVersion\": 3,\n  \"requires\": true,\n  \"packages\": {\n    \"\": {\n      \"dependencies\": {\n        \"@playwright/test\": \"^1.56.1\"\n      }\n    },\n    \"node_modules/@playwright/test\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/@playwright/test/-/test-1.56.1.tgz\",\n      \"integrity\": \"sha512-vSMYtL/zOcFpvJCW71Q/OEGQb7KYBPAdKh35WNSkaZA75JlAO8ED8UN6GUNTm3drWomcbcqRPFqQbLae8yBTdg==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"playwright\": \"1.56.1\"\n      },\n      \"bin\": {\n        \"playwright\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      }\n    },\n    \"node_modules/fsevents\": {\n      \"version\": \"2.3.2\",\n      \"resolved\": \"https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz\",\n      \"integrity\": \"sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==\",\n      \"hasInstallScript\": true,\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"darwin\"\n      ],\n      \"engines\": {\n        \"node\": \"^8.16.0 || ^10.6.0 || >=11.0.0\"\n      }\n    },\n    \"node_modules/playwright\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/playwright/-/playwright-1.56.1.tgz\",\n      \"integrity\": \"sha512-aFi5B0WovBHTEvpM3DzXTUaeN6eN0qWnTkKx4NQaH4Wvcmc153PdaY2UBdSYKaGYw+UyWXSVyxDUg5DoPEttjw==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"playwright-core\": \"1.56.1\"\n      },\n      \"bin\": {\n        \"playwright\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      },\n      \"optionalDependencies\": {\n        \"fsevents\": \"2.3.2\"\n      }\n    },\n    \"node_modules/playwright-core\": {\n      \"version\": \"1.56.1\",\n      \"resolved\": \"https://registry.npmjs.org/playwright-core/-/playwright-core-1.56.1.tgz\",\n      \"integrity\": \"sha512-hutraynyn31F+Bifme+Ps9Vq59hKuUCz7H1kDOcBs+2oGguKkWTU50bBWrtz34OUWmIwpBTWDxaRPXrIXkgvmQ==\",\n      \"license\": \"Apache-2.0\",\n      \"bin\": {\n        \"playwright-core\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=18\"\n      }\n    }\n  }\n}\n",
    "pyproject.toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"paddock-parser-ng\"\nversion = \"0.1.0\"\ndescription = \"A toolkit to identify the best racecards for betting.\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\n\n[project.scripts]\npaddock_parser_ui = \"paddock_parser.entry_points:run_terminal_ui\"\npaddock_parser_dashboard = \"paddock_parser.entry_points:run_dashboard\"\npaddock_parser_predict = \"paddock_parser.entry_points:run_prediction_engine\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n# Configuration for the Ruff linter\n[tool.ruff]\n# Allow lines to be up to 120 characters long.\nline-length = 120\n\n[tool.ruff.lint]\n# Enable Pyflakes (F), pycodestyle (E, W), and isort (I) rules.\nselect = [\"E\", \"F\", \"W\", \"I\"]\nignore = []\n\n[tool.ruff.lint.isort]\n# Sort imports within their sections alphabetically.\nforce-single-line = true\n",
    "python_service/adapters/betfair_datascientist_adapter.py": "# python_service/adapters/betfair_datascientist_adapter.py\n\nfrom datetime import datetime\nfrom io import StringIO\nfrom typing import List\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BetfairDataScientistAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Betfair Data Scientist CSV models, migrated to BaseAdapterV3.\n    \"\"\"\n\n    ADAPTER_NAME = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str, url: str, config=None):\n        source_name = f\"{self.ADAPTER_NAME}_{model_name}\"\n        super().__init__(source_name=source_name, base_url=url, config=config)\n        self.model_name = model_name\n\n    async def _fetch_data(self, date: str) -> Optional[StringIO]:\n        \"\"\"Fetches the raw CSV data from the Betfair Data Scientist model endpoint.\"\"\"\n        endpoint = f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n        self.logger.info(f\"Fetching data from {self.base_url}{endpoint}\")\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return StringIO(response.text) if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[StringIO]) -> List[Race]:\n        \"\"\"Parses the raw CSV data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n        try:\n            df = pd.read_csv(raw_data)\n            if df.empty:\n                self.logger.warning(\"Received empty CSV from Betfair Data Scientist.\")\n                return []\n\n            df = df.rename(\n                columns={\n                    \"meetings.races.bfExchangeMarketId\": \"market_id\",\n                    \"meetings.races.runners.bfExchangeSelectionId\": \"selection_id\",\n                    \"meetings.races.runners.ratedPrice\": \"rated_price\",\n                    \"meetings.races.raceName\": \"race_name\",\n                    \"meetings.name\": \"meeting_name\",\n                    \"meetings.races.raceNumber\": \"race_number\",\n                    \"meetings.races.runners.runnerName\": \"runner_name\",\n                    \"meetings.races.runners.clothNumber\": \"saddle_cloth\",\n                }\n            )\n            races: List[Race] = []\n            for market_id, group in df.groupby(\"market_id\"):\n                race_info = group.iloc[0]\n                runners = []\n                for _, row in group.iterrows():\n                    rated_price = row.get(\"rated_price\")\n                    odds_data = {}\n                    if pd.notna(rated_price):\n                        odds_data[self.source_name] = OddsData(\n                            win=float(rated_price),\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(\n                        Runner(\n                            name=str(row.get(\"runner_name\", \"Unknown\")),\n                            number=int(row.get(\"saddle_cloth\", 0)),\n                            odds=odds_data,\n                        )\n                    )\n\n                race = Race(\n                    id=str(market_id),\n                    venue=normalize_venue_name(str(race_info.get(\"meeting_name\", \"\"))),\n                    race_number=int(race_info.get(\"race_number\", 0)),\n                    start_time=datetime.now(),  # Placeholder, not provided in source\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n            self.logger.info(f\"Normalized {len(races)} races from {self.model_name}.\")\n            return races\n        except (pd.errors.ParserError, KeyError) as e:\n            self.logger.error(\n                \"Failed to parse Betfair Data Scientist CSV.\",\n                exc_info=True,\n                error=str(e),\n            )\n            return []\n",
    "python_service/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairGreyhounds\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Failed to parse a Betfair Greyhound market.\",\n                    exc_info=True,\n                    market=market,\n                )\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Optional[Race]:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "python_service/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass EquibaseAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        d = datetime.strptime(date, \"%Y-%m-%d\").date()\n        index_url = f\"/entries/Entries.cfm?ELEC_DATE={d.month}/{d.day}/{d.year}&STYLE=EQB\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Equibase index page\", url=index_url)\n            return None\n\n        parser = HTMLParser(index_response.text)\n        track_links = [\n            link.attributes[\"href\"]\n            for link in parser.css(\"div.track-information a\")\n            if \"race=\" not in link.attributes.get(\"href\", \"\")\n        ]\n\n        async def get_race_links_from_track(track_url: str):\n            response = await self.make_request(self.http_client, \"GET\", track_url, headers=self._get_headers())\n            if not response:\n                return []\n            parser = HTMLParser(response.text)\n            return [link.attributes[\"href\"] for link in parser.css(\"a.program-race-link\")]\n\n        tasks = [get_race_links_from_track(link) for link in track_links]\n        results = await asyncio.gather(*tasks)\n        race_links = [f\"{self.base_url}{link}\" for sublist in results for link in sublist]\n\n        async def fetch_single_html(race_url: str):\n            response = await self.make_request(self.http_client, \"GET\", race_url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first(\"div.track-information strong\")\n                if not venue_node:\n                    continue\n                venue = clean_text(venue_node.text())\n\n                race_number_node = parser.css_first(\"div.race-information strong\")\n                if not race_number_node:\n                    continue\n                race_number_text = race_number_node.text().replace(\"Race\", \"\").strip()\n                if not race_number_text.isdigit():\n                    continue\n                race_number = int(race_number_text)\n\n                post_time_node = parser.css_first(\"p.post-time span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    if runner := self._parse_runner(node):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first(\"td:nth-child(1)\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            name_node = node.css_first(\"td:nth-child(3)\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            odds_node = node.css_first(\"td:nth-child(10)\")\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError, IndexError):\n            self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n            return None\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        time_part = time_str.split(\" \")[-2] + \" \" + time_str.split(\" \")[-1]\n        dt_str = f\"{date_str} {time_part}\"\n        return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, \"GREYHOUND_API_URL\") or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "python_service/api.py": "# python_service/api.py\n\nimport asyncio\nimport os\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import List\nfrom typing import Optional\n\nimport aiosqlite\nimport structlog\nfrom fastapi import Depends\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import Request\nfrom fastapi import WebSocket\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom slowapi import Limiter\nfrom slowapi import _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom starlette.websockets import WebSocketDisconnect\n\n# --- PyInstaller Explicit Imports ---\nfrom .analyzer import AnalyzerEngine\nfrom .cache_manager import cache_manager\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .engine import OddsEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .manual_override_manager import ManualOverrideManager\nfrom .middleware.error_handler import UserFriendlyException\nfrom .middleware.error_handler import user_friendly_exception_handler\nfrom .middleware.error_handler import validation_exception_handler\nfrom .models import AggregatedResponse\nfrom .models import ManualParseRequest\nfrom .models import QualifiedRacesResponse\nfrom .models import Race\nfrom .models import TipsheetRace\nfrom .security import verify_api_key\n\n# ------------------------------------\n\nlog = structlog.get_logger()\n\n# Create a fixed thread pool for blocking calls at the module level\nexecutor = ThreadPoolExecutor(max_workers=1)\n\n\ndef _initialize_heavy_resources_sync(app: FastAPI):\n    \"\"\"\n    This synchronous function contains the blocking I/O and CPU-intensive\n    initialization of the OddsEngine and its ~25 adapters. By isolating it,\n    we can run it in a background thread without stalling the main Uvicorn event loop.\n    \"\"\"\n    log.info(\"Background initialization of heavy resources started.\")\n    try:\n        settings = get_settings()\n\n        # Initialize WebSocket connection manager\n        connection_manager = ConnectionManager()\n\n        # Initialize manual override manager\n        manual_override_manager = ManualOverrideManager()\n\n        # Initialize engine with manual override and WebSocket support\n        engine = OddsEngine(\n            config=settings,\n            manual_override_manager=manual_override_manager,\n            connection_manager=connection_manager,\n        )\n\n        # Store the initialized components on the app state\n        app.state.engine = engine\n        app.state.analyzer_engine = AnalyzerEngine()\n        app.state.manual_override_manager = manual_override_manager\n        app.state.connection_manager = connection_manager\n        log.info(\"Background initialization of heavy resources completed successfully.\")\n    except Exception:\n        log.critical(\"CRITICAL: Background initialization failed.\", exc_info=True)\n        # In a real-world scenario, you might want a more robust way\n        # to signal this failure to the main application.\n        app.state.engine = None\n\n\nclass ConnectionManager:\n    \"\"\"Manages active WebSocket connections.\"\"\"\n\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        log.info(\"WebSocket ConnectionManager initialized.\")\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        log.info(\"New WebSocket connection established.\")\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n        log.info(\"WebSocket connection closed.\")\n\n    async def broadcast(self, message: dict):\n        \"\"\"Broadcasts a message to all connected clients.\"\"\"\n        if not self.active_connections:\n            return\n\n        log.info(\n            \"Broadcasting message to connected clients\",\n            client_count=len(self.active_connections),\n        )\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(message)\n            except Exception:\n                log.error(\"Error sending message to a WebSocket client.\", exc_info=True)\n\n\n# Lifespan context manager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    configure_logging()\n    log.info(\"Uvicorn is online, starting lifespan hook.\")\n\n    # 1. Perform lightweight, non-blocking startup tasks\n    settings = get_settings()\n    await cache_manager.connect(settings.REDIS_URL)\n    log.info(\"Fast, non-blocking startup tasks complete (Redis connected).\")\n\n    # 2. Schedule the heavy, synchronous initialization to run in a background thread\n    loop = asyncio.get_event_loop()\n    loop.run_in_executor(executor, _initialize_heavy_resources_sync, app)\n    log.info(\"Heavy resource initialization has been scheduled in a background thread.\")\n\n    # 3. Wait for the engine to be initialized before yielding control.\n    start_time = time.time()\n    while not hasattr(app.state, \"engine\") or app.state.engine is None:\n        if time.time() - start_time > 30:  # 30-second timeout\n            raise RuntimeError(\"Engine initialization timed out.\")\n        await asyncio.sleep(0.1)\n\n    log.info(\"Engine is initialized. Server is ready to accept requests.\")\n    yield\n\n    # --- Shutdown Sequence ---\n    log.info(\"Server shutdown sequence initiated.\")\n    if hasattr(app.state, \"engine\") and app.state.engine:\n        log.info(\"Closing HTTP client resources.\")\n        await app.state.engine.close()\n\n    await cache_manager.disconnect()\n    executor.shutdown(wait=False)\n    log.info(\"Server shutdown sequence complete.\")\n\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI(\n    title=\"Fortuna Faucet API\",\n    version=\"2.1\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\",\n)\n\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\napp.add_exception_handler(UserFriendlyException, user_friendly_exception_handler)\napp.include_router(health_router)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"http://localhost:3001\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\"],\n    allow_headers=[\"*\"],\n)\n\n# --- Robust Pathing for Frozen Executables ---\ndef resource_path(relative_path: str) -> str:\n    \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n    if getattr(sys, 'frozen', False):\n        # PyInstaller creates a temp folder and stores path in _MEIPASS\n        base_path = sys._MEIPASS\n    else:\n        # In development, the base path is the project root.\n        # This assumes the script is run from the project root.\n        base_path = os.path.abspath(\".\")\n    return os.path.join(base_path, relative_path)\n\n# --- Conditional UI Serving for Web Service Mode ---\nif os.environ.get(\"FORTUNA_MODE\") == \"webservice\":\n    import sys\n    from fastapi.staticfiles import StaticFiles\n    from starlette.responses import FileResponse\n    from starlette.middleware.base import BaseHTTPMiddleware\n\n    log.info(\"Application running in 'webservice' mode, configuring static file serving.\")\n\n    # Use the robust resource_path function to find the 'ui' directory.\n    # The spec file bundles 'staging/ui' into the 'ui' folder in the executable's root.\n    static_dir_key = \"ui\" if getattr(sys, 'frozen', False) else \"staging/ui\"\n    static_files_dir = resource_path(static_dir_key)\n    log.info(\"Resolved static files directory\", path=static_files_dir)\n\n    if not os.path.exists(static_files_dir):\n        log.error(\"Static files directory not found! UI will not be served.\", path=static_files_dir)\n    else:\n        class SpaMiddleware(BaseHTTPMiddleware):\n            async def dispatch(self, request, call_next):\n                response = await call_next(request)\n                if response.status_code == 404 and not request.url.path.startswith('/api'):\n                    index_path = os.path.join(static_files_dir, \"index.html\")\n                    if os.path.exists(index_path):\n                        return FileResponse(index_path)\n                return response\n\n        app.add_middleware(SpaMiddleware)\n\n        app.mount(\"/\", StaticFiles(directory=static_files_dir), name=\"static\")\n\ndef get_engine(request: Request) -> OddsEngine:\n    return request.app.state.engine\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        return engine.get_all_adapter_statuses()\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_favorite_odds: float = Query(2.5, ge=1.0, le=100.0),\n    min_second_favorite_odds: float = Query(4.0, ge=1.0, le=100.0),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        aggregated_data = await engine.fetch_all_odds(date_str)\n        races = [Race(**r) for r in aggregated_data.get(\"races\", [])]\n        analyzer_engine = request.app.state.analyzer_engine\n        custom_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds,\n        }\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(\"Error in /api/races/qualified\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\n@app.get(\"/api/races/filter-suggestions\")\nasync def get_filter_suggestions(engine: OddsEngine = Depends(get_engine)):\n    try:\n        date_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n        aggregated = await engine.fetch_all_odds(date_str)\n        if not aggregated or not aggregated.get(\"races\"):\n            return {\"suggestions\": {}}\n        field_sizes = [len(r[\"runners\"]) for r in aggregated[\"races\"]]\n        favorite_odds, second_favorite_odds = [], []\n        for race_data in aggregated[\"races\"]:\n            race = Race(**race_data)\n            runners = race.runners\n            if len(runners) >= 2:\n                odds_list = []\n                for runner in runners:\n                    if not runner.scratched and runner.odds:\n                        best_odd = min(\n                            (o.win for o in runner.odds.values() if o.win is not None),\n                            default=None,\n                        )\n                        if best_odd is not None:\n                            odds_list.append(float(best_odd))\n                if len(odds_list) >= 2:\n                    odds_list.sort()\n                    favorite_odds.append(odds_list[0])\n                    second_favorite_odds.append(odds_list[1])\n        return {\n            \"suggestions\": {\n                \"max_field_size\": {\"recommended\": (int(sum(field_sizes) / len(field_sizes)) if field_sizes else 10)},\n                \"min_favorite_odds\": {\"recommended\": 2.5},\n                \"min_second_favorite_odds\": {\"recommended\": 4.0},\n            }\n        }\n    except Exception:\n        log.error(\"Error generating filter suggestions\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Failed to generate suggestions\")\n\n\n@app.get(\"/api/races/source/{source_name}\", response_model=AggregatedResponse)\n@limiter.limit(\"60/minute\")\nasync def get_races_by_source(\n    source_name: str,\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(date_str, source=source_name)\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(f\"Error in /api/races/source/{source_name}\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n    ),\n    source: Optional[str] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_obj = race_date or datetime.now().date()\n        date_str = date_obj.strftime(\"%Y-%m-%d\")\n        return await engine.fetch_all_odds(date_str, source)\n    except (AdapterHttpError, AdapterConfigError) as e:\n        raise UserFriendlyException(error_key=e.__class__.__name__, details=str(e))\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise UserFriendlyException(error_key=\"default\")\n\n\nDB_PATH = \"fortuna.db\"\n\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = \"SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC\"\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    return results\n\n\n# API Models\nclass ManualDataSubmission(BaseModel):\n    request_id: str\n    content: str\n    content_type: str = \"html\"\n\n\n# New endpoints\n@app.get(\"/api/manual-overrides/pending\")\n@limiter.limit(\"60/minute\")\nasync def get_pending_overrides(\n    request: Request,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Get all pending manual override requests\"\"\"\n    pending = manager.get_pending_requests()\n    return {\"pending_requests\": [req.model_dump() for req in pending]}\n\n\n@app.post(\"/api/manual-overrides/submit\")\n@limiter.limit(\"30/minute\")\nasync def submit_manual_data(\n    request: Request,\n    submission: ManualDataSubmission,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Submit manually-provided data for a failed fetch\"\"\"\n    success = manager.submit_manual_data(\n        request_id=submission.request_id,\n        raw_content=submission.content,\n        content_type=submission.content_type,\n    )\n\n    if success:\n        return {\"status\": \"success\", \"message\": \"Manual data submitted\"}\n    else:\n        raise HTTPException(status_code=404, detail=\"Request not found\")\n\n\n@app.post(\"/api/manual-overrides/skip/{request_id}\")\n@limiter.limit(\"60/minute\")\nasync def skip_manual_override(\n    request: Request,\n    request_id: str,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Skip a manual override request\"\"\"\n    success = manager.skip_request(request_id)\n\n    if success:\n        return {\"status\": \"success\", \"message\": \"Request skipped\"}\n    else:\n        raise HTTPException(status_code=404, detail=\"Request not found\")\n\n\n@app.post(\"/api/manual-overrides/cleanup\")\n@limiter.limit(\"60/minute\")\nasync def cleanup_old_overrides(\n    request: Request,\n    max_age_hours: int = 24,\n    api_key: str = Depends(verify_api_key),\n    manager: ManualOverrideManager = Depends(lambda: app.state.manual_override_manager),\n):\n    \"\"\"Clean up old manual override requests\"\"\"\n    manager.clear_old_requests(max_age_hours)\n    return {\"status\": \"success\", \"message\": \"Old requests cleaned\"}\n\n\n@app.websocket(\"/ws/live-updates\")\nasync def websocket_endpoint(websocket: WebSocket, api_key: str = Query(...)):\n    \"\"\"WebSocket endpoint for live race updates.\"\"\"\n    try:\n        # Use the existing API key verification logic\n        # This is a synchronous call, which is fine for auth at the start.\n        # In a real-world scenario with high connection rates, you might\n        # want to make this check asynchronous if it involved I/O.\n        verify_api_key(api_key)\n    except HTTPException as e:\n        log.warning(\"WebSocket connection rejected due to invalid API key.\")\n        await websocket.close(code=4001, reason=f\"Authentication failed: {e.detail}\")\n        return\n\n    manager = websocket.app.state.connection_manager\n    await manager.connect(websocket)\n    try:\n        # Keep the connection alive, listening for messages (if any)\n        while True:\n            # You could implement logic here to handle incoming messages if needed\n            # For now, it's just a broadcast-only connection\n            await websocket.receive_text()\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        log.info(\"Client disconnected from WebSocket.\")\n\n\n@app.post(\"/api/races/parse-manual\", response_model=List[Race])\n@limiter.limit(\"30/minute\")\nasync def parse_manual_html(\n    request: Request,\n    parse_request: ManualParseRequest,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    \"\"\"\n    Manually parses a block of HTML using a specified adapter.\n    \"\"\"\n    try:\n        adapter = engine.get_adapter(parse_request.adapter_name)\n        if not adapter:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Adapter '{parse_request.adapter_name}' not found.\",\n            )\n\n        # The _parse_races method is synchronous, so we run it in a thread\n        # to avoid blocking the asyncio event loop.\n        loop = asyncio.get_event_loop()\n        parsed_races_data = await loop.run_in_executor(executor, adapter._parse_races, parse_request.html_content)\n\n        # Validate the parsed data with the Race model\n        validated_races = [Race(**race_data) for race_data in parsed_races_data]\n        return validated_races\n\n    except Exception as e:\n        log.error(\"Error during manual parsing\", exc_info=True)\n        raise HTTPException(status_code=500, detail=f\"An error occurred: {e}\")\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport json\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport redis\nimport redis.asyncio as redis_async\nimport structlog\nfrom pydantic import ValidationError\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base_adapter_v3 import BaseAdapterV3\nfrom .adapters.betfair_adapter import BetfairAdapter\n\n# from .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.fanduel_adapter import FanDuelAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.horseracingnation_adapter import HorseRacingNationAdapter\nfrom .adapters.nyrabets_adapter import NYRABetsAdapter\nfrom .adapters.oddschecker_adapter import OddscheckerAdapter\nfrom .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .adapters.punters_adapter import PuntersAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.racingtv_adapter import RacingTVAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.tab_adapter import TabAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\nfrom .adapters.xpressbet_adapter import XpressbetAdapter\nfrom .config import get_settings\nfrom .core.exceptions import AdapterConfigError\nfrom .core.exceptions import AdapterHttpError\nfrom .manual_override_manager import ManualOverrideManager\nfrom .models import AggregatedResponse\nfrom .models import Race\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddsEngine:\n    def __init__(\n        self,\n        config=None,\n        manual_override_manager: ManualOverrideManager = None,\n        connection_manager=None,\n    ):\n        # THE FIX: Import the cache_manager singleton here to ensure tests can\n        # patch and reload it *before* the engine is initialized.\n        from .cache_manager import cache_manager\n\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n        self.connection_manager = connection_manager\n        self.cache_manager = cache_manager\n\n        try:\n            try:\n                self.config = config or get_settings()\n                self.logger.info(\"Configuration loaded.\")\n            except ValidationError as e:\n                self.logger.warning(\n                    \"Could not load settings, possibly in test environment.\",\n                    error=str(e),\n                )\n                # Create a default/mock config or re-raise if not in a test context\n                from .config import Settings\n\n                self.config = Settings(API_KEY=\"a_secure_test_api_key_that_is_long_enough\")\n\n            # Redis is now handled entirely by the CacheManager.\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter,\n                BetfairAdapter,\n                BetfairGreyhoundAdapter,\n                BrisnetAdapter,\n                EquibaseAdapter,\n                FanDuelAdapter,\n                GbgbApiAdapter,\n                GreyhoundAdapter,\n                HarnessAdapter,\n                HorseRacingNationAdapter,\n                NYRABetsAdapter,\n                OddscheckerAdapter,\n                PuntersAdapter,\n                RacingAndSportsAdapter,\n                RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter,\n                RacingTVAdapter,\n                SportingLifeAdapter,\n                TabAdapter,\n                TheRacingApiAdapter,\n                TimeformAdapter,\n                TwinSpiresAdapter,\n                TVGAdapter,\n                XpressbetAdapter,\n                PointsBetGreyhoundAdapter,\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    self.logger.info(f\"Attempting to initialize adapter: {adapter_cls.__name__}\")\n                    adapter_instance = adapter_cls(config=self.config)\n                    self.logger.info(f\"Successfully initialized adapter: {adapter_cls.__name__}\")\n                    if manual_override_manager and getattr(adapter_instance, \"supports_manual_override\", False):\n                        adapter_instance.enable_manual_override(manual_override_manager)\n                    self.adapters.append(adapter_instance)\n                except AdapterConfigError as e:\n                    self.logger.warning(\n                        \"Skipping adapter due to configuration error\",\n                        adapter=adapter_cls.__name__,\n                        error=str(e),\n                    )\n                except Exception:\n                    self.logger.error(\n                        f\"An unexpected error occurred while initializing {adapter_cls.__name__}\",\n                        exc_info=True,\n                    )\n\n            # Special case for BetfairDataScientistAdapter with extra args - DISABLED\n            # try:\n            #     bds_adapter = BetfairDataScientistAdapter(\n            #         model_name=\"ThoroughbredModel\",\n            #         url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n            #         config=self.config,\n            #     )\n            #     if manual_override_manager and getattr(bds_adapter, \"supports_manual_override\", False):\n            #         bds_adapter.enable_manual_override(manual_override_manager)\n            #     self.adapters.append(bds_adapter)\n            # except Exception:\n            #     self.logger.warning(\n            #         \"Failed to initialize adapter: BetfairDataScientistAdapter\",\n            #         exc_info=True,\n            #     )\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            # Initialize semaphore for concurrency limiting\n            self.semaphore = asyncio.Semaphore(self.config.MAX_CONCURRENT_REQUESTS)\n            self.logger.info(\n                \"Concurrency semaphore initialized\",\n                limit=self.config.MAX_CONCURRENT_REQUESTS,\n            )\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def get_from_cache(self, key):\n        return await self.cache_manager.get(key)\n\n    async def set_in_cache(self, key, value, ttl=300):\n        # THE FIX: The keyword argument is 'ttl_seconds', not 'ttl'.\n        await self.cache_manager.set(key, value, ttl_seconds=ttl)\n\n    async def _fetch_with_semaphore(self, adapter: BaseAdapterV3, date: str):\n        \"\"\"Acquires the semaphore before fetching data from an adapter.\"\"\"\n        async with self.semaphore:\n            return await self._time_adapter_fetch(adapter, date)\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapterV3, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n        attempted_url = None\n\n        try:\n            race_data_list = await adapter.get_races(date)\n            races = [Race(**race_data) for race_data in race_data_list]\n            is_success = True\n        except AdapterHttpError as e:\n            self.logger.error(\n                \"HTTP failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                status_code=e.status_code,\n                url=e.url,\n                exc_info=False,\n            )\n            error_message = f\"HTTP Error {e.status_code} for {e.url}\"\n            attempted_url = e.url\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True,\n            )\n            error_message = str(e)\n            races = [\n                Race(\n                    id=f\"error_{adapter.source_name.lower()}\",\n                    venue=adapter.source_name,\n                    race_number=0,\n                    start_time=datetime.now(),\n                    runners=[],\n                    source=adapter.source_name,\n                    is_error_placeholder=True,\n                    error_message=error_message,\n                )\n            ]\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n                \"attempted_url\": attempted_url,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        races_copy = deepcopy(races)\n        race_map: Dict[str, Race] = {}\n        for race in races_copy:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def _broadcast_update(self, data: Dict[str, Any]):\n        \"\"\"Helper to broadcast data if the connection manager is available.\"\"\"\n        if self.connection_manager:\n            await self.connection_manager.broadcast(data)\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached and broadcasted via WebSocket.\n        \"\"\"\n        # Construct a cache key\n        cache_key = f\"fortuna_engine_races:{date}:{source_filter or 'all'}\"\n        cached_data = await self.get_from_cache(cache_key)\n        if cached_data:\n            log.info(\"Cache hit for fetch_all_odds\", key=cache_key)\n            return json.loads(cached_data)\n\n        log.info(\"Cache miss for fetch_all_odds\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._fetch_with_semaphore(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        errors = []\n\n        for i, result in enumerate(results):\n            adapter = target_adapters[i]\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed with an unhandled exception\", adapter=adapter.source_name, error=result)\n                errors.append({\n                    \"adapter_name\": adapter.source_name,\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                    \"attempted_url\": \"Unknown\"\n                })\n                source_infos.append({\n                    \"name\": adapter.source_name,\n                    \"status\": \"FAILED\",\n                    \"error_message\": f\"Unhandled exception: {str(result)}\",\n                })\n            else:\n                _adapter_name, adapter_result, _duration = result\n                source_info = adapter_result.get(\"source_info\", {})\n                source_infos.append(source_info)\n                if source_info.get(\"status\") == \"SUCCESS\":\n                    all_races.extend(adapter_result.get(\"races\", []))\n                else:\n                    errors.append({\n                        \"adapter_name\": adapter.source_name,\n                        \"error_message\": source_info.get(\"error_message\", \"Unknown error\"),\n                        \"attempted_url\": source_info.get(\"attempted_url\")\n                    })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            errors=errors,\n            source_info=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n                \"total_errors\": len(errors),\n            },\n        )\n\n        response_data = response_obj.model_dump(by_alias=True)\n\n        # Set the result in the cache\n        await self.set_in_cache(cache_key, json.dumps(response_data, default=str), ttl=300)\n        await self._broadcast_update(response_data)\n        return response_data\n",
    "python_service/manual_override_manager.py": "# python_service/manual_override_manager.py\nimport hashlib\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n\nclass ManualOverrideRequest(BaseModel):\n    request_id: str\n    adapter_name: str\n    url: str\n    timestamp: datetime = Field(default_factory=datetime.now)\n    status: str = \"pending\"  # pending, submitted, skipped\n\n\nclass ManualOverrideManager:\n    def __init__(self):\n        self._requests: Dict[str, ManualOverrideRequest] = {}\n        self._data: Dict[str, Tuple[str, str]] = {}  # request_id -> (content, content_type)\n\n    def _generate_id(self, adapter_name: str, url: str) -> str:\n        \"\"\"Generates a consistent ID for a given adapter and URL.\"\"\"\n        return hashlib.sha256(f\"{adapter_name}:{url}\".encode()).hexdigest()[:16]\n\n    def register_failure(self, adapter_name: str, url: str) -> str:\n        \"\"\"\n        Registers a failed fetch attempt and returns a unique request ID.\n        If a pending request for this exact resource already exists, it returns the existing ID.\n        \"\"\"\n        request_id = self._generate_id(adapter_name, url)\n        if request_id not in self._requests or self._requests[request_id].status != \"pending\":\n            request = ManualOverrideRequest(request_id=request_id, adapter_name=adapter_name, url=url)\n            self._requests[request_id] = request\n        return request_id\n\n    def submit_manual_data(self, request_id: str, raw_content: str, content_type: str) -> bool:\n        \"\"\"Submits manual data for a pending request.\"\"\"\n        if request_id in self._requests and self._requests[request_id].status == \"pending\":\n            self._data[request_id] = (raw_content, content_type)\n            self._requests[request_id].status = \"submitted\"\n            return True\n        return False\n\n    def skip_request(self, request_id: str) -> bool:\n        \"\"\"Marks a pending request as skipped.\"\"\"\n        if request_id in self._requests and self._requests[request_id].status == \"pending\":\n            self._requests[request_id].status = \"skipped\"\n            return True\n        return False\n\n    def get_pending_requests(self) -> List[ManualOverrideRequest]:\n        \"\"\"Returns a list of all requests that are currently pending.\"\"\"\n        return [req for req in self._requests.values() if req.status == \"pending\"]\n\n    def get_manual_data(self, adapter_name: str, url: str) -> Optional[Tuple[str, str]]:\n        \"\"\"\n        Retrieves submitted manual data for a given adapter and URL, if it exists.\n        Once retrieved, the data is consumed and will not be returned again.\n        \"\"\"\n        request_id = self._generate_id(adapter_name, url)\n        if request_id in self._data:\n            # Data is single-use; remove it after retrieval.\n            return self._data.pop(request_id)\n        return None\n\n    def clear_old_requests(self, max_age_hours: int = 24):\n        \"\"\"Removes requests and associated data older than a specified age.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=max_age_hours)\n        old_request_ids = [req_id for req_id, req in self._requests.items() if req.timestamp < cutoff]\n        for req_id in old_request_ids:\n            self._requests.pop(req_id, None)\n            self._data.pop(req_id, None)\n",
    "python_service/notifications.py": "# python_service/notifications.py\n\nimport sys\n\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\n\ndef send_toast(title: str, message: str):\n    \"\"\"\n    Sends a desktop notification. This function is platform-aware and will only\n    attempt to send a toast on Windows. On other operating systems, it will\n    log the notification content.\n    \"\"\"\n    if sys.platform == \"win32\":\n        try:\n            from windows_toasts import Toast\n            from windows_toasts import WindowsToaster\n\n            toaster = WindowsToaster(title)\n            new_toast = Toast()\n            new_toast.text_fields = [message]\n            toaster.show_toast(new_toast)\n            log.info(\"Sent Windows toast notification.\", title=title, message=message)\n        except ImportError:\n            log.warning(\n                \"windows_toasts library not found, skipping notification.\",\n                recommendation=\"Install with: pip install windows-toasts\",\n            )\n        except Exception:\n            log.error(\"Failed to send Windows toast notification.\", exc_info=True)\n    else:\n        log.info(\n            \"Skipping toast notification on non-Windows platform.\",\n            platform=sys.platform,\n            title=title,\n            message=message,\n        )\n",
    "python_service/run_api.py": "# python_service/run_api.py\n\nimport uvicorn\n\n\ndef main():\n    # This entry point is for the packaged application\n    uvicorn.run(\"python_service.api:app\", host=\"127.0.0.1\", port=8000, reload=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/install_fortuna_gui.bat": "@echo off\nREM Interactive MSI installation with standard Windows UI\n\ntitle Fortuna Faucet Installation Wizard\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Administrator privileges required\n    echo Please right-click this file and select \"Run as Administrator\"\n    pause\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" /L*v \"%TEMP%\\fortuna_install.log\"\n\nif %errorlevel% equ 0 (\n    echo Installation completed successfully!\n    echo Access dashboard at: http://localhost:3000\n) else (\n    echo Installation failed. Log: %TEMP%\\fortuna_install.log\n)\npause",
    "web_platform/api_gateway/src/server.ts": "// server.ts - Complete API Gateway with Database Integration and WebSocket\n\nimport express from 'express';\nimport { createServer } from 'http';\nimport { Server as SocketServer } from 'socket.io';\nimport cors from 'cors';\nimport sqlite3 from 'sqlite3';\nimport { open, Database } from 'sqlite';\nimport path from 'path';\n\n// Types\ninterface Race {\n  race_id: string;\n  track_name: string;\n  race_number: number | null;\n  post_time: string | null;\n  checkmate_score: number;\n  qualified: boolean;\n  trifecta_factors_json: string | null;\n  raw_data_json: string | null;\n  updated_at: string;\n}\n\ninterface AdapterStatus {\n  adapter_name: string;\n  status: string;\n  last_run: string;\n  races_found: number;\n  execution_time_ms: number;\n  error_message: string | null;\n}\n\n// Database Service\nclass DatabaseService {\n  private db: Database | null = null;\n  private dbPath: string;\n\n  constructor() {\n    this.dbPath = process.env.FORTUNA_DB_PATH || path.join(process.cwd(), '..', '..', 'shared_database', 'races.db');\n  }\n\n  async connect(): Promise<void> {\n    try {\n      this.db = await open({\n        filename: this.dbPath,\n        driver: sqlite3.Database\n      });\n      console.log(`[INFO] Connected to database: ${this.dbPath}`);\n    } catch (error) {\n      console.error('[ERROR] Failed to connect to database:', error);\n      throw error;\n    }\n  }\n\n  async getQualifiedRaces(): Promise<Race[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const races = await this.db.all<Race[]>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        WHERE qualified = 1\n        ORDER BY checkmate_score DESC, post_time ASC\n      `);\n      return races;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch qualified races:', error);\n      return [];\n    }\n  }\n\n  async getAllRaces(): Promise<Race[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const races = await this.db.all<Race[]>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        ORDER BY post_time ASC\n      `);\n      return races;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch all races:', error);\n      return [];\n    }\n  }\n\n  async getAdapterStatuses(): Promise<AdapterStatus[]> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const statuses = await this.db.all<AdapterStatus[]>(`\n        SELECT adapter_name, status, last_run, races_found,\n               execution_time_ms, error_message\n        FROM adapter_status\n        ORDER BY last_run DESC\n      `);\n      return statuses;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch adapter statuses:', error);\n      return [];\n    }\n  }\n\n  async getRaceById(raceId: string): Promise<Race | null> {\n    if (!this.db) throw new Error('Database not connected');\n    try {\n      const race = await this.db.get<Race>(`\n        SELECT race_id, track_name, race_number, post_time,\n               checkmate_score, qualified, trifecta_factors_json,\n               raw_data_json, updated_at\n        FROM live_races\n        WHERE race_id = ?\n      `, raceId);\n      return race || null;\n    } catch (error) {\n      console.error('[ERROR] Failed to fetch race by ID:', error);\n      return null;\n    }\n  }\n}\n\n// Initialize Express and Socket.IO\nconst app = express();\nconst httpServer = createServer(app);\nconst io = new SocketServer(httpServer, {\n  cors: { origin: process.env.ALLOWED_ORIGINS || 'http://localhost:3000' }\n});\n\napp.use(cors());\napp.use(express.json());\n\nconst dbService = new DatabaseService();\n\n// API Endpoints\napp.get('/api/status', (req, res) => {\n  res.json({\n    status: 'online',\n    timestamp: new Date().toISOString(),\n    service: 'Checkmate API Gateway'\n  });\n});\n\napp.get('/api/races', async (req, res) => {\n  try {\n    const races = await dbService.getAllRaces();\n    res.json({ success: true, count: races.length, races });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch races' });\n  }\n});\n\napp.get('/api/races/qualified', async (req, res) => {\n  try {\n    const races = await dbService.getQualifiedRaces();\n    res.json({ success: true, count: races.length, races });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch qualified races' });\n  }\n});\n\napp.get('/api/races/:raceId', async (req, res) => {\n  try {\n    const race = await dbService.getRaceById(req.params.raceId);\n    if (race) {\n      res.json({ success: true, race });\n    } else {\n      res.status(404).json({ success: false, error: 'Race not found' });\n    }\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch race' });\n  }\n});\n\napp.get('/api/adapters/status', async (req, res) => {\n  try {\n    const statuses = await dbService.getAdapterStatuses();\n    res.json({ success: true, count: statuses.length, adapters: statuses });\n  } catch (error) {\n    res.status(500).json({ success: false, error: 'Failed to fetch adapter statuses' });\n  }\n});\n\n// WebSocket Connection Handling\nio.on('connection', (socket) => {\n  console.log(`[WebSocket] Client connected: ${socket.id}`);\n\n  dbService.getQualifiedRaces().then(races => {\n    socket.emit('races_update', { races });\n  });\n\n  dbService.getAdapterStatuses().then(statuses => {\n    socket.emit('adapters_update', { adapters: statuses });\n  });\n\n  socket.on('disconnect', () => {\n    console.log(`[WebSocket] Client disconnected: ${socket.id}`);\n  });\n\n  socket.on('request_update', async () => {\n    const races = await dbService.getQualifiedRaces();\n    const statuses = await dbService.getAdapterStatuses();\n    socket.emit('races_update', { races });\n    socket.emit('adapters_update', { adapters: statuses });\n  });\n});\n\n// Broadcast updates to all clients periodically\nasync function broadcastUpdates() {\n  try {\n    const races = await dbService.getQualifiedRaces();\n    const statuses = await dbService.getAdapterStatuses();\n\n    io.emit('races_update', { races });\n    io.emit('adapters_update', { adapters: statuses });\n  } catch (error) {\n    console.error('[ERROR] Failed to broadcast updates:', error);\n  }\n}\n\n// Start Server\nconst PORT = process.env.PORT || 8080;\n\nasync function startServer() {\n  try {\n    await dbService.connect();\n\n    httpServer.listen(PORT, () => {\n      console.log('='.repeat(70));\n      console.log(`  Checkmate API Gateway`);\n      console.log(`  Running on port ${PORT}`);\n      console.log(`  Database: ${dbService['dbPath']}`);\n      console.log('='.repeat(70));\n    });\n\n    setInterval(broadcastUpdates, 15000);\n\n  } catch (error) {\n    console.error('[FATAL] Failed to start server:', error);\n    process.exit(1);\n  }\n}\n\n// Graceful shutdown\nprocess.on('SIGINT', async () => {\n  console.log('\\n[INFO] Shutting down gracefully...');\n  httpServer.close();\n  process.exit(0);\n});\n\nprocess.on('SIGTERM', async () => {\n  console.log('\\n[INFO] Shutting down gracefully...');\n  httpServer.close();\n  process.exit(0);\n});\n\nstartServer();",
    "web_platform/frontend/.gitignore": "# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.\n\n# Dependencies\n/node_modules\n/.pnp\n.pnp.js\n\n# Testing\n/coverage\n\n# Next.js\n/.next/\n/out/\n\n# Production\n/build\n\n# Misc\n.DS_Store\n*.pem\n\n# Local .env files\n.env.local\n.env.development.local\n.env.test.local\n.env.production.local\n\n# Log files\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nlerna-debug.log*\n\n# Editor directories and files\n.vscode\n.idea\n*.suo\n*.ntvs*\n*.njsproj\n*.sln\n*.sw?",
    "web_platform/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_platform/frontend/next-env.d.ts": "/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.\n",
    "web_platform/frontend/src/components/LiveRaceDashboard.tsx": "// web_platform/frontend/src/components/LiveRaceDashboard.tsx\n'use client';\n\nimport React, { useState, useEffect, useCallback } from 'react';\nimport { useQuery, useQueryClient } from '@tanstack/react-query';\nimport { RaceFilters } from './RaceFilters';\nimport { RaceCard } from './RaceCard';\nimport { RaceCardSkeleton } from './RaceCardSkeleton';\nimport { EmptyState } from './EmptyState';\nimport { ErrorDisplay } from './ErrorDisplay';\nimport { Race, SourceInfo, AdapterError, AggregatedRacesResponse } from '../types/racing';\nimport { useWebSocket } from '../hooks/useWebSocket';\nimport { StatusDetailModal } from './StatusDetailModal';\nimport ManualOverridePanel from './ManualOverridePanel';\nimport { LiveModeToggle } from './LiveModeToggle';\nimport { AdapterStatusPanel } from './AdapterStatusPanel';\n\n// Type for the backend process status received from Electron main\ntype BackendState = 'starting' | 'running' | 'error' | 'stopped';\ninterface BackendStatus {\n  state: BackendState;\n  logs: string[];\n}\n\ninterface RaceFilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nconst fetchAdapterStatuses = async (apiKey: string | null, port: number | null): Promise<SourceInfo[]> => {\n  if (!apiKey || !port) {\n    throw new Error('API key or port not available.');\n  }\n  const response = await fetch(`http://localhost:${port}/api/adapters/status`, {\n    headers: { 'X-API-Key': apiKey, 'Content-Type': 'application/json' },\n  });\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n  return response.json();\n};\n\nconst fetchQualifiedRaces = async (apiKey: string | null, port: number | null, params: RaceFilterParams): Promise<AggregatedRacesResponse> => {\n  if (!apiKey || !port) {\n    throw new Error('API key or port not available');\n  }\n  const url = new URL(`http://localhost:${port}/api/races`);\n\n  const response = await fetch(url.toString(), {\n    headers: { 'X-API-Key': apiKey },\n  });\n\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n\n  return response.json();\n};\n\n\nconst BackendErrorPanel = ({ logs, onRestart }: { logs: string[]; onRestart: () => void }) => (\n  <div className=\"bg-slate-800 p-6 rounded-lg border border-red-500/50 text-white\">\n    <h2 className=\"text-2xl font-bold text-red-400 mb-4\">Backend Service Error</h2>\n    <p className=\"text-slate-400 mb-4\">The backend data service failed to start or has crashed. Below are the most recent diagnostic messages.</p>\n    <div className=\"bg-black p-4 rounded-md font-mono text-sm text-slate-300 h-64 overflow-y-auto mb-4\">\n      {logs.map((log, index) => (\n        <p key={index} className=\"whitespace-pre-wrap\">{`> ${log}`}</p>\n      ))}\n    </div>\n    <button\n      onClick={onRestart}\n      className=\"w-full px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 transition-colors\"\n    >\n      Restart Backend Service\n    </button>\n  </div>\n);\n\n// New Sub-Component to display an error from a specific adapter\nconst ErrorCard = ({ source, message }: { source: string; message: string }) => (\n  <div className=\"bg-slate-800 rounded-lg p-4 border border-red-500/50 flex flex-col justify-between\">\n    <div>\n      <h3 className=\"font-bold text-red-400 text-lg\">{source} Failed</h3>\n      <p className=\"text-slate-400 text-sm mt-2\">{message}</p>\n    </div>\n    <div className=\"mt-4 text-xs text-slate-500\">\n      <p>This adapter failed to fetch data. This is not a critical error; other adapters may provide the necessary data.</p>\n    </div>\n  </div>\n);\n\n// New Sub-Component to render the grid of races or error cards\nconst RaceGrid = ({ races }: { races: Race[] }) => (\n  <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n    {races.map(race =>\n      race.isErrorPlaceholder ? (\n        <ErrorCard key={race.id} source={race.venue} message={race.errorMessage || 'An unknown error occurred.'} />\n      ) : (\n        <RaceCard key={race.id} race={race} />\n      )\n    )}\n  </div>\n);\n\nconst useBackendStatus = () => {\n  const [backendStatus, setBackendStatus] = useState<BackendStatus>({ state: 'starting', logs: [] });\n\n  useEffect(() => {\n    let isMounted = true;\n\n    const getInitialStatus = async () => {\n      if (window.electronAPI?.getBackendStatus) {\n        try {\n          const initialStatus = await window.electronAPI.getBackendStatus();\n          if (isMounted) setBackendStatus(initialStatus);\n        } catch (error) {\n          console.error(\"Failed to get initial backend status:\", error);\n          if (isMounted) setBackendStatus({ state: 'error', logs: ['Failed to query backend status from main process.'] });\n        }\n      } else {\n          setBackendStatus({ state: 'running', logs: ['In a web-only environment, backend is assumed to be running.'] });\n      }\n    };\n\n    if (window.electronAPI?.onBackendStatusUpdate) {\n      const unsubscribe = window.electronAPI.onBackendStatusUpdate((status: BackendStatus) => {\n        if (isMounted) setBackendStatus(status);\n      });\n      getInitialStatus();\n      return () => {\n        isMounted = false;\n        if (typeof unsubscribe === 'function') {\n          unsubscribe();\n        }\n      };\n    } else {\n        getInitialStatus();\n    }\n  }, []);\n\n  return backendStatus;\n};\n\nexport const LiveRaceDashboard = React.memo(() => {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [adapterErrors, setAdapterErrors] = useState<AdapterError[]>([]);\n  const backendStatus = useBackendStatus();\n  const [apiKey, setApiKey] = useState<string | null>(null);\n  const [apiPort, setApiPort] = useState<number | null>(null);\n  const queryClient = useQueryClient();\n\n  // Get API key and port on component mount\n  useEffect(() => {\n    const fetchApiConfig = async () => {\n      try {\n        // Use a single check for electronAPI\n        const isElectron = typeof window.electronAPI !== 'undefined';\n\n        const key = isElectron\n          ? await window.electronAPI.getApiKey()\n          : null;\n\n        if (key) {\n          setApiKey(key);\n        } else {\n          console.error('API key could not be retrieved.');\n          // Fallback to a default or test key if necessary for web-only mode\n          setApiKey(\"a_secure_test_api_key_that_is_long_enough\");\n        }\n\n        const port = isElectron\n          ? await window.electronAPI.getApiPort()\n          : 8000; // Default port for web-only development\n\n        if (port) {\n          setApiPort(port);\n        } else {\n          console.error('API port could not be retrieved, defaulting to 8000.');\n          setApiPort(8000);\n        }\n\n      } catch (error) {\n        console.error('Error fetching API config:', error);\n        // Provide fallbacks in case of an error during retrieval\n        setApiKey(\"a_secure_test_api_key_that_is_long_enough\");\n        setApiPort(8000);\n      }\n    };\n    fetchApiConfig();\n  }, []);\n\n  const [params, setParams] = useState<RaceFilterParams>({\n    maxFieldSize: 10,\n    minFavoriteOdds: 2.5,\n    minSecondFavoriteOdds: 4.0,\n  });\n\n  const {\n    data,\n    status: connectionStatus,\n    error: errorDetails,\n    refetch,\n  } = useQuery({\n    queryKey: ['aggregatedRaces', apiKey, apiPort], // Updated query key\n    queryFn: () => fetchQualifiedRaces(apiKey, apiPort, params),\n    enabled: backendStatus.state === 'running' && !!apiKey && !!apiPort,\n    refetchOnWindowFocus: true,\n  });\n\n  // Update state when data is successfully fetched\n  useEffect(() => {\n    if (data) {\n      setRaces(data.races || []);\n      setAdapterErrors(data.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [data]);\n\n  const { data: liveData, isConnected: isLiveConnected } = useWebSocket<AggregatedRacesResponse>(\n    '/ws/live-updates',\n    { apiKey, port: apiPort }\n  );\n\n  // Effect to update state when new live data arrives\n  useEffect(() => {\n    if (liveData) {\n      console.log('Received live data update:', liveData);\n      // Update the query cache and local state with the new data\n      queryClient.setQueryData(['aggregatedRaces', apiKey], liveData);\n      setRaces(liveData.races || []);\n      setAdapterErrors(liveData.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [liveData, queryClient, apiKey]);\n\n  const [lastUpdate, setLastUpdate] = useState<Date | null>(null);\n  const [isModalOpen, setIsModalOpen] = useState(false);\n\n\n  const handleParamsChange = useCallback((newParams: RaceFilterParams) => {\n    setParams(newParams);\n  }, []);\n\n  const handleParseSuccess = (adapterName: string, parsedRaces: Race[]) => {\n    queryClient.setQueryData(['qualifiedRaces', apiKey, params], (oldData: { races: Race[], source_info: SourceInfo[] } | undefined) => {\n      if (!oldData) return { races: parsedRaces, source_info: [] };\n\n      // 1. Remove the placeholder error card for this adapter\n      const otherRaces = oldData.races.filter(race => race.source !== adapterName);\n\n      // 2. Merge the new races in\n      const updatedRaces = [...otherRaces, ...parsedRaces].sort(\n        (a, b) => new Date(a.start_time).getTime() - new Date(b.start_time).getTime()\n      );\n\n      // 3. Update source_info to remove the failed source\n      const updatedSourceInfo = oldData.source_info.filter(s => s.name !== adapterName);\n\n      return { races: updatedRaces, source_info: updatedSourceInfo };\n    });\n  };\n\n  const renderContent = () => {\n    // Priority 1: Backend process has failed.\n    if (backendStatus.state === 'error') {\n      return <BackendErrorPanel logs={backendStatus.logs} onRestart={() => window.electronAPI.restartBackend()} />;\n    }\n\n    if (backendStatus.state === 'stopped') {\n        return <EmptyState\n            title=\"Backend Service Stopped\"\n            message=\"The backend data service is not running. Please start it to see live race data.\"\n            actionButton={<button onClick={() => window.electronAPI.restartBackend()} className=\"mt-4 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700\">Start Backend Service</button>}\n        />;\n    }\n\n    // Priority 2: Backend is starting or initial fetch is happening.\n    const isLoading = backendStatus.state === 'starting' || (connectionStatus === 'pending' && !data);\n    if (isLoading) {\n        return (\n            <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n                {[...Array(8)].map((_, i) => <RaceCardSkeleton key={i} />)}\n            </div>\n        );\n    }\n\n    // Priority 3: API connection is offline.\n    if (connectionStatus === 'error') {\n      try {\n        const errorInfo = JSON.parse((errorDetails as Error).message);\n        return <ErrorDisplay error={errorInfo.error} />;\n      } catch (e) {\n        return <EmptyState\n            title=\"API Connection Offline\"\n            message={(errorDetails as Error)?.message || \"The backend is running, but the dashboard could not connect to its API.\"}\n            actionButton={<button onClick={() => refetch()} className=\"mt-4 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700\">Retry Connection</button>}\n        />;\n      }\n    }\n\n    // Priority 4: No races found after a successful fetch.\n    if (!races || races.length === 0) {\n      return <EmptyState\n          title=\"No Races Found\"\n          message=\"No races matched the specified criteria for the selected date. Please try different filters.\"\n      />;\n    }\n\n    // Priority 5: Display the races (and any error placeholders).\n    return <RaceGrid races={races} />;\n  };\n\n  const getStatusIndicator = () => {\n    if (backendStatus.state === 'error') {\n      return { color: 'bg-red-500', text: 'Backend Error' };\n    }\n    if (backendStatus.state === 'stopped') {\n        return { color: 'bg-gray-500', text: 'Stopped' };\n    }\n    if (backendStatus.state === 'starting') {\n      return { color: 'bg-yellow-500', text: 'Backend Starting...' };\n    }\n    if (isLiveConnected) {\n      return { color: 'bg-cyan-500', text: 'Live' };\n    }\n    return { color: 'bg-yellow-500', text: 'Connecting...' };\n  };\n\n  const { color: statusColor, text: statusText } = getStatusIndicator();\n\n  return (\n    <>\n      <div className=\"space-y-6\">\n        <div className=\"flex justify-between items-start\">\n            <div className=\"text-left space-y-2\">\n                <h1 className=\"text-4xl font-bold text-white\">\ud83c\udfc7 Fortuna Faucet</h1>\n                <p className=\"text-slate-400\">\n                Last updated: {lastUpdate ? lastUpdate.toLocaleTimeString() : 'N/A'}\n                </p>\n            </div>\n            <div className=\"flex items-center gap-4\">\n                <button\n                    onClick={() => (connectionStatus === 'error' || backendStatus.state === 'error') && setIsModalOpen(true)}\n                    className={`flex items-center gap-2 px-3 py-1.5 rounded-full text-sm font-medium text-white ${statusColor} ${(connectionStatus === 'error' || backendStatus.state === 'error') ? 'cursor-pointer hover:opacity-80' : 'cursor-default'}`}\n                    data-testid=\"status-indicator\"\n                >\n                    <span className={`w-2.5 h-2.5 rounded-full bg-white ${isLiveConnected ? 'animate-pulse' : ''}`}></span>\n                    {statusText}\n                </button>\n            </div>\n        </div>\n\n        <RaceFilters onParamsChange={handleParamsChange} isLoading={connectionStatus === 'pending'} refetch={refetch} />\n\n        {adapterErrors.map(error => (\n          <ManualOverridePanel\n            key={error.adapterName}\n            adapterName={error.adapterName}\n            attemptedUrl={error.attemptedUrl || 'URL not available'}\n            apiKey={apiKey}\n            onParseSuccess={handleParseSuccess}\n          />\n        ))}\n\n        {renderContent()}\n      </div>\n\n      <StatusDetailModal\n        isOpen={isModalOpen}\n        onClose={() => setIsModalOpen(false)}\n        status={{ title: 'Connection Error', details: (errorDetails as Error)?.message || 'No specific error message was provided.' }}\n      />\n    </>\n  );\n});\n",
    "web_platform/frontend/src/components/RaceFilters.tsx": "// web_platform/frontend/src/components/RaceFilters.tsx\n'use client';\n\nimport { useState, useCallback } from 'react';\nimport { Settings, RotateCcw } from 'lucide-react';\n\ninterface FilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nexport interface RaceFiltersProps {\n  onParamsChange: (params: FilterParams) => void;\n  isLoading: boolean;\n  refetch: () => void;\n}\n\nconst DEFAULT_PARAMS: FilterParams = {\n  maxFieldSize: 10,\n  minFavoriteOdds: 2.5,\n  minSecondFavoriteOdds: 4.0,\n};\n\nexport function RaceFilters({ onParamsChange, isLoading, refetch }: RaceFiltersProps) {\n  const [params, setParams] = useState<FilterParams>(DEFAULT_PARAMS);\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  // Handle individual parameter changes\n  const handleChange = useCallback((key: keyof FilterParams, value: number) => {\n    setParams(prev => {\n      const updated = { ...prev, [key]: value };\n      onParamsChange(updated);\n      return updated;\n    });\n    // Debounce the refetch call\n    const timer = setTimeout(() => {\n      refetch();\n    }, 500);\n    return () => clearTimeout(timer);\n  }, [onParamsChange, refetch]);\n\n  // Reset to defaults\n  const handleReset = useCallback(() => {\n    setParams(DEFAULT_PARAMS);\n    onParamsChange(DEFAULT_PARAMS);\n    refetch();\n  }, [onParamsChange, refetch]);\n\n  return (\n    <div className=\"bg-gradient-to-r from-slate-800 to-slate-900 rounded-lg p-4 mb-6 border border-slate-700\">\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-2\">\n          <Settings className=\"w-5 h-5 text-amber-500\" />\n          <h3 className=\"text-lg font-semibold text-white\">Race Filters</h3>\n        </div>\n        <button\n          onClick={() => setIsExpanded(!isExpanded)}\n          className=\"text-sm text-slate-400 hover:text-slate-200 transition\"\n        >\n          {isExpanded ? 'Hide' : 'Show'}\n        </button>\n      </div>\n\n      {isExpanded && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 pt-4 border-t border-slate-700\">\n          {/* Max Field Size */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Max Field Size\n              <span className=\"text-amber-500 ml-2\">{params.maxFieldSize}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2\"\n              max=\"20\"\n              value={params.maxFieldSize}\n              onChange={(e) => handleChange('maxFieldSize', parseInt(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Filters races with larger fields</p>\n          </div>\n\n          {/* Min Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"1.5\"\n              max=\"5\"\n              step=\"0.1\"\n              value={params.minFavoriteOdds}\n              onChange={(e) => handleChange('minFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = pickier favorites</p>\n          </div>\n\n          {/* Min Second Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min 2nd Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minSecondFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2.0\"\n              max=\"8\"\n              step=\"0.1\"\n              value={params.minSecondFavoriteOdds}\n              onChange={(e) => handleChange('minSecondFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = better odds separation</p>\n          </div>\n\n          {/* Reset Button */}\n          <div className=\"md:col-span-3 flex justify-end pt-4 border-t border-slate-700\">\n            <button\n              onClick={handleReset}\n              disabled={isLoading}\n              className=\"inline-flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 text-slate-200 rounded text-sm font-medium transition disabled:opacity-50\"\n            >\n              <RotateCcw className=\"w-4 h-4\" />\n              Reset to Defaults\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n}\n",
    "web_platform/frontend/src/components/TrifectaFactors.tsx": "// TrifectaFactors.tsx - FINAL, DYNAMIC VERSION\n'use client';\nimport React from 'react';\n\ninterface TrifectaFactorsProps {\n  factorsJson: string | null;\n}\n\nexport function TrifectaFactors({ factorsJson }: TrifectaFactorsProps) {\n  if (!factorsJson) {\n    return <div className=\"text-sm text-gray-500\">No analysis factors available.</div>;\n  }\n\n  try {\n    const factors = JSON.parse(factorsJson);\n    const positiveFactors = Object.entries(factors).filter(([key, value]: [string, any]) => value.ok);\n\n    if (positiveFactors.length === 0) {\n      return <div className=\"text-sm text-gray-500\">No positive factors identified.</div>;\n    }\n\n    return (\n      <div className=\"mt-2 text-xs\">\n        <h4 className=\"font-semibold mb-1\">Key Factors:</h4>\n        <ul className=\"list-disc list-inside space-y-1\">\n          {positiveFactors.map(([key, value]: [string, any]) => (\n            <li key={key} className=\"text-gray-700\">\n              <span className=\"font-medium text-green-600\">\u2713</span> {value.reason} ({value.points > 0 ? `+${value.points}` : value.points} pts)\n            </li>\n          ))}\n        </ul>\n      </div>\n    );\n  } catch (error) {\n    console.error(\"Failed to parse trifecta factors:\", error);\n    return <div className=\"text-sm text-red-500\">Error displaying analysis factors.</div>;\n  }\n}",
    "web_platform/frontend/src/hooks/useRealTimeRaces.ts": "import { useState, useEffect } from 'react';\nimport { io, Socket } from 'socket.io-client';\nimport { Race } from '../types/racing';\n\nconst API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8080';\n\nexport function useRealTimeRaces() {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [isConnected, setIsConnected] = useState(false);\n\n  useEffect(() => {\n    const socket: Socket = io(API_URL);\n\n    socket.on('connect', () => setIsConnected(true));\n    socket.on('disconnect', () => setIsConnected(false));\n\n    socket.on('races_update', (data: { races: Race[] }) => {\n      if (data && Array.isArray(data.races)) {\n        setRaces(data.races);\n      }\n    });\n\n    // Cleanup on component unmount\n    return () => {\n      socket.disconnect();\n    };\n  }, []);\n\n  return { races, isConnected };\n}",
    "web_platform/frontend/src/types/racing.ts": "// web_platform/frontend/src/types/racing.ts\n// This file is the central source of truth for frontend racing data types.\n\n// --- Runner & Odds Interfaces ---\nexport interface OddsData {\n  win: number | null;\n  place: number | null;\n  show: number | null;\n  source: string;\n  last_updated: string;\n}\n\nexport interface Runner {\n  number: number;\n  name: string;\n  scratched: boolean;\n  selection_id?: number;\n  odds: Record<string, OddsData>;\n  jockey?: string;\n  trainer?: string;\n}\n\n// --- Race Interface ---\n// This interface matches the shape of the data returned by the API for the dashboard.\nexport interface Race {\n  id: string;\n  venue: string;\n  race_number: number;\n  start_time: string;\n  runners: Runner[];\n  source: string;\n  qualification_score?: number;\n  distance?: string;\n  surface?: string;\n  favorite?: Runner;\n  isErrorPlaceholder?: boolean;\n  errorMessage?: string;\n}\n\n// --- API Response Interfaces ---\nexport interface SourceInfo {\n  name: string;\n  status: 'SUCCESS' | 'FAILED' | 'CONFIG_ERROR' | 'PENDING';\n  racesFetched: number;\n  fetchDuration: number;\n  errorMessage?: string;\n  attemptedUrl?: string;\n}\n\nexport interface AdapterError {\n  adapterName: string;\n  errorMessage: string;\n  attemptedUrl?: string;\n}\n\nexport interface AggregatedRacesResponse {\n  races: Race[];\n  errors: AdapterError[];\n  source_info: SourceInfo[];\n}\n\n// --- Analysis Factor Interfaces (retained from previous version) ---\nexport interface Factor {\n    points: number;\n    ok: boolean;\n    reason: string;\n}\n\nexport interface TrifectaFactors {\n    [key: string]: Factor;\n}\n",
    "web_service/backend/adapters/base_adapter_v3.py": "# python_service/adapters/base_v3.py\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import Any\nfrom typing import AsyncGenerator\nfrom typing import List\n\nimport httpx\nimport structlog\nfrom tenacity import RetryError\nfrom tenacity import retry\nfrom tenacity import stop_after_attempt\nfrom tenacity import wait_exponential\n\nfrom ..core.exceptions import AdapterHttpError\nfrom ..manual_override_manager import ManualOverrideManager\nfrom ..models import Race\n\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    Abstract base class for all V3 data adapters.\n    Enforces a standardized fetch/parse pattern and includes robust request handling.\n    \"\"\"\n\n    def __init__(self, source_name: str, base_url: str, config=None, timeout: int = 20):\n        self.source_name = source_name\n        self.base_url = base_url\n        self.config = config\n        self.timeout = timeout\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient = None  # Injected by the engine\n        self.manual_override_manager: ManualOverrideManager = None\n        self.supports_manual_override = True  # Can be overridden by subclasses\n\n    def enable_manual_override(self, manager: ManualOverrideManager):\n        \"\"\"Injects the manual override manager into the adapter.\"\"\"\n        self.manual_override_manager = manager\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw data (e.g., HTML, JSON) for the given date.\n        This is the only method that should perform network operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        Parses the raw data retrieved by _fetch_data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> AsyncGenerator[Race, None]:\n        \"\"\"\n        Orchestrates the fetch-then-parse pipeline for the adapter.\n        This public method should not be overridden by subclasses.\n        \"\"\"\n        raw_data = None\n\n        if self.manual_override_manager:\n            # This is not a full URL, but a representative key for the fetch operation\n            # Subclasses might need to override get_races to provide a more specific URL if needed\n            lookup_key = f\"{self.base_url}/racecards/{date}\"\n            manual_data = self.manual_override_manager.get_manual_data(self.source_name, lookup_key)\n            if manual_data:\n                self.logger.info(\"Using manually submitted data for request\", url=lookup_key)\n                # Reconstruct a dictionary similar to what _fetch_data would return\n                # This may need adjustment based on adapter specifics\n                raw_data = {\"pages\": [manual_data[0]], \"date\": date}\n\n        if raw_data is None:\n            try:\n                raw_data = await self._fetch_data(date)\n            except AdapterHttpError as e:\n                if self.manual_override_manager and self.supports_manual_override:\n                    self.manual_override_manager.register_failure(self.source_name, e.url)\n                raise  # Reraise the exception to be handled by the OddsEngine\n\n        if raw_data is not None:\n            parsed_races = self._parse_races(raw_data)\n            for race in parsed_races:\n                yield race\n\n    @retry(\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        stop=stop_after_attempt(3),\n        reraise=True,  # Reraise the final exception to be caught by get_races\n    )\n    async def make_request(self, http_client: httpx.AsyncClient, method: str, url: str, **kwargs) -> httpx.Response:\n        \"\"\"\n        Makes a resilient HTTP request with built-in retry logic using tenacity.\n        \"\"\"\n        # Ensure the URL is correctly formed, whether it's relative or absolute\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url.rstrip('/')}/{url.lstrip('/')}\"\n\n        try:\n            self.logger.info(\"Making request\", method=method.upper(), url=full_url)\n            response = await http_client.request(method, full_url, timeout=self.timeout, **kwargs)\n            response.raise_for_status()  # Raise an exception for 4xx/5xx responses\n            return response\n        except httpx.HTTPStatusError as e:\n            self.logger.error(\n                \"HTTP Status Error during request\",\n                status_code=e.response.status_code,\n                url=str(e.request.url),\n            )\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=e.response.status_code,\n                url=str(e.request.url),\n            ) from e\n        except (httpx.RequestError, RetryError) as e:\n            self.logger.error(\"Request Error or Retry Error\", error=str(e))\n            raise AdapterHttpError(\n                adapter_name=self.source_name,\n                status_code=503,  # Service Unavailable\n                url=full_url,\n            ) from e\n\n    def get_status(self) -> dict:\n        \"\"\"\n        Returns a dictionary representing the adapter's current status.\n        Subclasses can extend this to include more specific health checks.\n        \"\"\"\n        return {\n            \"adapter_name\": self.source_name,\n            \"status\": \"OK\",  # Basic status; can be enhanced in subclasses\n        }\n",
    "web_service/backend/adapters/brisnet_adapter.py": "# python_service/adapters/brisnet_adapter.py\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BrisnetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for brisnet.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Brisnet\"\n    BASE_URL = \"https://www.brisnet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetches the raw HTML from the Brisnet race page.\"\"\"\n        # Note: Brisnet URL structure seems to require a track code, e.g., 'CD' for Churchill Downs.\n        # This implementation will need to be improved to dynamically handle different tracks.\n        # For now, it is hardcoded to Churchill Downs as a placeholder.\n        url = f\"/race/{date}/CD\"\n        response = await self.make_request(self.http_client, \"GET\", url)\n        return {\"html\": response.text, \"date\": date} if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[dict]) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        venue_text_node = soup.select_one(\"header h1\")\n        if not venue_text_node:\n            self.logger.warning(\"Could not find venue name on Brisnet page.\")\n            return []\n\n        venue_text = venue_text_node.text\n        venue = normalize_venue_name(venue_text.split(\" - \")[0])\n\n        races = []\n        for race_section in soup.select(\"section.race\"):\n            try:\n                race_number_str = race_section.get(\"data-racenumber\")\n                if not race_number_str or not race_number_str.isdigit():\n                    continue\n                race_number = int(race_number_str)\n\n                post_time_node = race_section.select_one(\".race-title span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text.replace(\"Post Time: \", \"\").strip()\n                start_time = parse(f\"{race_date} {post_time_str}\")\n\n                runners = []\n                for row in race_section.select(\"tbody tr\"):\n                    if \"scratched\" in row.get(\"class\", []):\n                        continue\n\n                    cells = row.find_all(\"td\")\n                    if len(cells) < 3:\n                        continue\n\n                    number_text = cells[0].text.strip()\n                    if not number_text.isdigit():\n                        continue\n                    number = int(number_text)\n\n                    name = cells[1].text.strip()\n                    odds_str = cells[2].text.strip()\n\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    odds = {}\n                    if win_odds:\n                        odds[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(Runner(number=number, name=name, odds=odds))\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"brisnet_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                    field_size=len(runners),\n                )\n                races.append(race)\n            except (ValueError, IndexError, TypeError):\n                self.logger.warning(\"Failed to parse a race on Brisnet, skipping.\", exc_info=True)\n                continue\n\n        return races\n",
    "web_service/backend/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Note: FanDuel's API is not date-centric. Event discovery would be needed for a robust implementation.\n        # This uses a hardcoded eventId as a placeholder.\n        event_id = \"38183.3\"\n        self.logger.info(f\"Fetching races from FanDuel for event_id: {event_id}\")\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            self.logger.warning(\"FanDuel response missing 'marketGroups' key\")\n            return []\n\n        races = []\n        for group in raw_data.get(\"marketGroups\", []):\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\n                            \"Failed to parse a FanDuel market\",\n                            market=market,\n                            exc_info=True,\n                        )\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            self.logger.warning(f\"Could not parse race and track from FanDuel market name: {market_name}\")\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\").strip()\n        if not race_number_str.isdigit():\n            return None\n        race_number = int(race_number_str)\n\n        track_name = parts[1]\n\n        # Placeholder for start_time - FanDuel's market API doesn't provide it directly\n        start_time = datetime.now(timezone.utc) + timedelta(hours=race_number)\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_runner_odds = runner_data.get(\"winRunnerOdds\", {})\n                current_price = win_runner_odds.get(\"currentPrice\")\n\n                if not runner_name or not current_price:\n                    continue\n\n                numerator, denominator = map(int, current_price.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                odds = OddsData(\n                    win=decimal_odds,\n                    source=self.source_name,\n                    last_updated=datetime.now(timezone.utc),\n                )\n\n                name_parts = runner_name.split(\".\", 1)\n                if len(name_parts) < 2:\n                    continue\n                program_number_str = name_parts[0].strip()\n                horse_name = name_parts[1].strip()\n\n                runners.append(\n                    Runner(\n                        name=horse_name,\n                        number=(int(program_number_str) if program_number_str.isdigit() else 0),\n                        odds={self.source_name: odds},\n                    )\n                )\n            except (ValueError, ZeroDivisionError, IndexError, TypeError):\n                self.logger.warning(\n                    \"Could not parse FanDuel runner\",\n                    runner_data=runner_data,\n                    exc_info=True,\n                )\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/horseracingnation_adapter.py": "# python_service/adapters/horseracingnation_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass HorseRacingNationAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for horseracingnation.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"HorseRacingNation\"\n    BASE_URL = \"https://www.horseracingnation.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/oddschecker_adapter.py": "# python_service/adapters/oddschecker_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass OddscheckerAdapter(BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        # Note: Oddschecker doesn't seem to support historical dates well in its main nav,\n        # but we build the URL as if it does for future compatibility.\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        # Find all links to individual race pages\n        race_links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                race = self._parse_race_page(soup, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, soup: BeautifulSoup, race_date) -> Optional[Race]:\n        track_name_node = soup.select_one(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.get_text(strip=True)\n\n        race_time_node = soup.select_one(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.get_text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = soup.select_one(\"a.race-time-link.active\")\n        race_number = 1\n        if active_link:\n            all_links = soup.select(\"a.race-time-link\")\n            try:\n                race_number = all_links.index(active_link) + 1\n            except ValueError:\n                pass  # Keep default race number if active link not in all links\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in soup.select(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.get_text(strip=True)\n\n            odds_node = row.select_one(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.get_text(strip=True)\n\n            number_node = row.select_one(\"td.runner-number\")\n            if not number_node or not number_node.get_text(strip=True).isdigit():\n                return None\n            number = int(number_node.get_text(strip=True))\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if win_odds and win_odds < 999:\n                odds_dict[self.source_name] = OddsData(\n                    win=win_odds, source=self.source_name, last_updated=datetime.now()\n                )\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/punters_adapter.py": "# python_service/adapters/punters_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass PuntersAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for punters.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Punters\"\n    BASE_URL = \"https://www.punters.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/tab_adapter.py": "# python_service/adapters/tab_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TabAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for tab.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/cache_manager.py": "# python_service/cache_manager.py\nimport asyncio\nimport hashlib\nimport json\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self):\n        self.redis_client = None\n        self.memory_cache = {}\n        self.is_configured = False\n        log.info(\"CacheManager initialized (not connected).\")\n\n    async def connect(self, redis_url: str):\n        if self.is_configured or not REDIS_AVAILABLE or not redis_url:\n            return\n\n        try:\n            log.info(\"Attempting to connect to Redis...\", url=redis_url)\n            # Use the async version of the client\n            self.redis_client = redis.asyncio.from_url(redis_url, decode_responses=True)\n            await self.redis_client.ping()  # Verify connection asynchronously\n            self.is_configured = True\n            log.info(\"Redis cache connected successfully.\")\n        except (redis.exceptions.ConnectionError, asyncio.TimeoutError) as e:\n            log.warning(\n                \"Failed to connect to Redis. Falling back to in-memory cache.\",\n                error=str(e),\n            )\n            self.redis_client = None\n            self.is_configured = False\n\n    async def disconnect(self):\n        if self.redis_client:\n            await self.redis_client.close()\n            log.info(\"Redis connection closed.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    async def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = await self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis GET failed, falling back to memory cache.\", error=e)\n\n        entry = self.memory_cache.get(key)\n        if entry and entry.get(\"expires_at\", datetime.min) > datetime.now():\n            return entry.get(\"value\")\n        return None\n\n    async def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        try:\n            serialized = json.dumps(value, default=str)\n        except (TypeError, ValueError) as e:\n            log.error(\"Failed to serialize value for caching.\", value=value, error=str(e))\n            return\n\n        if self.redis_client:\n            try:\n                await self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis SET failed, falling back to memory cache.\", error=e)\n\n        self.memory_cache[key] = {\n            \"value\": value,\n            \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds),\n        }\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager()\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = await cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n\n            try:\n                await cache_manager.set(cache_key, result, ttl_seconds)\n            except Exception as e:\n                log.error(\"Failed to store result in cache.\", error=str(e), key=cache_key)\n\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "web_service/backend/credentials_manager.py": "# python_service/credentials_manager.py\ntry:\n    import keyring\n\n    # This check is crucial for cross-platform compatibility\n    import keyring.backends.windows\n\n    IS_WINDOWS = True\nexcept ImportError:\n    keyring = None\n    IS_WINDOWS = False\n\n\nclass SecureCredentialsManager:\n    \"\"\"Manages secrets in the system's native credential store.\"\"\"\n\n    SERVICE_NAME = \"Fortuna\"\n\n    @staticmethod\n    def save_credential(account: str, secret: str) -> bool:\n        \"\"\"Saves a secret for a given account (e.g., 'api_key', 'betfair_username').\"\"\"\n        if not IS_WINDOWS:\n            print(\"Credential storage is only supported on Windows.\")\n            return False\n        try:\n            keyring.set_password(SecureCredentialsManager.SERVICE_NAME, account, secret)\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credential for {account}: {e}\")\n            return False\n\n    @staticmethod\n    def get_credential(account: str) -> str:\n        \"\"\"Retrieves a secret for a given account.\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            return keyring.get_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credential for {account}: {e}\")\n            return None\n\n    @staticmethod\n    def get_betfair_credentials() -> tuple[str, str]:\n        \"\"\"Convenience method to retrieve both Betfair username and password.\"\"\"\n        username = SecureCredentialsManager.get_credential(\"betfair_username\")\n        password = SecureCredentialsManager.get_credential(\"betfair_password\")\n        return username, password\n\n    @staticmethod\n    def delete_credential(account: str):\n        \"\"\"Deletes a specific credential.\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception:\n            pass\n",
    "web_service/backend/db/init.py": "# python_service/db/init.py\nimport os\nimport sqlite3\n\nfrom ..config import get_settings\n\n\ndef initialize_database():\n    \"\"\"\n    Initializes the database based on the configuration.\n    Currently supports a simple SQLite fallback for local testing.\n    \"\"\"\n    settings = get_settings()\n    db_type = getattr(settings, \"DATABASE_TYPE\", \"sqlite\").lower()\n\n    if db_type == \"sqlite\":\n        # DATABASE_URL for sqlite will be like 'sqlite:///./fortuna.db'\n        db_path = settings.DATABASE_URL.split(\"///\")[1]\n\n        # Ensure the directory for the database exists\n        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n\n        try:\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n\n            # The schema is based on the provided pg_schemas, adapted for SQLite\n            # This is a simplified version for demonstration.\n            cursor.execute(\n                \"\"\"\n            CREATE TABLE IF NOT EXISTS races (\n                id TEXT PRIMARY KEY,\n                venue TEXT NOT NULL,\n                race_number INTEGER NOT NULL,\n                start_time TEXT NOT NULL,\n                source TEXT,\n                field_size INTEGER\n            )\n            \"\"\"\n            )\n\n            cursor.execute(\n                \"\"\"\n            CREATE TABLE IF NOT EXISTS runners (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                race_id TEXT,\n                number INTEGER,\n                name TEXT,\n                odds REAL,\n                FOREIGN KEY (race_id) REFERENCES races (id)\n            )\n            \"\"\"\n            )\n\n            conn.commit()\n            conn.close()\n            print(\"SQLite database initialized successfully.\")\n        except sqlite3.Error as e:\n            print(f\"Error initializing SQLite database: {e}\")\n            raise\n",
    "web_service/backend/fortuna_service.py": "# fortuna_service.py\n# The main service runner, upgraded to the final Endgame architecture.\n\nimport json\nimport logging\nimport os\nimport sqlite3\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom .analyzer import TrifectaAnalyzer\nfrom .engine import Race\nfrom .engine import Settings\nfrom .engine import SuperchargedOrchestrator\n\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            # Correctly resolve paths from the service's location\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n            schema_path = os.path.join(base_dir, \"shared_database\", \"schema.sql\")\n            web_schema_path = os.path.join(base_dir, \"shared_database\", \"web_schema.sql\")\n\n            # Read both schema files\n            with open(schema_path, \"r\") as f:\n                schema = f.read()\n            with open(web_schema_path, \"r\") as f:\n                web_schema = f.read()\n\n            # Apply both schemas in a single transaction\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.executescript(schema)\n                cursor.executescript(web_schema)\n                conn.commit()\n            self.logger.info(\"CRITICAL SUCCESS: All database schemas (base + web) applied successfully.\")\n        except Exception as e:\n            self.logger.critical(\n                f\"FATAL: Database setup failed. Other platforms will fail. Error: {e}\",\n                exc_info=True,\n            )\n            raise\n\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO live_races (\n                        race_id, track_name, race_number, post_time, raw_data_json,\n                        fortuna_score, qualified, trifecta_factors_json, updated_at\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        race.race_id,\n                        race.track_name,\n                        race.race_number,\n                        race.post_time,\n                        race.model_dump_json(),\n                        race.fortuna_score,\n                        race.is_qualified,\n                        race.trifecta_factors_json,\n                        datetime.now(),\n                    ),\n                )\n            for status in statuses:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO adapter_status (\n                        adapter_name, status, last_run, races_found, error_message,\n                        execution_time_ms\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        status.get(\"adapter_id\"),\n                        status.get(\"status\"),\n                        status.get(\"timestamp\"),\n                        status.get(\"races_found\"),\n                        status.get(\"error_message\"),\n                        int(status.get(\"response_time\", 0) * 1000),\n                    ),\n                )\n\n            if races or statuses:\n                cursor.execute(\n                    \"INSERT INTO events (event_type, payload) VALUES (?, ?)\",\n                    (\"RACES_UPDATED\", json.dumps({\"race_count\": len(races)})),\n                )\n\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\")\n\n\nclass FortunaBackgroundService:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        from dotenv import load_dotenv\n\n        dotenv_path = os.path.join(os.path.dirname(__file__), \"..\", \".env\")\n        load_dotenv(dotenv_path=dotenv_path)\n\n        db_path = os.getenv(\"FORTUNA_DB_PATH\")\n        if not db_path:\n            self.logger.critical(\"FATAL: FORTUNA_DB_PATH environment variable not set. Service cannot start.\")\n            raise ValueError(\"FORTUNA_DB_PATH is not configured.\")\n\n        self.logger.info(f\"Database path loaded from environment: {db_path}\")\n\n        self.settings = Settings()\n        self.db_handler = DatabaseHandler(db_path)\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\n        self.python_analyzer = TrifectaAnalyzer(self.settings)\n        self.stop_event = threading.Event()\n        self.rust_engine_path = os.path.join(\n            os.path.dirname(__file__),\n            \"..\",\n            \"rust_engine\",\n            \"target\",\n            \"release\",\n            \"fortuna_engine.exe\",\n        )\n\n    def _analyze_with_rust(self, races: List[Race]) -> Optional[List[Race]]:\n        self.logger.info(\"Attempting analysis with external Rust engine.\")\n        try:\n            race_data_json = json.dumps([r.model_dump() for r in races])\n            result = subprocess.run(\n                [self.rust_engine_path],\n                input=race_data_json,\n                capture_output=True,\n                text=True,\n                check=True,\n                timeout=30,\n            )\n            results_data = json.loads(result.stdout)\n            results_map = {res[\"race_id\"]: res for res in results_data}\n\n            for race in races:\n                if race.race_id in results_map:\n                    res = results_map[race.race_id]\n                    race.fortuna_score = res.get(\"fortuna_score\")\n                    race.is_qualified = res.get(\"qualified\")\n                    race.trifecta_factors_json = json.dumps(res.get(\"trifecta_factors\"))\n            return races\n        except FileNotFoundError:\n            self.logger.warning(\"Rust engine not found. Falling back to Python analyzer.\")\n            return None\n        except (\n            subprocess.CalledProcessError,\n            json.JSONDecodeError,\n            subprocess.TimeoutExpired,\n        ) as e:\n            self.logger.error(f\"Rust engine execution failed: {e}. Falling back to Python analyzer.\")\n            return None\n\n    def _analyze_with_python(self, races: List[Race]) -> List[Race]:\n        self.logger.info(\"Performing analysis with internal Python engine.\")\n        return [self.python_analyzer.analyze_race_advanced(race) for race in races]\n\n    def run_continuously(self, interval_seconds: int = 60):\n        self.logger.info(\"Background service thread starting continuous run.\")\n\n        while not self.stop_event.is_set():\n            try:\n                self.logger.info(\"Starting data collection and analysis cycle.\")\n                races, statuses = self.orchestrator.get_races_parallel()\n\n                analyzed_races = None\n                if os.path.exists(self.rust_engine_path):\n                    analyzed_races = self._analyze_with_rust(races)\n\n                if analyzed_races is None:  # Fallback condition\n                    analyzed_races = self._analyze_with_python(races)\n\n                if analyzed_races:  # Ensure we have something to update\n                    self.db_handler.update_races_and_status(analyzed_races, statuses)\n\n            except Exception as e:\n                self.logger.critical(f\"Unhandled exception in service loop: {e}\", exc_info=True)\n\n            self.logger.info(f\"Cycle complete. Sleeping for {interval_seconds} seconds.\")\n            self.stop_event.wait(interval_seconds)\n        self.logger.info(\"Background service run loop has terminated.\")\n\n    def start(self):\n        self.stop_event.clear()\n        self.thread = threading.Thread(target=self.run_continuously)\n        self.thread.daemon = True\n        self.thread.start()\n        self.logger.info(\"FortunaBackgroundService started.\")\n\n    def stop(self):\n        self.stop_event.set()\n        if hasattr(self, \"thread\") and self.thread.is_alive():\n            self.thread.join(timeout=10)\n        self.logger.info(\"FortunaBackgroundService stopped.\")\n",
    "web_service/backend/fortuna_windows_service.py": "# fortuna_windows_service.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Ensure the script's directory is at the front of the path\nscript_dir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, script_dir)\n\ntry:\n    from fortuna_service import FortunaBackgroundService\nexcept ImportError as e:\n    # Log a detailed error to the Windows Event Log if the import fails\n    servicemanager.LogErrorMsg(f\"FATAL: Could not import FortunaBackgroundService. Error: {e}\")\n    sys.exit(1)  # Exit with an error code\n\n\nclass FortunaWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaV8Service\"\n    _svc_display_name_ = \"Fortuna V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.fortuna_service = FortunaBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(name)s - %(levelname)s - %(message)s\",\n            handlers=[servicemanager.LogHandler()],\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.fortuna_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        self.fortuna_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaWindowsService)\n",
    "web_service/backend/health.py": "# python_service/health.py\nfrom datetime import datetime\nfrom typing import Dict\nfrom typing import List\n\nimport psutil\nimport structlog\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\nlog = structlog.get_logger(__name__)\n\n\nclass HealthMonitor:\n    def __init__(self):\n        self.adapter_health: Dict[str, Dict] = {}\n        self.system_metrics: List[Dict] = []\n        self.max_metrics_history = 100\n\n    def record_adapter_response(self, adapter_name: str, success: bool, duration: float):\n        if adapter_name not in self.adapter_health:\n            self.adapter_health[adapter_name] = {\n                \"total_requests\": 0,\n                \"successful_requests\": 0,\n                \"failed_requests\": 0,\n                \"avg_response_time\": 0.0,\n                \"last_success\": None,\n                \"last_failure\": None,\n            }\n\n        health = self.adapter_health[adapter_name]\n        health[\"total_requests\"] += 1\n\n        if success:\n            health[\"successful_requests\"] += 1\n            health[\"last_success\"] = datetime.now().isoformat()\n        else:\n            health[\"failed_requests\"] += 1\n            health[\"last_failure\"] = datetime.now().isoformat()\n\n        health[\"avg_response_time\"] = (\n            health[\"avg_response_time\"] * (health[\"total_requests\"] - 1) + duration\n        ) / health[\"total_requests\"]\n\n    def get_system_metrics(self) -> Dict:\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage(\"/\")\n\n        metrics = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_available_gb\": round(memory.available / (1024**3), 2),\n            \"disk_percent\": disk.percent,\n            \"disk_free_gb\": round(disk.free / (1024**3), 2),\n        }\n\n        self.system_metrics.append(metrics)\n        if len(self.system_metrics) > self.max_metrics_history:\n            self.system_metrics.pop(0)\n\n        return metrics\n\n    def get_health_report(self) -> Dict:\n        system_metrics = self.get_system_metrics()\n        return {\n            \"status\": \"healthy\" if self.is_system_healthy() else \"degraded\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"system\": system_metrics,\n            \"adapters\": self.adapter_health,\n            \"metrics_history\": self.system_metrics[-10:],\n        }\n\n    def is_system_healthy(self) -> bool:\n        if not self.system_metrics:\n            return True\n        latest = self.system_metrics[-1]\n        return latest[\"cpu_percent\"] < 80 and latest[\"memory_percent\"] < 85 and latest[\"disk_percent\"] < 90\n\n\n# Global instance for the application to use\nhealth_monitor = HealthMonitor()\n\n\n@router.get(\"/health/detailed\", tags=[\"Health\"])\nasync def get_detailed_health():\n    \"\"\"Provides a comprehensive health check of the system.\"\"\"\n    return health_monitor.get_health_report()\n\n\n@router.get(\"/health\", tags=[\"Health\"])\nasync def get_basic_health():\n    \"\"\"Provides a basic health check for load balancers and uptime monitoring.\"\"\"\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n",
    "web_service/backend/initialize_db.py": "# python_service/initialize_db.py\nfrom db.init import initialize_database\n\n\ndef main():\n    \"\"\"\n    This script exists solely to initialize the database.\n    It should be called before the main server process is started.\n    \"\"\"\n    print(\"Initializing database...\", flush=True)\n    initialize_database()\n    print(\"Database initialization complete.\", flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "web_service/backend/main.py": "import uvicorn\nimport sys\nimport os\nfrom multiprocessing import freeze_support\n\n# Force UTF-8 encoding for stdout and stderr, crucial for PyInstaller on Windows\nos.environ[\"PYTHONUTF8\"] = \"1\"\n\n# This is the definitive entry point for the Fortuna Faucet backend service.\n# It is designed to be compiled with PyInstaller.\n\ndef _configure_sys_path():\n    \"\"\"\n    Dynamically adds project paths to sys.path.\n    This is the robust solution to ensure that string-based imports like\n    \"web_service.backend.api:app\" work correctly when the application is\n    run from a PyInstaller executable. The `_MEIPASS` attribute is a temporary\n    directory created by PyInstaller at runtime.\n    \"\"\"\n    if getattr(sys, \"frozen\", False) and hasattr(sys, \"_MEIPASS\"):\n        # Running in a PyInstaller bundle. The project root is the _MEIPASS directory.\n        project_root = os.path.abspath(sys._MEIPASS)\n\n        # Aggressively add paths to resolve potential module lookup issues in frozen mode.\n        paths_to_add = [\n            project_root,\n            os.path.join(project_root, \"web_service\"),\n            os.path.join(project_root, \"web_service\", \"backend\"),\n        ]\n\n        # Insert paths at the beginning of sys.path in reverse order\n        # to maintain the desired precedence.\n        for path in reversed(paths_to_add):\n            if path not in sys.path:\n                sys.path.insert(0, path)\n                print(f\"INFO: Added path to sys.path: {path}\")\n\n    else:\n        # Running as a normal script. The project root is two levels up.\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n        if project_root not in sys.path:\n            sys.path.insert(0, project_root)\n            print(f\"INFO: Added project root to sys.path: {project_root}\")\n\ndef main():\n    \"\"\"\n    Primary entry point for the Fortuna Faucet backend application.\n    This function configures and runs the Uvicorn server.\n    \"\"\"\n    # CRITICAL: This must be called before any other application imports.\n    _configure_sys_path()\n\n    # When packaged, we need to ensure multiprocessing and asyncio work correctly.\n    if getattr(sys, \"frozen\", False):\n        # CRITICAL for multiprocessing support in frozen mode on Windows.\n        freeze_support()\n        # CRITICAL for asyncio server behavior in frozen mode on Windows.\n        import asyncio\n        print(\"[BOOT] Applied WindowsSelectorEventLoopPolicy for PyInstaller\")\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n\n    from web_service.backend.config import get_settings\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    # The 'app' object is needed for the server to run. Importing it here ensures\n    # all its dependencies are resolved after the sys.path modification.\n    from web_service.backend.api import app\n\n    settings = get_settings()\n\n    # In CI/CD, binding to 0.0.0.0 is more robust than 127.0.0.1.\n    # We will override the host setting for the smoke test environment.\n    run_host = settings.UVICORN_HOST\n    if os.environ.get(\"FORTUNA_ENV\") == \"smoke-test\":\n        run_host = \"0.0.0.0\"\n        print(f\"INFO: Smoke test environment detected. Overriding host to '{run_host}'\")\n\n\n    # --- Port Sanity Check ---\n    check_port_and_exit_if_in_use(settings.FORTUNA_PORT, run_host)\n\n    print(f\"INFO: Starting Uvicorn server...\")\n    print(f\"      APP: web_service.backend.api:app\")\n    print(f\"      HOST: {run_host}\")\n    print(f\"      PORT: {settings.FORTUNA_PORT}\")\n\n    # For PyInstaller, it's more reliable to pass the app object directly\n    # rather than a string, as string-based imports can be fragile.\n    uvicorn.run(\n        app,\n        host=run_host,\n        port=settings.FORTUNA_PORT,\n        log_level=\"info\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "web_service/backend/security.py": "# python_service/security.py\n\nimport secrets\n\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Security\nfrom fastapi import status\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings\nfrom .config import get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\n\nasync def verify_api_key(key: str = Security(api_key_header), settings: Settings = Depends(get_settings)):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n    \"\"\"\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid or missing API Key\")\n",
    "web_service/backend/utils/odds.py": "# Centralized odds parsing utility, created by Operation: The A+ Trifecta\nfrom decimal import Decimal\nfrom decimal import InvalidOperation\nfrom typing import Optional\nfrom typing import Union\n\n\ndef parse_odds_to_decimal(odds: Union[str, int, float, None]) -> Optional[Decimal]:\n    \"\"\"\n    Parse various odds formats to Decimal for precise financial calculations.\n    Handles fractional, decimal, and special cases ('EVS', 'SP', etc.).\n    Returns None for unparseable or invalid values.\n    \"\"\"\n    if odds is None:\n        return None\n\n    if isinstance(odds, (int, float)):\n        return Decimal(str(odds))\n\n    odds_str = str(odds).strip().upper()\n\n    SPECIAL_CASES = {\n        \"EVS\": Decimal(\"2.0\"),\n        \"EVENS\": Decimal(\"2.0\"),\n        \"SP\": None,\n        \"SCRATCHED\": None,\n        \"SCR\": None,\n        \"\": None,\n    }\n\n    if odds_str in SPECIAL_CASES:\n        return SPECIAL_CASES[odds_str]\n\n    if \"/\" in odds_str:\n        try:\n            parts = odds_str.split(\"/\")\n            if len(parts) != 2:\n                return None\n            num, den = map(Decimal, parts)\n            if den <= 0:\n                return None\n            return Decimal(\"1.0\") + (num / den)\n        except (ValueError, InvalidOperation):\n            return None\n\n    try:\n        return Decimal(odds_str)\n    except (ValueError, InvalidOperation):\n        return None\n",
    "web_service/frontend/app/Providers.tsx": "// web_platform/frontend/app/Providers.tsx\n'use client';\n\nimport { QueryClientProvider } from '@tanstack/react-query';\nimport { queryClient } from './lib/queryClient';\nimport React from 'react';\n\nexport default function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>\n  );\n}\n",
    "web_service/frontend/app/components/AdapterStatusPanel.tsx": "// web_platform/frontend/src/components/AdapterStatusPanel.tsx\n'use client';\n\nimport React from 'react';\nimport { SourceInfo } from '../types/racing';\n\ninterface AdapterStatusPanelProps {\n  adapter: SourceInfo;\n  onFetchRaces: (sourceName: string) => void;\n}\n\nexport const AdapterStatusPanel: React.FC<AdapterStatusPanelProps> = ({ adapter, onFetchRaces }) => {\n  const isConfigured = adapter.status !== 'CONFIG_ERROR';\n\n  return (\n    <div className={`p-4 rounded-lg border ${isConfigured ? 'bg-slate-800 border-slate-700' : 'bg-yellow-900/20 border-yellow-700/50'}`}>\n      <div className=\"flex justify-between items-center\">\n        <h3 className=\"font-bold text-lg text-white\">{adapter.name}</h3>\n        <span className={`px-2 py-0.5 rounded-full text-xs font-medium ${isConfigured ? 'bg-green-500/20 text-green-300' : 'bg-yellow-500/20 text-yellow-300'}`}>\n          {isConfigured ? 'Ready' : 'Not Configured'}\n        </span>\n      </div>\n      <div className=\"mt-4 flex gap-2\">\n        <button\n          onClick={() => onFetchRaces(adapter.name)}\n          disabled={!isConfigured}\n          className=\"flex-1 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 disabled:bg-slate-700 disabled:text-slate-400 disabled:cursor-not-allowed\"\n        >\n          Automatic Load\n        </button>\n        <button\n          disabled\n          className=\"flex-1 px-4 py-2 bg-slate-700 text-slate-400 rounded cursor-not-allowed\"\n        >\n          Manual Entry (Coming Soon)\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/LiveRaceDashboard.tsx": "// web_platform/frontend/src/components/LiveRaceDashboard.tsx\n'use client';\n\nimport React, { useState, useEffect, useCallback } from 'react';\nimport { useQuery, useQueryClient } from '@tanstack/react-query';\nimport { RaceFilters } from './RaceFilters';\nimport { RaceCard } from './RaceCard';\nimport { RaceCardSkeleton } from './RaceCardSkeleton';\nimport { EmptyState } from './EmptyState';\nimport { ErrorDisplay } from './ErrorDisplay';\nimport { Race, SourceInfo, AdapterError, AggregatedRacesResponse } from '../types/racing';\nimport { useWebSocket } from '../hooks/useWebSocket';\nimport { StatusDetailModal } from './StatusDetailModal';\nimport ManualOverridePanel from './ManualOverridePanel';\nimport { LiveModeToggle } from './LiveModeToggle';\nimport { AdapterStatusPanel } from './AdapterStatusPanel';\n\n// Type for the backend process status received from Electron main\ntype BackendState = 'starting' | 'running' | 'error' | 'stopped';\ninterface BackendStatus {\n  state: BackendState;\n  logs: string[];\n}\n\ninterface RaceFilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nconst fetchAdapterStatuses = async (apiKey: string | null): Promise<SourceInfo[]> => {\n  if (!apiKey) {\n    throw new Error('API key not available.');\n  }\n  const response = await fetch(`/api/adapters/status`, {\n    headers: { 'X-API-Key': apiKey, 'Content-Type': 'application/json' },\n  });\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n  return response.json();\n};\n\nconst fetchQualifiedRaces = async (apiKey: string | null, params: RaceFilterParams): Promise<AggregatedRacesResponse> => {\n  if (!apiKey) {\n    throw new Error('API key not available');\n  }\n  // In web service mode, API calls are relative to the current origin.\n  const response = await fetch(`/api/races`, {\n    headers: { 'X-API-Key': apiKey },\n  });\n\n  if (!response.ok) {\n    const errorData = await response.json();\n    throw new Error(JSON.stringify(errorData));\n  }\n\n  return response.json();\n};\n\n\nconst BackendErrorPanel = ({ logs }: { logs: string[] }) => (\n  <div className=\"bg-slate-800 p-6 rounded-lg border border-red-500/50 text-white\">\n    <h2 className=\"text-2xl font-bold text-red-400 mb-4\">Backend Service Error</h2>\n    <p className=\"text-slate-400 mb-4\">The backend data service failed to start or has crashed. Below are the most recent diagnostic messages.</p>\n    <div className=\"bg-black p-4 rounded-md font-mono text-sm text-slate-300 h-64 overflow-y-auto mb-4\">\n      {logs.map((log, index) => (\n        <p key={index} className=\"whitespace-pre-wrap\">{`> ${log}`}</p>\n      ))}\n    </div>\n    <p className=\"text-sm text-slate-500 text-center mt-4\">Please check the server logs for more information.</p>\n  </div>\n);\n\n// New Sub-Component to display an error from a specific adapter\nconst ErrorCard = ({ source, message }: { source: string; message: string }) => (\n  <div className=\"bg-slate-800 rounded-lg p-4 border border-red-500/50 flex flex-col justify-between\">\n    <div>\n      <h3 className=\"font-bold text-red-400 text-lg\">{source} Failed</h3>\n      <p className=\"text-slate-400 text-sm mt-2\">{message}</p>\n    </div>\n    <div className=\"mt-4 text-xs text-slate-500\">\n      <p>This adapter failed to fetch data. This is not a critical error; other adapters may provide the necessary data.</p>\n    </div>\n  </div>\n);\n\n// New Sub-Component to render the grid of races or error cards\nconst RaceGrid = ({ races }: { races: Race[] }) => (\n  <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n    {races.map(race =>\n      race.isErrorPlaceholder ? (\n        <ErrorCard key={race.id} source={race.venue} message={race.errorMessage || 'An unknown error occurred.'} />\n      ) : (\n        <RaceCard key={race.id} race={race} />\n      )\n    )}\n  </div>\n);\n\n// In a pure web service, the frontend cannot know the backend's process status.\n// We assume it's always running if the frontend is served.\nconst useBackendStatus = (): BackendStatus => {\n  return { state: 'running', logs: ['Running in web service mode. Backend status is assumed to be active.'] };\n};\n\nexport const LiveRaceDashboard = React.memo(() => {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [adapterErrors, setAdapterErrors] = useState<AdapterError[]>([]);\n  const backendStatus = useBackendStatus();\n  // In a web service, the API key might be hardcoded, come from a meta tag, or an auth flow.\n  // For this version, we'll rely on the backend not requiring one from the same origin or use a known key.\n  const [apiKey, setApiKey] = useState<string | null>(\"a_secure_test_api_key_that_is_long_enough\");\n  const queryClient = useQueryClient();\n\n  const [params, setParams] = useState<RaceFilterParams>({\n    maxFieldSize: 10,\n    minFavoriteOdds: 2.5,\n    minSecondFavoriteOdds: 4.0,\n  });\n\n  const {\n    data,\n    status: connectionStatus,\n    error: errorDetails,\n    refetch,\n  } = useQuery({\n    queryKey: ['aggregatedRaces', apiKey],\n    queryFn: () => fetchQualifiedRaces(apiKey, params),\n    enabled: backendStatus.state === 'running' && !!apiKey,\n    refetchOnWindowFocus: true,\n  });\n\n  // Update state when data is successfully fetched\n  useEffect(() => {\n    if (data) {\n      setRaces(data.races || []);\n      setAdapterErrors(data.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [data]);\n\n  const { data: liveData, isConnected: isLiveConnected } = useWebSocket<AggregatedRacesResponse>(\n    '/ws/live-updates',\n    { apiKey } // Port is not needed for same-origin WebSockets\n  );\n\n  // Effect to update state when new live data arrives\n  useEffect(() => {\n    if (liveData) {\n      console.log('Received live data update:', liveData);\n      // Update the query cache and local state with the new data\n      queryClient.setQueryData(['aggregatedRaces', apiKey], liveData);\n      setRaces(liveData.races || []);\n      setAdapterErrors(liveData.errors || []);\n      setLastUpdate(new Date());\n    }\n  }, [liveData, queryClient, apiKey]);\n\n  const [lastUpdate, setLastUpdate] = useState<Date | null>(null);\n  const [isModalOpen, setIsModalOpen] = useState(false);\n\n\n  const handleParamsChange = useCallback((newParams: RaceFilterParams) => {\n    setParams(newParams);\n  }, []);\n\n  const handleParseSuccess = (adapterName: string, parsedRaces: Race[]) => {\n    queryClient.setQueryData(['qualifiedRaces', apiKey, params], (oldData: { races: Race[], source_info: SourceInfo[] } | undefined) => {\n      if (!oldData) return { races: parsedRaces, source_info: [] };\n\n      // 1. Remove the placeholder error card for this adapter\n      const otherRaces = oldData.races.filter(race => race.source !== adapterName);\n\n      // 2. Merge the new races in\n      const updatedRaces = [...otherRaces, ...parsedRaces].sort(\n        (a, b) => new Date(a.start_time).getTime() - new Date(b.start_time).getTime()\n      );\n\n      // 3. Update source_info to remove the failed source\n      const updatedSourceInfo = oldData.source_info.filter(s => s.name !== adapterName);\n\n      return { races: updatedRaces, source_info: updatedSourceInfo };\n    });\n  };\n\n  const renderContent = () => {\n    // Priority 1: Backend process has failed.\n    if (backendStatus.state === 'error') {\n      return <BackendErrorPanel logs={backendStatus.logs} />;\n    }\n\n    if (backendStatus.state === 'stopped') {\n        return <EmptyState\n            title=\"Backend Service Stopped\"\n            message=\"The backend data service is not running. Please start it to see live race data.\"\n        />;\n    }\n\n    // Priority 2: Backend is starting or initial fetch is happening.\n    const isLoading = backendStatus.state === 'starting' || (connectionStatus === 'pending' && !data);\n    if (isLoading) {\n        return (\n            <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4\">\n                {[...Array(8)].map((_, i) => <RaceCardSkeleton key={i} />)}\n            </div>\n        );\n    }\n\n    // Priority 3: API connection is offline.\n    if (connectionStatus === 'error') {\n      try {\n        const errorInfo = JSON.parse((errorDetails as Error).message);\n        return <ErrorDisplay error={errorInfo.error} />;\n      } catch (e) {\n        return <EmptyState\n            title=\"API Connection Offline\"\n            message={(errorDetails as Error)?.message || \"The backend is running, but the dashboard could not connect to its API.\"}\n            actionButton={<button onClick={() => refetch()} className=\"mt-4 px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700\">Retry Connection</button>}\n        />;\n      }\n    }\n\n    // Priority 4: No races found after a successful fetch.\n    if (!races || races.length === 0) {\n      return <EmptyState\n          title=\"No Races Found\"\n          message=\"No races matched the specified criteria for the selected date. Please try different filters.\"\n      />;\n    }\n\n    // Priority 5: Display the races (and any error placeholders).\n    return <RaceGrid races={races} />;\n  };\n\n  const getStatusIndicator = () => {\n    if (backendStatus.state === 'error') {\n      return { color: 'bg-red-500', text: 'Backend Error' };\n    }\n    if (backendStatus.state === 'stopped') {\n        return { color: 'bg-gray-500', text: 'Stopped' };\n    }\n    if (backendStatus.state === 'starting') {\n      return { color: 'bg-yellow-500', text: 'Backend Starting...' };\n    }\n    if (isLiveConnected) {\n      return { color: 'bg-cyan-500', text: 'Live' };\n    }\n    return { color: 'bg-yellow-500', text: 'Connecting...' };\n  };\n\n  const { color: statusColor, text: statusText } = getStatusIndicator();\n\n  return (\n    <>\n      <div className=\"space-y-6\">\n        <div className=\"flex justify-between items-start\">\n            <div className=\"text-left space-y-2\">\n                <h1 className=\"text-4xl font-bold text-white\">\ud83c\udfc7 Fortuna Faucet</h1>\n                <p className=\"text-slate-400\">\n                Last updated: {lastUpdate ? lastUpdate.toLocaleTimeString() : 'N/A'}\n                </p>\n            </div>\n            <div className=\"flex items-center gap-4\">\n                <button\n                    onClick={() => (connectionStatus === 'error' || backendStatus.state === 'error') && setIsModalOpen(true)}\n                    className={`flex items-center gap-2 px-3 py-1.5 rounded-full text-sm font-medium text-white ${statusColor} ${(connectionStatus === 'error' || backendStatus.state === 'error') ? 'cursor-pointer hover:opacity-80' : 'cursor-default'}`}\n                    data-testid=\"status-indicator\"\n                >\n                    <span className={`w-2.5 h-2.5 rounded-full bg-white ${isLiveConnected ? 'animate-pulse' : ''}`}></span>\n                    {statusText}\n                </button>\n            </div>\n        </div>\n\n        <RaceFilters onParamsChange={handleParamsChange} isLoading={connectionStatus === 'pending'} refetch={refetch} />\n\n        {adapterErrors.map(error => (\n          <ManualOverridePanel\n            key={error.adapterName}\n            adapterName={error.adapterName}\n            attemptedUrl={error.attemptedUrl || 'URL not available'}\n            apiKey={apiKey}\n            onParseSuccess={handleParseSuccess}\n          />\n        ))}\n\n        {renderContent()}\n      </div>\n\n      <StatusDetailModal\n        isOpen={isModalOpen}\n        onClose={() => setIsModalOpen(false)}\n        status={{ title: 'Connection Error', details: (errorDetails as Error)?.message || 'No specific error message was provided.' }}\n      />\n    </>\n  );\n});\n",
    "web_service/frontend/app/components/LiveRaceDashboardNoSSR.tsx": "// web_platform/frontend/src/components/LiveRaceDashboardNoSSR.tsx\nimport dynamic from 'next/dynamic';\n\nconst LiveRaceDashboardNoSSR = dynamic(\n  () => import('./LiveRaceDashboard').then((mod) => mod.LiveRaceDashboard),\n  { ssr: false }\n);\n\nexport default LiveRaceDashboardNoSSR;\n",
    "web_service/frontend/next.config.mjs": "/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  output: 'export',  // Critical for static HTML export\n  distDir: 'out',\n  trailingSlash: true,\n  images: {\n    unoptimized: true  // Required for static export\n  },\n  async rewrites() {\n    return [\n      {\n        source: '/api/:path*',\n        destination: 'http://127.0.0.1:8000/api/:path*',\n      },\n    ]\n  },\n};\n\nexport default nextConfig;\n",
    "web_service/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"@tanstack/react-query\": \"^5.28.9\",\n    \"file-saver\": \"^2.0.5\",\n    \"lucide-react\": \"^0.548.0\",\n    \"next\": \"^14.2.33\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"socket.io-client\": \"^4.7.4\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"file-saver\": \"^2.0.5\",\n    \"next-pwa\": \"^5.6.0\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.3.0\",\n    \"typescript\": \"^5\"\n  }\n}\n",
    "web_service/frontend/public/sw.js": "if(!self.define){let e,s={};const a=(a,n)=>(a=new URL(a+\".js\",n).href,s[a]||new Promise(s=>{if(\"document\"in self){const e=document.createElement(\"script\");e.src=a,e.onload=s,document.head.appendChild(e)}else e=a,importScripts(a),s()}).then(()=>{let e=s[a];if(!e)throw new Error(`Module ${a} didn\u2019t register its module`);return e}));self.define=(n,t)=>{const i=e||(\"document\"in self?document.currentScript.src:\"\")||location.href;if(s[i])return;let c={};const r=e=>a(e,i),o={module:{uri:i},exports:c,require:r};s[i]=Promise.all(n.map(e=>o[e]||r(e))).then(e=>(t(...e),c))}}define([\"./workbox-4754cb34\"],function(e){\"use strict\";importScripts(),self.skipWaiting(),e.clientsClaim(),e.precacheAndRoute([{url:\"/_next/app-build-manifest.json\",revision:\"b6130f23369e5df052a4061c412f24fa\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_buildManifest.js\",revision:\"c155cce658e53418dec34664328b51ac\"},{url:\"/_next/static/YkCCvmjhdkIswKuIgvFNH/_ssgManifest.js\",revision:\"b6652df95db52feb4daf4eca35380933\"},{url:\"/_next/static/chunks/117-6326cd814d964913.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/816-7254031126ac0a96.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/928.d7f641058b89a54a.js\",revision:\"d7f641058b89a54a\"},{url:\"/_next/static/chunks/app/_not-found/page-e7dc36cd5a340c38.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/layout-605479d07717f01e.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/app/page-a2c385e93bfc2dac.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/fd9d1056-af804af0be509bea.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/framework-f66176bb897dc684.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-8563e00d234bd632.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/main-app-e0b3e4e952d25145.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_app-72b849fbd24ac258.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/pages/_error-7ba65e1336b92748.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/chunks/polyfills-42372ed130431b0a.js\",revision:\"846118c33b2c0e922d7b3a7676f81f6f\"},{url:\"/_next/static/chunks/webpack-d92cdde7bb2319ca.js\",revision:\"YkCCvmjhdkIswKuIgvFNH\"},{url:\"/_next/static/css/a55e4893d0564dbf.css\",revision:\"a55e4893d0564dbf\"},{url:\"/_next/static/media/19cfc7226ec3afaa-s.woff2\",revision:\"9dda5cfc9a46f256d0e131bb535e46f8\"},{url:\"/_next/static/media/21350d82a1f187e9-s.woff2\",revision:\"4e2553027f1d60eff32898367dd4d541\"},{url:\"/_next/static/media/8e9860b6e62d6359-s.woff2\",revision:\"01ba6c2a184b8cba08b0d57167664d75\"},{url:\"/_next/static/media/ba9851c3c22cd980-s.woff2\",revision:\"9e494903d6b0ffec1a1e14d34427d44d\"},{url:\"/_next/static/media/c5fe6dc8356a8c31-s.woff2\",revision:\"027a89e9ab733a145db70f09b8a18b42\"},{url:\"/_next/static/media/df0a9ae256c0569c-s.woff2\",revision:\"d54db44de5ccb18886ece2fda72bdfe0\"},{url:\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",revision:\"65850a373e258f1c897a2b3d75eb74de\"},{url:\"/manifest.json\",revision:\"23bffdb04aba9b85948642cffa772eae\"}],{ignoreURLParametersMatching:[]}),e.cleanupOutdatedCaches(),e.registerRoute(\"/\",new e.NetworkFirst({cacheName:\"start-url\",plugins:[{cacheWillUpdate:async({request:e,response:s,event:a,state:n})=>s&&\"opaqueredirect\"===s.type?new Response(s.body,{status:200,statusText:\"OK\",headers:s.headers}):s}]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:gstatic)\\.com\\/.*/i,new e.CacheFirst({cacheName:\"google-fonts-webfonts\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:31536e3})]}),\"GET\"),e.registerRoute(/^https:\\/\\/fonts\\.(?:googleapis)\\.com\\/.*/i,new e.StaleWhileRevalidate({cacheName:\"google-fonts-stylesheets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:eot|otf|ttc|ttf|woff|woff2|font.css)$/i,new e.StaleWhileRevalidate({cacheName:\"static-font-assets\",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),\"GET\"),e.registerRoute(/\\.(?:jpg|jpeg|gif|png|svg|ico|webp)$/i,new e.StaleWhileRevalidate({cacheName:\"static-image-assets\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/image\\?url=.+$/i,new e.StaleWhileRevalidate({cacheName:\"next-image\",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp3|wav|ogg)$/i,new e.CacheFirst({cacheName:\"static-audio-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:mp4)$/i,new e.CacheFirst({cacheName:\"static-video-assets\",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:js)$/i,new e.StaleWhileRevalidate({cacheName:\"static-js-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:css|less)$/i,new e.StaleWhileRevalidate({cacheName:\"static-style-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\/_next\\/data\\/.+\\/.+\\.json$/i,new e.StaleWhileRevalidate({cacheName:\"next-data\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(/\\.(?:json|xml|csv)$/i,new e.NetworkFirst({cacheName:\"static-data-assets\",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;const s=e.pathname;return!s.startsWith(\"/api/auth/\")&&!!s.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"apis\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:16,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>{if(!(self.origin===e.origin))return!1;return!e.pathname.startsWith(\"/api/\")},new e.NetworkFirst({cacheName:\"others\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),\"GET\"),e.registerRoute(({url:e})=>!(self.origin===e.origin),new e.NetworkFirst({cacheName:\"cross-origin\",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:3600})]}),\"GET\")});\n"
}