{
    ".github/dependabot.yml": "# To get started with Dependabot version updates, you'll need to specify which\n# package ecosystems to update and where the package manifests are located.\n# Please see the documentation for all configuration options:\n# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates\n\nversion: 2\nupdates:\n  - package-ecosystem: \"pip\" # See documentation for possible values\n    directory: \"/\" # Location of package manifests\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/web_platform/frontend\"\n    schedule:\n      interval: \"daily\"\n\n  - package-ecosystem: \"npm\"\n    directory: \"/electron\"\n    schedule:\n      interval: \"daily\"\n",
    ".github/workflows/codeql.yml": "# For most projects, this workflow file will not need changing; you simply need\n# to commit it to your repository.\n#\n# You may plcace this file in any folder within the .github/workflows folder.\n# GitHub will find and execute it.\n#\n# To learn more about the language matrix, please visit https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#changing-the-languages-that-are-analyzed\n\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    # The branches below must be a subset of the branches above\n    branches: [ main ]\n  schedule:\n    - cron: '22 5 * * 1'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'python', 'javascript' ]\n        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]\n        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    # Initializes the CodeQL tools for scanning.\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n        # If you wish to specify custom queries, you can do so here or in a config file.\n        # By default, queries are pulled from a suite stored in GitHub.\n        # See https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs\n        # queries: security-extended,security-and-quality\n\n    # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).\n    # If this step fails, then you should remove it and run the build manually (see below)\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v3\n\n    # \u2139\ufe0f Command-line programs to run using the OS shell.\n    # \ud83d\udcda See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun\n\n    #   If the Autobuild fails above, remove it and uncomment the following three lines.\n    #   and modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.\n\n    # - run: |\n    #     echo \"Run, Build Application using script\"\n    #     ./build.sh\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v3\n      with:\n        category: \"/language:${{matrix.language}}\"\n",
    "JSON_BACKUP_MANIFEST.md": "# Checkmate Ultimate Solo: JSON Backup Manifest (Total Recall Edition)\n\n**Purpose:** To provide a single, complete, and verified list of direct links to the JSON backups of all CORE and Operational files. This is the definitive entry point for external AI code review.\n\n---\n\n## 1.0 CORE Architecture (JSON Backups)\n\n### Python Backend\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/api.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/engine.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/models.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/__init__.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/base.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/utils.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/betfair_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/pointsbet_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/racing_and_sports_adapter.py.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/adapters/tvg_adapter.py.json\n\n### TypeScript Frontend\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/package.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/package-lock.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/next.config.mjs.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/tailwind.config.ts.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/tsconfig.json.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/page.tsx.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/layout.tsx.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/web_platform/frontend/src/app/globals.css.json\n\n---\n\n## 2.0 Operational & Tooling (JSON Backups)\n\n### Project Tooling\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/.gitignore.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/convert_to_json.py.json\n\n### Environment & Setup\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/setup_windows.bat.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/.env.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/python_service/requirements.txt.json\n\n### Strategic Blueprints\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/README.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/ARCHITECTURAL_MANDATE.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/HISTORY.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/STATUS.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/WISDOM.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/PROJECT_MANIFEST.md.json\nhttps://raw.githubusercontent.com/masonj0/fortuna/refs/heads/main/ReviewableJSON/ROADMAP_APPENDICES.md.json",
    "REBRANDING_AUDIT.md": "# Fortuna Faucet: Rebranding Audit Report\n\nThis report lists all files containing legacy branding terms (`checkmate`, `solo`).\n\n---\n\n- `./.env`\n- `./AGENTS.md`\n- `./ARCHITECTURAL_MANDATE_V8.1.md`\n- `./GEMINI_ONBOARDING.md`\n- `./HISTORY.md`\n- `./JSON_BACKUP_MANIFEST.md`\n- `./MANIFEST2.md`\n- `./MANIFEST3.md`\n- `./PROJECT_MANIFEST.md`\n- `./Procfile`\n- `./ROADMAP.md`\n- `./ROADMAP_APPENDICES.md`\n- `./WISDOM.md`\n- `./attic/ARCHITECTURAL_MANDATE_V7.2.md`\n- `./attic/build_python_service.py`\n- `./attic/checkmate_app.py`\n- `./attic/checkmate_engine.py`\n- `./attic/checkmate_monitor_v1.html`\n- `./attic/dashboard.py`\n- `./attic/desktop_app/App.xaml`\n- `./attic/desktop_app/App.xaml.cs`\n- `./attic/desktop_app/CheckmateDeck.csproj`\n- `./attic/desktop_app/Models/AdapterStatusDisplay.cs`\n- `./attic/desktop_app/Models/DisplayRace.cs`\n- `./attic/desktop_app/Services/DatabaseService.cs`\n- `./attic/desktop_app/Services/IDatabaseService.cs`\n- `./attic/desktop_app/ViewModels/MainViewModel.cs`\n- `./attic/desktop_app/Views/MainWindow.xaml`\n- `./attic/desktop_app/Views/MainWindow.xaml.cs`\n- `./attic/launcher.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_api.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_betfair_modern_adapter.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_fanduel_api_adapter.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_logic.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_models.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_racingpost_modern_adapter.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_run.py`\n- `./attic/legacy_tests_pre_triage/checkmate_v7/test_services.py`\n- `./attic/legacy_tests_pre_triage/test_checkmate_v7.py`\n- `./attic/legacy_tests_pre_triage/test_python_service.py`\n- `./attic/portable_demo_v2.py`\n- `./attic/rust_engine/rust_engine/Cargo.toml`\n- `./attic/rust_engine/rust_engine/benches/analysis_benchmark.rs`\n- `./attic/rust_engine/rust_engine/src/lib.rs`\n- `./attic/rust_engine/rust_engine/src/main.rs`\n- `./attic/the_one_script.py`\n- `./attic/tipsheet_generator.py`\n- `./attic/vba_source/Module_Charts.bas`\n- `./attic/vba_source/Module_DB.bas`\n- `./attic/vba_source/Module_UI.bas`\n- `./checkmate_web/engine.py`\n- `./checkmate_web/main.py`\n- `./checkmate_web/static/app.js`\n- `./checkmate_web/static/index.html`\n- `./command_deck.py`\n- `./diagnostic_report.txt`\n- `./launch_and_hunt.py`\n- `./launch_checkmate.bat`\n- `./launch_command_deck.bat`\n- `./manual_override_tool.py`\n- `./pg_schemas/historical_races.sql`\n- `./pytest.ini`\n- `./python_service/api.py`\n- `./python_service/checkmate_service.py`\n- `./python_service/minimal_service.py`\n- `./python_service/windows_service_wrapper.py`\n- `./run_server.py`\n- `./rust_engine/tests/integration_tests.rs`\n- `./shared_database/schema.sql`\n- `./src/checkmate_v7/adapters/AndWereOff.py`\n- `./src/checkmate_v7/adapters/Stablemates.py`\n- `./src/checkmate_v7/adapters/__init__.py`\n- `./src/checkmate_v7/api.py`\n- `./src/checkmate_v7/base.py`\n- `./src/checkmate_v7/cockpit.py`\n- `./src/checkmate_v7/config.py`\n- `./src/checkmate_v7/database.py`\n- `./src/checkmate_v7/headless_monitor.py`\n- `./src/checkmate_v7/logic.py`\n- `./src/checkmate_v7/models.py`\n- `./src/checkmate_v7/run.py`\n- `./src/checkmate_v7/services.py`\n- `./src/checkmate_v7/settings.py`\n- `./src/paddock_parser/prediction_engine.py`\n- `./web_platform/api_gateway/package-lock.json`\n- `./web_platform/api_gateway/src/server.ts`\n- `./web_platform/api_gateway/src/services/DatabaseService.ts`\n- `./web_platform/frontend/.next/server/app/page.js`\n- `./web_platform/frontend/app/layout.tsx`\n- `./web_platform/frontend/src/components/RaceCard.tsx`\n- `./web_platform/frontend/src/types/racing.ts`\n",
    "USER_GUIDE.md": "# \ud83c\udfaf Fortuna Faucet: Complete User Guide for Windows Hobbyists\n\n## What Is This Amazing Software?\n\n**Fortuna Faucet** is a professional-grade horse racing analysis platform that:\n- \ud83d\udcca Aggregates data from **20+ global racing sources** simultaneously\n- \ud83e\udd16 Uses AI-powered analysis to find value betting opportunities\n- \ud83d\udcc8 Provides live odds monitoring via Betfair Exchange\n- \ud83c\udf10 Features a beautiful web dashboard for real-time insights\n- \ud83d\udd04 Runs automatically in the background like a professional service\n\nThink of it as your personal racing intelligence agency!\n\n---\n\n## \ud83d\ude80 Quick Start (15 Minutes to Racing!)\n\n### Step 1: One-Click Installation\n1. Extract all files to `C:\\FortunaFaucet` (or your preferred location)\n2. **Right-click** `INSTALL_FORTUNA.bat` \u2192 **Run as Administrator**\n3. Wait 3-5 minutes while it automatically installs:\n   - Python 3.11 (if needed)\n   - Node.js (if needed)\n   - All required packages\n\n### Step 2: Quick Configuration\n1. **Double-click** `setup_wizard.py` in your folder\n2. Follow the friendly prompts to configure:\n   - Your private API key (auto-generated)\n   - Betfair credentials (optional, for live odds)\n3. The wizard creates your `.env` file automatically!\n\n### Step 3: Launch!\n- **Double-click** the \"Launch Fortuna\" shortcut on your desktop\n- Wait 10 seconds for services to start\n- Your dashboard opens automatically in your browser! \ud83c\udf89\n\n---\n\n## \ud83c\udfae Using Your New Command Center\n\n### The Dashboard (http://localhost:3000)\nYour racing command center features:\n\n**\ud83d\udcca Statistics Panel** (Top of screen)\n- **Qualified Races**: How many races meet your criteria\n- **Premium Targets**: High-score opportunities (80%+)\n- **Next Race**: Countdown to the next qualifying race\n- **Avg Field Size**: Average number of horses\n\n**\ud83c\udf9b\ufe0f Smart Filters** (Middle section)\nCustomize what you see:\n- **Min Score Slider**: Only show races above X% match\n- **Max Field Size**: Filter by number of runners (8, 10, 12, or Any)\n- **Sort By**: Order by score, time, or track name\n\n**\ud83c\udfc7 Race Cards** (Main display)\nEach card shows:\n- Track name and race number\n- Qualification score (color-coded!)\n- Race conditions (distance, surface)\n- Top 3 contenders with best odds\n- Data source count\n\n### Color Coding System\n- \ud83d\udd34 **Red (80%+)**: Premium betting opportunity!\n- \ud83d\udfe1 **Yellow (60-79%)**: Good value potential\n- \ud83d\udfe2 **Green (<60%)**: Meets minimum criteria\n\n---\n\n## \ud83d\udd27 Advanced Features\n\n### Live Odds Monitoring\nOnce you've added Betfair credentials:\n1. The system automatically tracks races approaching post time\n2. Updates odds every 30 seconds for races within 5 minutes\n3. Highlights dramatic odds movements\n\n### Desktop Monitor Tool\nRun `fortuna_monitor.py` for a real-time status window:\n- Shows all data source health\n- Performance graphs (with matplotlib)\n- Success rates and fetch durations\n- Quick \"Refresh Now\" button\n\n### Auto-Start on Windows Boot\nRun `SCHEDULE_FORTUNA.bat` (as Administrator):\n- Fortuna starts when you log into Windows\n- Daily 3 AM restart for fresh data\n- Runs silently in the background\n\n---\n\n## \ud83c\udfaf Understanding the \"Trifecta Analyzer\"\n\nThis is the brain! It scores races on three factors:\n\n### Factor 1: Field Size (smaller is better)\n- **Why**: Fewer horses = easier to predict\n- **Default**: Maximum 10 runners\n\n### Factor 2: Favorite's Odds (higher is better)\n- **Why**: If the favorite is 2.5+, the race is wide open\n- **Default**: Minimum 2.5\n\n### Factor 3: Second Favorite's Odds (higher is better)\n- **Why**: Confirms multiple horses are competitive\n- **Default**: Minimum 4.0\n\n**The Score**: Combines all three into a 0-100% match rating!\n\n---\n\n## \ud83d\udcda System Architecture (Simplified)\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     \ud83c\udf10 Next.js Dashboard (Port 3000)    \u2502\n\u2502     Your beautiful web interface        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502 API Calls\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   \ud83d\udc0d Python FastAPI Backend (Port 8000) \u2502\n\u2502   - OddsEngine: Fetches from 20+ sources\u2502\n\u2502   - TrifectaAnalyzer: Scores races      \u2502\n\u2502   - LiveOddsMonitor: Betfair tracking   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502 Async Requests\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     \ud83d\udd0c Adapter Fleet (20+ sources)      \u2502\n\u2502  TVG \u2022 Betfair \u2022 TimeForm \u2022 GBGB       \u2502\n\u2502  RacingAndSports \u2022 USTA \u2022 And more!     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udd10 Security Notes\n\n### API Keys\n- **Your local API_KEY**: Only for communication between YOUR backend and frontend\n- Never shared online, never exposed\n- Auto-generated during setup\n\n### External API Keys (Optional)\nAdd these to `.env` for more data sources:\n```\nTVG_API_KEY=\"your_tvg_key\"\nRACING_AND_SPORTS_TOKEN=\"your_ras_token\"\nTHE_RACING_API_KEY=\"your_theracingapi_key\"\n```\n\nGet keys from:\n- TVG: https://www.tvg.com/promos/developer-api\n- Racing and Sports: https://www.racingandsports.com/data-api/\n- The Racing API: https://www.theracingapi.com/\n\n---\n\n## \ud83d\udee0\ufe0f Troubleshooting\n\n### \"Backend Offline\" Error\n```batch\n# Stop everything cleanly\nSTOP_FORTUNA.bat\n\n# Wait 10 seconds, then restart\nLAUNCH_FORTUNA.bat\n```\n\n### Dashboard Loads But No Data\n1. Open `http://localhost:8000/health` in browser\n2. Should show: `{\"status\": \"OK\"}`\n3. If not, check Python backend window for errors\n\n### \"Port Already In Use\" Error\nSomeone else is using port 8000 or 3000:\n```batch\n# Windows: Kill processes on those ports\nnetstat -ano | findstr :8000\ntaskkill /PID [number] /F\n\nnetstat -ano | findstr :3000\ntaskkill /PID [number] /F\n```\n\n### Reset Everything\n```batch\n# Nuclear option: Clean slate\nSTOP_FORTUNA.bat\ndel .env\nsetup_wizard.py\nINSTALL_FORTUNA.bat\n```\n\n---\n\n## \ud83d\udcd6 File Structure Explained\n\n### Critical Files (Don't Delete!)\n- `.env` - Your configuration (API keys, settings)\n- `requirements.txt` - Python packages list\n- `package.json` - Node.js packages list\n\n### Convenience Scripts\n- `LAUNCH_FORTUNA.bat` - Start everything\n- `STOP_FORTUNA.bat` - Stop everything\n- `RESTART_FORTUNA.bat` - Clean restart\n- `setup_wizard.py` - Interactive config tool\n\n### Python Backend (`python_service/`)\n- `api.py` - Web server (FastAPI)\n- `engine.py` - Master data orchestrator\n- `analyzer.py` - Race scoring logic\n- `models.py` - Data structure definitions\n- `adapters/` - Individual data source plugins\n\n### Frontend (`web_platform/frontend/`)\n- `src/app/page.tsx` - Main dashboard\n- `src/components/RaceCard.tsx` - Individual race display\n- `.env.local` - Frontend API key\n\n---\n\n## \ud83c\udf93 Customization Ideas\n\n### Change Analyzer Thresholds\nEdit `python_service/analyzer.py`:\n```python\nclass TrifectaAnalyzer(BaseAnalyzer):\n    def __init__(self,\n                 max_field_size: int = 8,      # \u2190 Change this\n                 min_favorite_odds: float = 3.0, # \u2190 Or this\n                 min_second_favorite_odds: float = 5.0): # \u2190 Or this\n```\n\n### Add New Data Sources\n1. Copy `python_service/adapters/template_adapter.py`\n2. Rename and implement the `fetch_races()` method\n3. Register in `python_service/adapters/__init__.py`\n4. Add to `python_service/engine.py` adapter list\n\n### Customize Dashboard Colors\nEdit `web_platform/frontend/tailwind.config.ts`:\n```typescript\ntheme: {\n  extend: {\n    colors: {\n      'fortuna-primary': '#your-hex-color',\n    }\n  }\n}\n```\n\n---\n\n## \ud83d\udca1 Pro Tips\n\n### Tip 1: Use Windows Task Scheduler\nRun `SCHEDULE_FORTUNA.bat` for:\n- Auto-start on login\n- Daily 3 AM maintenance restart\n\n### Tip 2: Monitor Multiple Days\nThe analyzer works for \"today\" by default, but you can query any date:\n```\nhttp://localhost:8000/api/races/qualified/trifecta?race_date=2025-10-25\n```\n\n### Tip 3: Export Data\nThe API returns pure JSON. Use tools like:\n- **Postman** for testing\n- **PowerShell** for scripting:\n```powershell\nInvoke-RestMethod -Uri \"http://localhost:8000/api/races/qualified/trifecta\" `\n  -Headers @{\"X-API-Key\"=\"your_key\"} | ConvertTo-Json -Depth 10\n```\n\n### Tip 4: Mobile Access\nIf you want to check from your phone on the same WiFi:\n1. Find your PC's IP: `ipconfig` in Command Prompt\n2. Open firewall port 3000\n3. Access from phone: `http://192.168.1.X:3000`\n\n---\n\n## \ud83c\udf89 You're Ready!\n\nThis is a **professional-grade** system that you now control. It was built with years of racing analytics experience and modern software practices.\n\n### What You Can Do Now:\n\u2705 Track races from 20+ global sources\n\u2705 Identify value opportunities with AI scoring\n\u2705 Monitor live odds movements\n\u2705 Run 24/7 as a background service\n\u2705 Customize thresholds and filters\n\u2705 Expand with new data sources\n\n**Welcome to the world of algorithmic racing analysis!** \ud83c\udfc7\ud83d\ude80\n\n---\n\n## \ud83d\udcde Additional Resources\n\n### Project Documentation\n- `HISTORY.md` - Project evolution story\n- `ARCHITECTURAL_MANDATE.md` - System design principles\n- `WISDOM.md` - Developer best practices\n- `ROADMAP_APPENDICES.md` - Future expansion ideas\n\n### Useful Commands\n```batch\n# View all active Python processes\ntasklist | findstr python\n\n# Check if ports are available\nnetstat -ano | findstr :8000\nnetstat -ano | findstr :3000\n\n# Update Python packages\n.venv\\Scripts\\activate\npip install --upgrade -r requirements.txt\n```\n\n### Need Help?\n1. Check `fortuna_restart.log` for error history\n2. Run `fortuna_monitor.py` to see real-time system status\n3. Verify `.env` file has all required keys\n\nHappy Racing! \ud83c\udfb0\ud83c\udfc6",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2)---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "assets/sounds/.gitkeep": "# This directory is for audio alert sound files (e.g., alert_premium.wav)",
    "check_port.py": "# check_port.py\nimport socket\nimport time\n\n\ndef check_server_status(host, port):\n    \"\"\"Checks if the server is accessible.\"\"\"\n    time.sleep(5)  # Give server time to start\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.connect((host, port))\n            print(\"SERVER CHECK: SUCCESS! Server is running and accessible.\")\n            return True\n        except ConnectionRefusedError:\n            print(\"SERVER CHECK: FAILED! Server is not accessible.\")\n            return False\n\n\nif __name__ == \"__main__\":\n    check_server_status(\"127.0.0.1\", 8000)\n",
    "create_fortuna_json.py": "# DEPRECATED\n# This script is no longer in use and is superseded by ARCHIVE_PROJECT.py\n\nimport sys\n\nprint(\"ERROR: This script is deprecated.\", file=sys.stderr)\nprint(\n    \"Please use 'python ARCHIVE_PROJECT.py' to generate the project archives.\",\n    file=sys.stderr,\n)\nsys.exit(1)\n",
    "electron/electron-builder-config.yml": "appId: com.fortuna.faucet\nproductName: \"Fortuna Faucet\"\ndirectories:\n  buildResources: \"assets\"\n  output: \"dist\"\nfiles:\n  - \"main.js\"\n  - \"preload.js\"\n  - \"package.json\"\n  - \"secure-settings-manager.js\"\n  - \"web-ui-build/out/**/*\"\n  - \"assets/**/*\"\nextraResources:\n  - from: \"resources/fortuna-backend.exe\"\n    to: \"fortuna-backend.exe\"\nwin:\n  target: \"msi\"\n  icon: \"assets/icon.ico\"\nmsi:\n  oneClick: false\n  perMachine: true\n  runAfterFinish: true\n  createDesktopShortcut: true\n  createStartMenuShortcut: true\n  shortcutName: \"Fortuna Faucet\"\n  menuCategory: \"Fortuna Faucet\"\n",
    "fortuna_monitor.py": "# fortuna_monitor.py - Windows-Optimized Version\n\nimport os\nimport sys\nimport threading\nimport time\nimport tkinter as tk\nfrom collections import deque\nfrom datetime import datetime\nfrom tkinter import filedialog\nfrom tkinter import messagebox\n\nimport httpx\nimport psutil\n\n# Try to import matplotlib for graphs\ntry:\n    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n    from matplotlib.figure import Figure\n\n    GRAPHS_AVAILABLE = True\nexcept ImportError:\n    GRAPHS_AVAILABLE = False\n\nAPI_BASE_URL = \"http://localhost:8000\"\n\n\nclass PerformanceTracker:\n    def __init__(self, max_history=100):\n        self.timestamps = deque(maxlen=max_history)\n        self.race_counts = deque(maxlen=max_history)\n        self.fetch_durations = deque(maxlen=max_history)\n        self.success_rates = deque(maxlen=max_history)\n        self.cpu_usage = deque(maxlen=max_history)\n        self.memory_usage = deque(maxlen=max_history)\n\n    def add_datapoint(self, races, duration, success_rate):\n        self.timestamps.append(datetime.now())\n        self.race_counts.append(races)\n        self.fetch_durations.append(duration)\n        self.success_rates.append(success_rate)\n        self.cpu_usage.append(psutil.cpu_percent(interval=None))\n        process = psutil.Process(os.getpid())\n        self.memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n\n    def export_to_csv(self, filename):\n        import csv\n\n        history = self.get_history()\n        with open(filename, \"w\", newline=\"\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"Timestamp\", \"Races\", \"Duration\", \"Success Rate\", \"CPU %\", \"Memory MB\"])\n            for i in range(len(history[\"times\"])):\n                writer.writerow(\n                    [\n                        history[\"times\"][i].isoformat(),\n                        history[\"races\"][i],\n                        history[\"durations\"][i],\n                        history[\"success\"][i],\n                        history[\"cpu\"][i],\n                        history[\"memory\"][i],\n                    ]\n                )\n\n    def get_history(self):\n        return {\n            \"times\": list(self.timestamps),\n            \"races\": list(self.race_counts),\n            \"durations\": list(self.fetch_durations),\n            \"success\": list(self.success_rates),\n            \"cpu\": list(self.cpu_usage),\n            \"memory\": list(self.memory_usage),\n        }\n\n\nclass FortunaAdvancedMonitor(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"Fortuna Faucet - Advanced Monitor\")\n        self.geometry(\"900x650\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.performance = PerformanceTracker()\n        self.running = True\n        self._create_widgets()\n        self.after(100, self.start_fetch_thread)\n\n    def _create_widgets(self):\n        self._create_control_panel()\n        # ... (rest of the widget creation)\n\n    def _create_control_panel(self):\n        control_frame = tk.Frame(self, bg=\"#1a1a2e\")\n        control_frame.pack(fill=tk.X, padx=15, pady=10)\n\n        tk.Button(\n            control_frame,\n            text=\"\ud83d\udcca Export Performance Data\",\n            command=self.export_data,\n            bg=\"#0f3460\",\n            fg=\"#ffffff\",\n            font=(\"Segoe UI\", 10, \"bold\"),\n            relief=tk.FLAT,\n            padx=25,\n            pady=10,\n        ).pack(side=tk.LEFT, padx=5)\n\n        tk.Button(\n            control_frame,\n            text=\"\ud83d\udcbb System Info\",\n            command=self.show_system_info,\n            bg=\"#0f3460\",\n            fg=\"#ffffff\",\n            font=(\"Segoe UI\", 10, \"bold\"),\n            relief=tk.FLAT,\n            padx=25,\n            pady=10,\n        ).pack(side=tk.LEFT, padx=5)\n\n    def start_fetch_thread(self):\n        self.fetch_thread = threading.Thread(target=self._fetch_data_loop, daemon=True)\n        self.fetch_thread.start()\n\n    def _fetch_data_loop(self):\n        while self.running:\n            try:\n                # Use httpx for async requests\n                with httpx.Client(headers={\"X-API-KEY\": self.api_key}, timeout=5) as client:\n                    response = client.get(f\"{API_BASE_URL}/api/adapters/status\")\n                if response.status_code == 200:\n                    data = response.json()\n                    # Add performance datapoint\n                    total_races = sum(a.get(\"races_fetched\", 0) for a in data)\n                    successful_adapters = [a for a in data if a.get(\"status\") == \"SUCCESS\"]\n                    success_rate = (len(successful_adapters) / len(data) * 100) if data else 0\n                    avg_duration = (\n                        sum(a.get(\"fetch_duration\", 0) for a in successful_adapters) / len(successful_adapters)\n                        if successful_adapters\n                        else 0\n                    )\n                    self.performance.add_datapoint(total_races, avg_duration, success_rate)\n\n                    self.after(0, self.update_ui, data)\n            except httpx.RequestError:\n                pass\n            time.sleep(10)  # Refresh interval\n\n    def update_ui(self, data):\n        # This is where you would update the tkinter UI with the new data\n        # For example, you might update a treeview or a graph\n        pass\n\n    def export_data(self):\n        filename = filedialog.asksaveasfilename(\n            defaultextension=\".csv\",\n            filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")],\n            initialfile=f\"fortuna_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n        )\n        if filename:\n            try:\n                self.performance.export_to_csv(filename)\n                messagebox.showinfo(\"Success\", f\"Data exported to {filename}\")\n            except Exception as e:\n                messagebox.showerror(\"Error\", f\"Export failed: {e}\")\n\n    def show_system_info(self):\n        vm = psutil.virtual_memory()\n        info = f\"\"\"\nSystem Information:\n\nCPU Usage: {psutil.cpu_percent(interval=1)}%\nCPU Cores: {psutil.cpu_count()}\nMemory Total: {vm.total / 1024 / 1024 / 1024:.2f} GB\nMemory Available: {vm.available / 1024 / 1024 / 1024:.2f} GB\nMemory Used: {vm.percent}%\n\nDisk Usage: {psutil.disk_usage(\"/\").percent}%\nPython Version: {sys.version.split()[0]}\n\"\"\"\n        messagebox.showinfo(\"System Information\", info)\n\n    def on_closing(self):\n        self.running = False\n        self.destroy()\n\n\nif __name__ == \"__main__\":\n    # Load .env variables\n    try:\n        from dotenv import load_dotenv\n\n        load_dotenv()\n    except ImportError:\n        print(\"Warning: dotenv is not installed. Script assumes environment variables are set.\")\n    app = FortunaAdvancedMonitor()\n    app.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n    app.mainloop()\n",
    "jules-scratch/verification/verify_error_handling.py": "# jules-scratch/verification/verify_error_handling.py\nfrom playwright.sync_api import expect\nfrom playwright.sync_api import sync_playwright\n\n\ndef run(playwright):\n    browser = playwright.chromium.launch(headless=True)\n    page = browser.new_page()\n\n    # Mock the API call to return an error\n    page.route(\n        \"**/api/races/qualified/trifecta?**\",\n        lambda route: route.fulfill(\n            status=500,\n            json={\n                \"error\": {\n                    \"message\": \"A data source is currently unavailable.\",\n                    \"suggestion\": \"This is usually temporary. Please try again in a few minutes.\",\n                    \"details\": \"AdapterHttpError: HTTP Error 503 for https://example.com\",\n                }\n            },\n        ),\n    )\n\n    page.goto(\"http://localhost:3000\")\n\n    # Wait for the status indicator to show \"Offline\"\n    offline_indicator = page.get_by_text(\"Offline\")\n    expect(offline_indicator).to_be_visible()\n\n    # Now that we know the app is in an error state, check for the detailed message\n    error_message = page.get_by_text(\"A data source is currently unavailable.\")\n    expect(error_message).to_be_visible()\n\n    page.screenshot(path=\"jules-scratch/verification/error_handling.png\")\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n",
    "jules-scratch/verification/verify_frontend.py": "from playwright.sync_api import sync_playwright\n\n\ndef run():\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        page = browser.new_page()\n        page.goto(\"http://localhost:3000\")\n        page.screenshot(path=\"jules-scratch/verification/verification.png\")\n        browser.close()\n\n\nif __name__ == \"__main__\":\n    run()\n",
    "jules-scratch/verification/verify_websockets.py": "from playwright.sync_api import sync_playwright, expect\nimport time\n\ndef run(playwright):\n    browser = playwright.chromium.launch(headless=True)\n    page = browser.new_page()\n\n    # Mock the Electron API and the WebSocket connection\n    page.add_init_script(\"\"\"\n        window.electronAPI = {\n            getApiKey: () => Promise.resolve('test-api-key'),\n            getBackendStatus: () => Promise.resolve({ state: 'running', logs: [] }),\n            onBackendStatusUpdate: (callback) => {\n                // Do nothing, assume it's always running\n                return () => {}; // Return an unsubscribe function\n            }\n        };\n\n        const mockSocket = {\n            listeners: {},\n            on(event, callback) {\n                this.listeners[event] = callback;\n            },\n            close() {},\n            readyState: 0, // Initially connecting\n            send() {},\n        };\n        window.mockSocket = mockSocket;\n        window.WebSocket = function(url) {\n            console.log('Mock WebSocket created for:', url);\n            setTimeout(() => {\n                mockSocket.readyState = 1; // OPEN\n                if(mockSocket.listeners.open) {\n                    mockSocket.listeners.open();\n                }\n            }, 100);\n            return mockSocket;\n        };\n    \"\"\")\n\n    # Intercept the API call and return a mock response\n    page.route(\"**/api/adapters/status\", lambda route: route.fulfill(\n        status=200,\n        json=[]\n    ))\n\n    page.goto(\"http://localhost:3000\")\n\n    # Wait for the status indicator to be visible\n    page.wait_for_selector('[data-testid=\"status-indicator\"]')\n\n    # Check that the status indicator eventually shows \"Live\"\n    expect(page.locator('[data-testid=\"status-indicator\"]')).to_have_text(\"Live\", timeout=5000)\n\n    # Simulate a WebSocket message with race data\n    page.evaluate(\"\"\"\n        window.mockSocket.listeners.message({\n            data: JSON.stringify({\n                races: [{\n                    id: 'test-race-1',\n                    venue: 'Test Park',\n                    raceNumber: 1,\n                    isErrorPlaceholder: false,\n                    startTime: new Date().toISOString(),\n                    runners: [{\n                        number: 1,\n                        name: 'Test Horse',\n                        odds: { 'TestSource': { win: 5.0, source: 'TestSource', last_updated: new Date().toISOString() } }\n                    }]\n                }],\n                source_info: [{\n                    name: 'TestSource',\n                    status: 'SUCCESS',\n                    races_fetched: 1,\n                    fetch_duration: 0.1\n                }]\n            })\n        });\n    \"\"\")\n\n    # Check that the UI has updated with the new data\n    expect(page.get_by_text(\"Test Park\")).to_be_visible()\n    expect(page.get_by_text(\"Test Horse\")).to_be_visible()\n\n    page.screenshot(path=\"jules-scratch/verification/websockets.png\")\n    browser.close()\n\nwith sync_playwright() as playwright:\n    run(playwright)\n",
    "playwright_test.js": "const { chromium } = require('playwright');\nconst { test, expect } = require('@playwright/test');\n\n(async () => {\n  const browser = await chromium.launch();\n  const page = await browser.newPage();\n\n  // Navigate to the dashboard\n  await page.goto('http://localhost:3001');\n\n  // Wait for the initial loading to complete.\n  // We expect the skeleton loaders to disappear.\n  await expect(page.locator('div:has-text(\"Loading races...\")')).toHaveCount(0, { timeout: 15000 });\n\n  // Check for the manual override panel\n  const overridePanel = page.locator('div:has-text(\"Fetch Failed: AtTheRaces\")');\n  await expect(overridePanel).toBeVisible({ timeout: 10000 });\n\n  // Check for the text area with the correct URL\n  // The date is a placeholder, as it can change. The important part is the base URL.\n  const textArea = overridePanel.locator('textarea');\n  await expect(textArea).toHaveAttribute('value', /https:\\/\\/www\\.attheraces\\.com\\/racecards\\/\\d{4}-\\d{2}-\\d{2}/);\n\n\n  // Take a screenshot for visual confirmation\n  await page.screenshot({ path: 'manual-override-panel.png' });\n\n  await browser.close();\n})();\n",
    "pyproject.toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"paddock-parser-ng\"\nversion = \"0.1.0\"\ndescription = \"A toolkit to identify the best racecards for betting.\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\n\n[project.scripts]\npaddock_parser_ui = \"paddock_parser.entry_points:run_terminal_ui\"\npaddock_parser_dashboard = \"paddock_parser.entry_points:run_dashboard\"\npaddock_parser_predict = \"paddock_parser.entry_points:run_prediction_engine\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n# Configuration for the Ruff linter\n[tool.ruff]\n# Allow lines to be up to 120 characters long.\nline-length = 120\n\n[tool.ruff.lint]\n# Enable Pyflakes (F), pycodestyle (E, W), and isort (I) rules.\nselect = [\"E\", \"F\", \"W\", \"I\"]\nignore = []\n\n[tool.ruff.lint.isort]\n# Sort imports within their sections alphabetically.\nforce-single-line = true\n",
    "python_service/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass AtTheRacesAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for attheraces.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"AtTheRaces\"\n    BASE_URL = \"https://www.attheraces.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch AtTheRaces index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        all_races = []\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to AtTheRacesAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                header_element = soup.select_one(\"h1.heading-racecard-title\")\n                if not header_element:\n                    continue\n                header = header_element.get_text()\n                track_name_raw, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n                track_name = normalize_venue_name(track_name_raw)\n                active_link = soup.select_one(\"a.race-time-link.active\")\n                race_number = 1\n                if active_link:\n                    parent_div = active_link.find_parent(\"div\", \"races\")\n                    if parent_div:\n                        all_links = parent_div.select(\"a.race-time-link\")\n                        race_number = all_links.index(active_link) + 1\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time, \"%H:%M\").time())\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n                race = Race(\n                    id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from AtTheRaces, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_element = row.select_one(\"h3.horse-name a\")\n            if not name_element:\n                return None\n            name = clean_text(name_element.get_text())\n\n            num_element = row.select_one(\"span.horse-number\")\n            if not num_element:\n                return None\n            num_str = clean_text(num_element.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_element = row.select_one(\"button.best-odds\")\n            odds_str = clean_text(odds_element.get_text()) if odds_element else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on AtTheRaces, skipping runner.\")\n            return None\n",
    "python_service/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairExchange\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "python_service/adapters/drf_adapter.py": "# python_service/adapters/drf_adapter.py\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass DRFAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for drf.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"DRF\"\n    BASE_URL = \"https://www.drf.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetches the raw HTML from the DRF entries page.\"\"\"\n        url = f\"/entries/{date}/USA\"\n        response = await self.make_request(self.http_client, \"GET\", url)\n        return {\"html\": response.text, \"date\": date} if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[dict]) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        venue_node = soup.select_one(\"div.track-info h1\")\n        if not venue_node:\n            self.logger.warning(\"Could not find venue name on DRF page.\")\n            return []\n\n        venue_text = venue_node.text\n        venue = normalize_venue_name(venue_text.split(\" - \")[0].replace(\"Entries for \", \"\"))\n\n        races = []\n        for race_entry in soup.select(\"div.race-entries\"):\n            try:\n                race_number_str = race_entry.get(\"data-race-number\")\n                if not race_number_str or not race_number_str.isdigit():\n                    continue\n                race_number = int(race_number_str)\n\n                post_time_node = race_entry.select_one(\".post-time\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text.replace(\"Post Time: \", \"\").strip()\n                start_time = parse(f\"{race_date} {post_time_str}\")\n\n                runners = []\n                for entry in race_entry.select(\"li.entry\"):\n                    if \"scratched\" in entry.get(\"class\", []):\n                        continue\n\n                    number_node = entry.select_one(\".program-number\")\n                    if not number_node or not number_node.text.isdigit():\n                        continue\n                    number = int(number_node.text)\n\n                    name_node = entry.select_one(\".horse-name\")\n                    if not name_node:\n                        continue\n                    name = name_node.text\n\n                    odds_node = entry.select_one(\".odds\")\n                    odds_str = odds_node.text.replace(\"-\", \"/\") if odds_node else \"\"\n\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    odds = {}\n                    if win_odds:\n                        odds[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(Runner(number=number, name=name, odds=odds))\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"drf_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                    field_size=len(runners),\n                )\n                races.append(race)\n            except (ValueError, KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a race on DRF, skipping.\", exc_info=True)\n                continue\n        return races\n",
    "python_service/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass SportingLifeAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to SportingLifeAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"a.hr-race-header-course-name__link\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.hr-race-header-time__time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n                race_number = 1\n                if active_link:\n                    all_links = soup.select(\"a.hr-race-header-navigation-link\")\n                    try:\n                        race_number = all_links.index(active_link) + 1\n                    except ValueError:\n                        pass  # Keep default race number if active link not in all links\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from SportingLife, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.hr-racing-runner-horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.hr-racing-runner-saddle-cloth-no\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.select_one(\"span.hr-racing-runner-odds\")\n            odds_str = clean_text(odds_node.get_text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {\n                    self.source_name: OddsData(\n                        win=win_odds,\n                        source=self.source_name,\n                        last_updated=datetime.now(),\n                    )\n                }\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on SportingLife, skipping runner.\")\n            return None\n",
    "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for The Racing API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(self.http_client, \"GET\", endpoint, headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in TheRacingAPI response.\")\n            return []\n\n        races = []\n        for race_data in raw_data.get(\"racecards\", []):\n            try:\n                race_id = race_data.get(\"race_id\")\n                off_time = race_data.get(\"off_time\")\n                course = race_data.get(\"course\")\n                race_no = race_data.get(\"race_no\")\n\n                if not all([race_id, off_time, course, race_no]):\n                    continue\n\n                start_time = datetime.fromisoformat(off_time.replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_id}\",\n                    venue=course,\n                    race_number=race_no,\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_data.get(\"race_id\"),\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                horse = runner_data.get(\"horse\")\n                if not horse:\n                    continue\n\n                odds_data = {}\n                odds_list = runner_data.get(\"odds\", [])\n                if odds_list:\n                    odds_decimal_str = odds_list[0].get(\"odds_decimal\")\n                    if odds_decimal_str:\n                        win_odds = Decimal(str(odds_decimal_str))\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=horse,\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI runner\",\n                    runner_name=runner_data.get(\"horse\"),\n                    exc_info=True,\n                )\n        return runners\n",
    "python_service/adapters/twinspires_adapter.py": "# python_service/adapters/twinspires_adapter.py\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for twinspires.com.\n    This is a placeholder for a full implementation using the discovered JSON API.\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        [MODIFIED FOR OFFLINE DEVELOPMENT]\n        Reads HTML content from a local fixture file instead of making a live API call.\n        This is a temporary measure to allow development while the live API is blocking requests.\n        \"\"\"\n        # Read the local HTML fixture\n        try:\n            with open(\"tests/fixtures/twinspires_sample.html\", \"r\") as f:\n                html_content = f.read()\n        except FileNotFoundError:\n            self.logger.error(\"TwinSpires test fixture not found.\")\n            return None\n\n        # To maintain the data structure the parser expects, we will create a mock\n        # raw_data object that resembles the original API response, but includes\n        # the HTML content.\n        return {\n            \"html_content\": html_content,\n            \"mock_track_data\": {\n                \"trackId\": \"cd\",\n                \"trackName\": \"Churchill Downs\",\n                \"raceType\": \"Thoroughbred\"\n            },\n            \"mock_race_card\": {\n                \"raceNumber\": 5,\n                \"postTime\": \"2025-10-26T16:30:00Z\"\n            }\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        [MODIFIED FOR OFFLINE DEVELOPMENT]\n        Parses race and runner data from the mock raw_data object, which now\n        includes the HTML content from the local fixture.\n        \"\"\"\n        if not raw_data or \"html_content\" not in raw_data:\n            return []\n\n        self.logger.info(\"Parsing TwinSpires data from local fixture.\")\n\n        html_content = raw_data[\"html_content\"]\n        track = raw_data[\"mock_track_data\"]\n        race_card = raw_data[\"mock_race_card\"]\n\n        # Parse the runners from the HTML content\n        runners = self._parse_runners_from_html(html_content)\n\n        try:\n            start_time = datetime.fromisoformat(race_card.get(\"postTime\").replace(\"Z\", \"+00:00\"))\n\n            race = Race(\n                id=f\"ts_{track.get('trackId')}_{race_card.get('raceNumber')}\",\n                venue=track.get(\"trackName\"),\n                race_number=race_card.get(\"raceNumber\"),\n                start_time=start_time,\n                discipline=track.get(\"raceType\", \"Unknown\"),\n                runners=runners,\n                source=self.SOURCE_NAME,\n            )\n            return [race]\n        except Exception as e:\n            self.logger.warning(\n                \"Failed to parse race card from mock data.\",\n                error=e,\n                exc_info=True,\n            )\n            return []\n\n    def _parse_runners_from_html(self, html_content: str) -> List[Runner]:\n        \"\"\"Parses runner data from a race card's HTML content.\"\"\"\n        runners = []\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        runner_elements = soup.select(\"li.runner\")\n\n        for element in runner_elements:\n            try:\n                scratched = \"scratched\" in element.get(\"class\", [])\n\n                number_tag = element.select_one(\"span.runner-number\")\n                name_tag = element.select_one(\"span.runner-name\")\n                odds_tag = element.select_one(\"span.runner-odds\")\n\n                if not all([number_tag, name_tag, odds_tag]):\n                    continue\n\n                number = int(number_tag.text.strip())\n                name = name_tag.text.strip()\n                odds_str = odds_tag.text.strip()\n\n                odds = {}\n                if not scratched and odds_str not in [\"SCR\", \"\"]:\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    if win_odds:\n                        odds[self.SOURCE_NAME] = OddsData(\n                            win=win_odds,\n                            source=self.SOURCE_NAME,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=number,\n                        name=name,\n                        scratched=scratched,\n                        odds=odds,\n                    )\n                )\n            except (ValueError, TypeError) as e:\n                self.logger.warning(\"Failed to parse a runner, skipping.\", error=e, exc_info=True)\n                continue\n\n        return runners\n\n    async def _get_races_async(self, date: str) -> List[Race]:\n        raw_data = await self._fetch_data(date)\n        return self._parse_races(raw_data)\n\n    def get_races(self, date: str) -> List[Race]:\n        \"\"\"\n        Orchestrates the fetching and parsing of race data for a given date.\n        This method will be called by the FortunaEngine.\n        \"\"\"\n        self.logger.info(f\"Getting races for {date} from {self.SOURCE_NAME}\")\n        # This is a synchronous wrapper for the async orchestrator\n        # It's a temporary measure to allow me to see the API response.\n        import asyncio\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n        races = loop.run_until_complete(self._get_races_async(date))\n        return races\n",
    "python_service/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "python_service/core/errors.py": "# python_service/core/errors.py\nfrom enum import Enum\n\n\nclass ErrorCategory(Enum):\n    CONFIGURATION_ERROR = \"Configuration missing or invalid\"\n    NETWORK_ERROR = \"HTTP/Network request failed\"\n    PARSING_ERROR = \"Data parsing or validation unsuccessful\"\n    UNEXPECTED_ERROR = \"An unhandled exception occurred\"\n",
    "python_service/credentials_manager.py": "# python_service/credentials_manager.py\ntry:\n    import keyring\n    # This check is crucial for cross-platform compatibility\n    import keyring.backends.windows\n\n    IS_WINDOWS = True\nexcept ImportError:\n    keyring = None\n    IS_WINDOWS = False\n\n\nclass SecureCredentialsManager:\n    \"\"\"Manages secrets in the system's native credential store.\"\"\"\n\n    SERVICE_NAME = \"Fortuna\"\n\n    @staticmethod\n    def save_credential(account: str, secret: str) -> bool:\n        \"\"\"Saves a secret for a given account (e.g., 'api_key', 'betfair_username').\"\"\"\n        if not IS_WINDOWS:\n            print(\"Credential storage is only supported on Windows.\")\n            return False\n        try:\n            keyring.set_password(SecureCredentialsManager.SERVICE_NAME, account, secret)\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credential for {account}: {e}\")\n            return False\n\n    @staticmethod\n    def get_credential(account: str) -> str:\n        \"\"\"Retrieves a secret for a given account.\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            return keyring.get_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credential for {account}: {e}\")\n            return None\n\n    @staticmethod\n    def get_betfair_credentials() -> tuple[str, str]:\n        \"\"\"Convenience method to retrieve both Betfair username and password.\"\"\"\n        username = SecureCredentialsManager.get_credential(\"betfair_username\")\n        password = SecureCredentialsManager.get_credential(\"betfair_password\")\n        return username, password\n\n    @staticmethod\n    def delete_credential(account: str):\n        \"\"\"Deletes a specific credential.\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception:\n            pass\n",
    "python_service/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    # Keep the processor chain simple for maximum reliability in bundled executables.\n    # More complex processors like StackInfoRenderer can cause issues in\n    # constrained environments.\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "python_service/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Annotated\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import Field\nfrom pydantic import WrapSerializer\n\n\ndef decimal_serializer(value: Decimal, handler: Callable[[Decimal], Any]) -> Any:\n    \"\"\"Custom serializer for Decimal to float conversion.\"\"\"\n    return float(value)\n\n\nJsonDecimal = Annotated[Decimal, WrapSerializer(decimal_serializer, when_used=\"json\")]\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[JsonDecimal] = None\n    place: Optional[JsonDecimal] = None\n    show: Optional[JsonDecimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n    is_error_placeholder: bool = Field(False, alias=\"isErrorPlaceholder\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n",
    "python_service/requirements.in": "#\n# Fortuna Faucet - High-Level Backend Dependencies\n# This is the source of truth. Run 'pip-compile' to generate requirements.txt.\n#\n\n# --- Core Application Framework (Hard Pins) ---\nfastapi\nuvicorn==0.30.1\ncryptography\n\n# --- Core Application Dependencies (Flexible) ---\ntenacity\npydantic-settings\nhttpx[http2]\nselectolax==0.4.0\nbeautifulsoup4\nslowapi\nredis\npandas\nnumpy\nscipy\naiosqlite\nSQLAlchemy\npsycopg2-binary\nstructlog\ncertifi\n\n# --- Desktop & OS Integration (Flexible) ---\npsutil\npywin32 ; sys_platform == 'win32'\nwindows-toasts ; sys_platform == 'win32'\nkeyring\npynput ; sys_platform == 'win32'\n\n# --- Development & Testing Dependencies ---\npytest\npytest-asyncio\nblack\n\n# --- Build Dependencies (Hard Pins) ---\n# THE FIX: Upgraded to 6.6.0 for official Python 3.12 support.\npyinstaller==6.6.0\nwheel\nsetuptools>=78.1.1\npip-tools\nrequests>=2.32.4\nurllib3>=2.5.0\n",
    "python_service/tests/test_manual_override.py": "# python_service/tests/test_manual_override.py\nimport pytest\n\nfrom python_service.manual_override_manager import ManualOverrideManager\n\n\n@pytest.fixture\ndef manager():\n    # The manager is now in-memory and doesn't need a path\n    return ManualOverrideManager()\n\n\ndef test_register_and_retrieve(manager):\n    adapter = \"TestAdapter\"\n    url = \"https://example.com/blocked\"\n\n    request_id = manager.register_failure(\n        adapter_name=adapter,\n        url=url,\n    )\n\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager):\n    adapter = \"TestAdapter\"\n    url = \"https://example.com/blocked\"\n    content = \"<html>Manual content</html>\"\n    content_type = \"text/html\"\n\n    request_id = manager.register_failure(\n        adapter_name=adapter,\n        url=url,\n    )\n\n    success = manager.submit_manual_data(\n        request_id=request_id,\n        raw_content=content,\n        content_type=content_type,\n    )\n\n    assert success\n\n    # Verify that the data can be retrieved correctly\n    retrieved_data = manager.get_manual_data(adapter_name=adapter, url=url)\n    assert retrieved_data is not None\n    retrieved_content, retrieved_type = retrieved_data\n    assert retrieved_content == content\n    assert retrieved_type == content_type\n\n    # Verify that data is consumed after retrieval\n    assert manager.get_manual_data(adapter_name=adapter, url=url) is None\n",
    "python_service/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "tests/fixtures/timeform_legacy_sample.html": "<!DOCTYPE html><html><body><div class='race-card'><div class='runner'><span class='runner-name'>Braveheart</span><span class='runner-odds'>5/2</span></div><div class='runner'><span class='runner-name'>Speedster</span><span class='runner-odds'>10/1</span></div><div class='runner'><span class='runner-name'>Steady Eddy</span><span class='runner-odds'>EVENS</span></div></div></body></html>",
    "tests/fixtures/twinspires_sample.html": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <title>Race Results - Twinspires</title>\n</head>\n<body>\n    <div id=\"race-card\">\n        <h1>Race 5 - Churchill Downs - 2025-10-26</h1>\n        <div class=\"race-details\">\n            <span class=\"post-time\">Post Time: 04:30 PM</span>\n            <span class=\"distance\">1 Mile</span>\n            <span class=\"surface\">Dirt</span>\n        </div>\n        <ul class=\"runners-list\">\n            <li class=\"runner\">\n                <span class=\"runner-number\">1</span>\n                <span class=\"runner-name\">Braveheart</span>\n                <span class=\"runner-odds\">5/2</span>\n            </li>\n            <li class=\"runner\">\n                <span class=\"runner-number\">2</span>\n                <span class=\"runner-name\">Speedster</span>\n                <span class=\"runner-odds\">10/1</span>\n            </li>\n            <li class=\"runner scratched\">\n                <span class=\"runner-number\">3</span>\n                <span class=\"runner-name\">Steady Eddy</span>\n                <span class=\"runner-odds\">SCR</span>\n            </li>\n             <li class=\"runner\">\n                <span class=\"runner-number\">4</span>\n                <span class=\"runner-name\">Gallant Gus</span>\n                <span class=\"runner-odds\">3/1</span>\n            </li>\n        </ul>\n    </div>\n</body>\n</html>\n",
    "tests/test_api.py": "# tests/test_api.py\nfrom datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import patch\n\nimport aiosqlite\nimport pytest\n\n# --- Fixtures ---\nfrom python_service.models import AggregatedResponse\n\n# The client fixture is now correctly sourced from conftest.py,\n# which handles the settings override globally.\n\n# --- API Tests ---\n\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine.fetch_all_odds\", new_callable=AsyncMock)\nasync def test_get_races_endpoint_success(mock_fetch_all_odds, client):\n    \"\"\"\n    SPEC: The /api/races endpoint should return data with a valid API key.\n    \"\"\"\n    # ARRANGE\n    today = date.today()\n    mock_response = AggregatedResponse(\n        date=today,\n        races=[],\n        sources=[],\n        metadata={},\n        # This was the missing field causing the validation error\n        source_info=[],\n    )\n    mock_fetch_all_odds.return_value = mock_response.model_dump()\n    headers = {\"X-API-Key\": \"a_secure_test_api_key_that_is_long_enough\"}\n\n    # ACT\n    response = client.get(f\"/api/races?race_date={today.isoformat()}\", headers=headers)\n\n    # ASSERT\n    assert response.status_code == 200\n    mock_fetch_all_odds.assert_awaited_once()\n\n\n@pytest.mark.asyncio\nasync def test_get_tipsheet_endpoint_success(tmp_path, client):\n    \"\"\"\n    SPEC: The /api/tipsheet endpoint should return a list of tipsheet races from the database.\n    \"\"\"\n    db_path = tmp_path / \"test.db\"\n    post_time = datetime.now()\n\n    with patch(\"python_service.api.DB_PATH\", db_path):\n        async with aiosqlite.connect(db_path) as db:\n            await db.execute(\n                \"\"\"\n                CREATE TABLE tipsheet (\n                    race_id TEXT PRIMARY KEY,\n                    track_name TEXT,\n                    race_number INTEGER,\n                    post_time TEXT,\n                    score REAL,\n                    factors TEXT\n                )\n            \"\"\"\n            )\n            await db.execute(\n                \"INSERT INTO tipsheet VALUES (?, ?, ?, ?, ?, ?)\",\n                (\"test_race_1\", \"Test Park\", 1, post_time.isoformat(), 85.5, \"{}\"),\n            )\n            await db.commit()\n\n        # ACT\n        response = client.get(f\"/api/tipsheet?date={post_time.date().isoformat()}\")\n\n        # ASSERT\n        assert response.status_code == 200\n        response_data = response.json()\n        assert len(response_data) == 1\n        # The database returns snake_case, but the Pydantic model is camelCase\n        assert response_data[0][\"raceId\"] == \"test_race_1\"\n        assert response_data[0][\"score\"] == 85.5\n\n\ndef test_health_check_unauthenticated(client):\n    \"\"\"Ensures the /health endpoint is accessible without an API key.\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    json_response = response.json()\n    assert json_response[\"status\"] == \"ok\"\n    assert \"timestamp\" in json_response\n\n\ndef test_api_key_authentication_failure(client):\n    \"\"\"Ensures that endpoints are protected and fail with an invalid API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\", headers={\"X-API-KEY\": \"invalid_key\"})\n    assert response.status_code == 403\n    assert \"Invalid or missing API Key\" in response.json()[\"detail\"]\n\n\ndef test_api_key_authentication_missing(client):\n    \"\"\"Ensures that endpoints are protected and fail with a missing API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\")\n    assert response.status_code == 403\n    assert \"Not authenticated\" in response.json()[\"detail\"]\n",
    "tests/test_models.py": "# Test suite for Pydantic models, resurrected from attic/legacy_tests_pre_triage/checkmate_v7/test_models.py\nimport datetime\n\nimport pytest\nfrom pydantic import ValidationError\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\n\ndef test_runner_model_creation():\n    \"\"\"Tests basic successful creation of the Runner model.\"\"\"\n    from datetime import datetime\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds_data = {\"TestOdds\": OddsData(win=Decimal(\"6.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    runner = Runner(number=5, name=\"Test Horse\", odds=odds_data, scratched=False)\n    assert runner.number == 5\n    assert runner.name == \"Test Horse\"\n    assert not runner.scratched\n\n\ndef test_race_model_with_valid_runners():\n    \"\"\"Tests basic successful creation of the Race model.\"\"\"\n    from datetime import datetime\n    from decimal import Decimal\n\n    from python_service.models import OddsData\n\n    odds1 = {\"TestOdds\": OddsData(win=Decimal(\"3.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    odds2 = {\"TestOdds\": OddsData(win=Decimal(\"4.0\"), source=\"TestOdds\", last_updated=datetime.now())}\n    runner1 = Runner(number=1, name=\"A\", odds=odds1, scratched=False)\n    runner2 = Runner(number=2, name=\"B\", odds=odds2, scratched=False)\n    race = Race(\n        id=\"test-race-1\",\n        venue=\"TEST\",\n        race_number=1,\n        start_time=datetime.now(),\n        runners=[runner1, runner2],\n        source=\"test\",\n    )\n    assert race.venue == \"TEST\"\n    assert len(race.runners) == 2\n\n\ndef test_model_validation_fails_on_missing_required_field():\n    \"\"\"Ensures Pydantic's validation fires for missing required fields.\"\"\"\n    with pytest.raises(ValidationError):\n        # 'name' is a required field for a Runner\n        Runner(number=3, odds=\"3/1\", scratched=False)\n\n    with pytest.raises(ValidationError):\n        # 'venue' is a required field for a Race\n        Race(\n            id=\"test-race-2\",\n            race_number=2,\n            start_time=datetime.datetime.now(),\n            runners=[],\n            source=\"test\",\n        )\n",
    "tests/test_msi_installation.ps1": "param([string]$MsiPath = \".\\dist\\Fortuna-Faucet-2.1.0-x64.msi\")\n\nWrite-Host \"Testing MSI Installation...\" -ForegroundColor Cyan\n\n# Test 1: File integrity\nWrite-Host \"\u2022 Verifying MSI structure...\"\nif (Test-Path $MsiPath) {\n    Write-Host \"\u2713 MSI file exists\"\n} else {\n    Write-Error \"MSI file not found\"\n    exit 1\n}\n\n# Test 2: Installation\nWrite-Host \"\u2022 Testing interactive installation...\"\n& msiexec.exe /i $MsiPath /l*v \"test_install.log\"\n\n# Test 3: Verify installation\nWrite-Host \"\u2022 Verifying files were installed...\"\n$programFiles = \"$env:PROGRAMFILES\\Fortuna Faucet\"\nif (Test-Path $programFiles) {\n    Write-Host \"\u2713 Installation successful\"\n} else {\n    Write-Error \"Installation failed\"\n    exit 1\n}\n\n# Test 4: Registry entries\nWrite-Host \"\u2022 Checking registry entries...\"\n$regPath = \"HKLM:\\Software\\Fortuna Faucet\"\nif (Test-Path $regPath) {\n    Write-Host \"\u2713 Registry entries found\"\n} else {\n    Write-Error \"Registry entries missing\"\n    exit 1\n}",
    "verify_dashboard.py": "\nfrom playwright.sync_api import sync_playwright\n\ndef verify_dashboard(page):\n    \"\"\"\n    Navigates to the dashboard and takes a screenshot.\n    \"\"\"\n    page.goto(\"http://localhost:3000\")\n    page.wait_for_selector(\"text=Fortuna Faucet\")\n    page.screenshot(path=\"verification.png\")\n\nif __name__ == \"__main__\":\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        page = browser.new_page()\n        try:\n            verify_dashboard(page)\n        finally:\n            browser.close()\n",
    "web_platform/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_platform/frontend/next.config.mjs": "/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  output: 'export',  // Critical for static HTML export\n  distDir: 'out',\n  trailingSlash: true,\n  images: {\n    unoptimized: true  // Required for static export\n  },\n  async rewrites() {\n    return [\n      {\n        source: '/api/:path*',\n        destination: 'http://127.0.0.1:8000/api/:path*',\n      },\n    ]\n  },\n};\n\nexport default nextConfig;\n",
    "web_platform/frontend/src/components/StatusDetailModal.tsx": "// web_platform/frontend/src/components/StatusDetailModal.tsx\nimport React from 'react';\n\ninterface StatusDetailModalProps {\n  isOpen: boolean;\n  onClose: () => void;\n  status: {\n      title: string;\n      details: string | Record<string, any>;\n  };\n}\n\nexport const StatusDetailModal: React.FC<StatusDetailModalProps> = ({ isOpen, onClose, status }) => {\n  if (!isOpen) {\n    return null;\n  }\n\n  const { title, details } = status;\n  const isDetailsString = typeof details === 'string';\n\n  // Determine status color only if details is an object with a status property\n  const statusColor = !isDetailsString && (details.status === 'SUCCESS' || details.status === 'OK')\n    ? 'text-green-400'\n    : 'text-gray-300'; // Default color\n\n  return (\n    <div className=\"fixed inset-0 bg-black/60 flex items-center justify-center z-50\" onClick={onClose}>\n      <div className=\"bg-gray-800 border border-gray-700 rounded-lg shadow-xl p-6 max-w-lg w-full\" onClick={e => e.stopPropagation()}>\n        <div className=\"flex justify-between items-start mb-4\">\n          <h3 className=\"text-xl font-bold text-white\">{title}</h3>\n          <button onClick={onClose} className=\"text-gray-400 hover:text-white\">&times;</button>\n        </div>\n        <div className=\"space-y-2 text-sm max-h-96 overflow-y-auto pr-2\">\n            {isDetailsString ? (\n                <div className=\"text-gray-300 whitespace-pre-wrap bg-gray-900/50 p-4 rounded-md\">{details}</div>\n            ) : (\n                Object.entries(details).map(([key, value]) => (\n                    <div key={key} className=\"grid grid-cols-3 gap-4 border-b border-gray-700/50 py-2\">\n                    <span className=\"font-semibold text-gray-400 capitalize\">{key.replace(/_/g, ' ')}</span>\n                    <span className={`col-span-2 break-words ${key === 'status' ? statusColor : 'text-gray-300'}`}>\n                        {typeof value === 'object' ? JSON.stringify(value, null, 2) : String(value)}\n                    </span>\n                    </div>\n                ))\n            )}\n        </div>\n        <button\n          onClick={onClose}\n          className=\"bg-gray-600 hover:bg-gray-700 text-white font-bold py-2 px-4 rounded w-full mt-6\"\n        >\n          Close\n        </button>\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n"
}