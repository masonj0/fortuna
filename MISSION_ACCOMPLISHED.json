{"fortuna_analytics.py": "#!/usr/bin/env python3\n\"\"\"\nfortuna_analytics.py\nRace result harvesting and performance analysis engine for Fortuna.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport asyncio\nimport json\nimport logging\nimport os\nimport re\nfrom contextlib import asynccontextmanager\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, Final, List, Optional, Set, Tuple, Type\nfrom zoneinfo import ZoneInfo\n\nimport structlog\nfrom pydantic import Field, model_validator\nfrom selectolax.parser import HTMLParser, Node\n\nimport fortuna\n\n# -- CONSTANTS ----------------------------------------------------------------\n\nEASTERN = ZoneInfo(\"America/New_York\")\nDEFAULT_DB_PATH: Final[str] = os.environ.get(\"FORTUNA_DB_PATH\", \"fortuna.db\")\nSTANDARD_BET: Final[float] = 2.00\nDEFAULT_REGION: Final[str] = \"GLOBAL\"\n\nPLACE_POSITIONS_BY_FIELD_SIZE: Final[Dict[int, int]] = {\n    4: 1,           # \u22644 runners: win only\n    7: 2,           # 5-7 runners: top 2\n    10000: 3,       # 8+: top 3\n}\n\n_currency_logger = structlog.get_logger(\"currency_parser\")\n\n_CASHED_VERDICTS: Final[frozenset] = frozenset({\"CASHED\", \"CASHED_ESTIMATED\"})\n_LOSS_VERDICTS:   Final[frozenset] = frozenset({\"BURNED\"})\n\n_VERDICT_DISPLAY: Final[Dict[str, str]] = {\n    \"CASHED\":           \"\u2705 WIN \",\n    \"CASHED_ESTIMATED\": \"\u2705 WIN~\",\n    \"BURNED\":           \"\u274c LOSS\",\n    \"VOID\":             \"\u26aa VOID\",\n}\n\n\n# -- HELPER FUNCTIONS ---------------------------------------------------------\n\ndef now_eastern() -> datetime:\n    return datetime.now(EASTERN)\n\n\ndef to_eastern(dt: datetime) -> datetime:\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=EASTERN)\n    return dt.astimezone(EASTERN)\n\n\n\n\ndef parse_position(pos_str: Optional[str]) -> Optional[int]:\n    \"\"\"``'1st'`` \u2192 1, ``'2/12'`` \u2192 2, ``'W'`` \u2192 1, etc.\"\"\"\n    if not pos_str:\n        return None\n    s = str(pos_str).upper().strip()\n    direct = {\n        \"W\": 1, \"1\": 1, \"1ST\": 1,\n        \"P\": 2, \"2\": 2, \"2ND\": 2,\n        \"S\": 3, \"3\": 3, \"3RD\": 3,\n        \"4\": 4, \"4TH\": 4,\n        \"5\": 5, \"5TH\": 5,\n    }\n    if s in direct:\n        return direct[s]\n    m = re.search(r\"^(\\d+)\", s)\n    return int(m.group(1)) if m else None\n\n\ndef get_places_paid(field_size: int) -> int:\n    for max_size, places in sorted(PLACE_POSITIONS_BY_FIELD_SIZE.items()):\n        if field_size <= max_size:\n            return places\n    return 3\n\n\ndef parse_currency_value(value_str: str) -> float:\n    \"\"\"``'$1,234.56'`` \u2192 1234.56\"\"\"\n    if not value_str:\n        return 0.0\n    try:\n        raw = str(value_str).strip()\n        # Allow standard currency symbols and codes (GBP, EUR, USD, ZAR)\n        if re.search(r\"[^\\d.,$\u00a3\u20ac\\sA-Z]\", raw):\n            _currency_logger.debug(\"unexpected_currency_format\", value=raw)\n            # If it contains truly invalid characters for currency, return 0\n            if re.search(r\"[^\\d.,$\u00a3\u20ac\\sA-Z\\-]\", raw):\n                 return 0.0\n\n        if \",\" in raw and \".\" in raw:\n            if raw.rfind(\",\") > raw.rfind(\".\"):\n                # European style: 1.234,56\n                cleaned = raw.replace(\".\", \"\").replace(\",\", \".\")\n            else:\n                # US style: 1,234.56\n                cleaned = raw.replace(\",\", \"\")\n        elif \",\" in raw and \".\" not in raw and re.search(r\",\\d{2}$\", raw):\n            # European format: 12,34\n            cleaned = raw.replace(\",\", \".\")\n        else:\n            cleaned = raw.replace(\",\", \"\")\n\n        cleaned = re.sub(r\"[^\\d.]\", \"\", cleaned)\n        return float(cleaned) if cleaned else 0.0\n    except (ValueError, TypeError):\n        _currency_logger.warning(\"failed_parsing_currency\", value=value_str)\n        return 0.0\n\n\ndef validate_date_format(date_str: str) -> bool:\n    try:\n        datetime.strptime(date_str, \"%Y-%m-%d\")\n        return True\n    except ValueError:\n        return False\n\n\n# -- MODELS -------------------------------------------------------------------\n\nclass ResultRunner(fortuna.Runner):\n    \"\"\"Runner extended with finishing position and payouts.\"\"\"\n\n    position: Optional[str] = None\n    position_numeric: Optional[int] = None\n    final_win_odds: Optional[float] = None\n    win_payout: Optional[float] = None\n    place_payout: Optional[float] = None\n    show_payout: Optional[float] = None\n\n    @model_validator(mode=\"after\")\n    def compute_position_numeric(self) -> ResultRunner:\n        if self.position and self.position_numeric is None:\n            self.position_numeric = parse_position(self.position)\n        return self\n\n\nclass ResultRace(fortuna.Race):\n    \"\"\"Race with full result data.\"\"\"\n\n    runners: List[ResultRunner] = Field(default_factory=list)\n    official_dividends: Dict[str, float] = Field(default_factory=dict)\n    chart_url: Optional[str] = None\n    is_fully_parsed: bool = False\n\n    trifecta_payout: Optional[float] = None\n    trifecta_cost: float = 1.00\n    trifecta_combination: Optional[str] = None\n    exacta_payout: Optional[float] = None\n    exacta_combination: Optional[str] = None\n    superfecta_payout: Optional[float] = None\n    superfecta_combination: Optional[str] = None\n\n    @property\n    def canonical_key(self) -> str:\n        d = self.start_time.strftime(\"%Y%m%d\")\n        t = self.start_time.strftime(\"%H%M\")\n        disc = (self.discipline or \"T\")[:1].upper()\n        return f\"{fortuna.get_canonical_venue(self.venue)}|{self.race_number}|{d}|{t}|{disc}\"\n\n    @property\n    def relaxed_key(self) -> str:\n        d = self.start_time.strftime(\"%Y%m%d\")\n        disc = (self.discipline or \"T\")[:1].upper()\n        return f\"{fortuna.get_canonical_venue(self.venue)}|{self.race_number}|{d}|{disc}\"\n\n    def get_top_finishers(self, n: int = 5) -> List[ResultRunner]:\n        ranked = [r for r in self.runners if r.position_numeric is not None]\n        ranked.sort(key=lambda r: r.position_numeric)\n        return ranked[:n]\n\n\n# -- AUDITOR ENGINE -----------------------------------------------------------\n\nclass AuditorEngine:\n    \"\"\"Matches predicted tips against actual race results via SQLite.\"\"\"\n\n    def __init__(self, db_path: Optional[str] = None) -> None:\n        self.db = fortuna.FortunaDB(db_path or DEFAULT_DB_PATH)\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        await self.close()\n\n    # -- data access -------------------------------------------------------\n\n    async def get_unverified_tips(\n        self, lookback_hours: int = 48,\n    ) -> List[Dict[str, Any]]:\n        return await self.db.get_unverified_tips(lookback_hours)\n\n    async def get_all_audited_tips(self) -> List[Dict[str, Any]]:\n        return await self.db.get_all_audited_tips()\n\n    async def get_recent_tips(self, limit: int = 15) -> List[Dict[str, Any]]:\n        return await self.db.get_recent_tips(limit)\n\n    async def close(self) -> None:\n        await self.db.close()\n\n    # -- audit pipeline ----------------------------------------------------\n\n    async def audit_races(\n        self,\n        results: List[ResultRace],\n        unverified: Optional[List[Dict[str, Any]]] = None,\n    ) -> List[Dict[str, Any]]:\n        results_map = self._build_results_map(results)\n        self.logger.debug(\"=== MATCHING DIAGNOSTIC ===\")\n        self.logger.debug(\"Result keys available:\", keys=list(results_map.keys())[:20])\n\n        if unverified is None:\n            unverified = await self.get_unverified_tips()\n\n        for tip in unverified[:10]:\n            tip_key = self._tip_canonical_key(tip)\n            self.logger.debug(\n                \"Tip key vs results\",\n                tip_venue=tip.get(\"venue\"),\n                tip_key=tip_key,\n                matched=tip_key in results_map if tip_key else False,\n            )\n\n        audited: List[Dict[str, Any]] = []\n        outcomes_to_batch: List[Tuple[str, Dict[str, Any]]] = []\n\n        for tip in unverified:\n            try:\n                race_id = tip.get(\"race_id\")\n                if not race_id:\n                    continue\n\n                tip_key = self._tip_canonical_key(tip)\n                if not tip_key:\n                    continue\n\n                result = self._match_tip_to_result(tip_key, results_map, race_id)\n                if not result:\n                    continue\n\n                outcome = self._evaluate_tip(tip, result)\n                outcomes_to_batch.append((race_id, outcome))\n                audited.append({**tip, **outcome, \"audit_completed\": True})\n\n            except Exception as exc:\n                self.logger.error(\n                    \"Error during audit\",\n                    tip_id=tip.get(\"race_id\"),\n                    error=str(exc),\n                    exc_info=True,\n                )\n\n        if outcomes_to_batch:\n            self.logger.info(\"Updating audit results\", count=len(outcomes_to_batch))\n            if hasattr(self.db, \"update_audit_results_batch\"):\n                await self.db.update_audit_results_batch(outcomes_to_batch)\n            else:\n                for race_id, outcome in outcomes_to_batch:\n                    await self.db.update_audit_result(race_id, outcome)\n\n        return audited\n\n    @staticmethod\n    def _build_results_map(\n        results: List[ResultRace],\n    ) -> Dict[str, ResultRace]:\n        mapping: Dict[str, ResultRace] = {}\n        log = structlog.get_logger(\"AuditorEngine\")\n        for r in results:\n            # Canonical key: full precision\n            mapping[r.canonical_key] = r\n\n            if r.relaxed_key != r.canonical_key:\n                # Relaxed key: Venue|Race|Date|Disc (no time)\n                if r.relaxed_key in mapping:\n                    existing = mapping[r.relaxed_key]\n                    if existing.canonical_key != r.canonical_key:\n                        log.debug(\n                            \"Relaxed key collision\",\n                            key=r.relaxed_key,\n                            existing=existing.canonical_key,\n                            new=r.canonical_key,\n                        )\n                        # Prefer existing canonical over new relaxed if collision\n                        continue\n                mapping[r.relaxed_key] = r\n        return mapping\n\n\n    def _match_tip_to_result(\n        self,\n        tip_key: str,\n        results_map: Dict[str, ResultRace],\n        race_id: str,\n    ) -> Optional[ResultRace]:\n        # Exact match\n        result = results_map.get(tip_key)\n        if result:\n            return result\n\n        parts = tip_key.split(\"|\")\n\n        # Fallback 1: drop time (keep discipline)\n        if len(parts) >= 5:\n            relaxed = f\"{parts[0]}|{parts[1]}|{parts[2]}|{parts[4]}\"\n            result = results_map.get(relaxed)\n            if result:\n                self.logger.info(\n                    \"Time-relaxed fallback match\",\n                    race_id=race_id,\n                    match_key=result.canonical_key,\n                )\n                return result\n\n        # Fallback 2: drop discipline (keep time)\n        if len(parts) >= 4:\n            prefix = \"|\".join(parts[:4])\n            matches = [obj for key, obj in results_map.items() if key.startswith(prefix)]\n            if matches:\n                if len(matches) > 1:\n                    self.logger.warning(\n                        \"Multiple discipline fallback matches\",\n                        race_id=race_id,\n                        count=len(matches),\n                    )\n                # Deterministic return: the one with exact time match if available\n                return matches[0]\n\n        return None\n\n    # -- key generation ----------------------------------------------------\n\n    @staticmethod\n    def _tip_canonical_key(tip: Dict[str, Any]) -> Optional[str]:\n        venue = tip.get(\"venue\")\n        race_number = tip.get(\"race_number\")\n        start_raw = tip.get(\"start_time\")\n        disc = (tip.get(\"discipline\") or \"T\")[:1].upper()\n\n        if not all([venue, race_number, start_raw]):\n            return None\n        try:\n            st = datetime.fromisoformat(\n                str(start_raw).replace(\"Z\", \"+00:00\"),\n            )\n            return (\n                f\"{fortuna.get_canonical_venue(venue)}\"\n                f\"|{race_number}\"\n                f\"|{st.strftime('%Y%m%d')}\"\n                f\"|{st.strftime('%H%M')}\"\n                f\"|{disc}\"\n            )\n        except (ValueError, TypeError):\n            return None\n\n    # -- evaluation --------------------------------------------------------\n\n    def _evaluate_tip(\n        self, tip: Dict[str, Any], result: ResultRace,\n    ) -> Dict[str, Any]:\n        selection_num = self._extract_selection_number(tip)\n        selection_name = tip.get(\"selection_name\")\n\n        top_finishers = result.get_top_finishers(5)\n        actual_top_5 = [str(r.number) for r in top_finishers]\n\n        top1_place = (\n            top_finishers[0].place_payout if len(top_finishers) >= 1 else None\n        )\n        top2_place = (\n            top_finishers[1].place_payout if len(top_finishers) >= 2 else None\n        )\n\n        actual_2nd_fav_odds = self._find_actual_2nd_fav_odds(result)\n\n        # Find our selection in result runners\n        sel_result = self._find_selection_runner(\n            result, selection_num, selection_name,\n        )\n\n        verdict, profit = self._compute_verdict(sel_result, result)\n\n        return {\n            \"actual_top_5\": \", \".join(actual_top_5),\n            \"actual_2nd_fav_odds\": actual_2nd_fav_odds,\n            \"verdict\": verdict,\n            \"net_profit\": round(profit, 2),\n            \"selection_position\": (\n                sel_result.position_numeric if sel_result else None\n            ),\n            \"audit_timestamp\": datetime.now(EASTERN).isoformat(),\n            \"trifecta_payout\": result.trifecta_payout,\n            \"trifecta_combination\": result.trifecta_combination,\n            \"superfecta_payout\": result.superfecta_payout,\n            \"superfecta_combination\": result.superfecta_combination,\n            \"top1_place_payout\": top1_place,\n            \"top2_place_payout\": top2_place,\n        }\n\n    @staticmethod\n    def _find_actual_2nd_fav_odds(result: ResultRace) -> Optional[float]:\n        runners_list = sorted(\n            (\n                r for r in result.runners\n                if r.final_win_odds and r.final_win_odds > 0 and not r.scratched\n            ),\n            key=lambda r: r.final_win_odds,\n        )\n        if len(runners_list) < 2:\n            return None\n        fav_odds = runners_list[0].final_win_odds\n        higher = [r for r in runners_list if r.final_win_odds > fav_odds]\n        return higher[0].final_win_odds if higher else None\n\n    @staticmethod\n    def _find_selection_runner(\n        result: ResultRace,\n        number: Optional[int],\n        name: Optional[str],\n    ) -> Optional[ResultRunner]:\n        if number is not None:\n            by_num = next(\n                (r for r in result.runners if r.number == number), None,\n            )\n            if by_num:\n                return by_num\n        if name:\n            return next(\n                (\n                    r for r in result.runners\n                    if r.name.lower() == name.lower()\n                ),\n                None,\n            )\n        return None\n\n    @staticmethod\n    def _compute_verdict(\n        sel: Optional[ResultRunner],\n        result: ResultRace,\n    ) -> Tuple[str, float]:\n        if sel is None:\n            return \"VOID\", 0.0\n        if sel.position_numeric is None:\n            return \"BURNED\", -STANDARD_BET\n\n        active = [r for r in result.runners if not r.scratched]\n        places_paid = get_places_paid(len(active))\n\n        if sel.position_numeric > places_paid:\n            return \"BURNED\", -STANDARD_BET\n\n        # CASHED \u2014 calculate profit\n        if sel.place_payout and sel.place_payout > 0:\n            return \"CASHED\", sel.place_payout - STANDARD_BET\n\n        # Heuristic: ~1/5 of win odds for place\n        # Claude Fix: Mark as ESTIMATED to avoid corrupting real audit trail\n        odds = sel.final_win_odds or 2.75\n        place_roi = max(0.1, (odds - 1.0) / 5.0)\n        return \"CASHED_ESTIMATED\", place_roi * STANDARD_BET\n\n    @staticmethod\n    def _extract_selection_number(tip: Dict[str, Any]) -> Optional[int]:\n        sel = tip.get(\"selection_number\")\n        if sel is not None:\n            try:\n                return int(sel)\n            except (ValueError, TypeError):\n                pass\n        top_five = tip.get(\"top_five\", \"\")\n        if top_five:\n            first = str(top_five).split(\",\")[0].strip()\n            try:\n                return int(first)\n            except (ValueError, TypeError):\n                pass\n        return None\n\n\n# -- SHARED RESULT-PARSING UTILITIES ------------------------------------------\n\ndef parse_fractional_odds(text: str) -> float:\n    \"\"\"``'5/2'`` \u2192 3.5, ``'2.5'`` \u2192 2.5, anything else \u2192 0.0.\"\"\"\n    val = fortuna.parse_odds_to_decimal(text)\n    return float(val) if val is not None else 0.0\n\n\ndef build_start_time(\n    date_str: str,\n    time_str: Optional[str] = None,\n    *,\n    tz: ZoneInfo = EASTERN,\n) -> datetime:\n    \"\"\"Build a tz-aware datetime from ``YYYY-MM-DD`` + optional ``HH:MM``.\"\"\"\n    try:\n        base = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        structlog.get_logger(\"build_start_time\").warning(\"unparseable_date\", date=date_str)\n        base = datetime.now(tz)\n    hour, minute = 12, 0\n    if time_str:\n        try:\n            parts = time_str.strip().split(\":\")\n            hour, minute = int(parts[0]), int(parts[1])\n        except (ValueError, IndexError):\n            structlog.get_logger(\"build_start_time\").warning(\"malformed_time_string\", time=time_str)\n            pass\n    return base.replace(hour=hour, minute=minute, tzinfo=tz)\n\n\ndef find_nested_value(\n    obj: Any,\n    key_fragment: str,\n    *,\n    _depth: int = 0,\n    _max_depth: int = 20,\n) -> Optional[float]:\n    \"\"\"Recursively search dicts/lists for a key containing *key_fragment*\n    whose value is numeric.  Depth-guarded.\"\"\"\n    if _depth > _max_depth:\n        return None\n    frag = key_fragment.lower()\n    if isinstance(obj, dict):\n        for k, v in obj.items():\n            if frag in k.lower() and isinstance(v, (int, float, str)):\n                parsed = (\n                    parse_currency_value(str(v))\n                    if isinstance(v, str)\n                    else float(v)\n                )\n                if parsed:\n                    return parsed\n            found = find_nested_value(\n                v, key_fragment, _depth=_depth + 1, _max_depth=_max_depth,\n            )\n            if found is not None:\n                return found\n    elif isinstance(obj, (list, tuple)):\n        for item in obj:\n            found = find_nested_value(\n                item, key_fragment, _depth=_depth + 1, _max_depth=_max_depth,\n            )\n            if found is not None:\n                return found\n    return None\n\n\n_BET_ALIASES: Final[Dict[str, List[str]]] = {\n    \"superfecta\": [\"superfecta\", \"first 4\", \"first four\"],\n    \"trifecta\":   [\"trifecta\", \"tricast\"],\n    \"exacta\":     [\"exacta\", \"forecast\"],\n}\n\n\ndef extract_exotic_payouts(\n    tables: list[Node],\n) -> Dict[str, Tuple[Optional[float], Optional[str]]]:\n    \"\"\"Scan *tables* for exotic-bet dividend rows.\n\n    Returns ``{\"trifecta\": (payout, combination), ...}`` for each found type.\n    Callers with an ``HTMLParser`` pass ``parser.css(\"table\")``.\n    Equibase passes a slice of tables between race boundaries.\n    \"\"\"\n    results: Dict[str, Tuple[Optional[float], Optional[str]]] = {}\n    for table in tables:\n        text = table.text().lower()\n        for bet_type, aliases in _BET_ALIASES.items():\n            if bet_type in results:\n                continue\n            if not any(a in text for a in aliases):\n                continue\n            for row in table.css(\"tr\"):\n                row_text = row.text().lower()\n                if not any(a in row_text for a in aliases):\n                    continue\n                cols = row.css(\"td\")\n                combo: Optional[str] = None\n                payout = 0.0\n                if len(cols) >= 3:\n                    combo  = fortuna.clean_text(cols[1].text())\n                    payout = parse_currency_value(cols[2].text())\n                elif len(cols) >= 2:\n                    combo  = fortuna.clean_text(cols[0].text())\n                    payout = parse_currency_value(cols[1].text())\n                if payout > 0:\n                    results[bet_type] = (payout, combo)\n                    break\n    return results\n\n\ndef _extract_race_number_from_text(\n    parser: HTMLParser,\n    url: str = \"\",\n) -> Optional[int]:\n    \"\"\"Best-effort race-number from page text or URL.\"\"\"\n    m = re.search(r\"Race\\s+(\\d+)\", parser.text(), re.I)\n    if m:\n        return int(m.group(1))\n    m = re.search(r\"/R(\\d+)(?:[/?#]|$)\", url)\n    if m:\n        return int(m.group(1))\n    return None\n\n\n# -- PageFetchingResultsAdapter BASE ------------------------------------------\n\nclass PageFetchingResultsAdapter(\n    fortuna.BrowserHeadersMixin,\n    fortuna.DebugMixin,\n    fortuna.RacePageFetcherMixin,\n    fortuna.BaseAdapterV3,\n):\n    \"\"\"Common base for results adapters that:\n\n    1. Fetch an index / listing page for a date\n    2. Extract links to individual result pages\n    3. Fetch all pages concurrently\n    4. Parse each page into one or more :class:`ResultRace`\n\n    **Subclasses must set** ``SOURCE_NAME``, ``BASE_URL``, ``HOST``\n    and implement :meth:`_discover_result_links`.\n\n    For single-race pages override :meth:`_parse_race_page`.\n    For multi-race pages (e.g. Equibase) override :meth:`_parse_page`.\n    \"\"\"\n\n    ADAPTER_TYPE: Final[str] = \"results\"\n\n    _BLOCK_SIGNATURES = [\n        \"pardon our interruption\",\n        \"checking your browser\",\n        \"cloudflare\",\n        \"access denied\",\n        \"captcha\",\n        \"please verify\",\n    ]\n\n    def _check_for_block(self, html: str, url: str) -> bool:\n        lower = html.lower()\n        for sig in self._BLOCK_SIGNATURES:\n            if sig in lower and len(html) < 10000:\n                self.logger.error(\n                    \"BOT BLOCKED\",\n                    source=self.SOURCE_NAME,\n                    url=url,\n                    signature=sig,\n                    html_length=len(html),\n                )\n                return True\n        return False\n\n    # -- subclass must set -------------------------------------------------\n    SOURCE_NAME: str\n    BASE_URL: str\n    HOST: str\n\n    # -- subclass may override ---------------------------------------------\n    TIMEOUT: int = 60\n    IMPERSONATE: Optional[str] = None\n\n    def __init__(self, **kwargs: Any) -> None:\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            **kwargs,\n        )\n        self._target_venues: Optional[Set[str]] = None\n\n    # -- target venues property --------------------------------------------\n\n    @property\n    def target_venues(self) -> Optional[Set[str]]:\n        return self._target_venues\n\n    @target_venues.setter\n    def target_venues(self, value: Optional[Set[str]]) -> None:\n        self._target_venues = value\n\n    # -- framework hooks (identical across every legacy adapter) -----------\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        # Use CURL_CFFI as primary (faster) but keep PLAYWRIGHT as fallback via SmartFetcher (Project Hardening)\n        return fortuna.FetchStrategy(\n            primary_engine=fortuna.BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=self.TIMEOUT,\n            network_idle=True,\n        )\n\n    async def make_request(\n        self, method: str, url: str, **kwargs: Any,\n    ) -> Any:\n        if self.IMPERSONATE:\n            kwargs.setdefault(\"impersonate\", self.IMPERSONATE)\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=self.HOST)\n\n    def _validate_and_parse_races(self, raw_data: Any) -> List[ResultRace]:\n        return self._parse_races(raw_data)\n\n    # -- fetch pipeline ----------------------------------------------------\n\n    async def _fetch_data(\n        self, date_str: str,\n    ) -> Optional[Dict[str, Any]]:\n        links = await self._discover_result_links(date_str)\n        if not links:\n            self.logger.warning(\n                \"No result links found\",\n                source=self.SOURCE_NAME,\n                date=date_str,\n            )\n            return None\n        return await self._fetch_link_pages(links, date_str)\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        \"\"\"Return URLs for individual result pages.  **Must be overridden.**\"\"\"\n        raise NotImplementedError\n\n    async def _fetch_link_pages(\n        self, links: Set[str], date_str: str,\n    ) -> Optional[Dict[str, Any]]:\n        absolute = list(dict.fromkeys(\n            lnk if lnk.startswith(\"http\") else f\"{self.BASE_URL}{lnk}\"\n            for lnk in links\n        ))\n        self.logger.info(\n            \"Fetching result pages\",\n            source=self.SOURCE_NAME,\n            count=len(absolute),\n        )\n        metadata = [{\"url\": u, \"race_number\": 0} for u in absolute]\n        pages = await self._fetch_race_pages_concurrent(\n            metadata, self._get_headers(),\n        )\n        return {\"pages\": pages, \"date\": date_str}\n\n    # -- parse pipeline ----------------------------------------------------\n\n    def _parse_races(self, raw_data: Any) -> List[ResultRace]:\n        if not raw_data:\n            return []\n        date_str = raw_data.get(\n            \"date\", datetime.now(EASTERN).strftime(\"%Y-%m-%d\"),\n        )\n        races: List[ResultRace] = []\n        for item in raw_data.get(\"pages\", []):\n            html = item.get(\"html\") if isinstance(item, dict) else None\n            url  = item.get(\"url\", \"\") if isinstance(item, dict) else \"\"\n            if not html:\n                continue\n            \n            if self._check_for_block(html, url):\n                continue\n\n            try:\n                races.extend(self._parse_page(html, date_str, url))\n            except Exception as exc:\n                self.logger.warning(\n                    \"Failed to parse result page\",\n                    source=self.SOURCE_NAME,\n                    url=url,\n                    error=str(exc),\n                )\n        return races\n\n    def _parse_page(\n        self, html: str, date_str: str, url: str,\n    ) -> List[ResultRace]:\n        \"\"\"Parse a page into one or more races.\n\n        Default delegates to :meth:`_parse_race_page` (single race per page).\n        Override for multi-race pages (e.g. Equibase track summaries).\n        \"\"\"\n        race = self._parse_race_page(html, date_str, url)\n        return [race] if race else []\n\n    def _parse_race_page(\n        self, html: str, date_str: str, _url: str,\n    ) -> Optional[ResultRace]:\n        \"\"\"Parse a single-race page.  Override for most adapters.\"\"\"\n        raise NotImplementedError\n\n    # -- shared helpers ----------------------------------------------------\n\n    def _venue_matches(self, text: str, href: str = \"\") -> bool:\n        \"\"\"Check whether a link matches target venue filters.\"\"\"\n        if not self.target_venues:\n            # If no targets specified, we accept everything (Project Directive: Keep fetchers fetching)\n            return True\n\n        # Try exact canonical match on text (e.g. track name in link)\n        canon_text = fortuna.get_canonical_venue(text)\n        if canon_text != \"unknown\" and canon_text in self.target_venues:\n            return True\n\n        # Check if any target venue slug is present in the href\n        # This handles links that are just times (e.g. /results/track-slug/date/time)\n        href_clean = href.lower().replace(\"-\", \"\").replace(\"_\", \"\")\n        for v in self.target_venues:\n            if v and v in href_clean:\n                return True\n\n        return False\n\n    def _make_race_id(\n        self,\n        prefix: str,\n        venue: str,\n        date_str: str,\n        race_num: int,\n    ) -> str:\n        canon = fortuna.get_canonical_venue(venue)\n        return f\"{prefix}_{canon}_{date_str.replace('-', '')}_R{race_num}\"\n\n\n# -- EQUIBASE RESULTS ADAPTER ------------------------------------------------\n#\n# Special case: index \u2192 track summary pages \u2192 multiple race tables per page.\n# Overrides _parse_page (multi-race) instead of _parse_race_page.\n# -----------------------------------------------------------------------------\n\nclass EquibaseResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Equibase summary charts \u2014 primary US thoroughbred results source.\"\"\"\n\n    SOURCE_NAME = \"EquibaseResults\"\n    BASE_URL    = \"https://www.equibase.com\"\n    HOST        = \"www.equibase.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 60\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        # Equibase uses Instart Logic / Imperva; PLAYWRIGHT_LEGACY with network_idle is robust\n        return fortuna.FetchStrategy(\n            primary_engine=fortuna.BrowserEngine.PLAYWRIGHT_LEGACY,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=self.TIMEOUT,\n            network_idle=True,\n        )\n\n    def _get_headers(self) -> dict:\n        # Equibase is sensitive to header order/content; let SmartFetcher handle it via browserforge\n        return {\"Referer\": \"https://www.equibase.com/\"}\n\n    # -- link discovery (complex: multiple index URL patterns) -------------\n\n    async def _discover_result_links(self, date_str: str) -> set:\n        try:\n            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=date_str)\n            return set()\n\n        index_urls = [\n            f\"/static/chart/summary/index.html?SAP=TN\",\n            f\"/static/chart/summary/index.html?date={dt.strftime('%m/%d/%Y')}\",\n            f\"/static/chart/summary/{dt.strftime('%m%d%y')}sum.html\",\n            f\"/static/chart/summary/{dt.strftime('%Y%m%d')}sum.html\",\n        ]\n\n        resp = None\n        for url in index_urls:\n            # Try multiple impersonations to bypass Imperva/Cloudflare\n            for imp in [\"chrome120\", \"chrome110\", \"safari15_5\"]:\n                try:\n                    resp = await self.make_request(\n                        \"GET\", url, headers=self._get_headers(), impersonate=imp\n                    )\n                    if (\n                        resp and resp.text\n                        and len(resp.text) > 2000 # Increased threshold for real content\n                        and \"Pardon Our Interruption\" not in resp.text\n                        and \"<table\" in resp.text.lower() # Verify presence of data tables\n                    ):\n                        break\n                    else:\n                        resp = None\n                except Exception:\n                    continue\n            if resp:\n                break\n\n        if not resp or not resp.text:\n            self.logger.warning(\"No response from Equibase index\", date=date_str)\n            return set()\n\n        self._save_debug_snapshot(resp.text, f\"eqb_results_index_{date_str}\")\n        initial_links = self._extract_track_links(resp.text, dt)\n\n        # Resolve any RaceCardIndex links to actual sum.html files\n        resolved_links = set()\n        index_links = [ln for ln in initial_links if \"RaceCardIndex\" in ln]\n        sum_links = [ln for ln in initial_links if \"RaceCardIndex\" not in ln]\n\n        resolved_links.update(sum_links)\n\n        if index_links:\n            self.logger.info(\"Resolving track indices\", count=len(index_links))\n            metadata = [{\"url\": ln, \"race_number\": 0} for ln in index_links]\n            index_pages = await self._fetch_race_pages_concurrent(\n                metadata, self._get_headers(),\n            )\n            for p in index_pages:\n                html = p.get(\"html\")\n                if not html: continue\n                # Extract all sum.html links from this track index\n                date_short = dt.strftime(\"%m%d%y\")\n                for m in re.findall(r'href=\"([^\"]+)\"', html):\n                    normalised = m.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n                    if date_short in normalised and \"sum.html\" in normalised:\n                        resolved_links.add(self._normalise_eqb_link(normalised))\n                for m in re.findall(r'\"URL\":\"([^\"]+)\"', html):\n                    normalised = m.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n                    if date_short in normalised and \"sum.html\" in normalised:\n                        resolved_links.add(self._normalise_eqb_link(normalised))\n\n        return resolved_links\n\n    def _extract_track_links(self, html: str, dt: datetime) -> set:\n        \"\"\"Pull track-summary URLs from the index page.\"\"\"\n        parser = HTMLParser(html)\n        raw_links: set = set()\n        date_short = dt.strftime(\"%m%d%y\")\n\n        # Source 1 \u2014 inline JSON in <script> tags\n        for url_match in re.findall(r'\"URL\":\"([^\"]+)\"', html):\n            normalised = url_match.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n            if date_short in normalised and (\n                \"sum.html\" in normalised or \"EQB.html\" in normalised or \"RaceCardIndex\" in normalised\n            ):\n                raw_links.add(normalised)\n\n        # Source 2 \u2014 <a> tags matching known patterns\n        selectors_and_patterns = [\n            ('table.display a[href*=\"sum.html\"]', None),\n            ('a[href*=\"/static/chart/summary/\"]', lambda h: \"index.html\" not in h and \"calendar.html\" not in h),\n            (\"a\", lambda h: (\n                re.search(r\"[A-Z]{3}\\d{6}(?:sum|EQB)\\.html\", h)\n                or (date_short in h and (\"sum.html\" in h.lower() or \"eqb.html\" in h.lower()))\n            ) and \"index.html\" not in h and \"calendar.html\" not in h),\n        ]\n        for selector, extra_filter in selectors_and_patterns:\n            for a in parser.css(selector):\n                href = (a.attributes.get(\"href\") or \"\").replace(\"\\\\\", \"/\")\n                if not href:\n                    continue\n                if extra_filter and not extra_filter(href):\n                    continue\n                if not self._venue_matches(a.text(), href):\n                    continue\n                raw_links.add(href)\n\n        if not raw_links:\n            self.logger.warning(\"No track links found in index\", date=str(dt.date()))\n            return set()\n\n        self.logger.info(\"Track links extracted\", count=len(raw_links))\n        return {self._normalise_eqb_link(lnk) for lnk in raw_links}\n\n    def _normalise_eqb_link(self, link: str) -> str:\n        \"\"\"Turn a relative Equibase link into an absolute URL.\"\"\"\n        if link.startswith(\"http\"):\n            return link\n        path = link.lstrip(\"/\")\n        if \"static/chart/summary/\" not in path:\n            if path.startswith(\"../\"):\n                path = \"static/chart/\" + path.replace(\"../\", \"\")\n            elif not path.startswith(\"static/\"):\n                path = f\"static/chart/summary/{path}\"\n        path = re.sub(r\"/+\", \"/\", path)\n        return f\"{self.BASE_URL}/{path}\"\n\n    # -- multi-race page parsing -------------------------------------------\n\n    def _parse_page(\n        self, html: str, date_str: str, url: str,\n    ) -> List[ResultRace]:\n        \"\"\"A track summary page contains multiple race tables.\"\"\"\n        parser = HTMLParser(html)\n\n        # Venue from page header\n        track_node = parser.css_first(\"h3\") or parser.css_first(\"h2\")\n        if not track_node:\n            self.logger.debug(\"No track header found\", url=url)\n            return []\n        venue = fortuna.normalize_venue_name(track_node.text(strip=True))\n        if not venue:\n            return []\n\n        # Identify race tables and their indices among ALL tables\n        all_tables = parser.css(\"table\")\n        indexed_race_tables: List[Tuple[int, Node]] = []\n        for i, table in enumerate(all_tables):\n            header = table.css_first(\"thead tr th\")\n            if header and \"Race\" in header.text():\n                indexed_race_tables.append((i, table))\n\n        races: List[ResultRace] = []\n        for j, (idx, race_table) in enumerate(indexed_race_tables):\n            try:\n                # Dividend tables sit between this race and the next\n                next_idx = (\n                    indexed_race_tables[j + 1][0]\n                    if j + 1 < len(indexed_race_tables)\n                    else len(all_tables)\n                )\n                dividend_tables = all_tables[idx + 1 : next_idx]\n                exotics = extract_exotic_payouts(dividend_tables)\n\n                race = self._parse_race_table(\n                    race_table, venue, date_str, exotics,\n                )\n                if race:\n                    races.append(race)\n            except Exception as exc:\n                self.logger.debug(\n                    \"Failed to parse race table\", error=str(exc),\n                )\n        return races\n\n    def _parse_race_table(\n        self,\n        table: Node,\n        venue: str,\n        date_str: str,\n        exotics: Dict[str, Tuple[Optional[float], Optional[str]]],\n    ) -> Optional[ResultRace]:\n        header = table.css_first(\"thead tr th\")\n        if not header:\n            return None\n        header_text = header.text()\n\n        race_match = re.search(r\"Race\\s+(\\d+)\", header_text)\n        if not race_match:\n            return None\n        race_num = int(race_match.group(1))\n\n        # Start time from header or fallback\n        start_time = self._parse_header_time(header_text, date_str)\n\n        runners = [\n            r for row in table.css(\"tbody tr\")\n            if (r := self._parse_runner_row(row)) is not None\n        ]\n        if not runners:\n            return None\n\n        tri = exotics.get(\"trifecta\", (None, None))\n        exa = exotics.get(\"exacta\", (None, None))\n        sup = exotics.get(\"superfecta\", (None, None))\n\n        return ResultRace(\n            id=self._make_race_id(\"eqb_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n            is_fully_parsed=True,\n            trifecta_payout=tri[0],\n            trifecta_combination=tri[1],\n            exacta_payout=exa[0],\n            exacta_combination=exa[1],\n            superfecta_payout=sup[0],\n            superfecta_combination=sup[1],\n        )\n\n    @staticmethod\n    def _parse_header_time(header_text: str, date_str: str) -> datetime:\n        m = re.search(r\"(\\d{1,2}:\\d{2})\\s*([APM]{2})\", header_text, re.I)\n        if m:\n            try:\n                t = datetime.strptime(\n                    f\"{m.group(1)} {m.group(2).upper()}\", \"%I:%M %p\",\n                ).time()\n                d = datetime.strptime(date_str, \"%Y-%m-%d\")\n                return datetime.combine(d, t).replace(tzinfo=EASTERN)\n            except ValueError:\n                pass\n        return build_start_time(date_str)\n\n    def _parse_runner_row(self, row: Node) -> Optional[ResultRunner]:\n        try:\n            cols = row.css(\"td\")\n            if len(cols) < 3:\n                return None\n\n            name = fortuna.clean_text(cols[2].text())\n            if not name or name.upper() in (\"HORSE\", \"NAME\", \"RUNNER\"):\n                return None\n\n            pos_text = fortuna.clean_text(cols[0].text())\n            num_text = fortuna.clean_text(cols[1].text())\n            number = int(num_text) if num_text.isdigit() else 0\n\n            odds_text = (\n                fortuna.clean_text(cols[3].text()) if len(cols) > 3 else \"\"\n            )\n            final_odds = fortuna.parse_odds_to_decimal(odds_text)\n\n            win_pay = place_pay = show_pay = 0.0\n            if len(cols) >= 7:\n                win_pay   = parse_currency_value(cols[4].text())\n                place_pay = parse_currency_value(cols[5].text())\n                show_pay  = parse_currency_value(cols[6].text())\n\n            return ResultRunner(\n                name=name,\n                number=number,\n                position=pos_text,\n                final_win_odds=final_odds,\n                win_payout=win_pay,\n                place_payout=place_pay,\n                show_payout=show_pay,\n            )\n        except Exception as exc:\n            self.logger.warning(\n                \"Failed parsing runner row\", error=str(exc),\n            )\n            return None\n\n\n# -- RACING POST RESULTS ADAPTER ---------------------------------------------\n\nclass RacingPostResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Racing Post results \u2014 UK / IRE thoroughbred and jumps.\"\"\"\n\n    SOURCE_NAME = \"RacingPostResults\"\n    BASE_URL    = \"https://www.racingpost.com\"\n    HOST        = \"www.racingpost.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 60\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        strategy = super()._configure_fetch_strategy()\n        # RacingPost is JS-heavy and has strong bot detection; keep CURL_CFFI primary but ensure fallback (Project Hardening)\n        strategy.primary_engine = fortuna.BrowserEngine.CURL_CFFI\n        return strategy\n\n    # -- link discovery ----------------------------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        resp = await self.make_request(\n            \"GET\", f\"/results/{date_str}\", headers=self._get_headers(),\n        )\n        if not resp or not resp.text:\n            return set()\n        \n        if self._check_for_block(resp.text, f\"/results/{date_str}\"):\n            return set()\n\n        self._save_debug_snapshot(resp.text, f\"rp_results_index_{date_str}\")\n        parser = HTMLParser(resp.text)\n        return self._extract_rp_links(parser)\n\n    def _extract_rp_links(self, parser: HTMLParser) -> set:\n        links: set = set()\n\n        _SELECTORS = [\n            'a[data-test-selector=\"RC-meetingItem__link_race\"]',\n            'a[href*=\"/results/\"]',\n            \".ui-link.rp-raceCourse__panel__race__time\",\n            \"a.rp-raceCourse__panel__race__time\",\n            \".rp-raceCourse__panel__race__time a\",\n            \".RC-meetingItem__link\",\n        ]\n        for selector in _SELECTORS:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\", \"\")\n                if not href:\n                    continue\n                if not self._venue_matches(a.text(), href):\n                    continue\n                if self._is_rp_race_link(href):\n                    links.add(href)\n\n        # Last-resort fallback\n        if not links:\n            for a in parser.css('a[href*=\"/results/\"]'):\n                href = a.attributes.get(\"href\", \"\")\n                if len(href.split(\"/\")) >= 3:\n                    links.add(href)\n\n        return links\n\n    @staticmethod\n    def _is_rp_race_link(href: str) -> bool:\n        return bool(\n            re.search(r\"/results/.*?\\d{5,}\", href)\n            or re.search(r\"/results/\\d+/\", href)\n            or re.search(r\"/\\d{4}-\\d{2}-\\d{2}/\", href)\n            or (\"/results/\" in href and len(href.split(\"/\")) >= 4)\n        )\n\n    # -- single-race page parsing ------------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, _url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        venue_node = parser.css_first(\".rp-raceTimeCourseName__course\")\n        if not venue_node:\n            return None\n        venue = fortuna.normalize_venue_name(venue_node.text(strip=True))\n\n        dividends = self._parse_tote_dividends(parser)\n        trifecta_pay, trifecta_combo = self._exotic_from_dividends(\n            dividends, \"trifecta\",\n        )\n        superfecta_pay, superfecta_combo = self._exotic_from_dividends(\n            dividends, \"superfecta\",\n        )\n\n        race_num = self._extract_rp_race_number(parser)\n\n        runners = self._parse_rp_runners(parser, dividends)\n        if not runners:\n            return None\n\n        return ResultRace(\n            id=self._make_race_id(\"rp_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=build_start_time(date_str),\n            runners=runners,\n            source=self.SOURCE_NAME,\n            trifecta_payout=trifecta_pay,\n            trifecta_combination=trifecta_combo,\n            superfecta_payout=superfecta_pay,\n            superfecta_combination=superfecta_combo,\n            official_dividends={\n                k: parse_currency_value(v) for k, v in dividends.items()\n            },\n        )\n\n    # -- RP-specific helpers -----------------------------------------------\n\n    @staticmethod\n    def _parse_tote_dividends(parser: HTMLParser) -> Dict[str, str]:\n        \"\"\"Extract label\u2192value pairs from the Tote Returns panel.\"\"\"\n        container = (\n            parser.css_first('div[data-test-selector=\"RC-toteReturns\"]')\n            or parser.css_first(\".rp-toteReturns\")\n        )\n        if not container:\n            return {}\n\n        dividends: Dict[str, str] = {}\n        for row in (\n            container.css(\"div.rp-toteReturns__row\")\n            or container.css(\".rp-toteReturns__row\")\n        ):\n            label_node = (\n                row.css_first(\"div.rp-toteReturns__label\")\n                or row.css_first(\".rp-toteReturns__label\")\n            )\n            val_node = (\n                row.css_first(\"div.rp-toteReturns__value\")\n                or row.css_first(\".rp-toteReturns__value\")\n            )\n            if label_node and val_node:\n                label = fortuna.clean_text(label_node.text())\n                value = fortuna.clean_text(val_node.text())\n                if label and value:\n                    dividends[label] = value\n        return dividends\n\n    @staticmethod\n    def _exotic_from_dividends(\n        dividends: Dict[str, str],\n        bet_type: str,\n    ) -> Tuple[Optional[float], Optional[str]]:\n        aliases = _BET_ALIASES.get(bet_type, [bet_type])\n        for label, val in dividends.items():\n            if any(a in label.lower() for a in aliases):\n                payout = parse_currency_value(val)\n                combo = (\n                    val.split(\"\u00a3\")[-1].strip() if \"\u00a3\" in val else None\n                )\n                return payout, combo\n        return None, None\n\n    @staticmethod\n    def _extract_rp_race_number(parser: HTMLParser) -> int:\n        # Priority 1 \u2014 navigation bar active item\n        for i, link in enumerate(\n            parser.css('a[data-test-selector=\"RC-raceTime\"]'),\n        ):\n            cls = link.attributes.get(\"class\", \"\")\n            if \"active\" in cls or \"rp-raceTimeCourseName__time\" in cls:\n                return i + 1\n        # Priority 2 \u2014 text fallback\n        return _extract_race_number_from_text(parser) or 1\n\n    def _parse_rp_runners(\n        self,\n        parser: HTMLParser,\n        dividends: Dict[str, str],\n    ) -> List[ResultRunner]:\n        runners: List[ResultRunner] = []\n        for row in parser.css(\".rp-horseTable__table__row\"):\n            try:\n                name_node = row.css_first(\".rp-horseTable__horse__name\")\n                if not name_node:\n                    continue\n                name = fortuna.clean_text(name_node.text())\n\n                pos_node = row.css_first(\".rp-horseTable__pos__number\")\n                pos = (\n                    fortuna.clean_text(pos_node.text()) if pos_node else None\n                )\n\n                num_node = row.css_first(\".rp-horseTable__saddleClothNo\")\n                number = 0\n                if num_node:\n                    try:\n                        number = int(fortuna.clean_text(num_node.text()))\n                    except ValueError:\n                        pass\n\n                # Place payout from dividends map\n                place_payout: Optional[float] = None\n                for lbl, val in dividends.items():\n                    if (\n                        \"place\" in lbl.lower()\n                        and name.lower() in lbl.lower()\n                    ):\n                        place_payout = parse_currency_value(val)\n                        break\n\n                sp_node = row.css_first(\".rp-horseTable__horse__sp\")\n                final_odds = 0.0\n                if sp_node:\n                    final_odds = parse_fractional_odds(\n                        fortuna.clean_text(sp_node.text()),\n                    )\n\n                runners.append(ResultRunner(\n                    name=name,\n                    number=number,\n                    position=pos,\n                    place_payout=place_payout,\n                    final_win_odds=final_odds,\n                ))\n            except Exception:\n                continue\n        return runners\n\n\n# -- AT THE RACES RESULTS ADAPTER ---------------------------------------------\n\nclass AtTheRacesResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"At The Races results \u2014 UK / IRE.\"\"\"\n\n    SOURCE_NAME = \"AtTheRacesResults\"\n    BASE_URL    = \"https://www.attheraces.com\"\n    HOST        = \"www.attheraces.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 60\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        strategy = super()._configure_fetch_strategy()\n        # ATR uses Cloudflare; keep CURL_CFFI primary but ensure fallback (Project Hardening)\n        strategy.primary_engine = fortuna.BrowserEngine.CURL_CFFI\n        return strategy\n\n    # -- link discovery (multi-URL index) ----------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> set:\n        dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n        index_urls = [\n            f\"/results/{date_str}\",\n            f\"/results/{dt.strftime('%d-%B-%Y')}\",\n            f\"/results/international/{date_str}\",\n            f\"/results/international/{dt.strftime('%d-%B-%Y')}\",\n        ]\n\n        links: set = set()\n        for url in index_urls:\n            try:\n                resp = await self.make_request(\n                    \"GET\", url, headers=self._get_headers(),\n                )\n                if not resp or not resp.text:\n                    continue\n                self._save_debug_snapshot(\n                    resp.text,\n                    f\"atr_index_{date_str}_{url.replace('/', '_')}\",\n                )\n                links.update(self._extract_atr_links(resp.text))\n            except Exception as exc:\n                self.logger.debug(\n                    \"ATR index fetch failed\", url=url, error=str(exc),\n                )\n\n        return links\n\n    def _extract_atr_links(self, html: str) -> set:\n        parser = HTMLParser(html)\n        links: set = set()\n        # Broad selectors for all possible result links (Council of Superbrains Directive)\n        for selector in [\n            \"a[href*='/results/']\",\n            \"a[data-test-selector*='result']\",\n            \".meeting-summary a\",\n            \".p-results__item a\",\n            \".p-meetings__item a\",\n            \".p-results-meeting a\",\n        ]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\", \"\")\n                if not href:\n                    continue\n                if not self._venue_matches(a.text(), href):\n                    continue\n                if self._is_atr_race_link(href):\n                    links.add(\n                        href if href.startswith(\"http\")\n                        else f\"{self.BASE_URL}{href}\"\n                    )\n        return links\n\n    @staticmethod\n    def _is_atr_race_link(href: str) -> bool:\n        return bool(\n            re.search(r\"/results/.*?/\\d{4}\", href)\n            or re.search(r\"/results/\\d{2}-.*?-\\d{4}/\", href)\n            or re.search(r\"/results/.*?/\\d+$\", href)\n            or (\"/results/\" in href and len(href.split(\"/\")) >= 4)\n        )\n\n    # -- single-race page parsing ------------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        venue = self._extract_atr_venue(parser)\n        if not venue:\n            return None\n\n        race_num = 1\n        url_match = re.search(r\"/R(\\d+)$\", url)\n        if url_match:\n            race_num = int(url_match.group(1))\n\n        runners = self._parse_atr_runners(parser)\n\n        # Dividends \u2014 use the shared exotic exotic extractor on the\n        # dedicated dividends table, then enrich with place payouts\n        div_table = parser.css_first(\".result-racecard__dividends-table\")\n        exotics: Dict[str, Tuple[Optional[float], Optional[str]]] = {}\n        if div_table:\n            exotics = extract_exotic_payouts([div_table])\n            self._map_place_payouts(div_table, runners)\n\n        if not runners:\n            return None\n\n        tri = exotics.get(\"trifecta\", (None, None))\n        sup = exotics.get(\"superfecta\", (None, None))\n\n        return ResultRace(\n            id=self._make_race_id(\"atr_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=build_start_time(date_str),\n            runners=runners,\n            trifecta_payout=tri[0],\n            trifecta_combination=tri[1],\n            superfecta_payout=sup[0],\n            superfecta_combination=sup[1],\n            source=self.SOURCE_NAME,\n        )\n\n    # -- ATR-specific helpers ----------------------------------------------\n\n    @staticmethod\n    def _extract_atr_venue(parser: HTMLParser) -> Optional[str]:\n        header = (\n            parser.css_first(\".race-header__details--primary\")\n            or parser.css_first(\".racecard-header\")\n            or parser.css_first(\".race-header\")\n        )\n        if not header:\n            return None\n        venue_node = (\n            header.css_first(\"h2\")\n            or header.css_first(\"h1\")\n            or header.css_first(\".track-name\")\n        )\n        if not venue_node:\n            return None\n        return fortuna.normalize_venue_name(venue_node.text(strip=True))\n\n    def _parse_atr_runners(\n        self, parser: HTMLParser,\n    ) -> List[ResultRunner]:\n        rows = (\n            parser.css(\".result-racecard__row\")\n            or parser.css(\".card-cell--horse\")\n            or parser.css(\"atr-result-horse\")\n            or parser.css(\"div[class*='RacecardResultItem']\")\n            or parser.css(\".p-results__item\")\n        )\n\n        runners: List[ResultRunner] = []\n        for row in rows:\n            try:\n                name_node = (\n                    row.css_first(\".result-racecard__horse-name a\")\n                    or row.css_first(\".horse-name a\")\n                    or row.css_first(\"a[href*='/horse/']\")\n                    or row.css_first(\"[class*='HorseName']\")\n                )\n                if not name_node:\n                    continue\n                name = fortuna.clean_text(name_node.text())\n\n                pos_node = (\n                    row.css_first(\".result-racecard__pos\")\n                    or row.css_first(\".pos\")\n                    or row.css_first(\".position\")\n                    or row.css_first(\"[class*='Position']\")\n                )\n                pos = (\n                    fortuna.clean_text(pos_node.text()) if pos_node else None\n                )\n\n                num_node = row.css_first(\n                    \".result-racecard__saddle-cloth\",\n                )\n                number = 0\n                if num_node:\n                    try:\n                        number = int(fortuna.clean_text(num_node.text()))\n                    except ValueError:\n                        pass\n\n                odds_node = row.css_first(\".result-racecard__odds\")\n                final_odds = 0.0\n                if odds_node:\n                    final_odds = parse_fractional_odds(\n                        fortuna.clean_text(odds_node.text()),\n                    )\n\n                runners.append(ResultRunner(\n                    name=name,\n                    number=number,\n                    position=pos,\n                    final_win_odds=final_odds,\n                ))\n            except Exception:\n                continue\n        return runners\n\n    @staticmethod\n    def _map_place_payouts(\n        div_table: Node,\n        runners: List[ResultRunner],\n    ) -> None:\n        \"\"\"Enrich runners with place payouts from the dividends table.\"\"\"\n        for row in div_table.css(\"tr\"):\n            try:\n                row_text = row.text().lower()\n                if \"place\" not in row_text:\n                    continue\n                cols = row.css(\"td\")\n                if len(cols) < 2:\n                    continue\n                p_name = fortuna.clean_text(\n                    cols[0].text().replace(\"Place\", \"\").strip(),\n                )\n                p_val = parse_currency_value(cols[1].text())\n                for runner in runners:\n                    if (\n                        runner.name.lower() in p_name.lower()\n                        or p_name.lower() in runner.name.lower()\n                    ):\n                        runner.place_payout = p_val\n            except Exception:\n                continue\n\n\n# -- SPORTING LIFE RESULTS ADAPTER --------------------------------------------\n\n\nclass SportingLifeResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Sporting Life results (UK / IRE / International).\"\"\"\n\n    SOURCE_NAME = \"SportingLifeResults\"\n    BASE_URL    = \"https://www.sportinglife.com\"\n    HOST        = \"www.sportinglife.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 45\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        strategy = super()._configure_fetch_strategy()\n        # SportingLife is JS-heavy; keep CURL_CFFI primary but ensure fallback (Project Hardening)\n        strategy.primary_engine = fortuna.BrowserEngine.CURL_CFFI\n        return strategy\n\n    # -- link discovery ----------------------------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        resp = await self.make_request(\n            \"GET\",\n            f\"/racing/results/{date_str}\",\n            headers=self._get_headers(),\n        )\n        if not resp or not resp.text:\n            return set()\n        self._save_debug_snapshot(resp.text, f\"sl_results_index_{date_str}\")\n        return self._extract_sl_links(resp.text)\n\n    def _extract_sl_links(self, html: str) -> set:\n        links: set = set()\n        for a in HTMLParser(html).css(\"a[href*='/racing/results/']\"):\n            href = a.attributes.get(\"href\", \"\")\n            if not href: continue\n            if not self._venue_matches(a.text(), href):\n                continue\n            # /racing/results/2026-02-04/ludlow/901676/race-name\n            if re.search(r\"/results/\\d{4}-\\d{2}-\\d{2}/.+/\\d+/\", href):\n                links.add(href)\n        return links\n\n    # -- page parsing (two strategies) -------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        # Strategy 1 \u2014 Next.js JSON payload (most reliable)\n        script = parser.css_first(\"script#__NEXT_DATA__\")\n        if script:\n            race = self._parse_from_next_data(script.text(), date_str)\n            if race:\n                return race\n\n        # Strategy 2 \u2014 HTML scrape fallback\n        return self._parse_from_html(parser, date_str)\n\n    # -- Strategy 1: JSON -------------------------------------------------\n\n    def _parse_from_next_data(\n        self, script_text: str, date_str: str,\n    ) -> Optional[ResultRace]:\n        try:\n            data = json.loads(script_text)\n        except json.JSONDecodeError as exc:\n            self.logger.debug(\"Invalid __NEXT_DATA__\", error=str(exc))\n            return None\n\n        race_data = (\n            data.get(\"props\", {}).get(\"pageProps\", {}).get(\"race\", {})\n        )\n        if not race_data:\n            return None\n\n        summary    = race_data.get(\"race_summary\", {})\n        venue      = fortuna.normalize_venue_name(\n            summary.get(\"course_name\", \"Unknown\"),\n        )\n        race_num   = (\n            race_data.get(\"race_number\")\n            or summary.get(\"race_number\")\n            or 1\n        )\n        date_val   = summary.get(\"date\", date_str)\n        start_time = build_start_time(date_val, summary.get(\"time\"))\n\n        runners = self._runners_from_json(race_data)\n        if not runners:\n            return None\n\n        # Exotic payouts via recursive search\n        trifecta_pay   = find_nested_value(race_data, \"trifecta\")\n        superfecta_pay = find_nested_value(race_data, \"superfecta\")\n\n        # Place payouts from CSV field\n        self._apply_place_payouts_from_csv(\n            race_data.get(\"place_win\", \"\"), runners,\n        )\n\n        return ResultRace(\n            id=self._make_race_id(\"sl_res\", venue, date_val, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            trifecta_payout=trifecta_pay,\n            superfecta_payout=superfecta_pay,\n            source=self.SOURCE_NAME,\n        )\n\n    @staticmethod\n    def _runners_from_json(race_data: dict) -> List[ResultRunner]:\n        \"\"\"Extract runners from ``rides`` (result pages) or ``runners``.\"\"\"\n        items = race_data.get(\"rides\") or race_data.get(\"runners\", [])\n        runners: List[ResultRunner] = []\n        for item in items:\n            horse = item.get(\"horse\", {})\n            name  = horse.get(\"name\") or item.get(\"name\")\n            if not name:\n                continue\n            sp_raw = (\n                item.get(\"starting_price\")\n                or item.get(\"sp\")\n                or item.get(\"betting\", {}).get(\"current_odds\", \"\")\n            )\n            runners.append(ResultRunner(\n                name=name,\n                number=(\n                    item.get(\"cloth_number\")\n                    or item.get(\"saddle_cloth_number\", 0)\n                ),\n                position=str(item.get(\"finish_position\", item.get(\"position\", \"\"))),\n                final_win_odds=parse_fractional_odds(str(sp_raw)),\n            ))\n        return runners\n\n    @staticmethod\n    def _apply_place_payouts_from_csv(\n        place_csv: str,\n        runners: List[ResultRunner],\n    ) -> None:\n        \"\"\"Map comma-separated place payouts to runners by finishing position.\"\"\"\n        if not isinstance(place_csv, str) or not place_csv:\n            return\n        pays = [parse_currency_value(p) for p in place_csv.split(\",\")]\n        for runner in runners:\n            pos = runner.position_numeric\n            if pos and 1 <= pos <= len(pays):\n                runner.place_payout = pays[pos - 1]\n\n    # -- Strategy 2: HTML fallback ----------------------------------------\n\n    def _parse_from_html(\n        self, parser: HTMLParser, date_str: str,\n    ) -> Optional[ResultRace]:\n        header = parser.css_first(\"h1\")\n        if not header:\n            return None\n\n        match = re.match(\n            r\"(\\d{1,2}:\\d{2})\\s+(.+)\\s+Result\",\n            fortuna.clean_text(header.text()),\n        )\n        if not match:\n            return None\n\n        time_str   = match.group(1)\n        venue      = fortuna.normalize_venue_name(match.group(2))\n        start_time = build_start_time(date_str, time_str)\n\n        runners: List[ResultRunner] = []\n        for row in parser.css(\n            'div[class*=\"ResultRunner__StyledResultRunnerWrapper\"]',\n        ):\n            name_node = row.css_first(\n                'a[class*=\"ResultRunner__StyledHorseName\"]',\n            )\n            if not name_node:\n                continue\n            pos_node = row.css_first(\n                'div[class*=\"ResultRunner__StyledRunnerPositionContainer\"]',\n            )\n            runners.append(ResultRunner(\n                name=fortuna.clean_text(name_node.text()),\n                number=0,\n                position=(\n                    fortuna.clean_text(pos_node.text()) if pos_node else None\n                ),\n            ))\n\n        if not runners:\n            return None\n\n        return ResultRace(\n            id=self._make_race_id(\"sl_res\", venue, date_str, 1),\n            venue=venue,\n            race_number=1,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n\nclass SkySportsResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Sky Sports Racing results (UK / IRE).\"\"\"\n\n    SOURCE_NAME = \"SkySportsResults\"\n    BASE_URL    = \"https://www.skysports.com\"\n    HOST        = \"www.skysports.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 45\n\n    # -- link discovery ----------------------------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        try:\n            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n            url_dates = [dt.strftime(\"%d-%m-%Y\")]\n        except ValueError:\n            url_dates = [date_str]\n\n        links: set = set()\n        for url_date in url_dates:\n            try:\n                resp = await self.make_request(\n                    \"GET\", f\"/racing/results/{url_date}\", headers=self._get_headers(),\n                )\n                if not resp or not resp.text:\n                    continue\n                self._save_debug_snapshot(resp.text, f\"sky_results_index_{url_date}\")\n                parser = HTMLParser(resp.text)\n                links.update(self._extract_sky_links(parser, date_str, url_date))\n            except Exception as exc:\n                self.logger.debug(\"Sky index fetch failed\", url_date=url_date, error=str(exc))\n\n        return links\n\n    def _extract_sky_links(self, parser: HTMLParser, date_str: str, url_date: str) -> set:\n        links: set = set()\n        # Broad selectors for SkySports results (Council of Superbrains Directive)\n        for a in (parser.css(\"a[href*='/racing/results/']\") + parser.css(\"a[href*='/full-result/']\")):\n            href = a.attributes.get(\"href\", \"\")\n            if not href: continue\n            if not self._venue_matches(a.text(), href):\n                continue\n\n            # Match various result path patterns\n            has_race_path = any(\n                p in href for p in (\"/full-result/\", \"/race-result/\", \"/results/full-result/\")\n            ) or re.search(r\"/\\d{6,}/\", href)\n\n            # Check if link belongs to requested date or is generally a result link\n            has_date = date_str in href or url_date in href or re.search(r\"/\\d{6,}/\", href)\n\n            if has_race_path and has_date:\n                links.add(href)\n        return links\n\n    # -- page parsing ------------------------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        header = parser.css_first(\".sdc-site-racing-header__name\")\n        if not header:\n            return None\n\n        match = re.match(\n            r\"(\\d{1,2}:\\d{2})\\s+(.+)\",\n            fortuna.clean_text(header.text()),\n        )\n        if not match:\n            return None\n\n        time_str   = match.group(1)\n        venue      = fortuna.normalize_venue_name(match.group(2))\n        start_time = build_start_time(date_str, time_str)\n\n        runners = self._parse_sky_runners(parser)\n        if not runners:\n            return None\n\n        exotics  = extract_exotic_payouts(parser.css(\"table\"))\n        race_num = self._extract_sky_race_number(parser, url)\n\n        tri = exotics.get(\"trifecta\", (None, None))\n        sup = exotics.get(\"superfecta\", (None, None))\n\n        return ResultRace(\n            id=self._make_race_id(\"sky_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            trifecta_payout=tri[0],\n            superfecta_payout=sup[0],\n            source=self.SOURCE_NAME,\n        )\n\n    @staticmethod\n    def _parse_sky_runners(parser: HTMLParser) -> List[ResultRunner]:\n        runners: List[ResultRunner] = []\n        for row in parser.css(\".sdc-site-racing-card__item\"):\n            name_node = row.css_first(\".sdc-site-racing-card__name\")\n            if not name_node:\n                continue\n\n            pos_node    = row.css_first(\".sdc-site-racing-card__position\")\n            number_node = row.css_first(\".sdc-site-racing-card__number\")\n            odds_node   = row.css_first(\".sdc-site-racing-card__odds\")\n\n            number = 0\n            if number_node:\n                try:\n                    number = int(re.sub(r\"\\D\", \"\", number_node.text()))\n                except (ValueError, TypeError):\n                    pass\n\n            runners.append(ResultRunner(\n                name=fortuna.clean_text(name_node.text()),\n                number=number,\n                position=(\n                    fortuna.clean_text(pos_node.text()) if pos_node else None\n                ),\n                final_win_odds=parse_fractional_odds(\n                    fortuna.clean_text(odds_node.text()) if odds_node else \"\",\n                ),\n            ))\n        return runners\n\n    @staticmethod\n    def _extract_sky_race_number(parser: HTMLParser, url: str) -> int:\n        \"\"\"Try navigation index, then URL ID, then text fallback.\"\"\"\n        url_match = re.search(r\"/(\\d+)/\", url)\n        if url_match:\n            nav_links = parser.css(\"a[href*='/racing/results/']\")\n            for i, link in enumerate(nav_links):\n                if url_match.group(0) in (\n                    link.attributes.get(\"href\") or \"\"\n                ):\n                    return i + 1\n\n        return _extract_race_number_from_text(parser, url) or 1\n\n\n# -- REPORT GENERATION --------------------------------------------------------\n\n_REPORT_WIDTH: Final[int] = 80\n_REPORT_SEP:   Final[str] = \"=\" * _REPORT_WIDTH\n_REPORT_DOT:   Final[str] = \".\" * _REPORT_WIDTH\n_SECTION_SEP:  Final[str] = \"-\" * 40\n\n\ndef _format_tip_time(tip: Dict[str, Any]) -> str:\n    \"\"\"Best-effort ET time string from a tip's ``start_time``.\"\"\"\n    raw = tip.get(\"start_time\", \"\")\n    try:\n        dt = datetime.fromisoformat(str(raw).replace(\"Z\", \"+00:00\"))\n        return to_eastern(dt).strftime(\"%Y-%m-%d %H:%M ET\")\n    except (ValueError, TypeError):\n        return str(raw)[:16].replace(\"T\", \" \")\n\n\ndef generate_analytics_report(\n    audited_tips: List[Dict[str, Any]],\n    recent_tips: Optional[List[Dict[str, Any]]] = None,\n    harvest_summary: Optional[Dict[str, Any]] = None,\n    *,\n    include_lifetime_stats: bool = False,\n) -> str:\n    \"\"\"Build the human-readable performance audit report.\"\"\"\n    now_str = datetime.now(EASTERN).strftime(\"%Y-%m-%d %H:%M ET\")\n    lines: list[str] = [\n        _REPORT_SEP,\n        \"\ud83d\udc0e FORTUNA INTELLIGENCE - PERFORMANCE AUDIT & VERIFICATION\".center(_REPORT_WIDTH),\n        f\"Generated: {now_str}\".center(_REPORT_WIDTH),\n        _REPORT_SEP,\n        \"\",\n    ]\n\n    if harvest_summary:\n        _append_harvest_proof(lines, harvest_summary)\n\n    if recent_tips:\n        _append_pending_section(lines, recent_tips)\n\n    audited_sorted = sorted(\n        audited_tips,\n        key=lambda t: t.get(\"start_time\", \"\"),\n        reverse=True,\n    )\n    if audited_sorted:\n        _append_recent_performance(lines, audited_sorted[:15])\n\n    _append_exotic_tracking(lines, audited_tips)\n\n    if include_lifetime_stats and audited_tips:\n        _append_lifetime_stats(lines, audited_tips)\n\n    return \"\\n\".join(lines)\n\n\ndef _append_harvest_proof(\n    lines: list[str],\n    harvest_summary: Dict[str, Any],\n) -> None:\n    lines.extend([\"\ud83d\udd0e LIVE ADAPTER HARVEST PROOF\", _SECTION_SEP])\n    for adapter, data in harvest_summary.items():\n        if isinstance(data, dict):\n            count    = data.get(\"count\", 0)\n            max_odds = data.get(\"max_odds\", 0.0)\n        else:\n            count, max_odds = data, 0.0\n\n        status   = \"\u2705 SUCCESS\" if count > 0 else \"\u23f3 PENDING/NO DATA\"\n        odds_str = (\n            f\"MaxOdds: {max_odds:>5.1f}\" if max_odds > 0 else \"Odds: N/A\"\n        )\n        lines.append(\n            f\"{adapter:<25} | {status:<15} | Records: {count:<4} | {odds_str}\",\n        )\n    lines.append(\"\")\n\n\ndef _append_pending_section(\n    lines: list[str],\n    recent_tips: List[Dict[str, Any]],\n) -> None:\n    lines.extend([\n        \"\u23f3 PENDING VERIFICATION - RECENT DISCOVERIES\",\n        _SECTION_SEP,\n        f\"{'RACE TIME':<18} | {'VENUE':<20} | {'R#':<3} | {'GM?':<4} | STATUS\",\n        _REPORT_DOT,\n    ])\n    for tip in recent_tips[:25]:\n        st_str = _format_tip_time(tip)\n        venue  = str(tip.get(\"venue\", \"Unknown\"))[:20]\n        rnum   = tip.get(\"race_number\", \"?\")\n        gm     = \"GOLD\" if tip.get(\"is_goldmine\") else \"----\"\n        status = (\n            tip.get(\"verdict\") if tip.get(\"audit_completed\") else \"WATCHING\"\n        )\n        lines.append(\n            f\"{st_str:<18} | {venue:<20} | {rnum:<3} | {gm:<4} | {status}\",\n        )\n    lines.append(\"\")\n\n\ndef _append_recent_performance(\n    lines: list[str],\n    tips: List[Dict[str, Any]],\n) -> None:\n    lines.extend([\n        \"\ud83d\udcb0 RECENT PERFORMANCE PROOF (MATCHED RESULTS)\",\n        _SECTION_SEP,\n        f\"{'RESULT':<6} | {'RACE':<25} | {'PROFIT':<8} | PAYOUT/DETAILS\",\n        _REPORT_DOT,\n    ])\n    for tip in tips:\n        verdict = tip.get(\"verdict\", \"?\")\n        emoji = _VERDICT_DISPLAY.get(verdict, \"\u26aa VOID\")\n        venue  = (\n            f\"{tip.get('venue', 'Unknown')[:18]} \"\n            f\"R{tip.get('race_number', '?')}\"\n        )\n        profit = f\"${tip.get('net_profit', 0.0):+.2f}\"\n\n        detail_parts: list[str] = []\n\n        # Place payouts\n        p1 = tip.get(\"top1_place_payout\")\n        p2 = tip.get(\"top2_place_payout\")\n        if p1 or p2:\n            detail_parts.append(f\"P: {p1 or 0:.2f}/{p2 or 0:.2f}\")\n\n        # Odds drift\n        po = tip.get(\"predicted_2nd_fav_odds\")\n        ao = tip.get(\"actual_2nd_fav_odds\")\n        if po is not None or ao is not None:\n            po_s = f\"{po:.1f}\" if po is not None else \"?\"\n            ao_s = f\"{ao:.1f}\" if ao is not None else \"?\"\n            detail_parts.append(f\"Odds: {po_s}->{ao_s}\")\n\n        # Exotics\n        if tip.get(\"superfecta_payout\"):\n            detail_parts.append(f\"Super: ${tip['superfecta_payout']:.2f}\")\n        elif tip.get(\"trifecta_payout\"):\n            detail_parts.append(f\"Tri: ${tip['trifecta_payout']:.2f}\")\n        elif tip.get(\"actual_top_5\"):\n            detail_parts.append(f\"Top 5: [{tip['actual_top_5']}]\")\n\n        payout_info = \" | \".join(detail_parts)\n        lines.append(\n            f\"{emoji:<6} | {venue:<25} | {profit:>8} | {payout_info}\",\n        )\n    lines.append(\"\")\n\n\ndef _append_exotic_tracking(\n    lines: list[str],\n    audited_tips: List[Dict[str, Any]],\n) -> None:\n    super_races = [t for t in audited_tips if t.get(\"superfecta_payout\")]\n    tri_races   = [t for t in audited_tips if t.get(\"trifecta_payout\")]\n\n    if super_races:\n        payouts = [t[\"superfecta_payout\"] for t in super_races]\n        lines.extend([\n            \"\ud83c\udfaf SUPERFECTA PERFORMANCE PROOF\",\n            _SECTION_SEP,\n            f\"Superfecta Matches: {len(super_races)}\",\n            f\"  Average Payout:   ${sum(payouts) / len(payouts):.2f}\",\n            f\"  Maximum Payout:   ${max(payouts):.2f}\",\n            \"\",\n        ])\n    elif tri_races:\n        payouts = [t[\"trifecta_payout\"] for t in tri_races]\n        lines.extend([\n            \"\ud83c\udfaf SECONDARY EXOTIC TRACKING (TRIFECTA)\",\n            _SECTION_SEP,\n            f\"Trifecta Matches:   {len(tri_races)} \"\n            f\"(Avg: ${sum(payouts) / len(payouts):.2f})\",\n            \"\",\n        ])\n\n\ndef _append_lifetime_stats(\n    lines: list[str],\n    audited_tips: List[Dict[str, Any]],\n) -> None:\n    total  = len(audited_tips)\n    cashed = sum(1 for t in audited_tips if t.get(\"verdict\") in _CASHED_VERDICTS)\n    profit = sum(t.get(\"net_profit\", 0.0) for t in audited_tips)\n    sr     = (cashed / total * 100) if total else 0.0\n    roi    = (profit / (total * 2.0) * 100) if total else 0.0\n\n    lines.extend([\n        \"\ud83d\udcca SUMMARY METRICS (LIFETIME)\",\n        _SECTION_SEP,\n        f\"Total Verified Races: {total}\",\n        f\"Overall Strike Rate:   {sr:.1f}%\",\n        f\"Total Net Profit:     ${profit:+.2f} (Using $2.00 Base Unit)\",\n        f\"Return on Investment:  {roi:+.1f}%\",\n        \"\",\n    ])\n\n\n# -- ADAPTER REGISTRY & LIFECYCLE ---------------------------------------------\n\ndef get_results_adapter_classes() -> List[Type[fortuna.BaseAdapterV3]]:\n    \"\"\"All concrete adapter classes with ``ADAPTER_TYPE == 'results'``.\"\"\"\n\n    def _all_subclasses(cls: type) -> set:\n        subs = set(cls.__subclasses__())\n        return subs.union(s for c in subs for s in _all_subclasses(c))\n\n    return [\n        c\n        for c in _all_subclasses(fortuna.BaseAdapterV3)\n        if not getattr(c, \"__abstractmethods__\", None)\n        and getattr(c, \"ADAPTER_TYPE\", \"discovery\") == \"results\"\n    ]\n\n\n@asynccontextmanager\nasync def managed_adapters(\n    region: Optional[str] = None,\n    target_venues: Optional[set] = None,\n):\n    \"\"\"Instantiate, optionally filter, yield, then tear down all results\n    adapters.\"\"\"\n    classes = get_results_adapter_classes()\n    logger = structlog.get_logger(\"managed_adapters\")\n\n    if region:\n        if region == \"GLOBAL\":\n            allowed = set(fortuna.USA_RESULTS_ADAPTERS) | set(fortuna.INT_RESULTS_ADAPTERS)\n        else:\n            allowed = (\n                set(fortuna.USA_RESULTS_ADAPTERS)\n                if region == \"USA\"\n                else set(fortuna.INT_RESULTS_ADAPTERS)\n            )\n        classes = [\n            c for c in classes\n            if getattr(c, \"SOURCE_NAME\", \"\") in allowed\n        ]\n\n    adapters: list[fortuna.BaseAdapterV3] = []\n    for cls in classes:\n        adapter = cls()\n        if target_venues:\n            adapter.target_venues = target_venues  # type: ignore[attr-defined]\n        adapters.append(adapter)\n\n    try:\n        yield adapters\n    finally:\n        for adapter in adapters:\n            try:\n                await adapter.close()\n            except Exception as exc:\n                logger.warning(\"Adapter cleanup failed\", adapter=adapter.source_name, error=str(exc))\n        try:\n            await fortuna.GlobalResourceManager.cleanup()\n        except Exception as exc:\n            logger.error(\"Global resource cleanup failed\", error=str(exc))\n\n\n# -- ORCHESTRATION HELPERS ----------------------------------------------------\n\n_analytics_logger = structlog.get_logger(\"run_analytics\")\n_MAX_CONCURRENT_FETCHES: Final[int] = 10\n\n\nasync def _harvest_results(\n    adapters: List[fortuna.BaseAdapterV3],\n    valid_dates: List[str],\n    harvest_summary: Dict[str, Dict[str, Any]],\n) -> List[ResultRace]:\n    \"\"\"Fetch results from all adapters \u00d7 dates; populate *harvest_summary*.\"\"\"\n    sem = asyncio.Semaphore(_MAX_CONCURRENT_FETCHES)\n\n    async def _fetch_one(\n        adapter: fortuna.BaseAdapterV3,\n        date_str: str,\n    ) -> Tuple[str, List[ResultRace]]:\n        async with sem:\n            try:\n                races = await adapter.get_races(date_str)\n                _analytics_logger.debug(\n                    \"Fetched results\",\n                    adapter=adapter.source_name,\n                    date=date_str,\n                    count=len(races),\n                )\n                return adapter.source_name, races\n            except Exception as exc:\n                _analytics_logger.warning(\n                    \"Adapter fetch failed\",\n                    adapter=adapter.source_name,\n                    date=date_str,\n                    error=str(exc),\n                )\n                return adapter.source_name, []\n\n    tasks = [\n        _fetch_one(adapter, d)\n        for d in valid_dates\n        for adapter in adapters\n    ]\n    raw_results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    all_races: List[ResultRace] = []\n    for res in raw_results:\n        if isinstance(res, Exception):\n            _analytics_logger.warning(\"Task raised exception\", error=str(res))\n            continue\n\n        name, races = res\n        all_races.extend(races)\n\n        # Track harvest metrics\n        max_odds = max(\n            (\n                float(r.final_win_odds)\n                for race in races\n                for r in race.runners\n                if r.final_win_odds\n            ),\n            default=0.0,\n        )\n        entry = harvest_summary.setdefault(\n            name, {\"count\": 0, \"max_odds\": 0.0},\n        )\n        entry[\"count\"] += len(races)\n        entry[\"max_odds\"] = max(entry[\"max_odds\"], max_odds)\n\n    return all_races\n\n\nasync def _save_harvest_summary(\n    harvest_summary: Dict[str, Dict[str, Any]],\n    auditor: AuditorEngine,\n    region: Optional[str],\n) -> None:\n    \"\"\"Persist harvest summary to JSON file and database.\"\"\"\n    try:\n        with open(\"results_harvest.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(harvest_summary, f)\n    except OSError as exc:\n        _analytics_logger.debug(\n            \"Failed to write results_harvest.json\", error=str(exc),\n        )\n\n    if harvest_summary:\n        try:\n            await auditor.db.log_harvest(harvest_summary, region=region)\n        except Exception as exc:\n            _analytics_logger.debug(\n                \"Failed to log harvest to DB\", error=str(exc),\n            )\n\n\nasync def _write_gha_summary(\n    auditor: AuditorEngine,\n    harvest_summary: dict,\n) -> None:\n    \"\"\"Build and write the GitHub Actions Job Summary.\"\"\"\n    if \"GITHUB_STEP_SUMMARY\" not in os.environ:\n        return\n\n    try:\n        pending_tips = await auditor.db.get_unverified_tips(lookback_hours=48)\n        qualified: list[fortuna.Race] = []\n\n        for tip in pending_tips:\n            try:\n                race = fortuna.Race(\n                    id=tip[\"race_id\"],\n                    venue=tip[\"venue\"],\n                    race_number=tip[\"race_number\"],\n                    start_time=datetime.fromisoformat(\n                        tip[\"start_time\"].replace(\"Z\", \"+00:00\"),\n                    ),\n                    runners=[],\n                    source=\"Database\",\n                )\n                race.metadata = {\n                    \"is_goldmine\": bool(tip.get(\"is_goldmine\")),\n                    \"selection_number\": tip.get(\"selection_number\"),\n                    \"selection_name\": tip.get(\"selection_name\"),\n                    \"predicted_2nd_fav_odds\": tip.get(\n                        \"predicted_2nd_fav_odds\",\n                    ),\n                }\n                race.top_five_numbers = tip.get(\"top_five\")\n                qualified.append(race)\n            except (KeyError, ValueError):\n                continue\n\n        predictions_md = fortuna.format_predictions_section(qualified)\n        proof_md       = await fortuna.format_proof_section(auditor.db)\n        harvest_md     = fortuna.build_harvest_table(\n            harvest_summary, \"\ud83d\udef0\ufe0f Results Harvest Performance\",\n        )\n        artifacts_md   = fortuna.format_artifact_links()\n        fortuna.write_job_summary(\n            predictions_md, harvest_md, proof_md, artifacts_md,\n        )\n        _analytics_logger.info(\"GHA Job Summary written\")\n    except Exception as exc:\n        _analytics_logger.error(\n            \"Failed to write GHA summary\", error=str(exc),\n        )\n\n\nasync def _generate_and_save_report(\n    auditor: AuditorEngine,\n    harvest_summary: Dict[str, Any],\n    *,\n    include_lifetime_stats: bool = False,\n) -> None:\n    \"\"\"Fetch all audited data, generate report, print and persist.\"\"\"\n    all_audited = await auditor.get_all_audited_tips()\n    recent_tips = await auditor.get_recent_tips(limit=20)\n\n    report = generate_analytics_report(\n        audited_tips=all_audited,\n        recent_tips=recent_tips,\n        harvest_summary=harvest_summary,\n        include_lifetime_stats=include_lifetime_stats,\n    )\n    print(report)\n\n    try:\n        Path(\"analytics_report.txt\").write_text(report, encoding=\"utf-8\")\n        _analytics_logger.info(\"Report saved\", path=\"analytics_report.txt\")\n    except OSError as exc:\n        _analytics_logger.error(\"Failed to save report\", error=str(exc))\n\n    if all_audited:\n        _analytics_logger.info(\n            \"Analytics complete\", total_audited=len(all_audited),\n        )\n    else:\n        _analytics_logger.info(\"No audited tips found in history\")\n\n\n# -- TOP-LEVEL ORCHESTRATION --------------------------------------------------\n\nasync def run_analytics(\n    target_dates: List[str],\n    region: Optional[str] = None,\n    *,\n    include_lifetime_stats: bool = False,\n) -> None:\n    \"\"\"Main analytics entry: harvest \u2192 audit \u2192 report \u2192 GHA summary.\"\"\"\n    valid_dates = [d for d in target_dates if validate_date_format(d)]\n    if not valid_dates:\n        _analytics_logger.error(\"No valid dates\", input_dates=target_dates)\n        return\n\n    target_region = region or DEFAULT_REGION\n    _analytics_logger.info(\n        \"Starting analytics audit\",\n        dates=valid_dates,\n        region=target_region,\n    )\n\n    # Pre-populate harvest summary for regional visibility\n    if target_region == \"GLOBAL\":\n        expected = set(fortuna.USA_RESULTS_ADAPTERS) | set(fortuna.INT_RESULTS_ADAPTERS)\n    else:\n        expected = (\n            set(fortuna.USA_RESULTS_ADAPTERS)\n            if target_region == \"USA\"\n            else set(fortuna.INT_RESULTS_ADAPTERS)\n        )\n    harvest_summary: Dict[str, Dict[str, Any]] = {\n        name: {\"count\": 0, \"max_odds\": 0.0} for name in expected\n    }\n\n    async with AuditorEngine() as auditor:\n        unverified = await auditor.get_unverified_tips()\n        target_venues = None\n\n        if not unverified:\n            _analytics_logger.info(\n                \"No unverified tips found in database; fetching all results for visibility (Project Directive)\",\n            )\n        else:\n            _analytics_logger.info(\"Tips to audit\", count=len(unverified))\n            target_venues = {\n                fortuna.get_canonical_venue(t.get(\"venue\"))\n                for t in unverified\n            }\n            # Remove only the sentinel value, not a real problem (Bug #6 Fix)\n            target_venues.discard(\"unknown\")\n\n            if not target_venues:\n                _analytics_logger.warning(\n                    \"All tip venues resolved to 'unknown' \u2014 fetching everything\",\n                )\n                target_venues = None\n            else:\n                _analytics_logger.info(\"Targeting venues\", venues=sorted(target_venues))\n\n        async with managed_adapters(\n            region=region, target_venues=target_venues,\n        ) as adapters:\n            try:\n                all_results = await _harvest_results(\n                    adapters, valid_dates, harvest_summary,\n                )\n                _analytics_logger.info(\n                    \"Total results harvested\",\n                    count=len(all_results),\n                )\n                if not all_results:\n                    _analytics_logger.error(\n                        \"ZERO results harvested \u2014 audit impossible\",\n                        adapters_tried=[a.source_name for a in adapters],\n                        dates=valid_dates,\n                        region=target_region,\n                    )\n                elif not unverified:\n                    _analytics_logger.warning(\"No unverified tips to audit against results\")\n                else:\n                    matched = await auditor.audit_races(all_results, unverified=unverified)\n                    _analytics_logger.info(\n                        \"Audit complete\",\n                        results_available=len(all_results),\n                        tips_checked=len(unverified),\n                        tips_matched=len(matched),\n                        tips_still_unmatched=len(unverified) - len(matched),\n                    )\n            finally:\n                await _save_harvest_summary(\n                    harvest_summary, auditor, region,\n                )\n\n        await _generate_and_save_report(\n            auditor,\n            harvest_summary,\n            include_lifetime_stats=include_lifetime_stats,\n        )\n        await _write_gha_summary(auditor, harvest_summary)\n\n\n\n# -- CLI ENTRY POINT ----------------------------------------------------------\n\ndef _build_target_dates(\n    explicit_date: Optional[str],\n    lookback_days: int,\n) -> List[str]:\n    \"\"\"Return a list of ``YYYY-MM-DD`` date strings.\"\"\"\n    if explicit_date:\n        if not validate_date_format(explicit_date):\n            raise ValueError(\n                f\"Invalid date format '{explicit_date}'.  Use YYYY-MM-DD.\",\n            )\n        return [explicit_date]\n    now = datetime.now(EASTERN)\n    return [\n        (now - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n        for i in range(lookback_days)\n    ]\n\n\ndef main() -> None:\n    \"\"\"CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Fortuna Analytics Engine \u2014 \"\n            \"Race result auditing and performance analysis\"\n        ),\n    )\n    parser.add_argument(\n        \"--date\", type=str, help=\"Target date (YYYY-MM-DD)\",\n    )\n    parser.add_argument(\n        \"--region\",\n        type=str,\n        choices=[\"USA\", \"INT\", \"GLOBAL\"],\n        help=\"Filter results by region\",\n    )\n    parser.add_argument(\n        \"--days\",\n        type=int,\n        default=2,\n        help=\"Number of days to look back (default: 2)\",\n    )\n    parser.add_argument(\n        \"--db-path\",\n        type=str,\n        default=DEFAULT_DB_PATH,\n        help=f\"Path to tip database (default: {DEFAULT_DB_PATH})\",\n    )\n    parser.add_argument(\n        \"-v\", \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose logging\",\n    )\n    parser.add_argument(\n        \"--migrate\",\n        action=\"store_true\",\n        help=\"Migrate data from legacy JSON to SQLite\",\n    )\n    parser.add_argument(\n        \"--include-lifetime_stats\",\n        action=\"store_true\",\n        help=\"Include lifetime summary statistics in report\",\n    )\n    args = parser.parse_args()\n\n    # Set DB path before any initialization\n    if args.db_path != DEFAULT_DB_PATH:\n        os.environ[\"FORTUNA_DB_PATH\"] = args.db_path\n\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    structlog.configure(\n        wrapper_class=structlog.make_filtering_bound_logger(log_level),\n    )\n\n    # Handle migration sub-command\n    if args.migrate:\n        async def _do_migrate() -> None:\n            db = fortuna.FortunaDB(args.db_path)\n            try:\n                await db.migrate_from_json()\n                print(\"Migration complete.\")\n            except Exception as exc:\n                print(f\"Migration failed: {exc}\")\n            finally:\n                await db.close()\n\n        asyncio.run(_do_migrate())\n        return\n\n    # Build target dates\n    try:\n        target_dates = _build_target_dates(args.date, args.days)\n    except ValueError as exc:\n        print(f\"Error: {exc}\")\n        return\n\n    # Use default region if not specified\n    if not args.region:\n        args.region = DEFAULT_REGION\n        structlog.get_logger().info(\n            \"Using default region\", region=args.region,\n        )\n\n    asyncio.run(\n        run_analytics(\n            target_dates,\n            region=args.region,\n            include_lifetime_stats=args.include_lifetime_stats,\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "fortuna.py": "from __future__ import annotations\n# fortuna_discovery_engine.py\n# Aggregated monolithic discovery adapters for Fortuna\n# This engine serves as a high-reliability fallback for the Fortuna discovery system.\n\n\"\"\"\nFortuna Discovery Engine - Production-grade racing data aggregation.\n\nThis module provides a unified collection of adapters for fetching racecard data\nfrom various racing websites. It serves as a high-reliability fallback system.\n\"\"\"\nimport argparse\nimport asyncio\nimport functools\nfrom functools import lru_cache\nimport html\nimport json\nimport logging\nimport os\nimport random\nimport re\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom datetime import date, datetime, timedelta, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom io import StringIO\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Annotated,\n    Callable,\n    ClassVar,\n    Dict,\n    Final,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nimport httpx\nimport pandas as pd\nimport sqlite3\nfrom zoneinfo import ZoneInfo\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\nimport structlog\nimport subprocess\nimport sys\nimport threading\nimport webbrowser\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    WrapSerializer,\n    field_validator,\n)\nfrom selectolax.parser import HTMLParser, Node\nfrom tenacity import (\n    RetryError,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\n# --- OPTIONAL IMPORTS ---\ntry:\n    from curl_cffi import requests as curl_requests\nexcept Exception:\n    curl_requests = None\n\ntry:\n    import tomli\n    HAS_TOML = True\nexcept ImportError:\n    HAS_TOML = False\n\ntry:\n    from scrapling import AsyncFetcher, Fetcher\n    from scrapling.parser import Selector\n    ASYNC_SESSIONS_AVAILABLE = True\nexcept Exception:\n    ASYNC_SESSIONS_AVAILABLE = False\n    Selector = None  # type: ignore\n\ntry:\n    from scrapling.fetchers import AsyncDynamicSession, AsyncStealthySession\nexcept Exception:\n    ASYNC_SESSIONS_AVAILABLE = False\n\ntry:\n    from scrapling.core.custom_types import StealthMode\nexcept Exception:\n    class StealthMode:  # type: ignore\n        FAST = \"fast\"\n        CAMOUFLAGE = \"camouflage\"\n\ntry:\n    import winsound\nexcept (ImportError, RuntimeError):\n    winsound = None\n\n\ndef get_resp_status(resp: Any) -> Union[int, str]:\n    if hasattr(resp, \"status_code\"): return resp.status_code\n    return getattr(resp, \"status\", \"unknown\")\n\ndef is_frozen() -> bool:\n    \"\"\"Check if running as a frozen executable (PyInstaller, cx_Freeze, etc.)\"\"\"\n    return getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS')\n\ndef get_base_path() -> Path:\n    \"\"\"Returns the base path of the application (frozen or source).\"\"\"\n    if is_frozen():\n        return Path(sys._MEIPASS)\n    return Path(__file__).parent\n\ndef load_config() -> Dict[str, Any]:\n    \"\"\"Loads configuration from config.toml with intelligent fallback.\"\"\"\n    config = {\n        \"analysis\": {\"trustworthy_ratio_min\": 0.7, \"max_field_size\": 11},\n        \"region\": {\"default\": \"GLOBAL\"},\n        \"ui\": {\"auto_open_report\": True, \"show_status_card\": True},\n        \"logging\": {\"level\": \"INFO\", \"save_to_file\": True}\n    }\n\n    config_paths = [Path(\"config.toml\")]\n    if is_frozen():\n        config_paths.insert(0, Path(sys.executable).parent / \"config.toml\")\n        config_paths.append(Path(sys._MEIPASS) / \"config.toml\")\n\n    selected_config = None\n    for cp in config_paths:\n        if cp.exists():\n            selected_config = cp\n            break\n\n    if selected_config and HAS_TOML:\n        try:\n            with open(selected_config, \"rb\") as f:\n                toml_data = tomli.load(f)\n                # Deep merge simple dict\n                for section, values in toml_data.items():\n                    if section in config and isinstance(values, dict):\n                        config[section].update(values)\n                    else:\n                        config[section] = values\n        except Exception as e:\n            print(f\"Warning: Failed to load config.toml: {e}\")\n\n    return config\n\ndef print_status_card(config: Dict[str, Any]):\n    \"\"\"Prints a friendly status card with application health and latest metrics.\"\"\"\n    if not config.get(\"ui\", {}).get(\"show_status_card\", True):\n        return\n\n    version = \"Unknown\"\n    version_file = get_base_path() / \"VERSION\"\n    if version_file.exists():\n        version = version_file.read_text().strip()\n\n    try:\n        from rich.console import Console\n        console = Console()\n        print_func = console.print\n    except ImportError:\n        print_func = print\n\n    print_func(\"\\n\" + \"\u2550\" * 60)\n    print_func(f\" \ud83d\udc0e FORTUNA FAUCET INTELLIGENCE - v{version} \".center(60, \"\u2550\"))\n    print_func(\"\u2550\" * 60)\n\n    # Region and active mode\n    region = config.get(\"region\", {}).get(\"default\", \"GLOBAL\")\n    print_func(f\" \ud83d\udccd Region: [bold cyan]{region}[/] | \ud83d\udd0d Status: [bold green]READY[/]\")\n\n    # Database status\n    db = FortunaDB()\n    # We'll use a sync helper or just run it\n    try:\n        # Simple sqlite check\n        conn = sqlite3.connect(db.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT COUNT(*) FROM tips\")\n        total_tips = cursor.fetchone()[0]\n        cursor.execute(\"SELECT COUNT(*) FROM tips WHERE audit_completed = 1\")\n        audited = cursor.fetchone()[0]\n        cursor.execute(\"SELECT COUNT(*) FROM tips WHERE is_goldmine = 1\")\n        goldmines = cursor.fetchone()[0]\n        conn.close()\n\n        print_func(f\" \ud83d\udcca Database: {total_tips} tips | \u2705 {audited} audited | \ud83d\udc8e {goldmines} goldmines\")\n    except Exception:\n        print_func(\" \ud83d\udcca Database: INITIALIZING\")\n\n    # Odds Hygiene\n    trust_min = config.get(\"analysis\", {}).get(\"trustworthy_ratio_min\", 0.7)\n    print_func(f\" \ud83d\udee1\ufe0f  Odds Hygiene: >{int(trust_min*100)}% trust ratio required\")\n\n    # Reports\n    reports = []\n    if Path(\"summary_grid.txt\").exists(): reports.append(\"Summary\")\n    if Path(\"fortuna_report.html\").exists(): reports.append(\"HTML\")\n    if reports:\n        print_func(f\" \ud83d\udcc1 Latest Reports: {', '.join(reports)}\")\n\n    print_func(\"\u2550\" * 60 + \"\\n\")\n\ndef print_quick_help():\n    \"\"\"Prints a friendly onboarding guide for new users.\"\"\"\n    try:\n        from rich.console import Console\n        from rich.panel import Panel\n        console = Console()\n        print_func = console.print\n    except ImportError:\n        print_func = print\n\n    help_text = \"\"\"\n    [bold yellow]Welcome to Fortuna Faucet Intelligence![/]\n\n    This app helps you discover \"Goldmine\" racing opportunities where the\n    second favorite has strong odds and a significant gap from the favorite.\n\n    [bold]Common Commands:[/]\n    \u2022 [cyan]Discovery:[/]  Just run the app! It will fetch latest races and find goldmines.\n    \u2022 [cyan]Monitor:[/]    Run with [green]--monitor[/] for a live-updating dashboard.\n    \u2022 [cyan]Analytics:[/]  Run [green]fortuna_analytics.py[/] to see how past predictions performed.\n\n    [bold]Useful Flags:[/]\n    \u2022 [green]--status:[/]    See your database stats and application health.\n    \u2022 [green]--show-log:[/]  See highlights from recent fetching and auditing.\n    \u2022 [green]--region:[/]    Force a region (USA, INT, or GLOBAL).\n\n    [italic]Predictions are saved to fortuna_report.html and summary_grid.txt[/]\n    \"\"\"\n    if 'Console' in globals() or 'console' in locals():\n        print_func(Panel(help_text, title=\"\ud83d\ude80 Quick Start Guide\", border_style=\"yellow\"))\n    else:\n        print_func(help_text)\n\nasync def print_recent_logs():\n    \"\"\"Prints recent fetch and audit highlights from the database.\"\"\"\n    db = FortunaDB()\n    try:\n        # We need to use sync connection here as it's called from main which is not in loop yet\n        # Actually main_all_in_one is async and called via asyncio.run\n        conn = sqlite3.connect(db.db_path)\n        conn.row_factory = sqlite3.Row\n\n        print(\"\\n\" + \"\u2500\" * 60)\n        print(\" \ud83d\udd0d RECENT ACTIVITY LOG \".center(60, \"\u2500\"))\n        print(\"\u2500\" * 60)\n\n        # Recent Harvests\n        cursor = conn.execute(\"SELECT timestamp, adapter_name, race_count, region FROM harvest_logs ORDER BY id DESC LIMIT 5\")\n        print(\"\\n [bold]Latest Fetches:[/]\")\n        for row in cursor.fetchall():\n            ts = row['timestamp'][:16].replace('T', ' ')\n            print(f\"  \u2022 {ts} | {row['adapter_name']:<20} | {row['race_count']} races ({row['region']})\")\n\n        # Recent Audits\n        cursor = conn.execute(\"SELECT audit_timestamp, venue, race_number, verdict, net_profit FROM tips WHERE audit_completed = 1 ORDER BY audit_timestamp DESC LIMIT 5\")\n        rows = cursor.fetchall()\n        if rows:\n            print(\"\\n [bold]Latest Audits:[/]\")\n            for row in rows:\n                ts = row['audit_timestamp'][:16].replace('T', ' ')\n                emoji = \"\u2705\" if row['verdict'] == \"CASHED\" else \"\u274c\"\n                print(f\"  \u2022 {ts} | {row['venue']:<15} R{row['race_number']} | {emoji} {row['verdict']} (${row['net_profit']:+.2f})\")\n\n        conn.close()\n        print(\"\\n\" + \"\u2500\" * 60 + \"\\n\")\n    except Exception as e:\n        print(f\"Error reading activity log: {e}\")\n\ndef open_report_in_browser():\n    \"\"\"Opens the HTML report in the default system browser.\"\"\"\n    html_path = Path(\"fortuna_report.html\")\n    if html_path.exists():\n        print(f\"Opening {html_path} in your browser...\")\n        try:\n            abs_path = html_path.absolute()\n            if sys.platform == \"win32\":\n                os.startfile(abs_path)\n            else:\n                import webbrowser\n                webbrowser.open(f\"file://{abs_path}\")\n        except Exception as e:\n            print(f\"Failed to open report: {e}\")\n    else:\n        print(\"No report found. Run discovery first!\")\n\ntry:\n    from notifications import DesktopNotifier\n    HAS_NOTIFICATIONS = True\nexcept Exception:\n    HAS_NOTIFICATIONS = False\n\ntry:\n    from browserforge.headers import HeaderGenerator\n    from browserforge.fingerprints import FingerprintGenerator\n    # Smoke test: HeaderGenerator often fails if data files are missing (frozen app issue)\n    _hg = HeaderGenerator()\n    BROWSERFORGE_AVAILABLE = True\nexcept Exception:\n    BROWSERFORGE_AVAILABLE = False\n\n\n# --- TYPE VARIABLES ---\nT = TypeVar(\"T\")\nRaceT = TypeVar(\"RaceT\", bound=\"Race\")\n\n# --- CONSTANTS ---\nEASTERN = ZoneInfo(\"America/New_York\")\nDEFAULT_REGION: Final[str] = \"GLOBAL\"\n\n# Region-based adapter lists (Refined by Council of Superbrains Directive)\n# Single-continent adapters remain in USA/INT jobs.\n# Multi-continental adapters move to the GLOBAL parallel fetch job.\n# AtTheRaces is duplicated into USA as per explicit request.\nUSA_DISCOVERY_ADAPTERS: Final[set] = {\"Equibase\", \"TwinSpires\", \"RacingPostB2B\", \"StandardbredCanada\", \"AtTheRaces\"}\nINT_DISCOVERY_ADAPTERS: Final[set] = {\"TAB\", \"BetfairDataScientist\"}\nGLOBAL_DISCOVERY_ADAPTERS: Final[set] = {\n    \"SkyRacingWorld\", \"AtTheRaces\", \"AtTheRacesGreyhound\", \"RacingPost\",\n    \"Oddschecker\", \"Timeform\", \"BoyleSports\", \"SportingLife\", \"SkySports\",\n    \"RacingAndSports\"\n}\n\nUSA_RESULTS_ADAPTERS: Final[set] = {\"EquibaseResults\", \"SportingLifeResults\"}\nINT_RESULTS_ADAPTERS: Final[set] = {\n    \"RacingPostResults\", \"RacingPostTote\", \"AtTheRacesResults\",\n    \"SportingLifeResults\", \"SkySportsResults\"\n}\n\nMAX_VALID_ODDS: Final[float] = 1000.0\nMIN_VALID_ODDS: Final[float] = 1.01\nDEFAULT_ODDS_FALLBACK: Final[float] = 2.75\nCOMMON_PLACEHOLDERS: Final[set] = {2.75}\nDEFAULT_CONCURRENT_REQUESTS: Final[int] = 5\nDEFAULT_REQUEST_TIMEOUT: Final[int] = 30\n\nDEFAULT_BROWSER_HEADERS: Final[Dict[str, str]] = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Connection\": \"keep-alive\",\n    \"Pragma\": \"no-cache\",\n    \"Sec-Fetch-Dest\": \"document\",\n    \"Sec-Fetch-Mode\": \"navigate\",\n    \"Sec-Fetch-Site\": \"none\",\n    \"Sec-Fetch-User\": \"?1\",\n    \"Upgrade-Insecure-Requests\": \"1\",\n}\n\nCHROME_USER_AGENT: Final[str] = (\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n)\n\nCHROME_SEC_CH_UA: Final[str] = (\n    '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"'\n)\n\n# Bet type keywords mapping (lowercase key -> display name)\nBET_TYPE_KEYWORDS: Final[Dict[str, str]] = {\n    \"superfecta\": \"Superfecta\",\n    \"spr\": \"Superfecta\",\n    \"trifecta\": \"Trifecta\",\n    \"tri\": \"Trifecta\",\n    \"exacta\": \"Exacta\",\n    \"ex\": \"Exacta\",\n    \"quinella\": \"Quinella\",\n    \"qn\": \"Quinella\",\n    \"daily double\": \"Daily Double\",\n    \"dbl\": \"Daily Double\",\n    \"pick 3\": \"Pick 3\",\n    \"pick 4\": \"Pick 4\",\n    \"pick 5\": \"Pick 5\",\n    \"pick 6\": \"Pick 6\",\n    \"first 4\": \"Superfecta\",\n    \"forecast\": \"Exacta\",\n    \"tricast\": \"Trifecta\",\n}\n\n# Discipline detection keywords\nDISCIPLINE_KEYWORDS: Final[Dict[str, List[str]]] = {\n    \"Harness\": [\"harness\", \"trotter\", \"pacer\", \"standardbred\", \"trot\", \"pace\"],\n    \"Greyhound\": [\"greyhound\", \"dog\", \"dogs\"],\n    \"Quarter Horse\": [\"quarter horse\", \"quarterhorse\"],\n}\n\n\n# --- EXCEPTIONS ---\nclass FortunaException(Exception):\n    \"\"\"Base exception for all Fortuna-related errors.\"\"\"\n    pass\n\n\nclass ErrorCategory(Enum):\n    \"\"\"Categories for classifying adapter errors.\"\"\"\n    BOT_DETECTION = \"bot_detection\"\n    NETWORK = \"network\"\n    STRUCTURE_CHANGE = \"structure_change\"\n    TIMEOUT = \"timeout\"\n    AUTHENTICATION = \"authentication\"\n    CONFIGURATION = \"configuration\"\n    PARSING = \"parsing\"\n    UNKNOWN = \"unknown\"\n\n\nclass AdapterError(FortunaException):\n    \"\"\"Base error for adapter-specific issues.\"\"\"\n    def __init__(self, adapter_name: str, message: str, category: ErrorCategory = ErrorCategory.UNKNOWN):\n        self.adapter_name = adapter_name\n        self.category = category\n        super().__init__(f\"[{adapter_name}] {message}\")\n\n\nclass AdapterRequestError(AdapterError):\n    def __init__(self, adapter_name: str, message: str):\n        super().__init__(adapter_name, message, ErrorCategory.NETWORK)\n\n\nclass AdapterHttpError(AdapterRequestError):\n    def __init__(self, adapter_name: str, status_code: int, url: str):\n        self.status_code = status_code\n        self.url = url\n        super().__init__(adapter_name, f\"Received HTTP {status_code} from {url}\")\n\n\nclass AdapterParsingError(AdapterError):\n    def __init__(self, adapter_name: str, message: str):\n        super().__init__(adapter_name, message, ErrorCategory.PARSING)\n\n\nclass FetchError(Exception):\n    def __init__(self, message: str, response: Optional[Any] = None, category: ErrorCategory = ErrorCategory.UNKNOWN):\n        super().__init__(message)\n        self.response = response\n        self.category = category\n\n\n# --- MODELS ---\ndef decimal_serializer(value: Any, handler: Callable[[Any], Any]) -> Any:\n    if value is None: return None\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return handler(value)\n\n\nJsonDecimal = Annotated[Any, WrapSerializer(decimal_serializer, when_used=\"json\")]\n\n\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n        str_strip_whitespace=True,\n    )\n\n\nclass OddsData(FortunaBaseModel):\n    win: Optional[JsonDecimal] = None\n    place: Optional[JsonDecimal] = None\n    source: str\n    last_updated: datetime = Field(default_factory=lambda: datetime.now(EASTERN))\n\n    @field_validator(\"last_updated\", mode=\"after\")\n    @classmethod\n    def validate_eastern(cls, v: datetime) -> datetime:\n        return ensure_eastern(v)\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = Field(default_factory=dict)\n    win_odds: Optional[float] = Field(None, alias=\"winOdds\")\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\n    @field_validator(\"name\", mode=\"before\")\n    @classmethod\n    def clean_name(cls, v: Any) -> str:\n        if not v:\n            return \"Unknown\"\n        name = str(v).strip()\n        # Handle non-breaking spaces\n        name = name.replace('\\xa0', ' ')\n        # Remove country suffixes in parentheses, e.g., \"Jay Bee (IRE)\" -> \"Jay Bee\"\n        name = re.sub(r\"\\s*\\([^)]*\\)\\s*$\", \"\", name)\n        # Remove leading numbers followed by a dot and space, e.g., \"1. Horse\" -> \"Horse\"\n        name = re.sub(r\"^\\d+\\.\\s*\", \"\", name)\n        # Remove unwanted punctuation/marks that might break parsing or Excel\n        # Keep letters, numbers, spaces, hyphens, and apostrophes.\n        name = re.sub(r\"[^a-zA-Z0-9\\s\\-\\'\\\\\\\"]\", \"\", name)\n        # Collapse multiple spaces\n        name = re.sub(r\"\\s+\", \" \", name)\n        return name.strip() or \"Unknown\"\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\", ge=1, le=100)\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner] = Field(default_factory=list)\n\n    @field_validator(\"start_time\", mode=\"after\")\n    @classmethod\n    def validate_eastern(cls, v: datetime) -> datetime:\n        \"\"\"Ensures all race start times are in US Eastern Time.\"\"\"\n        return ensure_eastern(v)\n    source: str\n    discipline: Optional[str] = None\n    distance: Optional[str] = None\n    field_size: Optional[int] = None\n    available_bets: List[str] = Field(default_factory=list, alias=\"availableBets\")\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    qualification_score: Optional[float] = None\n    is_error_placeholder: bool = False\n    top_five_numbers: Optional[str] = None\n    error_message: Optional[str] = None\n\n# --- UTILITIES ---\ndef get_field(obj: Any, field_name: str, default: Any = None) -> Any:\n    \"\"\"Helper to get a field from either an object or a dictionary.\"\"\"\n    if isinstance(obj, dict):\n        return obj.get(field_name, default)\n    return getattr(obj, field_name, default)\n\n\ndef clean_text(text: Any) -> str:\n    \"\"\"Strips leading/trailing whitespace and collapses internal whitespace.\"\"\"\n    if not text:\n        return \"\"\n    return \" \".join(str(text).strip().split())\n\n\ndef node_text(n: Any) -> str:\n    \"\"\"Consistently extracts text from Scrapling Selectors and Selectolax Nodes.\"\"\"\n    if n is None:\n        return \"\"\n    # Selectolax nodes have a .text() method, Scrapling Selectors have a .text property\n    txt = getattr(n, \"text\", None)\n    if txt is None:\n        return \"\"\n    return txt().strip() if callable(txt) else str(txt).strip()\n\n\n@lru_cache(maxsize=1024)\ndef get_canonical_venue(name: Optional[str]) -> str:\n    \"\"\"Returns a sanitized canonical form for deduplication keys.\"\"\"\n    if not name:\n        return \"unknown\"\n    # Call normalization first to strip race titles and ads\n    norm = normalize_venue_name(name)\n    # Remove everything in parentheses (extra safety)\n    norm = re.sub(r\"[\\(\\[\uff08].*?[\\)\\]\uff09]\", \"\", norm)\n    # Remove special characters, lowercase, strip\n    res = re.sub(r\"[^a-z0-9]\", \"\", norm.lower())\n    return res or \"unknown\"\n\n\ndef now_eastern() -> datetime:\n    \"\"\"Returns the current time in US Eastern Time.\"\"\"\n    return datetime.now(EASTERN)\n\n\n\n\ndef to_eastern(dt: datetime) -> datetime:\n    \"\"\"Converts a datetime object to US Eastern Time.\"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=EASTERN)\n    return dt.astimezone(EASTERN)\n\n\ndef ensure_eastern(dt: datetime) -> datetime:\n    \"\"\"Ensures datetime is timezone-aware and in Eastern time. More strict than to_eastern.\"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=EASTERN)\n    if dt.tzinfo is not EASTERN:\n        try:\n            return dt.astimezone(EASTERN)\n        except Exception:\n            # Fallback for rare cases where conversion fails (e.g. invalid times during DST transitions)\n            return dt.replace(tzinfo=EASTERN)\n    return dt\n\n\nRACING_KEYWORDS = [\n\"PRIX\", \"CHASE\", \"HURDLE\", \"HANDICAP\", \"STAKES\", \"CUP\", \"LISTED\", \"GBB\",\n\"RACE\", \"MEETING\", \"NOVICE\", \"TRIAL\", \"PLATE\", \"TROPHY\", \"CHAMPIONSHIP\",\n\"JOCKEY\", \"TRAINER\", \"BEST ODDS\", \"GUARANTEED\", \"PRO/AM\", \"AUCTION\",\n\"HUNT\", \"MARES\", \"FILLIES\", \"COLTS\", \"GELDINGS\", \"JUVENILE\", \"SELLING\",\n\"CLAIMING\", \"OPTIONAL\", \"ALLOWANCE\", \"MAIDEN\", \"OPEN\", \"INVITATIONAL\",\n\"CLASS \", \"GRADE \", \"GROUP \", \"DERBY\", \"OAKS\", \"GUINEAS\", \"ELIE DE\",\n\"FREDERIK\", \"CONNOLLY'S\", \"QUINNBET\", \"RED MILLS\", \"IRISH EBF\", \"SKY BET\",\n\"CORAL\", \"BETFRED\", \"WILLIAM HILL\", \"UNIBET\", \"PADDY POWER\", \"BETFAIR\",\n\"GET THE BEST\", \"CHELTENHAM TRIALS\", \"PORSCHE\", \"IMPORTED\", \"IMPORTE\", \"THE JOC\",\n\"PREMIO\", \"GRANDE\", \"CLASSIC\", \"SPRINT\", \"DASH\", \"MILE\", \"STAYERS\",\n\"BOWL\", \"MEMORIAL\", \"PURSE\", \"CONDITION\", \"NIGHT\", \"EVENING\", \"DAY\",\n\"4RACING\", \"WILGERBOSDRIFT\", \"YOUCANBETONUS\", \"FOR HOSPITALITY\", \"SA \", \"TAB \",\n\"DE \", \"DU \", \"DES \", \"LA \", \"LE \", \"AU \", \"WELCOME\", \"BET \", \"WITH \", \"AND \",\n\"NEXT\", \"WWW\", \"GAMBLE\", \"BETMGM\", \"TV\", \"ONLINE\", \"LUCKY\", \"RACEWAY\",\n\"SPEEDWAY\", \"DOWNS\", \"PARK\", \"HARNESS\", \" STANDARDBRED\", \"FORM GUIDE\", \"FULL FIELDS\"\n]\n\n\nVENUE_MAP = {\n\"ABU DHABI\": \"Abu Dhabi\",\n\"AQU\": \"Aqueduct\",\n\"AQUEDUCT\": \"Aqueduct\",\n\"ARGENTAN\": \"Argentan\",\n\"ASCOT\": \"Ascot\",\n\"AYR\": \"Ayr\",\n\"BAHRAIN\": \"Bahrain\",\n\"BANGOR ON DEE\": \"Bangor-on-Dee\",\n\"CATTERICK\": \"Catterick\",\n\"CATTERICK BRIDGE\": \"Catterick\",\n\"CT\": \"Charles Town\",\n\"CENTRAL PARK\": \"Central Park\",\n\"CHELMSFORD\": \"Chelmsford\",\n\"CHELMSFORD CITY\": \"Chelmsford\",\n\"CURRAGH\": \"Curragh\",\n\"DEAUVILLE\": \"Deauville\",\n\"DED\": \"Delta Downs\",\n\"DELTA DOWNS\": \"Delta Downs\",\n\"DONCASTER\": \"Doncaster\",\n\"DOVER DOWNS\": \"Dover Downs\",\n\"DOWN ROYAL\": \"Down Royal\",\n\"DUNDALK\": \"Dundalk\",\n\"DUNSTALL PARK\": \"Wolverhampton\",\n\"EPSOM\": \"Epsom\",\n\"EPSOM DOWNS\": \"Epsom\",\n\"FG\": \"Fair Grounds\",\n\"FAIR GROUNDS\": \"Fair Grounds\",\n\"FONTWELL\": \"Fontwell Park\",\n\"FONTWELL PARK\": \"Fontwell Park\",\n\"GREAT YARMOUTH\": \"Great Yarmouth\",\n\"GP\": \"Gulfstream Park\",\n\"GULFSTREAM\": \"Gulfstream Park\",\n\"GULFSTREAM PARK\": \"Gulfstream Park\",\n\"HAYDOCK\": \"Haydock Park\",\n\"HAYDOCK PARK\": \"Haydock Park\",\n\"HOOSIER PARK\": \"Hoosier Park\",\n\"HOVE\": \"Hove\",\n\"KEMPTON\": \"Kempton Park\",\n\"KEMPTON PARK\": \"Kempton Park\",\n\"LRL\": \"Laurel Park\",\n\"LAUREL PARK\": \"Laurel Park\",\n\"LINGFIELD\": \"Lingfield Park\",\n\"LINGFIELD PARK\": \"Lingfield Park\",\n\"LOS ALAMITOS\": \"Los Alamitos\",\n\"MARONAS\": \"Maronas\",\n\"MEADOWLANDS\": \"Meadowlands\",\n\"MEYDAN\": \"Meydan\",\n\"MIAMI VALLEY\": \"Miami Valley\",\n\"MIAMI VALLEY RACEWAY\": \"Miami Valley\",\n\"MVR\": \"Mahoning Valley\",\n\"MOHAWK\": \"Mohawk\",\n\"MOHAWK PARK\": \"Mohawk\",\n\"MUSSELBURGH\": \"Musselburgh\",\n\"NAAS\": \"Naas\",\n\"NEWCASTLE\": \"Newcastle\",\n\"NEWMARKET\": \"Newmarket\",\n\"NORTHFIELD PARK\": \"Northfield Park\",\n\"OXFORD\": \"Oxford\",\n\"PAU\": \"Pau\",\n\"OP\": \"Oaklawn Park\",\n\"PEN\": \"Penn National\",\n\"POCONO DOWNS\": \"Pocono Downs\",\n\"SAM HOUSTON\": \"Sam Houston\",\n\"SAM HOUSTON RACE PARK\": \"Sam Houston\",\n\"SANDOWN\": \"Sandown Park\",\n\"SANDOWN PARK\": \"Sandown Park\",\n\"SA\": \"Santa Anita\",\n\"SANTA ANITA\": \"Santa Anita\",\n\"SARATOGA\": \"Saratoga\",\n\"SARATOGA HARNESS\": \"Saratoga Harness\",\n\"SCIOTO DOWNS\": \"Scioto Downs\",\n\"SHEFFIELD\": \"Sheffield\",\n\"STRATFORD\": \"Stratford-on-Avon\",\n\"SUN\": \"Sunland Park\",\n\"SUNLAND PARK\": \"Sunland Park\",\n\"TAM\": \"Tampa Bay Downs\",\n\"TAMPA BAY DOWNS\": \"Tampa Bay Downs\",\n\"THURLES\": \"Thurles\",\n\"TP\": \"Turfway Park\",\n\"TUP\": \"Turf Paradise\",\n\"TURF PARADISE\": \"Turf Paradise\",\n\"TURFFONTEIN\": \"Turffontein\",\n\"UTTOXETER\": \"Uttoxeter\",\n\"VINCENNES\": \"Vincennes\",\n\"WARWICK\": \"Warwick\",\n\"WETHERBY\": \"Wetherby\",\n\"WOLVERHAMPTON\": \"Wolverhampton\",\n\"WO\": \"Woodbine\",\n\"WOODBINE\": \"Woodbine\",\n\"WOODBINE MOHAWK\": \"Mohawk\",\n\"WOODBINE MOHAWK PARK\": \"Mohawk\",\n\"YARMOUTH\": \"Great Yarmouth\",\n\"YONKERS\": \"Yonkers\",\n\"YONKERS RACEWAY\": \"Yonkers\",\n}\n\n\ndef normalize_venue_name(name: Optional[str]) -> str:\n    \"\"\"\n    Normalizes a racecourse name to a standard format.\n    Aggressively strips race names, sponsorships, and country noise.\n    \"\"\"\n    if not name:\n        return \"Unknown\"\n\n    # 1. Initial Cleaning: Replace dashes and strip all parenthetical info\n    # Handle full-width parentheses and brackets often found in international data\n    name = str(name).replace(\"-\", \" \")\n    name = re.sub(r\"[\\(\\[\uff08].*?[\\)\\]\uff09]\", \" \", name)\n\n    cleaned = clean_text(name)\n    if not cleaned:\n        return \"Unknown\"\n\n    # 2. Aggressive Race/Meeting Name Stripping\n    # If these keywords are found, assume everything after is the race name.\n\n    upper_name = cleaned.upper()\n    earliest_idx = len(cleaned)\n    for kw in RACING_KEYWORDS:\n        # Check for keyword with leading space\n        idx = upper_name.find(\" \" + kw)\n        if idx != -1:\n            earliest_idx = min(earliest_idx, idx)\n\n    track_part = cleaned[:earliest_idx].strip()\n    if not track_part:\n        track_part = cleaned\n\n    # Handle repetition check (e.g., \"Bahrain Bahrain\" -> \"Bahrain\")\n    words = track_part.split()\n    if len(words) > 1 and words[0].lower() == words[1].lower():\n        track_part = words[0]\n\n    upper_track = track_part.upper()\n\n    # 3. High-Confidence Mapping\n    # Map raw/cleaned names to canonical display names.\n\n    # Direct match\n    if upper_track in VENUE_MAP:\n        return VENUE_MAP[upper_track]\n\n    # Prefix match (sort by length desc to avoid partial matches on shorter names)\n    for known_track in sorted(VENUE_MAP.keys(), key=len, reverse=True):\n        if upper_name.startswith(known_track):\n            return VENUE_MAP[known_track]\n\n    return track_part.title()\n\n\ndef parse_odds_to_decimal(odds_str: Any) -> Optional[float]:\n    \"\"\"\n    Parses various odds formats (fractional, decimal) into a float decimal.\n    Uses advanced heuristics to extract odds from noisy strings.\n    \"\"\"\n    if odds_str is None: return None\n    s = str(odds_str).strip().upper()\n\n    # Remove common non-odds noise and currency symbols\n    s = re.sub(r\"[$\\s\\xa0]\", \"\", s)\n    s = re.sub(r\"(ML|MTP|AM|PM|LINE|ODDS|PRICE)[:=]*\", \"\", s)\n\n    if s in (\"EVN\", \"EVEN\", \"EVS\", \"EVENS\"): return 2.0\n    if any(kw in s for kw in (\"SCR\", \"SCRATCHED\", \"N/A\", \"NR\", \"VOID\")): return None\n\n    try:\n        # 1. Fractional Format: \"7/4\", \"7-4\", \"7 TO 4\"\n        groups = re.search(r\"(\\d+)\\s*(?:[/\\-]|TO)\\s*(\\d+)\", s)\n        if groups:\n            num, den = int(groups.group(1)), int(groups.group(2))\n            if den > 0: return round((num / den) + 1.0, 2)\n\n        # 2. Decimal Format: \"5.00\", \"10.5\"\n        decimal_match = re.search(r\"(\\d+\\.\\d+)\", s)\n        if decimal_match:\n            value = float(decimal_match.group(1))\n            if MIN_VALID_ODDS <= value < MAX_VALID_ODDS: return round(value, 2)\n\n        # 3. Simple Integer as fractional odds (e.g., \"5\" often means \"5/1\")\n        # Only apply if it's a likely odds value (not saddle cloth 1-20)\n        int_match = re.match(r\"^(\\d+)$\", s)\n        if int_match:\n            val = int(int_match.group(1))\n            # Heuristic: only treat as fractional odds if it's in a likely range (1-50)\n            # to avoid misinterpreting horse numbers or race numbers.\n            if 1 <= val <= 50:\n                return float(val + 1)\n\n    except Exception: pass\n    return None\n\n\ndef is_placeholder_odds(value: Optional[Union[float, Decimal]]) -> bool:\n    \"\"\"Detects if odds value is a known placeholder or default.\"\"\"\n    if value is None:\n        return True\n    try:\n        val_float = round(float(value), 2)\n        return val_float in COMMON_PLACEHOLDERS\n    except (ValueError, TypeError):\n        return True\n\n\ndef is_valid_odds(odds: Any) -> bool:\n    if odds is None: return False\n    try:\n        odds_float = float(odds)\n        if not (MIN_VALID_ODDS <= odds_float < MAX_VALID_ODDS):\n            return False\n        return not is_placeholder_odds(odds_float)\n    except Exception: return False\n\n\ndef create_odds_data(source_name: str, win_odds: Any, place_odds: Any = None) -> Optional[OddsData]:\n    if not is_valid_odds(win_odds):\n        if win_odds is not None and is_placeholder_odds(win_odds):\n            structlog.get_logger().warning(\"placeholder_odds_detected\", source=source_name, odds=win_odds)\n        return None\n    return OddsData(win=float(win_odds), place=float(place_odds) if is_valid_odds(place_odds) else None, source=source_name)\n\n\ndef scrape_available_bets(html_content: str) -> List[str]:\n    if not html_content: return []\n    available_bets: List[str] = []\n    html_lower = html_content.lower()\n    for kw, bet_name in BET_TYPE_KEYWORDS.items():\n        if re.search(rf\"\\b{re.escape(kw)}\\b\", html_lower) and bet_name not in available_bets:\n            available_bets.append(bet_name)\n    return available_bets\n\n\ndef detect_discipline(html_content: str) -> str:\n    if not html_content: return \"Thoroughbred\"\n    html_lower = html_content.lower()\n    for disc, keywords in DISCIPLINE_KEYWORDS.items():\n        if any(kw in html_lower for kw in keywords): return disc\n    return \"Thoroughbred\"\n\n\nclass SmartOddsExtractor:\n    \"\"\"\n    Advanced heuristics for extracting odds from noisy HTML or text.\n    Scans for various patterns and returns the first plausible odds found.\n    \"\"\"\n    @staticmethod\n    def extract_from_text(text: str) -> Optional[float]:\n        if not text: return None\n        # Try to find common odds patterns in the text\n        # 1. Decimal odds (e.g. 5.00, 10.5)\n        decimals = re.findall(r\"(\\d+\\.\\d+)\", text)\n        for d in decimals:\n            val = float(d)\n            if MIN_VALID_ODDS <= val < MAX_VALID_ODDS: return round(val, 2)\n\n        # 2. Fractional odds (e.g. 7/4, 10-1)\n        fractions = re.findall(r\"(\\d+)\\s*[/\\-]\\s*(\\d+)\", text)\n        for num, den in fractions:\n            n, d = int(num), int(den)\n            if d > 0 and (n/d) > 0.1: return round((n / d) + 1.0, 2)\n\n        return None\n\n    @staticmethod\n    def extract_from_node(node: Any) -> Optional[float]:\n        \"\"\"Scans a selectolax node for odds using multiple strategies.\"\"\"\n        # Strategy 1: Look at text content of the entire node\n        if hasattr(node, 'text'):\n            if val := SmartOddsExtractor.extract_from_text(node.text()):\n                return val\n\n        # Strategy 2: Look at attributes\n        if hasattr(node, 'attributes'):\n            for attr in [\"data-odds\", \"data-price\", \"data-bestprice\", \"title\"]:\n                if val_str := node.attributes.get(attr):\n                    if val := parse_odds_to_decimal(val_str):\n                        return val\n\n        return None\n\n\ndef generate_race_id(prefix: str, venue: str, start_time: datetime, race_number: int, discipline: Optional[str] = None) -> str:\n    venue_slug = re.sub(r\"[^a-z0-9]\", \"\", venue.lower())\n    date_str = start_time.strftime(\"%Y%m%d\")\n    time_str = start_time.strftime(\"%H%M\")\n\n    # Always include a discipline suffix for consistency and better matching\n    dl = (discipline or \"Thoroughbred\").lower()\n    if \"harness\" in dl: disc_suffix = \"_h\"\n    elif \"greyhound\" in dl: disc_suffix = \"_g\"\n    elif \"quarter\" in dl: disc_suffix = \"_q\"\n    else: disc_suffix = \"_t\"\n\n    return f\"{prefix}_{venue_slug}_{date_str}_{time_str}_R{race_number}{disc_suffix}\"\n\n\n# --- VALIDATORS ---\nclass RaceValidator(BaseModel):\n    venue: str = Field(..., min_length=1)\n    race_number: int = Field(..., ge=1, le=100)\n    start_time: datetime\n    runners: List[Runner] = Field(..., min_length=2)\n\n\nclass DataValidationPipeline:\n    @staticmethod\n    def validate_raw_response(adapter_name: str, raw_data: Any) -> tuple[bool, str]:\n        if raw_data is None: return False, \"Null response\"\n        return True, \"OK\"\n    @staticmethod\n    def validate_parsed_races(races: List[Race], adapter_name: str = \"Unknown\") -> tuple[List[Race], List[str]]:\n        valid_races: List[Race] = []\n        warnings: List[str] = []\n        for i, race in enumerate(races):\n            try:\n                data = race.model_dump() if hasattr(race, \"model_dump\") else race.dict()\n                RaceValidator(**data)\n                valid_races.append(race)\n            except Exception as e:\n                err_msg = f\"[{adapter_name}] Race {i} ({getattr(race, 'venue', 'Unknown')} R{getattr(race, 'race_number', '?')}) validation failed: {str(e)}\"\n                warnings.append(err_msg)\n                structlog.get_logger().error(\"race_validation_failed\", adapter=adapter_name, error=str(e), race_index=i, venue=getattr(race, 'venue', 'Unknown'))\n                continue\n        return valid_races, warnings\n\n\n# --- CORE INFRASTRUCTURE ---\nclass GlobalResourceManager:\n    \"\"\"Manages shared resources like HTTP clients and semaphores.\"\"\"\n    _httpx_client: Optional[httpx.AsyncClient] = None\n    _locks: ClassVar[dict[asyncio.AbstractEventLoop, asyncio.Lock]] = {}\n    _lock_initialized: ClassVar[threading.Lock] = threading.Lock()\n    _global_semaphore: Optional[asyncio.Semaphore] = None\n\n    @classmethod\n    async def _get_lock(cls) -> asyncio.Lock:\n        loop = asyncio.get_running_loop()\n        if loop not in cls._locks:\n            with cls._lock_initialized:\n                if loop not in cls._locks:\n                    cls._locks[loop] = asyncio.Lock()\n        return cls._locks[loop]\n\n    @classmethod\n    async def get_httpx_client(cls, timeout: Optional[int] = None) -> httpx.AsyncClient:\n        \"\"\"\n        Returns a shared httpx client.\n        If timeout is provided and differs from current client, the client is recreated.\n        \"\"\"\n        lock = await cls._get_lock()\n        async with lock:\n            if cls._httpx_client is not None:\n                if timeout is not None and abs(cls._httpx_client.timeout.read - timeout) > 0.001:\n                    try:\n                        await cls._httpx_client.aclose()\n                    except Exception:\n                        pass\n                    cls._httpx_client = None\n\n            if cls._httpx_client is None:\n                use_timeout = timeout or DEFAULT_REQUEST_TIMEOUT\n                cls._httpx_client = httpx.AsyncClient(\n                    follow_redirects=True,\n                    timeout=httpx.Timeout(use_timeout),\n                    headers={**DEFAULT_BROWSER_HEADERS, \"User-Agent\": CHROME_USER_AGENT},\n                    limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)\n                )\n        return cls._httpx_client\n\n    @classmethod\n    def get_global_semaphore(cls) -> asyncio.Semaphore:\n        if cls._global_semaphore is None:\n            try:\n                # Attempt to get running loop to ensure we are in async context\n                asyncio.get_running_loop()\n                cls._global_semaphore = asyncio.Semaphore(DEFAULT_CONCURRENT_REQUESTS * 2)\n            except RuntimeError:\n                # Fallback if called outside a loop\n                cls._global_semaphore = asyncio.Semaphore(DEFAULT_CONCURRENT_REQUESTS * 2)\n                return cls._global_semaphore\n        return cls._global_semaphore\n\n    @classmethod\n    async def cleanup(cls):\n        if cls._httpx_client:\n            await cls._httpx_client.aclose()\n            cls._httpx_client = None\n\n\nclass BrowserEngine(Enum):\n    CAMOUFOX = \"camoufox\"\n    PLAYWRIGHT = \"playwright\"\n    CURL_CFFI = \"curl_cffi\"\n    PLAYWRIGHT_LEGACY = \"playwright_legacy\"\n    HTTPX = \"httpx\"\n\n\n@dataclass\nclass UnifiedResponse:\n    \"\"\"Unified response object to normalize data across different fetch engines.\"\"\"\n    text: str\n    status: int\n    status_code: int\n    url: str\n    headers: Dict[str, str] = field(default_factory=dict)\n\n    def json(self) -> Any:\n        return json.loads(self.text)\n\n\nclass FetchStrategy(FortunaBaseModel):\n    primary_engine: BrowserEngine = BrowserEngine.PLAYWRIGHT\n    enable_js: bool = True\n    stealth_mode: str = \"fast\"\n    block_resources: bool = True\n    max_retries: int = Field(3, ge=0, le=10)\n    timeout: int = Field(DEFAULT_REQUEST_TIMEOUT, ge=1, le=300)\n    page_load_strategy: str = \"domcontentloaded\"\n    wait_until: Optional[str] = None\n    network_idle: bool = False\n    wait_for_selector: Optional[str] = None\n\n\nclass SmartFetcher:\n    BOT_DETECTION_KEYWORDS: ClassVar[List[str]] = [\"datadome\", \"perimeterx\", \"access denied\", \"captcha\", \"cloudflare\", \"please verify\"]\n    def __init__(self, strategy: Optional[FetchStrategy] = None):\n        self.strategy = strategy or FetchStrategy()\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self._engine_health = {\n            BrowserEngine.CAMOUFOX: 0.9,\n            BrowserEngine.CURL_CFFI: 0.8,\n            BrowserEngine.PLAYWRIGHT: 0.7,\n            BrowserEngine.PLAYWRIGHT_LEGACY: 0.6,\n            BrowserEngine.HTTPX: 0.5\n        }\n        self.last_engine: str = \"unknown\"\n        if BROWSERFORGE_AVAILABLE:\n            self.header_gen = HeaderGenerator()\n            self.fingerprint_gen = FingerprintGenerator()\n        else:\n            self.header_gen = None\n            self.fingerprint_gen = None\n\n    async def fetch(self, url: str, **kwargs: Any) -> Any:\n        method = kwargs.pop(\"method\", \"GET\").upper()\n        kwargs.pop(\"url\", None)\n        # Check if engines are available before sorting\n        available_engines = [e for e in self._engine_health.keys()]\n        if not curl_requests and BrowserEngine.CURL_CFFI in available_engines:\n            available_engines.remove(BrowserEngine.CURL_CFFI)\n        if not ASYNC_SESSIONS_AVAILABLE:\n            for e in [BrowserEngine.CAMOUFOX, BrowserEngine.PLAYWRIGHT]:\n                if e in available_engines: available_engines.remove(e)\n\n        if not available_engines:\n            self.logger.error(\"no_fetch_engines_available\", url=url)\n            raise FetchError(\"No fetch engines available (install curl_cffi or scrapling)\")\n\n        strategy = kwargs.get(\"strategy\", self.strategy)\n        engines = sorted(available_engines, key=lambda e: self._engine_health[e], reverse=True)\n        if strategy.primary_engine in engines:\n            engines.remove(strategy.primary_engine)\n            engines.insert(0, strategy.primary_engine)\n        self.logger.debug(\"Fetch engines ordered\", url=url, engines=[e.value for e in engines], primary=strategy.primary_engine.value)\n        last_error: Optional[Exception] = None\n        for engine in engines:\n            try:\n                response = await self._fetch_with_engine(engine, url, method=method, **kwargs)\n                self._engine_health[engine] = min(1.0, self._engine_health[engine] + 0.1)\n                self.last_engine = engine.value\n                return response\n            except Exception as e:\n                self.logger.debug(f\"Engine {engine.value} failed\", error=str(e))\n                self._engine_health[engine] = max(0.0, self._engine_health[engine] - 0.2)\n                last_error = e\n                continue\n        err_msg = repr(last_error) if last_error else \"All fetch engines failed\"\n        self.logger.error(\"all_engines_failed\", url=url, error=err_msg)\n        raise last_error or FetchError(\"All fetch engines failed\")\n\n    \n    async def _fetch_with_engine(self, engine: BrowserEngine, url: str, method: str, **kwargs: Any) -> Any:\n        # Generate browserforge headers if available\n        if BROWSERFORGE_AVAILABLE:\n            try:\n                # Generate headers and a corresponding user agent\n                fingerprint = self.fingerprint_gen.generate()\n                bf_headers = self.header_gen.generate()\n                # Ensure User-Agent is consistent between fingerprint and headers\n                ua = getattr(fingerprint.navigator, 'userAgent', getattr(fingerprint.navigator, 'user_agent', CHROME_USER_AGENT))\n                bf_headers['User-Agent'] = ua\n\n                if \"headers\" in kwargs:\n                    # Merge - browserforge headers complement provided ones\n                    for k, v in bf_headers.items():\n                        if k not in kwargs[\"headers\"]:\n                            kwargs[\"headers\"][k] = v\n                else:\n                    kwargs[\"headers\"] = bf_headers\n                self.logger.debug(\"Applied browserforge headers\", engine=engine.value)\n            except Exception as e:\n                self.logger.warning(\"Failed to generate browserforge headers\", error=str(e))\n\n        # Define browser-specific arguments to strip for non-browser engines\n        BROWSER_SPECIFIC_KWARGS = [\n            \"network_idle\", \"wait_selector\", \"wait_until\", \"impersonate\",\n            \"stealth\", \"block_resources\", \"wait_for_selector\", \"stealth_mode\",\n            \"strategy\"\n        ]\n\n        strategy = kwargs.get(\"strategy\", self.strategy)\n        if engine == BrowserEngine.HTTPX:\n            # Pass strategy timeout if present in kwargs or use default\n            timeout = kwargs.get(\"timeout\", strategy.timeout)\n            client = await GlobalResourceManager.get_httpx_client(timeout=timeout)\n\n            # Remove timeout and browser-specific keys from kwargs\n            req_kwargs = {\n                k: v for k, v in kwargs.items()\n                if k != \"timeout\" and k not in BROWSER_SPECIFIC_KWARGS\n            }\n            resp = await client.request(method, url, timeout=timeout, **req_kwargs)\n            return UnifiedResponse(resp.text, resp.status_code, resp.status_code, str(resp.url), resp.headers)\n        \n        if engine == BrowserEngine.CURL_CFFI:\n            if not curl_requests:\n                raise ImportError(\"curl_cffi is not available\")\n            \n            self.logger.debug(f\"Using curl_cffi for {url}\")\n            timeout = kwargs.get(\"timeout\", strategy.timeout)\n\n            # Default headers if still not present after browserforge attempt\n            headers = kwargs.get(\"headers\", {**DEFAULT_BROWSER_HEADERS, \"User-Agent\": CHROME_USER_AGENT})\n            # Respect impersonate if provided, otherwise default\n            impersonate = kwargs.get(\"impersonate\", \"chrome110\")\n            \n            # Remove keys that curl_requests.AsyncSession.request doesn't like\n            clean_kwargs = {\n                k: v for k, v in kwargs.items()\n                if k not in [\"timeout\", \"headers\", \"impersonate\"] + BROWSER_SPECIFIC_KWARGS\n            }\n            \n            async with curl_requests.AsyncSession() as s:\n                resp = await s.request(\n                    method, \n                    url, \n                    timeout=timeout, \n                    headers=headers, \n                    impersonate=impersonate,\n                    **clean_kwargs\n                )\n                return UnifiedResponse(resp.text, resp.status_code, resp.status_code, resp.url, resp.headers)\n\n        if not ASYNC_SESSIONS_AVAILABLE:\n            raise ImportError(\"scrapling not available\")\n\n        # Scrapling specific kwargs\n        SCRAPLING_KWARGS = [\"network_idle\", \"wait_selector\", \"wait_until\", \"stealth_mode\", \"block_resources\", \"timeout\"]\n        scrapling_kwargs = {k: v for k, v in kwargs.items() if k in SCRAPLING_KWARGS}\n\n        # Propagate strategy values to scrapling if not explicitly overridden in kwargs\n        if \"timeout\" not in scrapling_kwargs:\n            timeout_val = kwargs.get(\"timeout\", strategy.timeout)\n            # Scrapling/Playwright uses milliseconds for timeout\n            scrapling_kwargs[\"timeout\"] = timeout_val * 1000\n        if \"wait_until\" not in scrapling_kwargs:\n            scrapling_kwargs[\"wait_until\"] = strategy.wait_until or strategy.page_load_strategy\n        if \"network_idle\" not in scrapling_kwargs:\n            scrapling_kwargs[\"network_idle\"] = strategy.network_idle\n        if \"stealth_mode\" not in scrapling_kwargs:\n            scrapling_kwargs[\"stealth_mode\"] = strategy.stealth_mode\n        if \"block_resources\" not in scrapling_kwargs:\n            scrapling_kwargs[\"block_resources\"] = strategy.block_resources\n            \n        # For other engines, we use AsyncFetcher from scrapling\n        if engine == BrowserEngine.CAMOUFOX:\n            async with AsyncStealthySession(headless=True) as s:\n                resp = await s.fetch(url, method=method, **scrapling_kwargs)\n                content = str(getattr(resp, 'body', getattr(resp, 'html_content', \"\")))\n                return UnifiedResponse(content, resp.status, resp.status, resp.url, resp.headers)\n\n        elif engine == BrowserEngine.PLAYWRIGHT_LEGACY:\n            # Direct Playwright usage for cases where scrapling/camoufox fail\n            from playwright.async_api import async_playwright\n            async with async_playwright() as p:\n                browser = await p.chromium.launch(headless=True)\n                # Apply impersonation via context\n                ua = kwargs.get(\"headers\", {}).get(\"User-Agent\", CHROME_USER_AGENT)\n                context = await browser.new_context(user_agent=ua)\n                page = await context.new_page()\n\n                timeout = kwargs.get(\"timeout\", strategy.timeout) * 1000\n                wait_until = \"networkidle\" if strategy.network_idle else \"domcontentloaded\"\n\n                # Apply headers\n                if \"headers\" in kwargs:\n                    await context.set_extra_http_headers(kwargs[\"headers\"])\n\n                resp_obj = await page.goto(url, wait_until=wait_until, timeout=timeout)\n                content = await page.content()\n                status = resp_obj.status if resp_obj else 0\n                headers = resp_obj.headers if resp_obj else {}\n\n                await browser.close()\n                return UnifiedResponse(content, status, status, url, headers)\n\n        elif engine == BrowserEngine.PLAYWRIGHT:\n            async with AsyncDynamicSession(headless=True) as s:\n                resp = await s.fetch(url, method=method, **scrapling_kwargs)\n                # Scrapling responses have a .text object that sometimes returns length 0\n                # We ensure it's a string from .body or .html_content\n                content = str(getattr(resp, 'body', getattr(resp, 'html_content', \"\")))\n                return UnifiedResponse(content, resp.status, resp.status, resp.url, resp.headers)\n        else:\n            # Fallback to simple fetcher\n            async with AsyncFetcher() as fetcher:\n                if method.upper() == \"GET\":\n                    resp = await fetcher.get(url, **kwargs)\n                else:\n                    resp = await fetcher.post(url, **kwargs)\n\n                content = str(getattr(resp, 'body', getattr(resp, 'html_content', \"\")))\n                return UnifiedResponse(content, resp.status, resp.status, resp.url, resp.headers)\n\n\n    async def close(self) -> None:\n        \"\"\"\n        Shared resources are managed by GlobalResourceManager.\n        This remains for API compatibility.\n        \"\"\"\n        pass\n\n\n@dataclass\nclass CircuitBreaker:\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    state: str = \"closed\"\n    failure_count: int = 0\n    last_failure_time: Optional[float] = None\n    async def record_success(self) -> None:\n        self.failure_count = 0\n        self.state = \"closed\"\n    async def record_failure(self) -> None:\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        if self.failure_count >= self.failure_threshold: self.state = \"open\"\n    async def allow_request(self) -> bool:\n        if self.state == \"closed\": return True\n        if self.state == \"open\" and self.last_failure_time:\n            if time.time() - self.last_failure_time > self.recovery_timeout:\n                self.state = \"half-open\"\n                return True\n        return self.state == \"half-open\"\n\n\n@dataclass\nclass RateLimiter:\n    requests_per_second: float = 10.0\n    _tokens: float = field(default=10.0, init=False)\n    _last_update: float = field(default_factory=time.time, init=False)\n    _lock: Optional[asyncio.Lock] = field(default=None, init=False)\n    _lock_sentinel: ClassVar[threading.Lock] = threading.Lock()\n\n    def _get_lock(self) -> asyncio.Lock:\n        if self._lock is None:\n            with self._lock_sentinel:\n                if self._lock is None:\n                    try:\n                        loop = asyncio.get_running_loop()\n                        self._lock = asyncio.Lock()\n                    except RuntimeError:\n                        pass\n        return self._lock\n\n    async def acquire(self) -> None:\n        lock = self._get_lock()\n        for _ in range(1000): # Iteration limit to prevent potential hangs\n            wait_time = 0\n            async with lock:\n                now = time.time()\n                elapsed = now - self._last_update\n                self._tokens = min(self.requests_per_second, self._tokens + (elapsed * self.requests_per_second))\n                self._last_update = now\n                if self._tokens >= 1:\n                    self._tokens -= 1\n                    return\n                wait_time = (1 - self._tokens) / self.requests_per_second\n\n            if wait_time >= 0:\n                await asyncio.sleep(max(wait_time, 0.01))\n\n\nclass AdapterMetrics:\n    def __init__(self) -> None:\n        self._lock = threading.Lock()\n        self.total_requests = 0\n        self.successful_requests = 0\n        self.failed_requests = 0\n        self.total_latency_ms = 0.0\n        self.consecutive_failures = 0\n        self.last_failure_reason: Optional[str] = None\n    @property\n    def success_rate(self) -> float:\n        return self.successful_requests / self.total_requests if self.total_requests > 0 else 1.0\n    async def record_success(self, latency_ms: float) -> None:\n        with self._lock:\n            self.total_requests += 1\n        self.successful_requests += 1\n        self.total_latency_ms += latency_ms\n        self.consecutive_failures = 0\n        self.last_failure_reason: Optional[str] = None\n    async def record_failure(self, error: str) -> None:\n        with self._lock:\n            self.total_requests += 1\n        self.failed_requests += 1\n        self.consecutive_failures += 1\n        self.last_failure_reason = error\n    def snapshot(self) -> Dict[str, Any]:\n        return {\n            \"total_requests\": self.total_requests,\n            \"success_rate\": self.success_rate,\n            \"failed_requests\": self.failed_requests,\n            \"consecutive_failures\": self.consecutive_failures,\n            \"last_failure_reason\": getattr(self, \"last_failure_reason\", None)\n        }\n\n\n# --- MIXINS ---\nclass JSONParsingMixin:\n    \"\"\"Mixin for safe JSON extraction from HTML and scripts.\"\"\"\n    def _parse_json_from_script(self, parser: HTMLParser, selector: str, context: str = \"script\") -> Optional[Any]:\n        script = parser.css_first(selector)\n        if not script:\n            return None\n        try:\n            return json.loads(script.text())\n        except json.JSONDecodeError as e:\n            if hasattr(self, 'logger'):\n                self.logger.error(\"failed_parsing_json\", context=context, selector=selector, error=str(e))\n            return None\n\n    def _parse_json_from_attribute(self, parser: HTMLParser, selector: str, attribute: str, context: str = \"attribute\") -> Optional[Any]:\n        el = parser.css_first(selector)\n        if not el:\n            return None\n        raw = el.attributes.get(attribute)\n        if not raw:\n            return None\n        try:\n            return json.loads(html.unescape(raw))\n        except json.JSONDecodeError as e:\n            if hasattr(self, 'logger'):\n                self.logger.error(\"failed_parsing_json\", context=context, selector=selector, attribute=attribute, error=str(e))\n            return None\n\n    def _parse_all_jsons_from_scripts(self, parser: HTMLParser, selector: str, context: str = \"scripts\") -> List[Any]:\n        results = []\n        for script in parser.css(selector):\n            try:\n                results.append(json.loads(script.text()))\n            except json.JSONDecodeError as e:\n                if hasattr(self, 'logger'):\n                    self.logger.error(\"failed_parsing_json_in_list\", context=context, selector=selector, error=str(e))\n        return results\n\n\nclass BrowserHeadersMixin:\n    def _get_browser_headers(self, host: Optional[str] = None, referer: Optional[str] = None, **extra: str) -> Dict[str, str]:\n        h = {**DEFAULT_BROWSER_HEADERS, \"User-Agent\": CHROME_USER_AGENT, \"sec-ch-ua\": CHROME_SEC_CH_UA, \"sec-ch-ua-mobile\": \"?0\", \"sec-ch-ua-platform\": '\"Windows\"'}\n        if host: h[\"Host\"] = host\n        if referer: h[\"Referer\"] = referer\n        h.update(extra)\n        return h\n\n\nclass DebugMixin:\n    def _save_debug_snapshot(self, content: str, context: str, url: Optional[str] = None) -> None:\n        if not content or not os.getenv(\"DEBUG_SNAPSHOTS\"): return\n        try:\n            d = Path(\"debug_snapshots\")\n            d.mkdir(parents=True, exist_ok=True)\n            f = d / f\"{context}_{datetime.now(EASTERN).strftime('%Y%m%d_%H%M%S')}.html\"\n            with open(f, \"w\", encoding=\"utf-8\") as out:\n                if url: out.write(f\"<!-- URL: {url} -->\\n\")\n                out.write(content)\n        except Exception: pass\n    def _save_debug_html(self, content: str, filename: str, **kwargs) -> None:\n        self._save_debug_snapshot(content, filename)\n\n\nclass RacePageFetcherMixin:\n    async def _fetch_race_pages_concurrent(self, metadata: List[Dict[str, Any]], headers: Dict[str, str], semaphore_limit: int = 5, delay_range: tuple[float, float] = (0.5, 1.5)) -> List[Dict[str, Any]]:\n        local_sem = asyncio.Semaphore(semaphore_limit)\n        async def fetch_single(item):\n            url = item.get(\"url\")\n            if not url: return None\n\n            async with local_sem:\n                    # Stagger requests by sleeping inside the semaphore (Project Convention)\n                    await asyncio.sleep(delay_range[0] + random.random() * (delay_range[1] - delay_range[0]))\n                    try:\n                        if hasattr(self, 'logger'):\n                            self.logger.debug(\"fetching_race_page\", url=url)\n                        # make_request handles global_sem internally\n                        resp = None\n                        for attempt in range(2): # 1 retry\n                            resp = await self.make_request(\"GET\", url, headers=headers)\n                            if resp and hasattr(resp, \"text\") and resp.text and len(resp.text) > 500:\n                                break\n                            await asyncio.sleep(1 * (attempt + 1))\n\n                        if resp and hasattr(resp, \"text\") and resp.text:\n                            if hasattr(self, 'logger'):\n                                self.logger.debug(\"fetched_race_page\", url=url, status=getattr(resp, 'status', 'unknown'))\n                            return {**item, \"html\": resp.text}\n                        elif resp:\n                            if hasattr(self, 'logger'):\n                                self.logger.warning(\"failed_fetching_race_page_unexpected_status\", url=url, status=getattr(resp, 'status', 'unknown'))\n                    except Exception as e:\n                        if hasattr(self, 'logger'):\n                            self.logger.error(\"failed_fetching_race_page\", url=url, error=str(e))\n                    return None\n        tasks = [fetch_single(m) for m in metadata]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return [r for r in results if not isinstance(r, Exception) and r is not None]\n\n\n# --- BASE ADAPTER ---\nclass BaseAdapterV3(ABC):\n    ADAPTER_TYPE: ClassVar[str] = \"discovery\"\n\n    def __init__(self, source_name: str, base_url: str, rate_limit: float = 10.0, config: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:\n        self.source_name = source_name\n        self.base_url = base_url.rstrip(\"/\")\n        self.config = config or {}\n        # Merge kwargs into config\n        self.config.update(kwargs)\n        self.trust_ratio = 0.0 # Tracking odds quality ratio (0.0 to 1.0)\n\n        # Override rate_limit from config if present\n        actual_rate_limit = float(self.config.get(\"rate_limit\", rate_limit))\n\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.circuit_breaker = CircuitBreaker(\n            failure_threshold=int(self.config.get(\"failure_threshold\", 5)),\n            recovery_timeout=float(self.config.get(\"recovery_timeout\", 60.0))\n        )\n        self.rate_limiter = RateLimiter(requests_per_second=actual_rate_limit)\n        self.metrics = AdapterMetrics()\n        self.smart_fetcher = SmartFetcher(strategy=self._configure_fetch_strategy())\n        self.last_race_count = 0\n        self.last_duration_s = 0.0\n\n    @abstractmethod\n    def _configure_fetch_strategy(self) -> FetchStrategy: pass\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Optional[Any]: pass\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> List[Race]: pass\n\n    async def get_races(self, date: str) -> List[Race]:\n        start = time.time()\n        try:\n            # Check for browser requirement in monolith mode\n            strategy = self.smart_fetcher.strategy\n            if is_frozen() and strategy.primary_engine in [BrowserEngine.PLAYWRIGHT, BrowserEngine.CAMOUFOX]:\n                self.logger.info(\"Skipping browser-dependent adapter in monolith mode\")\n                return []\n\n            if not await self.circuit_breaker.allow_request(): return []\n            await self.rate_limiter.acquire()\n            raw = await self._fetch_data(date)\n            if not raw:\n                await self.circuit_breaker.record_failure()\n                return []\n            races = self._validate_and_parse_races(raw)\n            self.last_race_count = len(races)\n            self.last_duration_s = time.time() - start\n            await self.circuit_breaker.record_success()\n            await self.metrics.record_success(self.last_duration_s * 1000)\n            return races\n        except Exception as e:\n            self.logger.error(\"Adapter failed\", error=str(e))\n            await self.circuit_breaker.record_failure()\n            await self.metrics.record_failure(str(e))\n            return []\n\n    def _validate_and_parse_races(self, raw_data: Any) -> List[Race]:\n        races = self._parse_races(raw_data)\n        total_runners = 0\n        trustworthy_runners = 0\n\n        for r in races:\n            # Global heuristic for runner numbers (addressing \"impossible\" high numbers)\n            active_runners = [run for run in r.runners if not run.scratched]\n            field_size = len(active_runners)\n\n            # If any runner has a number > 20 and it's also > field_size + 10 (buffer)\n            # or if it's extremely high (> 100), re-index everything as it's likely a parsing error (horse IDs).\n            # Also re-index if all numbers are missing/zero.\n            suspicious = all(run.number == 0 or run.number is None for run in r.runners)\n            if not suspicious:\n                for run in r.runners:\n                    if run.number:\n                        if run.number > 100 or (run.number > 20 and run.number > field_size + 10):\n                            suspicious = True\n                            break\n\n            if suspicious:\n                self.logger.warning(\"suspicious_runner_numbers\", venue=r.venue, field_size=field_size)\n                for i, run in enumerate(r.runners):\n                    run.number = i + 1\n\n            for runner in r.runners:\n                if not runner.scratched:\n                    # Explicitly enrich win_odds using all available sources (including fallbacks)\n                    best = _get_best_win_odds(runner)\n                    # Untrustworthy odds should be flagged (Memory Directive Fix)\n                    is_trustworthy = best is not None\n                    runner.metadata[\"odds_source_trustworthy\"] = is_trustworthy\n                    if best:\n                        runner.win_odds = float(best)\n                        trustworthy_runners += 1\n                    total_runners += 1\n\n        if total_runners > 0:\n            self.trust_ratio = round(trustworthy_runners / total_runners, 2)\n            self.logger.info(\"adapter_odds_quality\", ratio=self.trust_ratio, source=self.source_name)\n\n        valid, warnings = DataValidationPipeline.validate_parsed_races(races, adapter_name=self.source_name)\n        return valid\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url}/{url.lstrip('/')}\"\n        self.logger.debug(\"Requesting\", method=method, url=full_url)\n        # Apply global concurrency limit (Memory Directive Fix)\n        async with GlobalResourceManager.get_global_semaphore():\n            try:\n                # Use adapter-specific strategy\n                kwargs.setdefault(\"strategy\", self.smart_fetcher.strategy)\n                resp = await self.smart_fetcher.fetch(full_url, method=method, **kwargs)\n                status = get_resp_status(resp)\n                self.logger.debug(\"Response received\", method=method, url=full_url, status=status)\n                return resp\n            except Exception as e:\n                self.logger.error(\"Request failed\", method=method, url=full_url, error=str(e))\n                return None\n\n    async def close(self) -> None: await self.smart_fetcher.close()\n    async def shutdown(self) -> None: await self.close()\n\n# ============================================================================\n# ADAPTER IMPLEMENTATIONS\n# ============================================================================\n\n# ----------------------------------------\n# EquibaseAdapter\n# ----------------------------------------\nclass RacingAndSportsAdapter(BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for Racing & Sports (RAS).\n    Note: Highly protected by Cloudflare; requires advanced impersonation.\n    \"\"\"\n    SOURCE_NAME: ClassVar[str] = \"RacingAndSports\"\n    BASE_URL: ClassVar[str] = \"https://www.racingandsports.com.au\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=60\n        )\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.racingandsports.com.au\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        url = f\"/racing-index?date={date}\"\n        resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n        if not resp or not resp.text:\n            return None\n\n        self._save_debug_snapshot(resp.text, f\"ras_index_{date}\")\n        parser = HTMLParser(resp.text)\n        metadata = []\n\n        # RAS uses tables for different regions (Australia, UK, etc.)\n        for table in parser.css(\"table.table-index\"):\n            for row in table.css(\"tbody tr\"):\n                venue_cell = row.css_first(\"td.venue-name\")\n                if not venue_cell: continue\n                venue_name = venue_cell.text(strip=True)\n\n                for link in row.css(\"td a.race-link\"):\n                    race_url = link.attributes.get(\"href\", \"\")\n                    if not race_url: continue\n                    if not race_url.startswith(\"http\"):\n                        race_url = self.BASE_URL + race_url\n\n                    r_num_match = re.search(r\"R(\\d+)\", link.text(strip=True))\n                    r_num = int(r_num_match.group(1)) if r_num_match else 0\n\n                    metadata.append({\n                        \"url\": race_url,\n                        \"venue\": venue_name,\n                        \"race_number\": r_num\n                    })\n\n        if not metadata:\n            return None\n\n        # Limit for sanity\n        pages = await self._fetch_race_pages_concurrent(metadata[:40], self._get_headers())\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except Exception: return []\n\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            html_content = item.get(\"html\")\n            if not html_content: continue\n            try:\n                race = self._parse_single_race(html_content, item.get(\"url\", \"\"), race_date, item.get(\"venue\"), item.get(\"race_number\"))\n                if race: races.append(race)\n            except Exception: pass\n        return races\n\n    def _parse_single_race(self, html_content: str, url: str, race_date: date, venue: str, race_num: int) -> Optional[Race]:\n        tree = HTMLParser(html_content)\n\n        runners = []\n        for row in tree.css(\"tr.runner-row\"):\n            name_node = row.css_first(\".runner-name\")\n            if not name_node: continue\n            name = clean_text(name_node.text())\n\n            num_node = row.css_first(\".runner-number\")\n            number = int(\"\".join(filter(str.isdigit, num_node.text()))) if num_node else 0\n\n            odds_node = row.css_first(\".odds-win\")\n            win_odds = parse_odds_to_decimal(clean_text(odds_node.text())) if odds_node else None\n\n            odds_data = {}\n            if ov := create_odds_data(self.SOURCE_NAME, win_odds):\n                odds_data[self.SOURCE_NAME] = ov\n\n            runners.append(Runner(name=name, number=number, odds=odds_data, win_odds=win_odds))\n\n        if not runners: return None\n\n        # Start time from page if available, else guess\n        start_time = datetime.combine(race_date, datetime.min.time())\n        # Try to find time in text\n        time_match = re.search(r\"(\\d{1,2}:\\d{2})\", html_content)\n        if time_match:\n            try:\n                start_time = datetime.combine(race_date, datetime.strptime(time_match.group(1), \"%H:%M\").time())\n            except Exception: pass\n\n        return Race(\n            id=generate_race_id(\"ras\", venue, start_time, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=ensure_eastern(start_time),\n            runners=runners,\n            source=self.SOURCE_NAME,\n            available_bets=scrape_available_bets(html_content)\n        )\n\nclass SkyRacingWorldAdapter(BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"SkyRacingWorld\"\n    BASE_URL: ClassVar[str] = \"https://www.skyracingworld.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=60\n        )\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.skyracingworld.com\")\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        kwargs.setdefault(\"impersonate\", \"chrome120\")\n        return await super().make_request(method, url, **kwargs)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        # Index for the day\n        index_url = f\"/form-guide/thoroughbred/{date}\"\n        resp = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not resp or not resp.text:\n            if resp: self.logger.warning(\"Unexpected status\", status=resp.status, url=index_url)\n            return None\n        self._save_debug_snapshot(resp.text, f\"skyracing_index_{date}\")\n\n        parser = HTMLParser(resp.text)\n        track_links = defaultdict(list)\n        now = now_eastern()\n        today_str = now.strftime(\"%Y-%m-%d\")\n\n        # Optimization: If it's late in ET, skip countries that are finished\n        # Europe/Turkey/SA usually finished by 18:00 ET\n        skip_finished_countries = (now.hour >= 18 or now.hour < 6) and (date == today_str)\n        finished_keywords = [\"turkey\", \"south-africa\", \"united-kingdom\", \"france\", \"germany\", \"dubai\", \"bahrain\"]\n\n        for link in parser.css(\"a.fg-race-link\"):\n            url = link.attributes.get(\"href\")\n            if url:\n                if not url.startswith(\"http\"):\n                    url = self.BASE_URL + url\n\n                if skip_finished_countries:\n                    if any(kw in url.lower() for kw in finished_keywords):\n                        continue\n\n                # Group by track (everything before R#)\n                track_key = re.sub(r'/R\\d+$', '', url)\n                track_links[track_key].append(url)\n\n        metadata = []\n        for t_url in track_links:\n            # For discovery, we usually only care about upcoming races.\n            # Without times in index, we pick R1 as a guess, but if we have multiple,\n            # R1 might be in the past. However, picking R1 is the safest if we want \"one per track\".\n            if track_links[t_url]:\n                metadata.append({\"url\": track_links[t_url][0]})\n\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"SRW Index Parsing\", url=index_url)\n            return None\n        # Limit to first 50 to avoid hammering\n        pages = await self._fetch_race_pages_concurrent(metadata[:50], self._get_headers(), semaphore_limit=5)\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except Exception: return []\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            html_content = item.get(\"html\")\n            if not html_content: continue\n            try:\n                race = self._parse_single_race(html_content, item.get(\"url\", \"\"), race_date)\n                if race: races.append(race)\n            except Exception: pass\n        return races\n\n    def _parse_single_race(self, html_content: str, url: str, race_date: date) -> Optional[Race]:\n        parser = HTMLParser(html_content)\n\n        # Extract venue and time from header\n        # Format usually: \"14:30 LINGFIELD\" or similar\n        header = parser.css_first(\".sdc-site-racing-header__name\") or parser.css_first(\"h1\") or parser.css_first(\"h2\")\n        if not header: return None\n\n        header_text = clean_text(header.text())\n        match = re.search(r\"(\\d{1,2}:\\d{2})\\s+(.+)\", header_text)\n        if match:\n            time_str = match.group(1)\n            venue = normalize_venue_name(match.group(2))\n        else:\n            venue = normalize_venue_name(header_text)\n            time_str = \"12:00\" # Fallback\n\n        try:\n            start_time = datetime.combine(race_date, datetime.strptime(time_str, \"%H:%M\").time())\n        except Exception:\n            start_time = datetime.combine(race_date, datetime.min.time())\n\n        # Race number from URL\n        race_num = 1\n        num_match = re.search(r'/R(\\d+)$', url)\n        if num_match:\n            race_num = int(num_match.group(1))\n\n        runners = []\n        # Try different selectors for runners\n        for row in parser.css(\".runner_row\") or parser.css(\".mobile-runner\"):\n            try:\n                name_node = row.css_first(\".horseName\") or row.css_first(\"a[href*='/horse/']\")\n                if not name_node: continue\n                name = clean_text(name_node.text())\n\n                num_node = row.css_first(\".tdContent b\") or row.css_first(\"[data-tab-no]\")\n                number = 0\n                if num_node:\n                    if num_node.attributes.get(\"data-tab-no\"):\n                        number = int(num_node.attributes.get(\"data-tab-no\"))\n                    else:\n                        digits = \"\".join(filter(str.isdigit, num_node.text()))\n                        if digits: number = int(digits)\n\n                scratched = \"strikeout\" in (row.attributes.get(\"class\") or \"\").lower() or row.attributes.get(\"data-scratched\") == \"True\"\n\n                win_odds = None\n                odds_node = row.css_first(\".pa_odds\") or row.css_first(\".odds\")\n                if odds_node:\n                    win_odds = parse_odds_to_decimal(clean_text(odds_node.text()))\n\n                if win_odds is None:\n                    win_odds = SmartOddsExtractor.extract_from_node(row)\n\n                od = {}\n                if ov := create_odds_data(self.SOURCE_NAME, win_odds):\n                    od[self.SOURCE_NAME] = ov\n\n                runners.append(Runner(name=name, number=number, scratched=scratched, odds=od, win_odds=win_odds))\n            except Exception: continue\n\n        if not runners: return None\n\n        disc = detect_discipline(html_content)\n        return Race(\n            id=generate_race_id(\"srw\", venue, start_time, race_num, disc),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            discipline=disc,\n            source=self.SOURCE_NAME,\n            available_bets=scrape_available_bets(html_content)\n        )\n\n# ----------------------------------------\n# AtTheRacesAdapter\n# ----------------------------------------\nclass AtTheRacesAdapter(BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"AtTheRaces\"\n    BASE_URL: ClassVar[str] = \"https://www.attheraces.com\"\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.CURL_CFFI, enable_js=True, stealth_mode=\"camouflage\")\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        kwargs.setdefault(\"impersonate\", \"chrome120\")\n        return await super().make_request(method, url, **kwargs)\n\n    SELECTORS: ClassVar[Dict[str, List[str]]] = {\n        \"race_links\": ['a.race-navigation-link', 'a.sidebar-racecardsigation-link', 'a[href^=\"/racecard/\"]', 'a[href*=\"/racecard/\"]'],\n        \"details_container\": [\".race-header__details--primary\", \"atr-racecard-race-header .container\", \".racecard-header .container\"],\n        \"track_name\": [\"h2\", \"h1 a\", \"h1\"],\n        \"race_time\": [\"h2 b\", \"h1 span\", \".race-time\"],\n        \"distance\": [\".race-header__details--secondary .p--large\", \".race-header__details--secondary div\"],\n        \"runners\": [\".card-cell--horse\", \".odds-grid-horse\", \"atr-horse-in-racecard\", \".horse-in-racecard\"],\n    }\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.attheraces.com\", referer=\"https://www.attheraces.com/racecards\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        index_url = f\"/racecards/{date}\"\n        intl_url = f\"/racecards/international/{date}\"\n\n        resp = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        intl_resp = await self.make_request(\"GET\", intl_url, headers=self._get_headers())\n\n        metadata = []\n        if resp and resp.text:\n            self._save_debug_snapshot(resp.text, f\"atr_index_{date}\")\n            parser = HTMLParser(resp.text)\n            metadata.extend(self._extract_race_metadata(parser, date))\n\n        elif resp:\n            self.logger.warning(\"Unexpected status\", status=resp.status, url=index_url)\n\n        if intl_resp and intl_resp.text:\n            self._save_debug_snapshot(intl_resp.text, f\"atr_intl_index_{date}\")\n            intl_parser = HTMLParser(intl_resp.text)\n            metadata.extend(self._extract_race_metadata(intl_parser, date))\n        elif intl_resp:\n            self.logger.warning(\"Unexpected status\", status=intl_resp.status, url=intl_url)\n\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"ATR Index Parsing\", date=date)\n            return None\n        pages = await self._fetch_race_pages_concurrent(metadata, self._get_headers(), semaphore_limit=5)\n        return {\"pages\": pages, \"date\": date}\n\n    def _extract_race_metadata(self, parser: HTMLParser, date_str: str) -> List[Dict[str, Any]]:\n        meta: List[Dict[str, Any]] = []\n        track_map = defaultdict(list)\n\n        try:\n            target_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        for link in parser.css('a[href*=\"/racecard/\"]'):\n            url = link.attributes.get(\"href\")\n            if not url: continue\n            # Look for time at end of URL: /racecard/venue/date/1330\n            time_match = re.search(r\"/(\\d{4})$\", url)\n            if not time_match:\n                # Might be just a race number: /racecard/venue/date/1\n                if not re.search(r\"/\\d{1,2}$\", url): continue\n\n            parts = url.split(\"/\")\n            if len(parts) >= 3:\n                track_name = parts[2]\n                time_str = time_match.group(1) if time_match else None\n                track_map[track_name].append({\"url\": url, \"time_str\": time_str})\n\n        # Site usually shows UK time\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        for track, race_infos in track_map.items():\n            # Broaden window to capture multiple races (Memory Directive Fix)\n            for r in race_infos:\n                if r[\"time_str\"]:\n                    try:\n                        rt = datetime.strptime(r[\"time_str\"], \"%H%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n                        meta.append({\"url\": r[\"url\"], \"race_number\": 1, \"venue_raw\": track})\n                    except Exception: pass\n\n        if not meta:\n            for meeting in (parser.css(\".meeting-summary\") or parser.css(\".p-meetings__item\")):\n                for link in meeting.css('a[href*=\"/racecard/\"]'):\n                    if url := link.attributes.get(\"href\"):\n                        meta.append({\"url\": url, \"race_number\": 1})\n        return meta\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except Exception: return []\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            html_content = item.get(\"html\")\n            if not html_content: continue\n            try:\n                race = self._parse_single_race(html_content, item.get(\"url\", \"\"), race_date, item.get(\"race_number\"))\n                if race: races.append(race)\n            except Exception: pass\n        return races\n\n    def _parse_single_race(self, html_content: str, url_path: str, race_date: date, race_number_fallback: Optional[int]) -> Optional[Race]:\n        parser = HTMLParser(html_content)\n        track_name, time_str, header_text = None, None, \"\"\n        header = parser.css_first(\".race-header__details\") or parser.css_first(\".racecard-header\")\n        if header:\n            header_text = clean_text(header.text()) or \"\"\n            time_match = re.search(r\"(\\d{1,2}:\\d{2})\", header_text)\n            if time_match:\n                time_str = time_match.group(1)\n                track_raw = re.sub(r\"\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4}\", \"\", header_text.replace(time_str, \"\")).strip()\n                track_raw = re.split(r\"\\s+Race\\s+\\d+\", track_raw, flags=re.I)[0]\n                track_raw = re.sub(r\"^\\d+\\s+\", \"\", track_raw).split(\" - \")[0].split(\"|\")[0].strip()\n                track_name = normalize_venue_name(track_raw)\n        if not track_name:\n            details = parser.css_first(\".race-header__details--primary\")\n            if details:\n                track_node = details.css_first(\"h2\") or details.css_first(\"h1 a\") or details.css_first(\"h1\")\n                if track_node: track_name = normalize_venue_name(clean_text(track_node.text()))\n                if not time_str:\n                    time_node = details.css_first(\"h2 b\") or details.css_first(\".race-time\")\n                    if time_node: time_str = clean_text(time_node.text()).replace(\" ATR\", \"\")\n        if not track_name:\n            parts = url_path.split(\"/\")\n            if len(parts) >= 3: track_name = normalize_venue_name(parts[2])\n        if not time_str:\n            parts = url_path.split(\"/\")\n            if len(parts) >= 5 and re.match(r\"\\d{4}\", parts[-1]):\n                raw_time = parts[-1]\n                time_str = f\"{raw_time[:2]}:{raw_time[2:]}\"\n        if not track_name or not time_str: return None\n        try: start_time = datetime.combine(race_date, datetime.strptime(time_str, \"%H:%M\").time())\n        except Exception: return None\n        race_number = race_number_fallback or 1\n        distance = None\n        dist_match = re.search(r\"\\|\\s*(\\d+[mfy].*)\", header_text, re.I)\n        if dist_match: distance = dist_match.group(1).strip()\n        runners = self._parse_runners(parser)\n        if not runners: return None\n        return Race(discipline=\"Thoroughbred\", id=generate_race_id(\"atr\", track_name, start_time, race_number), venue=track_name, race_number=race_number, start_time=start_time, runners=runners, distance=distance, source=self.source_name, available_bets=scrape_available_bets(html_content))\n\n    def _parse_runners(self, parser: HTMLParser) -> List[Runner]:\n        odds_map: Dict[str, float] = {}\n        for row in parser.css(\".odds-grid__row--horse\"):\n            if m := re.search(r\"row-(\\d+)\", row.attributes.get(\"id\", \"\")):\n                if price := row.attributes.get(\"data-bestprice\"):\n                    try:\n                        p_val = float(price)\n                        if is_valid_odds(p_val): odds_map[m.group(1)] = p_val\n                    except Exception: pass\n        runners: List[Runner] = []\n        for selector in self.SELECTORS[\"runners\"]:\n            nodes = parser.css(selector)\n            if nodes:\n                for i, node in enumerate(nodes):\n                    runner = self._parse_runner(node, odds_map, i + 1)\n                    if runner: runners.append(runner)\n                break\n        return runners\n\n    def _parse_runner(self, row: Node, odds_map: Dict[str, float], fallback_number: int = 0) -> Optional[Runner]:\n        try:\n            name_node = row.css_first(\"h3\") or row.css_first(\"a.horse__link\") or row.css_first('a[href*=\"/form/horse/\"]')\n            if not name_node: return None\n            name = clean_text(name_node.text())\n            if not name: return None\n            num_node = row.css_first(\".horse-in-racecard__saddle-cloth-number\") or row.css_first(\".odds-grid-horse__no\")\n            number = 0\n            if num_node:\n                ns = clean_text(num_node.text())\n                if ns:\n                    digits = \"\".join(filter(str.isdigit, ns))\n                    if digits: number = int(digits)\n\n            if number == 0 or number > 40:\n                number = fallback_number\n            win_odds = None\n            if horse_link := row.css_first('a[href*=\"/form/horse/\"]'):\n                if m := re.search(r\"/(\\d+)(\\?|$)\", horse_link.attributes.get(\"href\", \"\")):\n                    win_odds = odds_map.get(m.group(1))\n            if win_odds is None:\n                if odds_node := row.css_first(\".horse-in-racecard__odds\"):\n                    win_odds = parse_odds_to_decimal(clean_text(odds_node.text()))\n\n            # Advanced heuristic fallback\n            if win_odds is None:\n                win_odds = SmartOddsExtractor.extract_from_node(row)\n\n            odds: Dict[str, OddsData] = {}\n            if od := create_odds_data(self.source_name, win_odds): odds[self.source_name] = od\n            return Runner(number=number, name=name, odds=odds, win_odds=win_odds)\n        except Exception: return None\n\n# ----------------------------------------\n# AtTheRacesGreyhoundAdapter\n# ----------------------------------------\nclass AtTheRacesGreyhoundAdapter(JSONParsingMixin, BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"AtTheRacesGreyhound\"\n    BASE_URL: ClassVar[str] = \"https://greyhounds.attheraces.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.CURL_CFFI, enable_js=True, stealth_mode=\"camouflage\", timeout=45)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"greyhounds.attheraces.com\", referer=\"https://greyhounds.attheraces.com/racecards\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        index_url = f\"/racecards/{date}\" if date else \"/racecards\"\n        resp = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not resp or not resp.text:\n            if resp: self.logger.warning(\"Unexpected status\", status=resp.status, url=index_url)\n            return None\n        self._save_debug_snapshot(resp.text, f\"atr_grey_index_{date}\")\n        parser = HTMLParser(resp.text)\n        metadata = self._extract_race_metadata(parser, date)\n        if not metadata:\n            links = []\n            scripts = self._parse_all_jsons_from_scripts(parser, 'script[type=\"application/ld+json\"]', context=\"ATR Greyhound Index\")\n            for d in scripts:\n                items = d.get(\"@graph\", [d]) if isinstance(d, dict) else []\n                for item in items:\n                    if item.get(\"@type\") == \"SportsEvent\":\n                        loc = item.get(\"location\")\n                        if isinstance(loc, list):\n                            for l in loc:\n                                if u := l.get(\"url\"): links.append(u)\n                        elif isinstance(loc, dict):\n                            if u := loc.get(\"url\"): links.append(u)\n            metadata = [{\"url\": l, \"race_number\": 0} for l in set(links)]\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"ATR Greyhound Index Parsing\", url=index_url)\n            return None\n        pages = await self._fetch_race_pages_concurrent(metadata, self._get_headers(), semaphore_limit=5)\n        return {\"pages\": pages, \"date\": date}\n\n    def _extract_race_metadata(self, parser: HTMLParser, date_str: str) -> List[Dict[str, Any]]:\n        meta: List[Dict[str, Any]] = []\n        pc = parser.css_first(\"page-content\")\n        if not pc: return []\n        items_raw = pc.attributes.get(\":items\") or pc.attributes.get(\":modules\")\n        if not items_raw: return []\n\n        try:\n            target_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        # Usually UK time\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        try:\n            modules = json.loads(html.unescape(items_raw))\n            for module in modules:\n                for meeting in module.get(\"data\", {}).get(\"items\", []):\n                    # Broaden window to capture multiple races (Memory Directive Fix)\n                    races = [r for r in meeting.get(\"items\", []) if r.get(\"type\") == \"racecard\"]\n\n                    for race in races:\n                        r_time_str = race.get(\"time\") # Usually HH:MM\n                        if r_time_str:\n                            try:\n                                rt = datetime.strptime(r_time_str, \"%H:%M\").replace(\n                                    year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                                )\n                                diff = (rt - now_site).total_seconds() / 60\n                                if not (-45 < diff <= 1080):\n                                    continue\n\n                                r_num = race.get(\"raceNumber\") or race.get(\"number\") or 1\n                                if u := race.get(\"cta\", {}).get(\"href\"):\n                                    if \"/racecard/\" in u:\n                                        meta.append({\"url\": u, \"race_number\": r_num})\n                            except Exception: pass\n        except Exception: pass\n        return meta\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data.get(\"date\", \"\"), \"%Y-%m-%d\").date()\n        except Exception: race_date = datetime.now(EASTERN).date()\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            if not item or not item.get(\"html\"): continue\n            try:\n                race = self._parse_single_race(item[\"html\"], item.get(\"url\", \"\"), race_date, item.get(\"race_number\"))\n                if race: races.append(race)\n            except Exception: pass\n        return races\n\n    def _parse_single_race(self, html_content: str, url_path: str, race_date: date, race_number: Optional[int]) -> Optional[Race]:\n        parser = HTMLParser(html_content)\n        pc = parser.css_first(\"page-content\")\n        if not pc: return None\n        items_raw = pc.attributes.get(\":items\") or pc.attributes.get(\":modules\")\n        if not items_raw: return None\n        try: modules = json.loads(html.unescape(items_raw))\n        except Exception: return None\n        venue, race_time_str, distance, runners, odds_map = \"\", \"\", \"\", [], {}\n\n        # Try to extract venue from title as high-priority fallback\n        title_node = parser.css_first(\"title\")\n        if title_node:\n            title_text = title_node.text().strip()\n            # Title: \"14:26 Oxford Greyhound Racecard...\"\n            tm = re.search(r'\\d{1,2}:\\d{2}\\s+(.+?)\\s+Greyhound', title_text)\n            if tm:\n                venue = normalize_venue_name(tm.group(1))\n        for module in modules:\n            m_type, m_data = module.get(\"type\"), module.get(\"data\", {})\n            if m_type == \"RacecardHero\":\n                venue = normalize_venue_name(m_data.get(\"track\", \"\"))\n                race_time_str = m_data.get(\"time\", \"\")\n                distance = m_data.get(\"distance\", \"\")\n                if not race_number: race_number = m_data.get(\"raceNumber\") or m_data.get(\"number\")\n            elif m_type == \"OddsGrid\":\n                odds_grid = m_data.get(\"oddsGrid\", {})\n\n                # If venue still empty, try to get it from OddsGrid data\n                if not venue:\n                    venue = normalize_venue_name(odds_grid.get(\"track\", \"\"))\n                if not race_time_str:\n                    race_time_str = odds_grid.get(\"time\", \"\")\n                if not distance:\n                    distance = odds_grid.get(\"distance\", \"\")\n\n                partners = odds_grid.get(\"partners\", {})\n                all_partners = []\n                if isinstance(partners, dict):\n                    for p_list in partners.values(): all_partners.extend(p_list)\n                elif isinstance(partners, list): all_partners = partners\n                for partner in all_partners:\n                    for o in partner.get(\"odds\", []):\n                        g_id = o.get(\"betParams\", {}).get(\"greyhoundId\")\n                        price = o.get(\"value\", {}).get(\"decimal\")\n                        if g_id and price:\n                            p_val = parse_odds_to_decimal(price)\n                            if p_val and is_valid_odds(p_val): odds_map[str(g_id)] = p_val\n                for t in odds_grid.get(\"traps\", []):\n                    trap_num = t.get(\"trap\", 0)\n                    name = clean_text(t.get(\"name\", \"\")) or \"\"\n                    g_id_match = re.search(r\"/greyhound/(\\d+)\", t.get(\"href\", \"\"))\n                    g_id = g_id_match.group(1) if g_id_match else None\n                    win_odds = odds_map.get(str(g_id)) if g_id else None\n\n                    # Advanced heuristic fallback\n                    if win_odds is None:\n                        win_odds = SmartOddsExtractor.extract_from_text(str(t))\n\n\n                    odds_data = {}\n                    if ov := create_odds_data(self.source_name, win_odds): odds_data[self.source_name] = ov\n                    runners.append(Runner(number=trap_num or 0, name=name, odds=odds_data, win_odds=win_odds))\n\n        url_parts = url_path.split(\"/\")\n        if not venue:\n             # /racecard/GB/oxford/10-February-2026/1426\n             m = re.search(r'/(?:racecard|result)/[A-Z]{2,3}/([^/]+)', url_path)\n             if m:\n                 venue = normalize_venue_name(m.group(1))\n        if not race_time_str and len(url_parts) >= 5:\n             race_time_str = url_parts[-1]\n        if not venue or not runners: return None\n        try:\n            if \":\" not in race_time_str and len(race_time_str) == 4: race_time_str = f\"{race_time_str[:2]}:{race_time_str[2:]}\"\n            start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        except Exception: return None\n        return Race(discipline=\"Greyhound\", id=generate_race_id(\"atrg\", venue, start_time, race_number or 0, \"Greyhound\"), venue=venue, race_number=race_number or 0, start_time=start_time, runners=runners, distance=str(distance) if distance else None, source=self.source_name, available_bets=scrape_available_bets(html_content))\n\n# ----------------------------------------\n# BoyleSportsAdapter\n# ----------------------------------------\nclass BoyleSportsAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"BoyleSports\"\n    BASE_URL: ClassVar[str] = \"https://www.boylesports.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Use CURL_CFFI with chrome120 for better reliability against bot detection\n        return FetchStrategy(primary_engine=BrowserEngine.CURL_CFFI, enable_js=True, stealth_mode=\"camouflage\", timeout=45)\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        kwargs.setdefault(\"impersonate\", \"chrome120\")\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.boylesports.com\", referer=\"https://www.google.com/\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        url = \"/sports/horse-racing\"\n        resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n        if not resp or not resp.text:\n            if resp: self.logger.warning(\"Unexpected status\", status=resp.status, url=url)\n            return None\n        self._save_debug_snapshot(resp.text, f\"boylesports_index_{date}\")\n        return {\"pages\": [{\"url\": url, \"html\": resp.text}], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except Exception: race_date = datetime.now(EASTERN).date()\n        item = raw_data[\"pages\"][0]\n        parser = HTMLParser(item.get(\"html\", \"\"))\n        races: List[Race] = []\n        meeting_groups = parser.css('.meeting-group') or parser.css('.race-meeting') or parser.css('div[class*=\"meeting\"]')\n        for meeting in meeting_groups:\n            tnn = meeting.css_first('.meeting-name') or meeting.css_first('h2') or meeting.css_first('.title')\n            if not tnn: continue\n            trw = clean_text(tnn.text())\n            track_name = normalize_venue_name(trw)\n            if not track_name: continue\n            m_harness = any(kw in trw.lower() for kw in ['harness', 'trot', 'pace', 'standardbred'])\n            is_grey = any(kw in trw.lower() for kw in ['greyhound', 'dog'])\n            race_nodes = meeting.css('.race-time-row') or meeting.css('.race-details') or meeting.css('a[href*=\"/race/\"]')\n            for i, rn in enumerate(race_nodes):\n                txt = clean_text(rn.text())\n                r_harness = m_harness or any(kw in txt.lower() for kw in ['trot', 'pace', 'attele', 'mounted'])\n                tm = re.search(r'(\\d{1,2}:\\d{2})', txt)\n                if not tm: continue\n                fm = re.search(r'\\((\\d+)\\s+runners\\)', txt, re.I)\n                fs = int(fm.group(1)) if fm else 0\n                dm = re.search(r'(\\d+(?:\\.\\d+)?\\s*[kmf]|1\\s*mile)', txt, re.I)\n                dist = dm.group(1) if dm else None\n                try: st = datetime.combine(race_date, datetime.strptime(tm.group(1), \"%H:%M\").time())\n                except Exception: continue\n                runners = [Runner(number=j+1, name=f\"Runner {j+1}\", scratched=False, odds={}) for j in range(fs)]\n                disc = \"Harness\" if r_harness else \"Greyhound\" if is_grey else \"Thoroughbred\"\n                ab = []\n                if 'superfecta' in txt.lower(): ab.append('Superfecta')\n                elif r_harness or ' (us)' in trw.lower():\n                    if fs >= 6: ab.append('Superfecta')\n                races.append(Race(id=f\"boyle_{track_name.lower().replace(' ', '')}_{st:%Y%m%d_%H%M}\", venue=track_name, race_number=i + 1, start_time=st, runners=runners, distance=dist, source=self.source_name, discipline=disc, available_bets=ab))\n        return races\n\n\n# ----------------------------------------\n# SportingLifeAdapter\n# ----------------------------------------\nclass SportingLifeAdapter(JSONParsingMixin, BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"SportingLife\"\n    BASE_URL: ClassVar[str] = \"https://www.sportinglife.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX, enable_js=False, stealth_mode=\"camouflage\", timeout=30)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.sportinglife.com\", referer=\"https://www.sportinglife.com/racing/racecards\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        index_url = f\"/racing/racecards/{date}/\" if date else \"/racing/racecards/\"\n        resp = await self.make_request(\"GET\", index_url, headers=self._get_headers(), follow_redirects=True)\n        if not resp or not resp.text:\n            if resp: self.logger.warning(\"Unexpected status\", status=resp.status, url=index_url)\n            raise AdapterHttpError(self.source_name, getattr(resp, 'status', 500), index_url)\n        self._save_debug_snapshot(resp.text, f\"sportinglife_index_{date}\")\n        parser = HTMLParser(resp.text)\n        metadata = self._extract_race_metadata(parser, date)\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"SportingLife Index Parsing\", url=index_url)\n            return None\n        pages = await self._fetch_race_pages_concurrent(metadata, self._get_headers(), semaphore_limit=8)\n        return {\"pages\": pages, \"date\": date}\n\n    def _extract_race_metadata(self, parser: HTMLParser, date_str: str) -> List[Dict[str, Any]]:\n        meta: List[Dict[str, Any]] = []\n        data = self._parse_json_from_script(parser, \"script#__NEXT_DATA__\", context=\"SportingLife Index\")\n\n        try:\n            target_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        if data:\n            for meeting in data.get(\"props\", {}).get(\"pageProps\", {}).get(\"meetings\", []):\n                # Broaden window to capture multiple races (Memory Directive Fix)\n                races = meeting.get(\"races\", [])\n                for i, race in enumerate(races):\n                    r_time_str = race.get(\"time\") # Usually HH:MM\n                    if r_time_str:\n                        try:\n                            rt = datetime.strptime(r_time_str, \"%H:%M\").replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n\n                            if url := race.get(\"racecard_url\"):\n                                meta.append({\"url\": url, \"race_number\": i + 1})\n                        except Exception: pass\n        if not meta:\n            meetings = parser.css('section[class^=\"MeetingSummary\"]') or parser.css(\".meeting-summary\")\n            for meeting in meetings:\n                # In HTML fallback, just take the first upcoming link we find\n                for link in meeting.css('a[href*=\"/racecard/\"]'):\n                    if url := link.attributes.get(\"href\"):\n                        # Try to see if time is in link text\n                        txt = node_text(link)\n                        if re.match(r\"\\d{1,2}:\\d{2}\", txt):\n                            try:\n                                rt = datetime.strptime(txt, \"%H:%M\").replace(\n                                    year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                                )\n                                # Skip if in past (Today only)\n                                if target_date == now_site.date() and rt < now_site - timedelta(minutes=5):\n                                    continue\n                            except Exception: pass\n\n                        meta.append({\"url\": url, \"race_number\": 1})\n                        break\n        return meta\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except Exception: return []\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            html_content = item.get(\"html\")\n            if not html_content: continue\n            try:\n                parser = HTMLParser(html_content)\n                race = self._parse_from_next_data(parser, race_date, item.get(\"race_number\"), html_content)\n                if not race: race = self._parse_from_html(parser, race_date, item.get(\"race_number\"), html_content)\n                if race: races.append(race)\n            except Exception: pass\n        return races\n\n    def _parse_from_next_data(self, parser: HTMLParser, race_date: date, race_number_fallback: Optional[int], html_content: str) -> Optional[Race]:\n        data = self._parse_json_from_script(parser, \"script#__NEXT_DATA__\", context=\"SportingLife Race\")\n        if not data: return None\n        race_info = data.get(\"props\", {}).get(\"pageProps\", {}).get(\"race\")\n        if not race_info: return None\n        summary = race_info.get(\"race_summary\") or {}\n        track_name = normalize_venue_name(race_info.get(\"meeting_name\") or summary.get(\"course_name\") or \"Unknown\")\n        rt = race_info.get(\"time\") or summary.get(\"time\") or race_info.get(\"off_time\") or race_info.get(\"start_time\")\n        if not rt:\n            def f(o):\n                if isinstance(o, str) and re.match(r\"^\\d{1,2}:\\d{2}$\", o): return o\n                if isinstance(o, dict):\n                    for v in o.values():\n                        if t := f(v): return t\n                if isinstance(o, list):\n                    for v in o:\n                        if t := f(v): return t\n                return None\n            rt = f(race_info)\n        if not rt: return None\n        try: start_time = datetime.combine(race_date, datetime.strptime(rt, \"%H:%M\").time())\n        except Exception: return None\n        runners = []\n        for rd in (race_info.get(\"runners\") or race_info.get(\"rides\") or []):\n            name = clean_text(rd.get(\"horse_name\") or rd.get(\"horse\", {}).get(\"name\", \"\"))\n            if not name: continue\n            num = rd.get(\"saddle_cloth_number\") or rd.get(\"cloth_number\") or 0\n            wo = parse_odds_to_decimal(rd.get(\"betting\", {}).get(\"current_odds\") or rd.get(\"betting\", {}).get(\"current_price\") or rd.get(\"forecast_price\") or rd.get(\"forecast_odds\") or rd.get(\"betting_forecast_price\") or rd.get(\"odds\") or rd.get(\"bookmakerOdds\") or \"\")\n            odds_data = {}\n            if ov := create_odds_data(self.source_name, wo): odds_data[self.source_name] = ov\n            runners.append(Runner(number=num, name=name, scratched=rd.get(\"is_non_runner\") or rd.get(\"ride_status\") == \"NON_RUNNER\", odds=odds_data, win_odds=wo))\n        if not runners: return None\n        return Race(id=generate_race_id(\"sl\", track_name or \"Unknown\", start_time, race_info.get(\"race_number\") or race_number_fallback or 1), venue=track_name or \"Unknown\", race_number=race_info.get(\"race_number\") or race_number_fallback or 1, start_time=start_time, runners=runners, distance=summary.get(\"distance\") or race_info.get(\"distance\"), source=self.source_name, discipline=\"Thoroughbred\", available_bets=scrape_available_bets(html_content))\n\n    def _parse_from_html(self, parser: HTMLParser, race_date: date, race_number_fallback: Optional[int], html_content: str) -> Optional[Race]:\n        h1 = parser.css_first('h1[class*=\"RacingRacecardHeader__Title\"]')\n        if not h1: return None\n        ht = clean_text(h1.text())\n        if not ht: return None\n        parts = ht.split()\n        if not parts: return None\n        try: start_time = datetime.combine(race_date, datetime.strptime(parts[0], \"%H:%M\").time())\n        except Exception: return None\n        track_name = normalize_venue_name(\" \".join(parts[1:]))\n        runners = []\n        for row in parser.css('div[class*=\"RunnerCard\"]'):\n            try:\n                nn = row.css_first('a[href*=\"/racing/profiles/horse/\"]')\n                if not nn: continue\n                name = clean_text(nn.text()).splitlines()[0].strip()\n                num_node = row.css_first('span[class*=\"SaddleCloth__Number\"]')\n                number = int(\"\".join(filter(str.isdigit, clean_text(num_node.text())))) if num_node else 0\n                on = row.css_first('span[class*=\"Odds__Price\"]')\n                wo = parse_odds_to_decimal(clean_text(on.text()) if on else \"\")\n\n                # Advanced heuristic fallback\n                if wo is None:\n                    wo = SmartOddsExtractor.extract_from_node(row)\n\n                od = {}\n                if ov := create_odds_data(self.source_name, wo): od[self.source_name] = ov\n                runners.append(Runner(number=number, name=name, odds=od, win_odds=wo))\n            except Exception: continue\n        if not runners: return None\n        dn = parser.css_first('span[class*=\"RacecardHeader__Distance\"]') or parser.css_first(\".race-distance\")\n        return Race(id=generate_race_id(\"sl\", track_name or \"Unknown\", start_time, race_number_fallback or 1), venue=track_name or \"Unknown\", race_number=race_number_fallback or 1, start_time=start_time, runners=runners, distance=clean_text(dn.text()) if dn else None, source=self.source_name, available_bets=scrape_available_bets(html_content))\n\n# ----------------------------------------\n# SkySportsAdapter\n# ----------------------------------------\nclass SkySportsAdapter(JSONParsingMixin, BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"SkySports\"\n    BASE_URL: ClassVar[str] = \"https://www.skysports.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX, enable_js=False, stealth_mode=\"fast\", timeout=30)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.skysports.com\", referer=\"https://www.skysports.com/racing\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        dt = datetime.strptime(date, \"%Y-%m-%d\")\n        index_url = f\"/racing/racecards/{dt.strftime('%d-%m-%Y')}\"\n        resp = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not resp or not resp.text:\n            if resp: self.logger.warning(\"Unexpected status\", status=resp.status, url=index_url)\n            raise AdapterHttpError(self.source_name, getattr(resp, 'status', 500), index_url)\n        self._save_debug_snapshot(resp.text, f\"skysports_index_{date}\")\n        parser = HTMLParser(resp.text)\n        metadata = []\n\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        meetings = parser.css(\".sdc-site-concertina-block\") or parser.css(\".page-details__section\") or parser.css(\".racing-meetings__meeting\")\n        for meeting in meetings:\n            hn = meeting.css_first(\".sdc-site-concertina-block__title\") or meeting.css_first(\".racing-meetings__meeting-title\")\n            if not hn: continue\n            vr = clean_text(hn.text()) or \"\"\n            if \"ABD:\" in vr: continue\n\n            # Updated Sky Sports event discovery logic\n            events = meeting.css(\".sdc-site-racing-meetings__event\") or meeting.css(\".racing-meetings__event\")\n            if events:\n                for i, event in enumerate(events):\n                    tn = event.css_first(\".sdc-site-racing-meetings__event-time\") or event.css_first(\".racing-meetings__event-time\")\n                    ln = event.css_first(\".sdc-site-racing-meetings__event-link\") or event.css_first(\".racing-meetings__event-link\")\n                    if tn and ln:\n                        txt, h = clean_text(tn.text()), ln.attributes.get(\"href\")\n                        if h and re.match(r\"\\d{1,2}:\\d{2}\", txt):\n                            try:\n                                rt = datetime.strptime(txt, \"%H:%M\").replace(\n                                    year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                                )\n                                diff = (rt - now_site).total_seconds() / 60\n                                if not (-45 < diff <= 1080):\n                                    continue\n                                metadata.append({\"url\": h, \"venue_raw\": vr, \"race_number\": i + 1})\n                            except Exception: pass\n            else:\n                # Fallback to older anchor-based discovery\n                for i, link in enumerate(meeting.css('a[href*=\"/racecards/\"]')):\n                    if h := link.attributes.get(\"href\"):\n                        txt = node_text(link)\n                        if re.match(r\"\\d{1,2}:\\d{2}\", txt):\n                            try:\n                                rt = datetime.strptime(txt, \"%H:%M\").replace(\n                                    year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                                )\n                                diff = (rt - now_site).total_seconds() / 60\n                                if not (-45 < diff <= 1080):\n                                    continue\n                                metadata.append({\"url\": h, \"venue_raw\": vr, \"race_number\": i + 1})\n                            except Exception: pass\n\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"SkySports Index Parsing\", url=index_url)\n            return None\n        pages = await self._fetch_race_pages_concurrent(metadata, self._get_headers(), semaphore_limit=10)\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data.get(\"date\", \"\"), \"%Y-%m-%d\").date()\n        except Exception: race_date = datetime.now(EASTERN).date()\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            html_content = item.get(\"html\")\n            if not html_content: continue\n            parser = HTMLParser(html_content)\n            h = parser.css_first(\".sdc-site-racing-header__name\")\n            if not h: continue\n            ht = clean_text(h.text()) or \"\"\n            m = re.match(r\"(\\d{1,2}:\\d{2})\\s+(.+)\", ht)\n            if not m:\n                tn, cn = parser.css_first(\".sdc-site-racing-header__time\"), parser.css_first(\".sdc-site-racing-header__course\")\n                if tn and cn: rts, tnr = clean_text(tn.text()) or \"\", clean_text(cn.text()) or \"\"\n                else: continue\n            else: rts, tnr = m.group(1), m.group(2)\n            track_name = normalize_venue_name(tnr)\n            if not track_name: continue\n            try: start_time = datetime.combine(race_date, datetime.strptime(rts, \"%H:%M\").time())\n            except Exception: continue\n            dist = None\n            for d in parser.css(\".sdc-site-racing-header__detail-item\"):\n                dt = clean_text(d.text()) or \"\"\n                if \"Distance:\" in dt: dist = dt.replace(\"Distance:\", \"\").strip(); break\n            runners = []\n            for i, node in enumerate(parser.css(\".sdc-site-racing-card__item\")):\n                nn = node.css_first(\".sdc-site-racing-card__name a\")\n                if not nn: continue\n                name = clean_text(nn.text())\n                if not name: continue\n                nnode = node.css_first(\".sdc-site-racing-card__number strong\")\n                number = i + 1\n                if nnode:\n                    nt = clean_text(nnode.text())\n                    if nt:\n                        try: number = int(nt)\n                        except Exception: pass\n                onode = node.css_first(\".sdc-site-racing-card__betting-odds\")\n                wo = parse_odds_to_decimal(clean_text(onode.text()) if onode else \"\")\n\n                # Advanced heuristic fallback\n                if wo is None:\n                    wo = SmartOddsExtractor.extract_from_node(node)\n\n                ntxt = clean_text(node.text()) or \"\"\n                scratched = \"NR\" in ntxt or \"Non-runner\" in ntxt\n                od = {}\n                if ov := create_odds_data(self.source_name, wo): od[self.source_name] = ov\n                runners.append(Runner(number=number, name=name, scratched=scratched, odds=od, win_odds=wo))\n            if not runners: continue\n            disc = detect_discipline(html_content)\n            ab = scrape_available_bets(html_content)\n            if not ab and (disc == \"Harness\" or \"(us)\" in tnr.lower()) and len([r for r in runners if not r.scratched]) >= 6: ab.append(\"Superfecta\")\n            races.append(Race(id=generate_race_id(\"sky\", track_name, start_time, item.get(\"race_number\", 0), disc), venue=track_name, race_number=item.get(\"race_number\", 0), start_time=start_time, runners=runners, distance=dist, discipline=disc, source=self.source_name, available_bets=ab))\n        return races\n\n# ----------------------------------------\n# RacingPostB2BAdapter\n# ----------------------------------------\nclass RacingPostB2BAdapter(BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"RacingPostB2B\"\n    BASE_URL: ClassVar[str] = \"https://backend-us-racecards.widget.rpb2b.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config, enable_cache=True, cache_ttl=300.0, rate_limit=5.0)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX, enable_js=False, max_retries=3, timeout=20)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        endpoint = f\"/v2/racecards/daily/{date}\"\n        resp = await self.make_request(\"GET\", endpoint)\n        if not resp: return None\n        try: data = resp.json()\n        except Exception: return None\n        if not isinstance(data, list): return None\n        return {\"venues\": data, \"date\": date, \"fetched_at\": datetime.now(EASTERN).isoformat()}\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        if not raw_data or not raw_data.get(\"venues\"): return []\n        races: List[Race] = []\n        for vd in raw_data[\"venues\"]:\n            if vd.get(\"isAbandoned\"): continue\n            vn, cc, rd = vd.get(\"name\", \"Unknown\"), vd.get(\"countryCode\", \"USA\"), vd.get(\"races\", [])\n            for r in rd:\n                if r.get(\"raceStatusCode\") == \"ABD\": continue\n                parsed = self._parse_single_race(r, vn, cc)\n                if parsed: races.append(parsed)\n        return races\n\n    def _parse_single_race(self, rd: Dict[str, Any], vn: str, cc: str) -> Optional[Race]:\n        rid, rnum, dts, nr = rd.get(\"id\"), rd.get(\"raceNumber\"), rd.get(\"datetimeUtc\"), rd.get(\"numberOfRunners\", 0)\n        if not all([rid, rnum, dts]): return None\n        try: st = datetime.fromisoformat(dts.replace(\"Z\", \"+00:00\"))\n        except Exception: return None\n        # Only return race if we have real runners (avoid placeholder generic runners)\n        runners = []\n        if runners_raw := rd.get(\"runners\"):\n            for i, run_data in enumerate(runners_raw):\n                name = run_data.get(\"name\") or f\"Runner {i+1}\"\n                num = run_data.get(\"number\") or i + 1\n                runners.append(Runner(number=num, name=name))\n\n        if not runners:\n            return None\n\n        return Race(discipline=\"Thoroughbred\", id=f\"rpb2b_{rid.replace('-', '')[:16]}\", venue=normalize_venue_name(vn), race_number=rnum, start_time=st, runners=runners, source=self.source_name, metadata={\"original_race_id\": rid, \"country_code\": cc, \"num_runners\": nr})\n\n\n# ----------------------------------------\n# StandardbredCanadaAdapter\n# ----------------------------------------\nclass StandardbredCanadaAdapter(BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"StandardbredCanada\"\n    BASE_URL: ClassVar[str] = \"https://standardbredcanada.ca\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        self._semaphore = asyncio.Semaphore(3)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Use CURL_CFFI for robust HTTPS and connection handling\n        return FetchStrategy(primary_engine=BrowserEngine.CURL_CFFI, enable_js=False, stealth_mode=\"fast\", timeout=45)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"standardbredcanada.ca\", referer=\"https://standardbredcanada.ca/racing\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        dt = datetime.strptime(date, \"%Y-%m-%d\")\n        date_label = dt.strftime(f\"%A %b {dt.day}, %Y\")\n        date_short = dt.strftime(\"%m%d\") # e.g. 0208\n\n        index_html = None\n\n        # 1. Try browser-based fetch if available\n        try:\n            from playwright.async_api import async_playwright\n            async with async_playwright() as p:\n                browser = await p.chromium.launch(headless=True)\n                page = await browser.new_page()\n                try:\n                    await page.goto(f\"{self.base_url}/entries\", wait_until=\"networkidle\")\n                    await page.evaluate(\"() => { document.querySelectorAll('details').forEach(d => d.open = true); }\")\n                    try: await page.select_option(\"#edit-entries-track\", label=\"View All Tracks\")\n                    except Exception: pass\n                    try: await page.select_option(\"#edit-entries-date\", label=date_label)\n                    except Exception: pass\n                    try: await page.click(\"#edit-custom-submit-entries\", force=True, timeout=5000)\n                    except Exception: pass\n                    try: await page.wait_for_selector(\"#entries-results-container a[href*='/entries/']\", timeout=10000)\n                    except Exception: pass\n                    index_html = await page.content()\n                finally:\n                    await page.close()\n                    await browser.close()\n        except Exception as e:\n            self.logger.debug(\"Playwright index fetch failed, trying fallback\", error=str(e))\n\n        # 2. Fallback: Try to guess the data URL pattern if index fetch failed\n        if not index_html:\n            # Common tracks and their codes (heuristic)\n            tracks = [\n                (\"Western Fair\", f\"e{date_short}lonn.dat\"),\n                (\"Mohawk\", f\"e{date_short}wbsbsn.dat\"),\n                (\"Flamboro\", f\"e{date_short}flmn.dat\"),\n                (\"Rideau\", f\"e{date_short}ridcn.dat\"),\n            ]\n            metadata = []\n            for track_name, filename in tracks:\n                url = f\"/racing/entries/data/{filename}\"\n                metadata.append({\"url\": url, \"venue\": track_name, \"finalized\": True})\n\n            pages = await self._fetch_race_pages_concurrent(metadata, self._get_headers())\n            return {\"pages\": pages, \"date\": date}\n\n        if not index_html:\n            self.logger.warning(\"No index HTML found\", context=\"StandardbredCanada Index Fetch\")\n            return None\n        self._save_debug_snapshot(index_html, f\"sc_index_{date}\")\n        parser = HTMLParser(index_html)\n        metadata = []\n        for container in parser.css(\"#entries-results-container .racing-results-ex-wrap > div\"):\n            tnn = container.css_first(\"h4.track-name\")\n            if not tnn: continue\n            tn = clean_text(tnn.text()) or \"\"\n            isf = \"*\" in tn or \"*\" in (clean_text(container.text()) or \"\")\n            for link in container.css('a[href*=\"/entries/\"]'):\n                if u := link.attributes.get(\"href\"):\n                    metadata.append({\"url\": u, \"venue\": tn.replace(\"*\", \"\").strip(), \"finalized\": isf})\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"StandardbredCanada Index Parsing\")\n            return None\n        pages = await self._fetch_race_pages_concurrent(metadata, self._get_headers(), semaphore_limit=3)\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        try: race_date = datetime.strptime(raw_data.get(\"date\", \"\"), \"%Y-%m-%d\").date()\n        except Exception: race_date = datetime.now(EASTERN).date()\n        races: List[Race] = []\n        for item in raw_data[\"pages\"]:\n            html_content = item.get(\"html\")\n            if not html_content or (\"Final Changes Made\" not in html_content and not item.get(\"finalized\")): continue\n            track_name = normalize_venue_name(item[\"venue\"])\n            for pre in HTMLParser(html_content).css(\"pre\"):\n                text = pre.text()\n                race_chunks = re.split(r\"(\\d+)\\s+--\\s+\", text)\n                for i in range(1, len(race_chunks), 2):\n                    try:\n                        r = self._parse_single_race(race_chunks[i+1], int(race_chunks[i]), race_date, track_name)\n                        if r: races.append(r)\n                    except Exception: continue\n        return races\n\n    def _parse_single_race(self, content: str, race_num: int, race_date: date, track_name: str) -> Optional[Race]:\n        tm = re.search(r\"Post\\s+Time:\\s*(\\d{1,2}:\\d{2}\\s*[APM]{2})\", content, re.I)\n        st = None\n        if tm:\n            try: st = datetime.combine(race_date, datetime.strptime(tm.group(1), \"%I:%M %p\").time())\n            except Exception: pass\n        if not st: st = datetime.combine(race_date, datetime.min.time())\n        ab = scrape_available_bets(content)\n        dist = \"1 Mile\"\n        dm = re.search(r\"(\\d+(?:/\\d+)?\\s+(?:MILE|MILES|KM|F))\", content, re.I)\n        if dm: dist = dm.group(1)\n        runners = []\n        for line in content.split(\"\\n\"):\n            m = re.search(r\"^\\s*(\\d+)\\s+([^(]+)\", line)\n            if m:\n                num, name = int(m.group(1)), m.group(2).strip()\n                name = re.sub(r\"\\(L\\)$|\\(L\\)\\s+\", \"\", name).strip()\n                sc = \"SCR\" in line or \"Scratched\" in line\n                # Try smarter odds extraction from the line\n                wo = SmartOddsExtractor.extract_from_text(line)\n                if wo is None:\n                    om = re.search(r\"(\\d+-\\d+|[0-9.]+)\\s*$\", line)\n                    if om: wo = parse_odds_to_decimal(om.group(1))\n\n                odds_data = {}\n                if ov := create_odds_data(self.source_name, wo): odds_data[self.source_name] = ov\n                runners.append(Runner(number=num, name=name, scratched=sc, odds=odds_data, win_odds=wo))\n        if not runners: return None\n        return Race(discipline=\"Harness\", id=generate_race_id(\"sc\", track_name, st, race_num, \"Harness\"), venue=track_name, race_number=race_num, start_time=st, runners=runners, distance=dist, source=self.source_name, available_bets=ab)\n\n# ----------------------------------------\n# TabAdapter\n# ----------------------------------------\nclass TabAdapter(BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"TAB\"\n    # Note: api.tab.com.au often has DNS resolution issues in some environments.\n    # api.beta.tab.com.au is more reliable.\n    BASE_URL: ClassVar[str] = \"https://api.beta.tab.com.au/v1/tab-info-service/racing\"\n    BASE_URL_STABLE: ClassVar[str] = \"https://api.tab.com.au/v1/tab-info-service/racing\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config, rate_limit=2.0)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Switch to CURL_CFFI for TAB API to avoid DNS and TLS issues common in cloud environments\n        return FetchStrategy(primary_engine=BrowserEngine.CURL_CFFI, enable_js=False, stealth_mode=\"fast\", timeout=45)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        url = f\"{self.base_url}/dates/{date}/meetings\"\n        resp = await self.make_request(\"GET\", url, headers={\"Accept\": \"application/json\", \"User-Agent\": CHROME_USER_AGENT})\n\n        if not resp or resp.status != 200:\n            self.logger.info(\"Falling back to STABLE TAB API\")\n            url = f\"{self.BASE_URL_STABLE}/dates/{date}/meetings\"\n            resp = await self.make_request(\"GET\", url, headers={\"Accept\": \"application/json\", \"User-Agent\": CHROME_USER_AGENT})\n\n        if not resp: return None\n        try: data = resp.json() if hasattr(resp, \"json\") else json.loads(resp.text)\n        except Exception: return None\n        if not data or \"meetings\" not in data: return None\n\n        # TAB meetings often only have race headers. We need to fetch each meeting's details\n        # to get runners and odds.\n        all_meetings = []\n        for m in data[\"meetings\"]:\n            try:\n                vn = m.get(\"meetingName\")\n                mt = m.get(\"meetingType\")\n                if vn and mt:\n                    # Endpoint for meeting details (includes races and runners)\n                    m_url = f\"{self.base_url}/dates/{date}/meetings/{mt}/{vn}?jurisdiction=VIC\"\n                    m_resp = await self.make_request(\"GET\", m_url, headers={\"Accept\": \"application/json\", \"User-Agent\": CHROME_USER_AGENT})\n                    if m_resp:\n                        try:\n                            m_data = m_resp.json() if hasattr(m_resp, \"json\") else json.loads(m_resp.text)\n                            if m_data:\n                                all_meetings.append(m_data)\n                                continue\n                        except Exception: pass\n                # Fallback to the summary data if detail fetch fails\n                all_meetings.append(m)\n            except Exception:\n                all_meetings.append(m)\n\n        return {\"meetings\": all_meetings, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or \"meetings\" not in raw_data: return []\n        races: List[Race] = []\n        for m in raw_data[\"meetings\"]:\n            vn = normalize_venue_name(m.get(\"meetingName\"))\n            mt = m.get(\"meetingType\", \"R\")\n            disc = {\"R\": \"Thoroughbred\", \"H\": \"Harness\", \"G\": \"Greyhound\"}.get(mt, \"Thoroughbred\")\n\n            for rd in m.get(\"races\", []):\n                rn = rd.get(\"raceNumber\")\n                rst = rd.get(\"raceStartTime\")\n                if not rst or not rn: continue\n\n                try: st = datetime.fromisoformat(rst.replace(\"Z\", \"+00:00\"))\n                except Exception: continue\n\n                runners = []\n                # If detail data was fetched, extract runners\n                for runner_data in rd.get(\"runners\", []):\n                    name = runner_data.get(\"runnerName\", \"Unknown\")\n                    num = runner_data.get(\"runnerNumber\")\n\n                    # Try to get win odds\n                    win_odds = None\n                    fixed_odds = runner_data.get(\"fixedOdds\", {})\n                    if fixed_odds:\n                        win_odds = fixed_odds.get(\"returnWin\") or fixed_odds.get(\"win\")\n\n                    odds_dict = {}\n                    if win_odds:\n                        if ov := create_odds_data(self.source_name, win_odds):\n                            odds_dict[self.source_name] = ov\n\n                    runners.append(Runner(\n                        name=name,\n                        number=num,\n                        win_odds=win_odds,\n                        odds=odds_dict,\n                        scratched=runner_data.get(\"scratched\", False)\n                    ))\n\n                races.append(Race(\n                    id=generate_race_id(\"tab\", vn, st, rn, disc),\n                    venue=vn,\n                    race_number=rn,\n                    start_time=st,\n                    runners=runners,\n                    discipline=disc,\n                    source=self.source_name,\n                    available_bets=scrape_available_bets(str(rd))\n                ))\n        return races\n\n# ----------------------------------------\n# BetfairDataScientistAdapter\n# ----------------------------------------\nclass BetfairDataScientistAdapter(JSONParsingMixin, BaseAdapterV3):\n    ADAPTER_NAME: ClassVar[str] = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str = \"Ratings\", url: str = \"https://www.betfair.com.au/hub/ratings/model/horse-racing/\", config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=f\"{self.ADAPTER_NAME}_{model_name}\", base_url=url, config=config)\n        self.model_name = model_name\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[StringIO]:\n        endpoint = f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n        resp = await self.make_request(\"GET\", endpoint)\n        return StringIO(resp.text) if resp and resp.text else None\n\n    def _parse_races(self, raw_data: Optional[StringIO]) -> List[Race]:\n        if not raw_data: return []\n        try:\n            df = pd.read_csv(raw_data)\n            if df.empty: return []\n            df = df.rename(columns={\"meetings.races.bfExchangeMarketId\": \"market_id\", \"meetings.name\": \"meeting_name\", \"meetings.races.raceNumber\": \"race_number\", \"meetings.races.runners.runnerName\": \"runner_name\", \"meetings.races.runners.clothNumber\": \"saddle_cloth\", \"meetings.races.runners.ratedPrice\": \"rated_price\"})\n            races: List[Race] = []\n            for mid, group in df.groupby(\"market_id\"):\n                ri = group.iloc[0]\n                runners = []\n                for _, row in group.iterrows():\n                    rp, od = row.get(\"rated_price\"), {}\n                    if pd.notna(rp):\n                        if ov := create_odds_data(self.source_name, float(rp)): od[self.source_name] = ov\n                    runners.append(Runner(name=str(row.get(\"runner_name\", \"Unknown\")), number=int(row.get(\"saddle_cloth\", 0)), odds=od))\n\n                vn = normalize_venue_name(str(ri.get(\"meeting_name\", \"\")))\n\n                # Try to find a start time in the CSV\n                start_time = datetime.now(EASTERN)\n                for col in [\"meetings.races.startTime\", \"startTime\", \"start_time\", \"time\"]:\n                    if col in ri and pd.notna(ri[col]):\n                        try:\n                            # Assume UTC and convert to Eastern if it looks like ISO\n                            st_val = str(ri[col])\n                            if \"T\" in st_val:\n                                start_time = to_eastern(datetime.fromisoformat(st_val.replace(\"Z\", \"+00:00\")))\n                            break\n                        except Exception: pass\n\n                races.append(Race(id=str(mid), venue=vn, race_number=int(ri.get(\"race_number\", 0)), start_time=start_time, runners=runners, source=self.source_name, discipline=\"Thoroughbred\"))\n            return races\n        except Exception: return []\n\n# ----------------------------------------\n# EquibaseAdapter\n# ----------------------------------------\nclass EquibaseAdapter(BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"Equibase\"\n    BASE_URL: ClassVar[str] = \"https://www.equibase.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Equibase uses Instart Logic / Imperva; PLAYWRIGHT_LEGACY with network_idle is robust\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT_LEGACY,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=120,\n            network_idle=True\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        # Force chrome120 for Equibase as it's the most reliable impersonation for Imperva/Cloudflare\n        kwargs.setdefault(\"impersonate\", \"chrome120\")\n        # Let SmartFetcher/curl_cffi handle headers mostly, but provide minimal essentials if not already set\n        h = kwargs.get(\"headers\", {})\n        if \"Referer\" not in h: h[\"Referer\"] = \"https://www.equibase.com/\"\n        kwargs[\"headers\"] = h\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> Dict[str, str]:\n        return self._get_browser_headers(host=\"www.equibase.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        dt = datetime.strptime(date, \"%Y-%m-%d\")\n        date_str = dt.strftime(\"%m%d%y\")\n\n        # Try different possible index URLs\n        index_urls = [\n            f\"/static/entry/index.html?SAP=TN\",\n            f\"/static/entry/index.html\",\n            f\"/entries/{date}\",\n            f\"/entries/index.cfm?date={dt.strftime('%m/%d/%Y')}\",\n        ]\n\n        resp = None\n        for url in index_urls:\n            # Try multiple impersonations to bypass block (Memory Directive Fix)\n            for imp in [\"chrome120\", \"chrome110\", \"safari15_5\"]:\n                try:\n                    resp = await self.make_request(\"GET\", url, impersonate=imp)\n                    if resp and resp.status == 200 and resp.text and len(resp.text) > 1000 and \"Pardon Our Interruption\" not in resp.text:\n                        self.logger.info(\"Found Equibase index\", url=url, impersonate=imp)\n                        break\n                    else:\n                        text_len = len(resp.text) if resp and resp.text else 0\n                        has_pardon = \"Pardon Our Interruption\" in resp.text if resp and resp.text else False\n                        self.logger.debug(\"Equibase candidate blocked or invalid\", url=url, impersonate=imp, len=text_len, has_pardon=has_pardon)\n                        resp = None\n                except Exception as e:\n                    self.logger.debug(\"Equibase request exception\", url=url, impersonate=imp, error=str(e))\n                    resp = None\n            if resp: break\n\n        if not resp or not resp.text or resp.status != 200:\n            if resp: self.logger.warning(\"Unexpected status\", status=resp.status, url=getattr(resp, 'url', 'Unknown'))\n            return None\n\n        self._save_debug_snapshot(resp.text, f\"equibase_index_{date}\")\n        parser, links = HTMLParser(resp.text), []\n\n        # New: Look for links in JSON data within scripts (Common on Equibase)\n        # Handles escaped slashes and different path separators\n        script_json_matches = re.findall(r'\"URL\":\"([^\"]+)\"', resp.text)\n        for url in script_json_matches:\n            # Normalizing backslashes and escaped slashes in found URLs\n            url_norm = url.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n            # Restrict lookahead: ensure link is for the targeted date_str\n            if \"/static/entry/\" in url_norm and (date_str in url_norm or \"RaceCardIndex\" in url_norm):\n                links.append(url_norm)\n\n        for a in parser.css(\"a\"):\n            h = a.attributes.get(\"href\") or \"\"\n            c = a.attributes.get(\"class\") or \"\"\n            txt = node_text(a).lower()\n            # Normalize backslashes (Project fix for Equibase path separators)\n            h_norm = h.replace(\"\\\\\", \"/\")\n\n            # Restrict lookahead: ensure link strictly belongs to targeted date_str (Project Hardening)\n            if \"/static/entry/\" in h_norm and (date_str in h_norm or \"RaceCardIndex\" in h_norm):\n                self.logger.debug(\"Equibase link matched\", href=h_norm)\n                links.append(h_norm)\n            elif \"entry-race-level\" in c and date_str in h_norm:\n                links.append(h_norm)\n            elif (\"race-link\" in c or \"track-link\" in c) and date_str in h_norm:\n                links.append(h_norm)\n            elif \"entries\" in txt and \"/static/entry/\" in h_norm and date_str in h_norm:\n                links.append(h_norm)\n\n        if not links:\n            self.logger.warning(\"No links found\", context=\"Equibase Index Parsing\", date=date)\n            return None\n\n        # Fetch initial set of pages\n        pages = await self._fetch_race_pages_concurrent([{\"url\": l} for l in set(links)], self._get_headers(), semaphore_limit=5)\n\n        all_htmls = []\n        extra_links = []\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        now = now_eastern()\n        for p in pages:\n            html_content = p.get(\"html\")\n            if not html_content: continue\n\n            # If it's an index page for a track, we need to extract individual race links\n            if \"RaceCardIndex\" in p.get(\"url\", \"\"):\n                sub_parser = HTMLParser(html_content)\n                # Only take the \"next\" race link for this track (Memory Directive Fix)\n                track_races = []\n                for a in sub_parser.css(\"a\"):\n                    sh = (a.attributes.get(\"href\") or \"\").replace(\"\\\\\", \"/\")\n                    if \"/static/entry/\" in sh and date_str in sh and \"RaceCardIndex\" not in sh:\n                        # Try to find time in text nearby\n                        time_txt = \"\"\n                        parent = a.parent\n                        if parent:\n                            time_txt = node_text(parent)\n                        track_races.append({\"url\": sh, \"time_txt\": time_txt})\n\n                next_race = None\n                for r in track_races:\n                    # Look for 1:00 PM etc\n                    tm = re.search(r\"(\\d{1,2}:\\d{2}\\s*[APM]{2})\", r[\"time_txt\"], re.I)\n                    if tm:\n                        try:\n                            rt = datetime.strptime(tm.group(1).upper(), \"%I:%M %p\").replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=EASTERN\n                            )\n                            # Skip if in past (Today only)\n                            if target_date == now.date() and rt < now - timedelta(minutes=5):\n                                continue\n                            next_race = r\n                            break\n                        except Exception: pass\n\n                if next_race:\n                    extra_links.append(next_race[\"url\"])\n            else:\n                all_htmls.append(html_content)\n\n        if extra_links:\n            self.logger.info(\"Fetching extra race pages from track index\", count=len(extra_links))\n            extra_pages = await self._fetch_race_pages_concurrent([{\"url\": l} for l in set(extra_links)], self._get_headers(), semaphore_limit=5)\n            all_htmls.extend([p.get(\"html\") for p in extra_pages if p and p.get(\"html\")])\n\n        return {\"pages\": all_htmls, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"): return []\n        ds, races = raw_data.get(\"date\", \"\"), []\n        for html_content in raw_data[\"pages\"]:\n            if not html_content: continue\n            try:\n                p = HTMLParser(html_content)\n                vn = p.css_first(\"div.track-information strong\")\n                rn = p.css_first(\"div.race-information strong\")\n                pt = p.css_first(\"p.post-time span\")\n                if not vn or not rn or not pt: continue\n                venue = clean_text(vn.text())\n                rnum_txt = rn.text().replace(\"Race\", \"\").strip()\n                if not venue or not rnum_txt.isdigit(): continue\n                st = self._parse_post_time(ds, pt.text().strip())\n                ab = scrape_available_bets(html_content)\n                runners = [r for node in p.css(\"table.entries-table tbody tr\") if (r := self._parse_runner(node))]\n                if not runners: continue\n                races.append(Race(id=f\"eqb_{venue.lower().replace(' ', '')}_{ds}_{rnum_txt}\", venue=venue, race_number=int(rnum_txt), start_time=st, runners=runners, source=self.source_name, discipline=\"Thoroughbred\", available_bets=ab))\n            except Exception: continue\n        return races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            cols = node.css(\"td\")\n            if len(cols) < 3: return None\n\n            # P1: Try to find number in first col\n            number = 0\n            num_text = clean_text(cols[0].text())\n            if num_text.isdigit():\n                number = int(num_text)\n\n            # P2: Horse name usually in 3rd col, but can vary\n            name = None\n            for idx in [2, 1, 3]:\n                if len(cols) > idx:\n                    n_text = clean_text(cols[idx].text())\n                    if n_text and not n_text.isdigit() and len(n_text) > 2:\n                        name = n_text\n                        break\n\n            if not name: return None\n\n            sc = \"scratched\" in node.attributes.get(\"class\", \"\").lower() or \"SCR\" in (clean_text(node.text()) or \"\")\n\n            odds, wo = {}, None\n            if not sc:\n                # Odds column can be 9 or 10 (blind indexing fallback)\n                for idx in [9, 8, 10]:\n                    if len(cols) > idx:\n                        o_text = clean_text(cols[idx].text())\n                        if o_text:\n                            wo = parse_odds_to_decimal(o_text)\n                            if wo: break\n\n                if wo is None: wo = SmartOddsExtractor.extract_from_node(node)\n                if od := create_odds_data(self.source_name, wo): odds[self.source_name] = od\n\n            return Runner(number=number, name=name, odds=odds, win_odds=wo, scratched=sc)\n        except Exception as e:\n            self.logger.debug(\"equibase_runner_parse_failed\", error=str(e))\n            return None\n\n    def _parse_post_time(self, ds: str, ts: str) -> datetime:\n        try:\n            parts = ts.replace(\"Post Time:\", \"\").strip().split()\n            if len(parts) >= 2:\n                dt = datetime.strptime(f\"{ds} {parts[0]} {parts[1]}\", \"%Y-%m-%d %I:%M %p\")\n                return dt.replace(tzinfo=EASTERN)\n        except Exception: pass\n        # Fallback to noon UTC for the given date if time parsing fails\n        try:\n            dt = datetime.strptime(ds, \"%Y-%m-%d\")\n            return dt.replace(hour=12, minute=0, tzinfo=EASTERN)\n        except Exception:\n            return datetime.now(EASTERN)\n\n# ----------------------------------------\n# TwinSpiresAdapter\n# ----------------------------------------\nclass TwinSpiresAdapter(JSONParsingMixin, DebugMixin, BaseAdapterV3):\n    SOURCE_NAME: ClassVar[str] = \"TwinSpires\"\n    BASE_URL: ClassVar[str] = \"https://www.twinspires.com\"\n\n    RACE_CONTAINER_SELECTORS: ClassVar[List[str]] = ['div[class*=\"RaceCard\"]', 'div[class*=\"race-card\"]', 'div[data-testid*=\"race\"]', 'div[data-race-id]', 'section[class*=\"race\"]', 'article[class*=\"race\"]', \".race-container\", \"[data-race]\", 'div[class*=\"card\"][class*=\"race\" i]', 'div[class*=\"event\"]']\n    TRACK_NAME_SELECTORS: ClassVar[List[str]] = ['[class*=\"track-name\"]', '[class*=\"trackName\"]', '[data-track-name]', 'h2[class*=\"track\"]', 'h3[class*=\"track\"]', \".track-title\", '[class*=\"venue\"]']\n    RACE_NUMBER_SELECTORS: ClassVar[List[str]] = ['[class*=\"race-number\"]', '[class*=\"raceNumber\"]', '[class*=\"race-num\"]', '[data-race-number]', 'span[class*=\"number\"]']\n    POST_TIME_SELECTORS: ClassVar[List[str]] = [\"time[datetime]\", '[class*=\"post-time\"]', '[class*=\"postTime\"]', '[class*=\"mtp\"]', \"[data-post-time]\", '[class*=\"race-time\"]']\n    RUNNER_ROW_SELECTORS: ClassVar[List[str]] = ['tr[class*=\"runner\"]', 'div[class*=\"runner\"]', 'li[class*=\"runner\"]', \"[data-runner-id]\", 'div[class*=\"horse-row\"]', 'tr[class*=\"horse\"]', 'div[class*=\"entry\"]', \".runner-row\", \".horse-entry\"]\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config, enable_cache=True, cache_ttl=180.0, rate_limit=1.5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # TwinSpires is heavily JS-dependent; Playwright is essential\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            network_idle=True\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        # Force chrome120 for TwinSpires to bypass basic bot checks\n        kwargs.setdefault(\"impersonate\", \"chrome120\")\n        # Provide common browser-like headers for TwinSpires\n        h = kwargs.get(\"headers\", {})\n        if \"Referer\" not in h: h[\"Referer\"] = \"https://www.google.com/\"\n        kwargs[\"headers\"] = h\n        return await super().make_request(method, url, **kwargs)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        ard = []\n        last_err = None\n\n        # Respect region from config if provided\n        target_region = self.config.get(\"region\") # \"USA\", \"INT\", or None for both\n\n        async def fetch_disc(disc, region=\"USA\"):\n            suffix = \"\" if region == \"USA\" else \"?region=INT\"\n            # Try date-specific URL first, fallback to todays-races\n            # TwinSpires uses YYYY-MM-DD for races URL\n            if date == datetime.now(EASTERN).strftime(\"%Y-%m-%d\"):\n                url = f\"{self.BASE_URL}/bet/todays-races/{disc}{suffix}\"\n            else:\n                url = f\"{self.BASE_URL}/bet/races/{date}/{disc}{suffix}\"\n            try:\n                resp = await self.make_request(\"GET\", url, network_idle=True, wait_selector='div[class*=\"race\"], [class*=\"RaceCard\"], [class*=\"track\"]')\n                if resp and resp.status == 200:\n                    self._save_debug_snapshot(resp.text, f\"ts_{disc}_{region}_{date}\")\n                    dr = self._extract_races_from_page(resp, date)\n                    for r in dr: r[\"assigned_discipline\"] = disc.capitalize()\n                    return dr\n            except Exception as e:\n                self.logger.error(\"TwinSpires fetch failed\", discipline=disc, region=region, error=str(e))\n            return []\n\n        # Fetch both USA and International for all disciplines\n        tasks = []\n        for d in [\"thoroughbred\", \"harness\", \"greyhound\"]:\n            if target_region in [None, \"USA\"]:\n                tasks.append(fetch_disc(d, \"USA\"))\n            if target_region in [None, \"INT\"]:\n                tasks.append(fetch_disc(d, \"INT\"))\n        results = await asyncio.gather(*tasks)\n        for r_list in results:\n            ard.extend(r_list)\n\n        if not ard:\n            try:\n                resp = await self.make_request(\"GET\", f\"{self.BASE_URL}/bet/todays-races/time\", network_idle=True)\n                if resp and resp.status == 200: ard = self._extract_races_from_page(resp, date)\n            except Exception as e: last_err = last_err or e\n        if not ard and last_err: raise last_err\n        return {\"races\": ard, \"date\": date, \"source\": self.source_name} if ard else None\n\n    def _extract_races_from_page(self, resp, date: str) -> List[Dict[str, Any]]:\n        if Selector is not None:\n            page = Selector(resp.text)\n        else:\n            self.logger.warning(\"Scrapling Selector not available, falling back to selectolax\")\n            page = HTMLParser(resp.text)\n\n        rd = []\n        relems, used = [], None\n        for s in self.RACE_CONTAINER_SELECTORS:\n            try:\n                el = page.css(s)\n                if el:\n                    relems, used = el, s\n                    break\n            except Exception: continue\n\n        if not relems:\n            return [{\"html\": resp.text, \"selector\": page, \"track\": \"Unknown\", \"race_number\": 0, \"date\": date, \"full_page\": True}]\n\n        track_counters = defaultdict(int)\n        last_track = \"Unknown\"\n\n        for i, relem in enumerate(relems, 1):\n            try:\n                # Handle both Scrapling Selector and Selectolax Node\n                if hasattr(relem, 'html'):\n                    html_str = str(relem.html)\n                elif hasattr(relem, 'raw_html'):\n                     html_str = relem.raw_html.decode('utf-8', 'ignore') if isinstance(relem.raw_html, bytes) else str(relem.raw_html)\n                else:\n                    # Last resort for selectolax: reconstruct HTML or use text\n                    html_str = str(relem)\n\n                # Try to find track name in the card, but fallback to the last seen track\n                # (addressing grouped race cards)\n                tn = self._find_with_selectors(relem, self.TRACK_NAME_SELECTORS)\n                if tn:\n                    last_track = tn.strip()\n\n                venue = last_track\n\n                track_counters[venue] += 1\n                rnum = track_counters[venue] # Track-specific index as default (Fixes Race 20 issue)\n\n                rn_txt = self._find_with_selectors(relem, self.RACE_NUMBER_SELECTORS)\n                if rn_txt:\n                    digits = \"\".join(filter(str.isdigit, rn_txt))\n                    if digits: rnum = int(digits)\n\n                rd.append({\n                    \"html\": html_str,\n                    \"selector\": relem,\n                    \"track\": venue,\n                    \"race_number\": rnum,\n                    \"post_time_text\": self._find_with_selectors(relem, self.POST_TIME_SELECTORS),\n                    \"distance\": self._find_with_selectors(relem, ['[class*=\"distance\"]', '[class*=\"Distance\"]', '[data-distance]', \".race-distance\"]),\n                    \"date\": date,\n                    \"full_page\": False,\n                    \"available_bets\": scrape_available_bets(html_str)\n                })\n            except Exception: continue\n        return rd\n\n    def _find_with_selectors(self, el, selectors: List[str]) -> Optional[str]:\n        for s in selectors:\n            try:\n                f = el.css_first(s)\n                if f:\n                    t = node_text(f)\n                    if t: return t\n            except Exception: continue\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or \"races\" not in raw_data: return []\n        rl, ds, parsed = raw_data[\"races\"], raw_data.get(\"date\", datetime.now(EASTERN).strftime(\"%Y-%m-%d\")), []\n        for rd in rl:\n            try:\n                r = self._parse_single_race(rd, ds)\n                if r and r.runners: parsed.append(r)\n            except Exception: continue\n        return parsed\n\n    def _parse_single_race(self, rd: dict, ds: str) -> Optional[Race]:\n        page = rd.get(\"selector\")\n        hc = rd.get(\"html\", \"\")\n        if not page:\n            if not hc: return None\n            if Selector is not None:\n                page = Selector(hc)\n            else:\n                page = HTMLParser(hc)\n        tn, rnum = rd.get(\"track\", \"Unknown\"), rd.get(\"race_number\", 1)\n        st = self._parse_post_time(rd.get(\"post_time_text\"), page, ds)\n        runners = self._parse_runners(page)\n        disc = rd.get(\"assigned_discipline\") or detect_discipline(hc)\n        ab = scrape_available_bets(hc)\n        return Race(discipline=disc, id=generate_race_id(\"ts\", tn, st, rnum, disc), venue=tn, race_number=rnum, start_time=st, runners=runners, distance=rd.get(\"distance\"), source=self.source_name, available_bets=ab)\n\n    def _parse_post_time(self, tt: Optional[str], page, ds: str) -> datetime:\n        bd = datetime.strptime(ds, \"%Y-%m-%d\").date()\n        if tt:\n            p = self._parse_time_string(tt, bd)\n            if p: return p\n        for s in self.POST_TIME_SELECTORS:\n            try:\n                e = page.css_first(s)\n                if e:\n                    # Scrapling attrib vs Selectolax attributes\n                    da = getattr(e, 'attrib', getattr(e, 'attributes', {})).get('datetime')\n                    if da:\n                        try:\n                            dt = datetime.fromisoformat(da.replace('Z', '+00:00'))\n                            # Only trust the date from HTML if it's within 1 day of what we expected\n                            if abs((dt.date() - bd).days) <= 1:\n                                return dt\n                            else:\n                                self.logger.debug(\"Suspicious date in HTML datetime attribute\", html_dt=da, expected_date=bd)\n                        except Exception: pass\n                    p = self._parse_time_string(node_text(e), bd)\n                    if p: return p\n            except Exception: continue\n        return datetime.combine(bd, datetime.now(EASTERN).time()) + timedelta(hours=1)\n\n    def _parse_time_string(self, ts: str, bd) -> Optional[datetime]:\n        if not ts: return None\n        tc = re.sub(r\"\\s+(EST|EDT|CST|CDT|MST|MDT|PST|PDT|ET|PT|CT|MT)$\", \"\", ts, flags=re.I).strip()\n        m = re.search(r\"(\\d+)\\s*(?:min|mtp)\", tc, re.I)\n        if m: return now_eastern() + timedelta(minutes=int(m.group(1)))\n\n        for f in ['%I:%M %p', '%I:%M%p', '%H:%M', '%I:%M:%S %p']:\n            try:\n                t = datetime.strptime(tc, f).time()\n                # Heuristic: If time is between 1:00 and 7:00 and no AM/PM was explicitly in the format\n                # (or even if it was, but we are suspicious), for US night tracks like Turfway,\n                # it's likely PM. But %I requires %p. If %H was used and gave < 12, check if it should be PM.\n                if f == '%H:%M' and 1 <= t.hour <= 7:\n                    # In US horse racing, 1-7 AM is rare, 1-7 PM is common.\n                    t = t.replace(hour=t.hour + 12)\n\n                return datetime.combine(bd, t)\n            except Exception: continue\n        return None\n\n    def _parse_runners(self, page) -> List[Runner]:\n        runners = []\n        relems = []\n        for s in self.RUNNER_ROW_SELECTORS:\n            try:\n                el = page.css(s)\n                if el: relems = el; break\n            except Exception: continue\n        for i, e in enumerate(relems):\n            try:\n                r = self._parse_single_runner(e, i + 1)\n                if r: runners.append(r)\n            except Exception: continue\n        return runners\n\n    def _parse_single_runner(self, e, dn: int) -> Optional[Runner]:\n        # Scrapling Selector has .html property\n        es = str(getattr(e, 'html', e))\n        sc = any(s in es.lower() for s in ['scratched', 'scr', 'scratch'])\n        num = None\n        for s in ['[class*=\"program\"]', '[class*=\"saddle\"]', '[class*=\"post\"]', '[class*=\"number\"]', '[data-program-number]', 'td:first-child']:\n            try:\n                ne = e.css_first(s)\n                if ne:\n                    nt = node_text(ne)\n                    dig = \"\".join(filter(str.isdigit, nt))\n                    if dig:\n                        val = int(dig)\n                        if val <= 40:\n                            num = val\n                            break\n            except Exception: continue\n        name = None\n        for s in ['[class*=\"horse-name\"]', '[class*=\"horseName\"]', '[class*=\"runner-name\"]', 'a[class*=\"name\"]', '[data-horse-name]', 'td:nth-child(2)']:\n            try:\n                ne = e.css_first(s)\n                if ne:\n                    nt = node_text(ne)\n                    if nt and len(nt) > 1: name = re.sub(r\"\\(.*\\)\", \"\", nt).strip(); break\n            except Exception: continue\n        if not name: return None\n        odds, wo = {}, None\n        if not sc:\n            for s in ['[class*=\"odds\"]', '[class*=\"ml\"]', '[class*=\"morning-line\"]', '[data-odds]']:\n                try:\n                    oe = e.css_first(s)\n                    if oe:\n                        ot = node_text(oe)\n                        if ot and ot.upper() not in ['SCR', 'SCRATCHED', '--', 'N/A']:\n                            wo = parse_odds_to_decimal(ot)\n                            if od := create_odds_data(self.source_name, wo): odds[self.source_name] = od; break\n                except Exception: continue\n\n            # Advanced heuristic fallback\n            if wo is None:\n                wo = SmartOddsExtractor.extract_from_node(e)\n                if od := create_odds_data(self.source_name, wo): odds[self.source_name] = od\n\n        return Runner(number=num or dn, name=name, scratched=sc, odds=odds, win_odds=wo)\n\n    async def cleanup(self):\n        await self.close()\n        self.logger.info(\"TwinSpires adapter cleaned up\")\n\n\n# ----------------------------------------\n# ANALYZER LOGIC\n# ----------------------------------------\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        # Fallback to win_odds if available\n        if runner.win_odds and is_valid_odds(runner.win_odds):\n            return Decimal(str(runner.win_odds))\n\n    valid_odds = []\n    for source_data in runner.odds.values():\n        # Handle both dict and primitive formats\n        if isinstance(source_data, dict):\n            win = source_data.get('win')\n        elif hasattr(source_data, 'win'):\n            win = source_data.win\n        else:\n            win = source_data\n\n        if is_valid_odds(win):\n            valid_odds.append(Decimal(str(win)))\n\n    if valid_odds:\n        return min(valid_odds)\n\n    # Final fallback to win_odds if present\n    if runner.win_odds and is_valid_odds(runner.win_odds):\n        return Decimal(str(runner.win_odds))\n\n    return None\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None, **kwargs):\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self.config = config or {}\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(\n        self,\n        max_field_size: Optional[int] = None,\n        min_favorite_odds: float = 0.01,\n        min_second_favorite_odds: float = 0.01,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        # Use config value if provided and no explicit override (GPT5 Improvement)\n        self.max_field_size = max_field_size or self.config.get(\"analysis\", {}).get(\"max_field_size\", 11)\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        # Apply global timing cutoff (45m ago, 120m future)\n        now = datetime.now(EASTERN)\n        past_cutoff = now - timedelta(minutes=45)\n        future_cutoff = now + timedelta(minutes=120)\n        st = race.start_time\n        if st.tzinfo is None:\n            st = st.replace(tzinfo=EASTERN)\n        if st < past_cutoff or st > future_cutoff:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        qualified_races = []\n        TRUSTWORTHY_RATIO_MIN = self.config.get(\"analysis\", {}).get(\"trustworthy_ratio_min\", 0.7)\n\n        for race in races:\n            if not self.is_race_qualified(race):\n                continue\n\n            active_runners = [r for r in race.runners if not r.scratched]\n            total_active = len(active_runners)\n\n            # Trustworthiness Airlock (Success Playbook Item)\n            if total_active > 0:\n                trustworthy_count = sum(1 for r in active_runners if r.metadata.get(\"odds_source_trustworthy\"))\n                if trustworthy_count / total_active < TRUSTWORTHY_RATIO_MIN:\n                    log.warning(\"Not enough trustworthy odds for Trifecta; skipping\", venue=race.venue, race=race.race_number, ratio=round(trustworthy_count/total_active, 2))\n                    continue\n\n            # Uniform Odds Check\n            all_odds = []\n            for runner in active_runners:\n                odds = _get_best_win_odds(runner)\n                if odds: all_odds.append(odds)\n\n            if len(all_odds) >= 3 and len(set(all_odds)) == 1:\n                log.warning(\"Race contains uniform odds; likely placeholder. Skipping Trifecta.\", venue=race.venue, race=race.race_number)\n                continue\n\n            score = self._evaluate_race(race)\n            if score > 0:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\n            \"Universal scoring complete\",\n            total_races_scored=len(qualified_races),\n            criteria=criteria,\n        )\n\n        for race in qualified_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": qualified_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            if len(active_runners) >= 2:\n                # If we have runners but no odds, use fallbacks\n                favorite_odds = Decimal(str(DEFAULT_ODDS_FALLBACK))\n                second_favorite_odds = Decimal(str(DEFAULT_ODDS_FALLBACK))\n            else:\n                return 0.0\n        else:\n            runners_with_odds.sort(key=lambda x: x[1])\n            favorite_odds = runners_with_odds[0][1]\n            second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        # --- Apply hard filters before scoring ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < Decimal(\"2.0\")\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            return 0.0\n\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        field_score = max(0.0, field_score)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n        # To be safe:\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass TinyFieldTrifectaAnalyzer(TrifectaAnalyzer):\n    \"\"\"A specialized TrifectaAnalyzer that only considers races with 6 or fewer runners.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Override the max_field_size to 6 for \"tiny field\" analysis\n        # Set low odds thresholds to \"let them through\" as per user request\n        super().__init__(max_field_size=6, min_favorite_odds=0.01, min_second_favorite_odds=0.01, **kwargs)\n\n    @property\n    def name(self) -> str:\n        return \"tiny_field_trifecta_analyzer\"\n\n\nclass SimplySuccessAnalyzer(BaseAnalyzer):\n    \"\"\"An analyzer that qualifies every race to show maximum successes (HTTP 200).\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"simply_success\"\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Returns races with a perfect score, applying global timing and chalk filters.\"\"\"\n        qualified = []\n        now = datetime.now(EASTERN)\n\n        # Success Playbook Hardening (Council of Superbrains)\n        TRUSTWORTHY_RATIO_MIN = self.config.get(\"analysis\", {}).get(\"trustworthy_ratio_min\", 0.7)\n\n        for race in races:\n            # 1. Timing Filter: Relaxed for \"News\" mode (GPT5: Caller handles strict timing)\n            st = race.start_time\n            if st.tzinfo is None:\n                st = st.replace(tzinfo=EASTERN)\n\n            # Goldmine Detection: 2nd favorite >= 4.5 decimal\n            is_goldmine = False\n            is_best_bet = False\n            active_runners = [r for r in race.runners if not r.scratched]\n            total_active = len(active_runners)\n\n            # Trustworthiness Airlock (Success Playbook Item)\n            if total_active > 0:\n                trustworthy_count = sum(1 for r in active_runners if r.metadata.get(\"odds_source_trustworthy\"))\n                if trustworthy_count / total_active < TRUSTWORTHY_RATIO_MIN:\n                    self.logger.warning(\"Not enough trustworthy odds; skipping race\", venue=race.venue, race=race.race_number, ratio=round(trustworthy_count/total_active, 2))\n                    continue\n\n            gap12 = 0.0\n            all_odds = []\n\n            # 1. Collect and Enrich Odds\n            for runner in active_runners:\n                odds = _get_best_win_odds(runner)\n                if odds is not None:\n                    # Propagate fresh odds to runner object for reporting\n                    runner.win_odds = float(odds)\n                    all_odds.append(odds)\n\n            # Sort odds ascending\n            all_odds.sort()\n\n            # Uniform Odds Check: If all runners have identical odds, it's likely a placeholder card (Memory Directive Fix)\n            if len(all_odds) >= 3 and len(set(all_odds)) == 1:\n                self.logger.warning(\"Race contains uniform odds; likely placeholder data. Skipping.\", venue=race.venue, race=race.race_number, odds=float(all_odds[0]))\n                continue\n\n            # Stability Check: Ensure we have at least 2 active runners to compare\n            if len(active_runners) < 2:\n                log.debug(\"Excluding race with < 2 runners\", venue=race.venue)\n                continue\n\n            # 2. Derive Selection (2nd favorite) and Top 5\n            # Collect valid runners with their enriched odds\n            valid_r_with_odds = sorted(\n                [(r, Decimal(str(r.win_odds))) for r in active_runners if r.win_odds is not None],\n                key=lambda x: x[1]\n            )\n            race.top_five_numbers = \", \".join([str(r[0].number or '?') for r in valid_r_with_odds[:5]])\n\n            if len(valid_r_with_odds) >= 2:\n                sec_fav = valid_r_with_odds[1][0]\n                race.metadata['selection_number'] = sec_fav.number\n                race.metadata['selection_name'] = sec_fav.name\n\n            # 3. Apply Best Bet Logic\n            if len(all_odds) >= 2:\n                fav, sec = all_odds[0], all_odds[1]\n                gap12 = round(float(sec - fav), 2)\n\n                # Enforce gap requirement\n                if gap12 <= 0.25:\n                    log.debug(\"Insufficient gap detected (1Gap2 <= 0.25), ineligible for Best Bet treatment\", venue=race.venue, race=race.race_number, gap=gap12)\n                else:\n                    # Goldmine = 2nd Fav >= 4.5, Field <= 11, Gap > 0.25\n                    if len(active_runners) <= 11 and sec >= Decimal(\"4.5\"):\n                        is_goldmine = True\n\n                    # You Might Like = 2nd Fav >= 3.5, Field <= 11, Gap > 0.25\n                    if len(active_runners) <= 11 and sec >= Decimal(\"3.5\"):\n                        is_best_bet = True\n\n                race.metadata['predicted_2nd_fav_odds'] = float(sec)\n            else:\n                # Fallback if insufficient odds data\n                race.metadata['predicted_2nd_fav_odds'] = None\n\n            race.metadata['is_goldmine'] = is_goldmine\n            race.metadata['is_best_bet'] = is_best_bet\n            race.metadata['1Gap2'] = gap12\n            race.qualification_score = 100.0\n            qualified.append(race)\n\n        if not qualified:\n            log.warning(\"\ud83d\udd2d SimplySuccess analyzer pass returned 0 qualified races\", input_count=len(races))\n\n        return {\n            \"criteria\": {\n                \"mode\": \"simply_success\",\n                \"timing_filter\": \"45m_past_to_120m_future\",\n                \"chalk_filter\": \"disabled\",\n                \"goldmine_threshold\": 4.5\n            },\n            \"races\": qualified\n        }\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self.config = config or {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        self.register_analyzer(\"tiny_field_trifecta\", TinyFieldTrifectaAnalyzer)\n        self.register_analyzer(\"simply_success\", SimplySuccessAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(config=self.config, **kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).resolve().parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.notifier = DesktopNotifier() if HAS_NOTIFICATIONS else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n        self.notifications_enabled = self.notifier is not None\n        if not self.notifications_enabled:\n            log.debug(\"Native notifications disabled (platform not supported or library missing)\")\n\n    def notify_qualified_race(self, race):\n        if race.id in self.notified_races:\n            return\n\n        # Always log the high-value opportunity regardless of notification setting\n        log.info(\n            \"High-value opportunity identified\",\n            venue=race.venue,\n            race=race.race_number,\n            score=race.qualification_score\n        )\n\n        if not self.notifications_enabled or self.notifier is None:\n            return\n\n        title = \"\ud83d\udc0e High-Value Opportunity!\"\n        message = f\"{race.venue} - Race {race.race_number}\\nScore: {race.qualification_score:.0f}%\\nPost Time: {race.start_time.strftime('%I:%M %p')}\"\n\n        try:\n            # Use keyword arguments for better compatibility (AI Review Fix)\n            self.notifier.send(\n                title=title,\n                message=message,\n                urgency=\"high\" if race.qualification_score >= 80 else \"normal\"\n            )\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            log.error(\"Failed to send notification\", error=str(e))\n\n\n# ----------------------------------------\ndef get_track_category(races_at_track: List[Any]) -> str:\n    \"\"\"Categorize the track as T (Thoroughbred), H (Harness), or G (Greyhounds).\"\"\"\n    if not races_at_track:\n        return 'T'\n\n    # Never allow any track with a field size above 7 to be G\n    has_large_field = False\n    for r in races_at_track:\n        runners = get_field(r, 'runners', [])\n        active_runners = len([run for run in runners if not get_field(run, 'scratched', False)])\n        if active_runners > 7:\n            has_large_field = True\n            break\n\n    for race in races_at_track:\n        source = get_field(race, 'source', '') or \"\"\n        race_id = (get_field(race, 'id', '') or \"\").lower()\n        discipline = get_field(race, 'discipline', '') or \"\"\n\n        if discipline == \"Harness\" or '_h' in race_id: return 'H'\n        if (discipline == \"Greyhound\" or '_g' in race_id) and not has_large_field:\n            return 'G'\n\n        source_lower = source.lower()\n        if (\"greyhound\" in source_lower or source in [\"GBGB\", \"Greyhound\", \"AtTheRacesGreyhound\"]) and not has_large_field:\n            return 'G'\n        if source in [\"USTrotting\", \"StandardbredCanada\", \"Harness\"] or any(kw in source_lower for kw in ['harness', 'standardbred', 'trot', 'pace']):\n            return 'H'\n\n    # Distance consistency check (Disabled - was mis-identifying Thoroughbred tracks)\n    # dist_counts = defaultdict(int)\n    # for r in races_at_track:\n    #     dist = get_field(r, 'distance')\n    #     if dist:\n    #         dist_counts[dist] += 1\n    # if dist_counts and max(dist_counts.values()) >= 4:\n    #     return 'H'\n\n    return 'T'\n\n\ndef generate_fortuna_fives(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate the FORTUNA FIVES appendix.\"\"\"\n    lines = [\"\", \"\", \"FORTUNA FIVES\", \"-------------\"]\n    fives = []\n    for race in (all_races or races):\n        runners = get_field(race, 'runners', [])\n        field_size = len([r for r in runners if not get_field(r, 'scratched', False)])\n        if field_size == 5:\n            fives.append(race)\n\n    if not fives:\n        lines.append(\"No qualifying races.\")\n        return \"\\n\".join(lines)\n\n    track_odds_sums = defaultdict(float)\n    track_odds_counts = defaultdict(int)\n    stats_races = all_races if all_races is not None else races\n    for race in stats_races:\n        v = get_field(race, 'venue')\n        track = normalize_venue_name(v)\n        for runner in get_field(race, 'runners', []):\n            win_odds = get_field(runner, 'win_odds')\n            if not get_field(runner, 'scratched') and win_odds:\n                track_odds_sums[track] += float(win_odds)\n                track_odds_counts[track] += 1\n\n    track_avgs = {}\n    for track, total in track_odds_sums.items():\n        count = track_odds_counts[track]\n        if count > 0:\n            track_avgs[track] = str(int(total / count))\n\n    track_to_nums = defaultdict(list)\n    for r in fives:\n        v = get_field(r, 'venue')\n        if v:\n            track_to_nums[normalize_venue_name(v)].append(get_field(r, 'race_number'))\n\n    for track in sorted(track_to_nums.keys()):\n        nums = sorted(list(set(track_to_nums[track])))\n        avg_str = f\" [{track_avgs[track]}]\" if track in track_avgs else \"\"\n        lines.append(f\"{track}{avg_str}: {', '.join(map(str, nums))}\")\n\n    return \"\\n\".join(lines)\n\n\ndef generate_field_matrix(races: List[Any]) -> str:\n    \"\"\"\n    Generates a Markdown table matrix of races by Track and Field Size.\n    Cells contain alphabetic race codes (lowercase=normal, uppercase=goldmine).\n    \"\"\"\n    if not races:\n        return \"No races available for field matrix.\"\n\n    # Group races by Track and Field Size\n    matrix = defaultdict(lambda: defaultdict(list))\n\n    for r in races:\n        track = normalize_venue_name(get_field(r, 'venue'))\n        field_size = len([run for run in get_field(r, 'runners', []) if not get_field(run, 'scratched', False)])\n\n        # Only interested in field sizes 3-11 for this report\n        if 3 <= field_size <= 11:\n            is_gold = get_field(r, 'metadata', {}).get('is_goldmine', False)\n            race_num = get_field(r, 'race_number')\n            matrix[track][field_size].append((race_num, is_gold))\n\n    if not matrix:\n        return \"No qualifying races for field matrix (3-11 runners).\"\n\n    # Header: Display sizes 3 to 11\n    display_sizes = range(3, 12)\n\n    header = \"| TRACK / FIELD | \" + \" | \".join(map(str, display_sizes)) + \" |\"\n    separator = \"| :--- | \" + \" | \".join([\":---:\"] * len(display_sizes)) + \" |\"\n    lines = [header, separator]\n\n    for track in sorted(matrix.keys()):\n        row = [track]\n        for size in display_sizes:\n            race_list = matrix[track].get(size, [])\n            if race_list:\n                # Standardize formatting of race codes\n                code_parts = format_grid_code(race_list, wrap_width=12)\n                row.append(\"<br>\".join(code_parts))\n            else:\n                row.append(\" \")\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n\n\ndef generate_goldmines(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate the GOLDMINE RACES appendix, filtered to Superfecta races.\"\"\"\n    lines = [\"\", \"\", \"GOLDMINE RACES\", \"--------------\"]\n\n    # Pre-calculate track categories\n    track_categories = {}\n    source_races_for_cat = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races_for_cat:\n        v = get_field(r, 'venue')\n        track = normalize_venue_name(v)\n        races_by_track[track].append(r)\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    def is_superfecta_effective(r):\n        available_bets = get_field(r, 'available_bets', [])\n        metadata_bets = get_field(r, 'metadata', {}).get('available_bets', [])\n        if 'Superfecta' in available_bets or 'Superfecta' in metadata_bets:\n            return True\n\n        track = normalize_venue_name(get_field(r, 'venue'))\n        cat = track_categories.get(track, 'T')\n        runners = get_field(r, 'runners', [])\n        field_size = len([run for run in runners if not get_field(run, 'scratched', False)])\n        if cat == 'T' and field_size >= 6:\n            return True\n        return False\n\n    goldmines = [r for r in races if get_field(r, 'metadata', {}).get('is_goldmine') and is_superfecta_effective(r)]\n\n    if not goldmines:\n        lines.append(\"No qualifying races.\")\n        return \"\\n\".join(lines)\n\n    track_to_nums = defaultdict(list)\n    for r in goldmines:\n        v = get_field(r, 'venue')\n        if v:\n            track = normalize_venue_name(v)\n            track_to_nums[track].append(get_field(r, 'race_number'))\n\n    # Sort tracks descending by category (T > H > G)\n    cat_map = {'T': 3, 'H': 2, 'G': 1}\n\n    formatted_tracks = []\n    for track in track_to_nums.keys():\n        cat = track_categories.get(track, 'T')\n        display_name = f\"{cat}~{track}\"\n        formatted_tracks.append((cat, track, display_name))\n\n    # Sort: Category Descending, then Track Name Ascending\n    formatted_tracks.sort(key=lambda x: (-cat_map.get(x[0], 0), x[1]))\n\n    for cat, track, display_name in formatted_tracks:\n        nums = sorted(list(set(track_to_nums[track])))\n        lines.append(f\"{display_name}: {', '.join(map(str, nums))}\")\n    return \"\\n\".join(lines)\n\n\ndef generate_goldmine_report(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate a detailed report for Goldmine races.\"\"\"\n    # 1. Reuse category logic\n    track_categories = {}\n    source_races_for_cat = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races_for_cat:\n        v = get_field(r, 'venue')\n        track = normalize_venue_name(v)\n        races_by_track[track].append(r)\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    def is_superfecta_available(r):\n        available_bets = get_field(r, 'available_bets', [])\n        metadata_bets = get_field(r, 'metadata', {}).get('available_bets', [])\n        if 'Superfecta' in available_bets or 'Superfecta' in metadata_bets:\n            return True\n        track = normalize_venue_name(get_field(r, 'venue'))\n        cat = track_categories.get(track, 'T')\n        runners = get_field(r, 'runners', [])\n        field_size = len([run for run in runners if not get_field(run, 'scratched', False)])\n        return cat == 'T' and field_size >= 6\n\n    # Include all goldmines (2nd fav >= 4.5)\n    # Deduplicate to prevent double-reporting (e.g. from multiple sources)\n    goldmines = []\n    seen_gold = set()\n    for r in races:\n        if get_field(r, 'metadata', {}).get('is_goldmine'):\n            track = get_canonical_venue(get_field(r, 'venue'))\n            num = get_field(r, 'race_number')\n            st = get_field(r, 'start_time')\n            st_str = st.strftime('%Y%m%d') if isinstance(st, datetime) else str(st)\n            # Use canonical key for cross-adapter deduplication\n            key = (track, num, st_str)\n            if key not in seen_gold:\n                seen_gold.add(key)\n                goldmines.append(r)\n\n    if not goldmines:\n        return \"No Goldmine races found.\"\n\n    # Sort goldmines: Cat descending, Track asc, Race num asc\n    cat_map = {'T': 3, 'H': 2, 'G': 1}\n    def goldmine_sort_key(r):\n        track = normalize_venue_name(get_field(r, 'venue'))\n        cat = track_categories.get(track, 'T')\n        return (-cat_map.get(cat, 0), track, get_field(r, 'race_number', 0))\n\n    goldmines.sort(key=goldmine_sort_key)\n\n    now = datetime.now(EASTERN)\n    immediate_gold_superfecta = []\n    immediate_gold = []\n    remaining_gold = []\n\n    for r in goldmines:\n        start_time = get_field(r, 'start_time')\n        if isinstance(start_time, str):\n            try:\n                start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n            except ValueError:\n                remaining_gold.append(r)\n                continue\n\n        if start_time:\n            if start_time.tzinfo is None:\n                start_time = start_time.replace(tzinfo=EASTERN)\n\n            diff = (start_time - now).total_seconds() / 60\n            if 0 <= diff <= 20:\n                if is_superfecta_available(r):\n                    immediate_gold_superfecta.append(r)\n                else:\n                    immediate_gold.append(r)\n            else:\n                remaining_gold.append(r)\n        else:\n            remaining_gold.append(r)\n\n    report_lines = [\"LIST OF BEST BETS - GOLDMINE REPORT\", \"===================================\", \"\"]\n\n    def render_races(races_to_render, label):\n        if not races_to_render:\n            return\n        report_lines.append(f\"--- {label.upper()} ---\")\n        report_lines.append(\"-\" * (len(label) + 8))\n        report_lines.append(\"\")\n\n        for r in races_to_render:\n            track = normalize_venue_name(get_field(r, 'venue'))\n            cat = track_categories.get(track, 'T')\n            race_num = get_field(r, 'race_number')\n            start_time = get_field(r, 'start_time')\n            if isinstance(start_time, datetime):\n                # Ensure it's in Eastern for the display\n                st_eastern = to_eastern(start_time)\n                time_str = st_eastern.strftime(\"%H:%M ET\")\n            else:\n                time_str = str(start_time)\n\n            # Identify Top 5\n            runners = get_field(r, 'runners', [])\n            active_with_odds = []\n            for run in runners:\n                if get_field(run, 'scratched'): continue\n                wo = _get_best_win_odds(run)\n                if wo: active_with_odds.append((run, wo))\n\n            sorted_by_odds = sorted(active_with_odds, key=lambda x: x[1])\n            top_5_nums = \", \".join([str(get_field(run[0], 'number') or '?') for run in sorted_by_odds[:5]])\n            if hasattr(r, 'top_five_numbers'):\n                r.top_five_numbers = top_5_nums\n\n            gap12 = get_field(r, 'metadata', {}).get('1Gap2', 0.0)\n            report_lines.append(f\"{cat}~{track} - Race {race_num} ({time_str})\")\n            report_lines.append(f\"PREDICTED TOP 5: [{top_5_nums}] | 1Gap2: {gap12:.2f}\")\n            report_lines.append(\"-\" * 40)\n\n            # Sort runners by number\n            sorted_runners = sorted(runners, key=lambda x: get_field(x, 'number') or 0)\n\n            for run in sorted_runners:\n                if get_field(run, 'scratched'):\n                    continue\n                name = get_field(run, 'name')\n                num = get_field(run, 'number')\n                odds = get_field(run, 'win_odds')\n                odds_str = f\"{odds:.2f}\" if odds else \"N/A\"\n                report_lines.append(f\"  #{num:<2} {name:<25}  ~ {odds_str}\")\n\n            report_lines.append(\"\")\n\n    if immediate_gold_superfecta:\n        render_races(immediate_gold_superfecta, \"Immediate Gold (superfecta)\")\n\n    if immediate_gold:\n        render_races(immediate_gold, \"Immediate Gold\")\n\n    if remaining_gold:\n        render_races(remaining_gold, \"All Remaining Goldmine Races\")\n\n    return \"\\n\".join(report_lines)\n\n\ndef generate_historical_goldmine_report(audited_tips: List[Dict[str, Any]]) -> str:\n    \"\"\"Generate a report for recently audited Goldmine races.\"\"\"\n    if not audited_tips:\n        return \"\"\n\n    lines = [\"\", \"RECENT AUDITED GOLDMINES\", \"------------------------\"]\n\n    # Calculate simple stats\n    total = len(audited_tips)\n    cashed = sum(1 for t in audited_tips if t.get(\"verdict\") == \"CASHED\")\n    total_profit = sum((t.get(\"net_profit\") or 0.0) for t in audited_tips)\n    sr = (cashed / total * 100) if total > 0 else 0\n\n    lines.append(f\"Performance Summary (Last {total} Goldmines):\")\n    lines.append(f\"  Strike Rate: {sr:.1f}% | Total Net Profit: ${total_profit:+.2f}\")\n    lines.append(\"\")\n\n    for tip in audited_tips:\n        venue = tip.get(\"venue\", \"Unknown\")\n        race_num = tip.get(\"race_number\", \"?\")\n        verdict = tip.get(\"verdict\", \"?\")\n        profit = tip.get(\"net_profit\", 0.0)\n        start_time_raw = tip.get(\"start_time\", \"\")\n\n        try:\n            st = datetime.fromisoformat(start_time_raw.replace('Z', '+00:00'))\n            time_str = to_eastern(st).strftime(\"%Y-%m-%d %H:%M ET\")\n        except Exception:\n            time_str = str(start_time_raw)[:16]\n\n        emoji = \"\u2705\" if verdict == \"CASHED\" else \"\u274c\" if verdict == \"BURNED\" else \"\u26aa\"\n\n        line = f\"{emoji} {time_str} | {venue} R{race_num} | {verdict:<6} | Profit: ${profit:+.2f}\"\n\n        # Add top place payouts for proof\n        p1 = tip.get(\"top1_place_payout\")\n        p2 = tip.get(\"top2_place_payout\")\n        if p1 or p2:\n            line += f\" | Place: {p1 or 0:.2f}/{p2 or 0:.2f}\"\n\n        # Prioritize Superfecta info to \"prove\" with payouts\n        super_payout = tip.get(\"superfecta_payout\")\n        tri_payout = tip.get(\"trifecta_payout\")\n\n        if super_payout:\n            line += f\" | Super: ${super_payout:.2f}\"\n        elif tri_payout:\n            line += f\" | Tri: ${tri_payout:.2f}\"\n\n        lines.append(line)\n\n    return \"\\n\".join(lines)\n\n\ndef generate_next_to_jump(races: List[Any]) -> str:\n    \"\"\"Generate the NEXT TO JUMP section.\"\"\"\n    lines = [\"\", \"\", \"NEXT TO JUMP\", \"------------\"]\n    now = datetime.now(EASTERN)\n    upcoming = []\n    for r in races:\n        r_time = get_field(r, 'start_time')\n        if isinstance(r_time, str):\n            try:\n                r_time = datetime.fromisoformat(r_time.replace('Z', '+00:00'))\n            except ValueError:\n                continue\n\n        if r_time:\n            if r_time.tzinfo is None:\n                r_time = r_time.replace(tzinfo=EASTERN)\n            if r_time > now:\n                upcoming.append((r, r_time))\n\n    if upcoming:\n        next_r, next_r_time = min(upcoming, key=lambda x: x[1])\n        diff = next_r_time - now\n        minutes = int(diff.total_seconds() / 60)\n        lines.append(f\"{normalize_venue_name(get_field(next_r, 'venue'))} Race {get_field(next_r, 'race_number')} in {minutes}m\")\n    else:\n        lines.append(\"All races complete for today.\")\n\n    return \"\\n\".join(lines)\n\n\nasync def generate_friendly_html_report(races: List[Any], stats: Dict[str, Any]) -> str:\n    \"\"\"Generates a high-impact, friendly HTML report for the Fortuna Faucet.\"\"\"\n    now_str = datetime.now(EASTERN).strftime('%Y-%m-%d %H:%M:%S')\n\n    # 1. Best Bet Opportunities\n    rows = []\n    for r in sorted(races, key=lambda x: getattr(x, 'start_time', '')):\n        # Get selection (2nd favorite)\n        runners = getattr(r, 'runners', [])\n        active = [run for run in runners if not getattr(run, 'scratched', False)]\n        if len(active) < 2: continue\n\n        active.sort(key=lambda x: getattr(x, 'win_odds', 999.0) or 999.0)\n        sel = active[1]\n\n        st = getattr(r, 'start_time', '')\n        if isinstance(st, datetime):\n            # Ensure it's in Eastern for display (GPT5 Improvement)\n            st_str = to_eastern(st).strftime('%H:%M')\n        elif isinstance(st, str):\n            try:\n                dt = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                st_str = to_eastern(dt).strftime('%H:%M')\n            except Exception:\n                st_str = str(st)[11:16]\n        else:\n            st_str = str(st)[11:16]\n\n        is_gold = getattr(r, 'metadata', {}).get('is_goldmine', False)\n        gold_badge = '<span class=\"badge gold\">GOLD</span>' if is_gold else ''\n\n        d_str = '??/??'\n        if isinstance(st, datetime):\n            d_str = st.strftime('%m/%d')\n        elif isinstance(st, str):\n            try:\n                dt = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                d_str = dt.strftime('%m/%d')\n            except Exception: pass\n\n        rows.append(f\"\"\"\n            <tr>\n                <td>{st_str} ({d_str})</td>\n                <td>{getattr(r, 'venue', 'Unknown')}</td>\n                <td>R{getattr(r, 'race_number', '?')}</td>\n                <td>#{getattr(sel, 'number', '?')} {getattr(sel, 'name', 'Unknown')}</td>\n                <td>{ (getattr(sel, 'win_odds') or 0.0):.2f}</td>\n                <td>{gold_badge}</td>\n            </tr>\n        \"\"\")\n\n    tips_count = stats.get('tips', 0)\n    cashed_count = stats.get('cashed', 0)\n    profit = stats.get('profit', 0.0)\n\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Fortuna Faucet Intelligence Report</title>\n        <style>\n            body {{ font-family: 'Segoe UI', Arial, sans-serif; background-color: #0f172a; color: #f8fafc; margin: 0; padding: 20px; }}\n            .container {{ max-width: 1000px; margin: 0 auto; background-color: #1e293b; padding: 30px; border-radius: 12px; box-shadow: 0 10px 25px rgba(0,0,0,0.5); }}\n            h1 {{ color: #fbbf24; text-align: center; text-transform: uppercase; letter-spacing: 3px; border-bottom: 2px solid #fbbf24; padding-bottom: 15px; }}\n            .stats-grid {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 30px 0; }}\n            .stat-card {{ background-color: #334155; padding: 20px; border-radius: 8px; text-align: center; }}\n            .stat-value {{ font-size: 24px; font-weight: bold; color: #fbbf24; }}\n            .stat-label {{ font-size: 14px; color: #94a3b8; text-transform: uppercase; }}\n            table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}\n            th {{ background-color: #334155; color: #fbbf24; text-align: left; padding: 12px; }}\n            td {{ padding: 12px; border-bottom: 1px solid #334155; }}\n            tr:hover {{ background-color: #334155; }}\n            .badge {{ padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; }}\n            .gold {{ background-color: #fbbf24; color: #0f172a; }}\n            .footer {{ margin-top: 40px; text-align: center; font-size: 12px; color: #64748b; }}\n        </style>\n    </head>\n    <body>\n        <div class=\"container\">\n            <h1>Fortuna Faucet Intelligence</h1>\n            <p style=\"text-align:center;\">Real-time global racing analysis generated at {now_str} ET</p>\n\n            <div class=\"stats-grid\">\n                <div class=\"stat-card\">\n                    <div class=\"stat-value\">{tips_count}</div>\n                    <div class=\"stat-label\">Total Selections</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-value\">{cashed_count}</div>\n                    <div class=\"stat-label\">Recently Audited Wins</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-value\">${profit:+.2f}</div>\n                    <div class=\"stat-label\">Estimated Profit</div>\n                </div>\n            </div>\n\n            <h2>\ud83d\udd25 Best Bet Opportunities</h2>\n            <table>\n                <thead>\n                    <tr>\n                        <th>Time</th>\n                        <th>Venue</th>\n                        <th>Race</th>\n                        <th>Selection</th>\n                        <th>Odds</th>\n                        <th>Type</th>\n                    </tr>\n                </thead>\n                <tbody>\n                    {''.join(rows) if rows else '<tr><td colspan=\"6\" style=\"text-align:center;\">No immediate opportunities identified.</td></tr>'}\n                </tbody>\n            </table>\n\n            {await _generate_audit_history_html()}\n\n            <div class=\"footer\">\n                Fortuna Faucet Portable App - Sci-Fi Intelligence Edition<br>\n                Powered by the Council of Superbrains\n            </div>\n        </div>\n    </body>\n    </html>\n    \"\"\"\n    return html\n\n\nasync def _generate_audit_history_html() -> str:\n    \"\"\"Generates HTML for recent audited results.\"\"\"\n    db = FortunaDB()\n    history = await db.get_all_audited_tips()\n    if not history:\n        return \"\"\n\n    # Take latest 15\n    history = sorted(history, key=lambda x: x.get('audit_timestamp', ''), reverse=True)[:15]\n\n    rows = []\n    for t in history:\n        verdict = t.get(\"verdict\", \"?\")\n        emoji = \"\u2705\" if verdict == \"CASHED\" else \"\u274c\" if verdict == \"BURNED\" else \"\u26aa\"\n        profit = t.get(\"net_profit\", 0.0)\n        p_class = \"profit-pos\" if profit > 0 else \"profit-neg\" if profit < 0 else \"\"\n\n        po = t.get(\"predicted_2nd_fav_odds\")\n        ao = t.get(\"actual_2nd_fav_odds\")\n        odds_str = f\"{po or '?':.1f} \u2192 {ao or '?':.1f}\"\n\n        rows.append(f\"\"\"\n            <tr>\n                <td>{emoji} {verdict}</td>\n                <td>{t.get('venue', 'Unknown')}</td>\n                <td>R{t.get('race_number', '?')}</td>\n                <td>{odds_str}</td>\n                <td class=\"{p_class}\">${profit:+.2f}</td>\n            </tr>\n        \"\"\")\n\n    return f\"\"\"\n        <style>\n            .profit-pos {{ color: #4ade80; font-weight: bold; }}\n            .profit-neg {{ color: #f87171; }}\n        </style>\n        <h2 style=\"margin-top: 40px;\">\ud83d\udcb0 Recent Audit Results</h2>\n        <table>\n            <thead>\n                <tr>\n                    <th>Verdict</th>\n                    <th>Venue</th>\n                    <th>Race</th>\n                    <th>Odds (Pred \u2192 Act)</th>\n                    <th>Net Profit</th>\n                </tr>\n            </thead>\n            <tbody>\n                {''.join(rows)}\n            </tbody>\n        </table>\n    \"\"\"\n\n\ndef generate_summary_grid(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"\n    Generates a Markdown table summary of upcoming races.\n    Sorted by MTP, ceiling of 4 hours from now.\n    \"\"\"\n    now = datetime.now(EASTERN)\n    cutoff = now + timedelta(hours=18)\n\n    # 1. Pre-calculate track categories\n    track_categories = {}\n    source_races = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races:\n        venue = get_field(r, 'venue')\n        track = normalize_venue_name(venue)\n        races_by_track[track].append(r)\n\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    table_races = []\n    seen = set()\n    for race in (all_races or races):\n        st = get_field(race, 'start_time')\n        if isinstance(st, str):\n            try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n            except Exception: continue\n        if st and st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n        # Ceiling of 18 hours, ignore races more than 10 mins past\n        if not st or st < now - timedelta(minutes=10) or st > cutoff:\n            continue\n\n        track = normalize_venue_name(get_field(race, 'venue'))\n        canonical_track = get_canonical_venue(get_field(race, 'venue'))\n        num = get_field(race, 'race_number')\n        # Deduplication key: Use canonical track/num/date\n        key = (canonical_track, num, st.strftime('%Y%m%d'))\n        if key in seen: continue\n        seen.add(key)\n\n        mtp = int((st - now).total_seconds() / 60)\n        runners = get_field(race, 'runners', [])\n        field_size = len([run for run in runners if not get_field(run, 'scratched', False)])\n        top5 = getattr(race, 'top_five_numbers', 'N/A')\n        gap12 = get_field(race, 'metadata', {}).get('1Gap2', 0.0)\n        is_gold = get_field(race, 'metadata', {}).get('is_goldmine', False)\n\n        table_races.append({\n            'mtp': mtp,\n            'cat': track_categories.get(track, 'T'),\n            'track': track,\n            'num': num,\n            'field': field_size,\n            'top5': top5,\n            'gap': gap12,\n            'gold': '[G]' if is_gold else ''\n        })\n\n    # Sort by MTP\n    table_races.sort(key=lambda x: x['mtp'])\n\n    if not table_races:\n        return \"No upcoming races in the next 4 hours.\"\n\n    lines = [\n        \"| MTP | CAT | TRACK | R# | FLD | TOP 5 | GAP | |\",\n        \"|:---:|:---:|:---|:---:|:---:|:---|:---:|:---:|\"\n    ]\n    for tr in table_races:\n        # Better alignment: leading zero for single digits (Memory Directive Fix)\n        mtp_val = tr['mtp']\n        mtp_str = f\"{mtp_val:02d}\" if 0 <= mtp_val < 10 else str(mtp_val)\n        lines.append(f\"| {mtp_str}m | {tr['cat']} | {tr['track'][:20]} | {tr['num']} | {tr['field']} | `{tr['top5']}` | {tr['gap']:.2f} | {tr['gold']} |\")\n\n    return \"\\n\".join(lines)\n\n\ndef normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n\n\ndef num_to_alpha(n, is_goldmine=False):\n    \"\"\"Convert race number to alphabetic code. Goldmines are uppercase.\"\"\"\n    if not isinstance(n, int) or n < 1:\n        return '?'\n    letter = chr(ord('a') + n - 1) if n <= 26 else str(n)\n    return letter.upper() if is_goldmine else letter\n\n\ndef wrap_text(text, width):\n    \"\"\"Wrap string into a list of fixed-width segments.\"\"\"\n    if not text:\n        return [\"\"]\n    return [text[i:i+width] for i in range(0, len(text), width)]\n\n\ndef format_grid_code(race_info_list, wrap_width=4):\n    \"\"\"\n    Standardizes the formatting of race code strings for the grid.\n    Includes midpoint space for readability if length exceeds 5.\n\n    Args:\n        race_info_list: List of (race_num, is_goldmine) tuples\n        wrap_width: Width to wrap at\n    \"\"\"\n    if not race_info_list:\n        return [\"\"]\n\n    code = \"\".join([num_to_alpha(n, gm) for n, gm in sorted(list(set(race_info_list)))])\n\n    # Midpoint space logic for readability (Project Convention)\n    if len(code) > 5:\n        mid = len(code) // 2\n        code = code[:mid] + \" \" + code[mid:]\n\n    return wrap_text(code, wrap_width)\n\n\ndef format_prediction_row(race: Race) -> str:\n    \"\"\"Formats a single race prediction for the GHA Job Summary table.\"\"\"\n    metadata = getattr(race, 'metadata', {})\n\n    st = race.start_time\n    if isinstance(st, str):\n        try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n        except Exception: st = None\n    date_str = st.strftime('%m/%d') if st else '??/??'\n\n    gold = '\u2705' if metadata.get('is_goldmine') else '\u2014'\n    selection = metadata.get('selection_name') or f\"#{metadata.get('selection_number', '?')}\"\n    odds = metadata.get('predicted_2nd_fav_odds')\n    odds_str = f\"{odds:.2f}\" if odds else 'N/A'\n    top5 = getattr(race, 'top_five_numbers', 'TBD')\n    gap = metadata.get('1Gap2', 0.0)\n    gap_str = f\"{gap:.2f}\"\n\n    payouts = []\n    # Check both metadata and attributes for payouts\n    for label in ('top1_place_payout', 'trifecta_payout', 'superfecta_payout'):\n        val = metadata.get(label) or getattr(race, label, None)\n        if val:\n            display_label = label.replace('_', ' ').title().replace('Top1 ', '')\n            payouts.append(f\"{display_label}: ${float(val):.2f}\")\n\n    payout_text = ' | '.join(payouts) or 'Awaiting Results'\n    return f\"| {date_str} | {race.venue} | {race.race_number} | {selection} | {odds_str} | {gap_str} | {gold} | {top5} | {payout_text} |\"\n\n\ndef format_predictions_section(qualified_races: List[Race]) -> str:\n    \"\"\"Generates the Predictions & Proof section for the GHA Job Summary.\"\"\"\n    lines = [\"### \ud83d\udd2e Fortuna Predictions & Proof\", \"\"]\n    if not qualified_races:\n        lines.append(\"No Goldmine predictions available for this run.\")\n        return \"\\n\".join(lines)\n\n    now = datetime.now(EASTERN)\n\n    def get_mtp(r):\n        st = r.start_time\n        if isinstance(st, str):\n            try:\n                st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n            except Exception:\n                return 9999\n        if st and st.tzinfo is None:\n            st = st.replace(tzinfo=EASTERN)\n        return (st - now).total_seconds() / 60 if st else 9999\n\n    # Sort by MTP ascending\n    sorted_races = sorted(qualified_races, key=get_mtp)\n    # Take top 10 opportunities\n    top_10 = sorted_races[:10]\n\n    lines.extend([\n        \"| Date | Venue | Race# | Selection | Odds | Gap | Goldmine? | Pred Top 5 | Payout Proof |\",\n        \"| --- | --- | --- | --- | --- | --- | --- | --- | --- |\"\n    ])\n    for r in top_10:\n        lines.append(format_prediction_row(r))\n    return \"\\n\".join(lines)\n\n\nasync def format_proof_section(db: FortunaDB) -> str:\n    \"\"\"Generates the Recent Audited Proof subsection for the GHA Job Summary.\"\"\"\n    lines = [\"\", \"#### \ud83d\udcb0 Recent Audited Proof\", \"\"]\n    try:\n        # First attempt to get recent goldmines\n        tips = await db.get_recent_audited_goldmines(limit=10)\n        # Fallback to any audited tips if no goldmines found\n        if not tips:\n            tips = await db.get_all_audited_tips()\n            tips = tips[:10]\n\n        if not tips:\n            lines.append(\"Awaiting race results; nothing audited yet.\")\n            return \"\\n\".join(lines)\n\n        lines.extend([\n            \"| Verdict | Profit | Venue | R# | Actual Top 5 | Actual 2nd Fav Odds | Payout Details |\",\n            \"| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\"\n        ])\n        for tip in tips:\n            payouts = []\n            if tip.get('superfecta_payout'):\n                payouts.append(f\"Superfecta ${tip['superfecta_payout']:.2f}\")\n            if tip.get('trifecta_payout'):\n                payouts.append(f\"Trifecta ${tip['trifecta_payout']:.2f}\")\n            if tip.get('top1_place_payout'):\n                payouts.append(f\"Place ${tip['top1_place_payout']:.2f}\")\n\n            payout_text = ' / '.join(payouts) if payouts else 'No payout data'\n\n            verdict = tip.get(\"verdict\", \"?\")\n            emoji = \"\u2705\" if verdict == \"CASHED\" else \"\u274c\" if verdict == \"BURNED\" else \"\u26aa\"\n            profit = tip.get('net_profit', 0.0)\n            actual_odds = tip.get('actual_2nd_fav_odds')\n            actual_odds_str = f\"{actual_odds:.2f}\" if actual_odds else \"N/A\"\n\n            lines.append(\n                f\"| {emoji} {verdict} | ${profit:+.2f} | {tip['venue']} | {tip['race_number']} | {tip.get('actual_top_5', 'N/A')} | {actual_odds_str} | {payout_text} |\"\n            )\n    except Exception as e:\n        lines.append(f\"Error generating audited proof: {e}\")\n\n    return \"\\n\".join(lines)\n\n\ndef build_harvest_table(summary: Dict[str, Any], title: str) -> str:\n    \"\"\"Generates a harvest performance table for the GHA Job Summary.\"\"\"\n    lines = [f\"### {title}\", \"\"]\n    if not summary:\n        lines.extend([\n            \"| Adapter | Races | Max Odds | Status |\",\n            \"| --- | --- | --- | --- |\",\n            \"| N/A | 0 | 0.0 | \u26a0\ufe0f No harvest data |\"\n        ])\n        return \"\\n\".join(lines)\n\n    lines.extend([\n        \"| Adapter | Races | Max Odds | Status |\",\n        \"| --- | --- | --- | --- |\"\n    ])\n\n    # Sort by Records Found (descending), then alphabetically\n    def sort_key(item):\n        adapter, data = item\n        count = data.get('count', 0) if isinstance(data, dict) else data\n        return (-count, adapter)\n\n    sorted_adapters = sorted(summary.items(), key=sort_key)\n\n    for adapter, data in sorted_adapters:\n        if isinstance(data, dict):\n            count = data.get('count', 0)\n            max_odds = data.get('max_odds', 0.0)\n        else:\n            count = data\n            max_odds = 0.0\n\n        status = '\u2705' if count > 0 else '\u26a0\ufe0f No Data'\n        lines.append(f\"| {adapter} | {count} | {max_odds:.1f} | {status} |\")\n    return \"\\n\".join(lines)\n\n\ndef format_artifact_links() -> str:\n    \"\"\"Generates the report artifacts links for the GHA Job Summary.\"\"\"\n    return '\\n'.join([\n        \"### \ud83d\udcc1 Report Artifacts\",\n        \"\",\n        \"- [Summary Grid](summary_grid.txt)\",\n        \"- [Field Matrix](field_matrix.txt)\",\n        \"- [Goldmine Report](goldmine_report.txt)\",\n        \"- [HTML Report](fortuna_report.html)\",\n        \"- [Analytics Log](analytics_report.txt)\"\n    ])\n\n\ndef write_job_summary(predictions_md: str, harvest_md: str, proof_md: str, artifacts_md: str) -> None:\n    \"\"\"Writes the consolidated sections to $GITHUB_STEP_SUMMARY.\"\"\"\n    path = os.environ.get('GITHUB_STEP_SUMMARY')\n    if not path:\n        return\n\n    # Narrate the entire workflow\n    summary = '\\n'.join([\n        predictions_md,\n        '',\n        harvest_md,\n        '',\n        proof_md,\n        '',\n        artifacts_md,\n    ])\n\n    try:\n        with open(path, \"a\", encoding=\"utf-8\") as f:\n            f.write(summary + '\\n')\n    except Exception:\n        # Silently fail if writing summary fails\n        pass\n\n\ndef get_db_path() -> str:\n    \"\"\"Returns the path to the SQLite database, using AppData in frozen mode.\"\"\"\n    if is_frozen() and sys.platform == \"win32\":\n        appdata = os.getenv('APPDATA')\n        if appdata:\n            db_dir = Path(appdata) / \"Fortuna\"\n            db_dir.mkdir(parents=True, exist_ok=True)\n            return str(db_dir / \"fortuna.db\")\n\n    return os.environ.get(\"FORTUNA_DB_PATH\", \"fortuna.db\")\n\n\nclass FortunaDB:\n    \"\"\"\n    Thread-safe SQLite backend for Fortuna using the standard library.\n    Handles persistence for tips, predictions, and audit outcomes.\n    \"\"\"\n    def __init__(self, db_path: Optional[str] = None):\n        self.db_path = db_path or get_db_path()\n        self._executor = ThreadPoolExecutor(max_workers=1)\n        self._conn = None\n        self._conn_lock = threading.Lock()\n\n        self._initialized = False\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    def _get_conn(self):\n        with self._conn_lock:\n            if not self._conn:\n                self._conn = sqlite3.connect(self.db_path, check_same_thread=False)\n            self._conn.row_factory = sqlite3.Row\n            # Enable WAL mode for better concurrency\n            self._conn.execute(\"PRAGMA journal_mode=WAL\")\n        return self._conn\n\n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Returns an async context manager for a database connection.\"\"\"\n        try:\n            import aiosqlite\n        except ImportError:\n            self.logger.error(\"aiosqlite not installed. Async database features will fail.\")\n            raise\n\n        async with aiosqlite.connect(self.db_path) as conn:\n            conn.row_factory = aiosqlite.Row\n            yield conn\n\n    async def _run_in_executor(self, func, *args):\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(self._executor, func, *args)\n\n    async def initialize(self):\n        \"\"\"Creates the database schema if it doesn't exist.\"\"\"\n        if self._initialized: return\n\n        def _init():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS schema_version (\n                        version INTEGER PRIMARY KEY,\n                        applied_at TEXT NOT NULL\n                    )\n                \"\"\")\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS harvest_logs (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        timestamp TEXT NOT NULL,\n                        region TEXT,\n                        adapter_name TEXT NOT NULL,\n                        race_count INTEGER NOT NULL,\n                        max_odds REAL\n                    )\n                \"\"\")\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS tips (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        race_id TEXT NOT NULL,\n                        venue TEXT NOT NULL,\n                        race_number INTEGER NOT NULL,\n                        discipline TEXT,\n                        start_time TEXT NOT NULL,\n                        report_date TEXT NOT NULL,\n                        is_goldmine INTEGER NOT NULL,\n                        gap12 TEXT,\n                        top_five TEXT,\n                        selection_number INTEGER,\n                        selection_name TEXT,\n                        audit_completed INTEGER DEFAULT 0,\n                        verdict TEXT,\n                        net_profit REAL,\n                        selection_position INTEGER,\n                        actual_top_5 TEXT,\n                        actual_2nd_fav_odds REAL,\n                        trifecta_payout REAL,\n                        trifecta_combination TEXT,\n                        superfecta_payout REAL,\n                        superfecta_combination TEXT,\n                        top1_place_payout REAL,\n                        top2_place_payout REAL,\n                        predicted_2nd_fav_odds REAL,\n                        audit_timestamp TEXT\n                    )\n                \"\"\")\n                # Composite index for deduplication - changed to race_id only for better deduplication\n                conn.execute(\"DROP INDEX IF EXISTS idx_race_report\")\n\n                # Cleanup potential duplicates before creating unique index (Memory Directive Fix)\n                try:\n                    self.logger.info(\"Cleaning up duplicate race_ids before indexing\")\n                    conn.execute(\"\"\"\n                        DELETE FROM tips\n                        WHERE id NOT IN (\n                            SELECT MAX(id)\n                            FROM tips\n                            GROUP BY race_id\n                        )\n                    \"\"\")\n                    self.logger.info(\"Duplicates removed, creating unique index\")\n                    conn.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS idx_race_id ON tips (race_id)\")\n                except Exception as e:\n                    self.logger.error(\"Failed to cleanup or create unique index\", error=str(e))\n                    # If index exists but table has duplicates, we might get IntegrityError\n                    # Just log it and continue - better than crashing the whole app\n                # Composite index for audit performance\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_audit_time ON tips (audit_completed, start_time)\")\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_venue ON tips (venue)\")\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_discipline ON tips (discipline)\")\n\n                # Add missing columns for existing databases\n                cursor = conn.execute(\"PRAGMA table_info(tips)\")\n                columns = [column[1] for column in cursor.fetchall()]\n                if \"superfecta_payout\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN superfecta_payout REAL\")\n                if \"superfecta_combination\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN superfecta_combination TEXT\")\n                if \"top1_place_payout\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN top1_place_payout REAL\")\n                if \"top2_place_payout\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN top2_place_payout REAL\")\n                if \"discipline\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN discipline TEXT\")\n                if \"predicted_2nd_fav_odds\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN predicted_2nd_fav_odds REAL\")\n                if \"actual_2nd_fav_odds\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN actual_2nd_fav_odds REAL\")\n                if \"selection_name\" not in columns:\n                    conn.execute(\"ALTER TABLE tips ADD COLUMN selection_name TEXT\")\n\n                # Maintenance: Purge garbage data (Memory Directive Fix)\n                try:\n                    res = conn.execute(\"DELETE FROM tips WHERE selection_name = 'Runner 2' OR predicted_2nd_fav_odds IN (2.75)\")\n                    if res.rowcount > 0:\n                        self.logger.info(\"Garbage data purged\", count=res.rowcount)\n                except Exception as e:\n                    self.logger.error(\"Failed to purge garbage data\", error=str(e))\n\n        await self._run_in_executor(_init)\n\n        # Track and execute migrations based on schema version\n        def _get_version():\n            cursor = self._get_conn().execute(\"SELECT MAX(version) FROM schema_version\")\n            row = cursor.fetchone()\n            return row[0] if row and row[0] is not None else 0\n\n        current_version = await self._run_in_executor(_get_version)\n\n        if current_version < 2:\n            await self.migrate_utc_to_eastern()\n            def _update_version():\n                with self._get_conn() as conn:\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (2, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_update_version)\n            self.logger.info(\"Schema migrated to version 2\")\n\n        if current_version < 3:\n            def _declutter():\n                # Delete old records to keep database lean (30-day retention cleanup)\n                cutoff = (datetime.now(EASTERN) - timedelta(days=30)).isoformat()\n                with self._get_conn() as conn:\n                    cursor = conn.execute(\"DELETE FROM tips WHERE report_date < ?\", (cutoff,))\n                    self.logger.info(\"Database decluttered (30-day retention cleanup)\", deleted_count=cursor.rowcount)\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (3, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_declutter)\n            self.logger.info(\"Schema migrated to version 3\")\n\n        if current_version < 4:\n            # Migration to version 4: Housekeeping & Long-term retention.\n            # 1. Clear the tips table for a fresh start as requested by JB.\n            # 2. Historical retention is now enabled (auto-cleanup removed from future migrations).\n            def _housekeeping():\n                self.logger.warning(\"Applying destructive migration: Clearing all historical tips for version 4 fresh start.\")\n                with self._get_conn() as conn:\n                    conn.execute(\"DELETE FROM tips\")\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (4, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_housekeeping)\n            self.logger.info(\"Schema migrated to version 4 (Housekeeping complete, long-term retention enabled)\")\n\n        self._initialized = True\n        self.logger.info(\"Database initialized\", path=self.db_path, schema_version=max(current_version, 4))\n\n    async def migrate_utc_to_eastern(self) -> None:\n        \"\"\"Migrates existing database records from UTC to US Eastern Time.\"\"\"\n        def _migrate():\n            conn = self._get_conn()\n            cursor = conn.execute(\"\"\"\n                SELECT id, start_time, report_date, audit_timestamp FROM tips\n                WHERE start_time LIKE '%+00:00' OR start_time LIKE '%Z'\n                OR report_date LIKE '%+00:00' OR report_date LIKE '%Z'\n                OR audit_timestamp LIKE '%+00:00' OR audit_timestamp LIKE '%Z'\n            \"\"\")\n            rows = cursor.fetchall()\n            if not rows: return\n\n            total = len(rows)\n            self.logger.info(\"Migrating legacy UTC timestamps to Eastern\", count=total)\n            converted = 0\n            errors = 0\n\n            # Process in chunks of 1000 for safety (Memory Directive Fix)\n            for i in range(0, total, 1000):\n                chunk = rows[i:i+1000]\n                with conn:\n                    for row in chunk:\n                        updates = {}\n                        for col in [\"start_time\", \"report_date\", \"audit_timestamp\"]:\n                            if col not in row.keys(): continue\n                            val = row[col]\n                            if val:\n                                try:\n                                    dt = datetime.fromisoformat(val.replace(\"Z\", \"+00:00\"))\n                                    dt_eastern = ensure_eastern(dt)\n                                    updates[col] = dt_eastern.isoformat()\n                                except Exception: pass\n                        if updates:\n                            try:\n                                set_clause = \", \".join([f\"{k} = ?\" for k in updates.keys()])\n                                conn.execute(f\"UPDATE tips SET {set_clause} WHERE id = ?\", (*updates.values(), row[\"id\"]))\n                                converted += 1\n                            except Exception as e:\n                                errors += 1\n                                self.logger.warning(\"Failed to migrate row\", row_id=row[\"id\"], error=str(e))\n                self.logger.info(\"Migration progress\", processed=min(i + 1000, total), total=total)\n\n            self.logger.info(\"Migration complete\", total=total, converted=converted, errors=errors)\n        await self._run_in_executor(_migrate)\n\n    async def log_harvest(self, harvest_summary: Dict[str, Any], region: Optional[str] = None):\n        \"\"\"Logs harvest performance metrics to the database.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _log():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN).isoformat()\n            to_insert = []\n            for adapter, data in harvest_summary.items():\n                if isinstance(data, dict):\n                    count = data.get(\"count\", 0)\n                    max_odds = data.get(\"max_odds\", 0.0)\n                else:\n                    count = data\n                    max_odds = 0.0\n\n                to_insert.append((now, region, adapter, count, max_odds))\n\n            if to_insert:\n                with conn:\n                    conn.executemany(\"\"\"\n                        INSERT INTO harvest_logs (timestamp, region, adapter_name, race_count, max_odds)\n                        VALUES (?, ?, ?, ?, ?)\n                    \"\"\", to_insert)\n\n        await self._run_in_executor(_log)\n\n    async def get_adapter_scores(self, days: int = 30) -> Dict[str, float]:\n        \"\"\"Calculates historical performance scores for each adapter.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _get():\n            conn = self._get_conn()\n            cutoff = (datetime.now(EASTERN) - timedelta(days=days)).isoformat()\n            cursor = conn.execute(\"\"\"\n                SELECT adapter_name,\n                       AVG(race_count) as avg_count,\n                       AVG(max_odds) as avg_max_odds\n                FROM harvest_logs\n                WHERE timestamp > ?\n                GROUP BY adapter_name\n            \"\"\", (cutoff,))\n\n            scores = {}\n            for row in cursor.fetchall():\n                # Heuristic: Score = Avg Race Count + (Avg Max Odds * 2)\n                # This prioritizes adapters that find races and high longshots\n                scores[row[\"adapter_name\"]] = (row[\"avg_count\"] or 0) + ((row[\"avg_max_odds\"] or 0) * 2)\n            return scores\n\n        return await self._run_in_executor(_get)\n\n    async def log_tips(self, tips: List[Dict[str, Any]], dedup_window_hours: int = 12):\n        \"\"\"Logs new tips to the database with batch deduplication.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _log():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN)\n\n            # Batch check for recently logged tips to avoid redundant entries\n            race_ids = [t.get(\"race_id\") for t in tips if t.get(\"race_id\")]\n            if not race_ids: return\n\n            placeholders = \",\".join([\"?\"] * len(race_ids))\n\n            # Use a more absolute check to ensure distinct races across all time\n            cursor = conn.execute(\n                f\"SELECT race_id FROM tips WHERE race_id IN ({placeholders})\",\n                (*race_ids,)\n            )\n            already_logged = {row[\"race_id\"] for row in cursor.fetchall()}\n\n            to_insert = []\n            for tip in tips:\n                rid = tip.get(\"race_id\")\n                if rid and rid not in already_logged:\n                    report_date = tip.get(\"report_date\") or now.isoformat()\n                    to_insert.append((\n                        rid, tip.get(\"venue\"), tip.get(\"race_number\"),\n                        tip.get(\"discipline\"), tip.get(\"start_time\"), report_date,\n                        1 if tip.get(\"is_goldmine\") else 0,\n                        str(tip.get(\"1Gap2\", 0.0)),\n                        tip.get(\"top_five\"), tip.get(\"selection_number\"), tip.get(\"selection_name\"),\n                        float(tip.get(\"predicted_2nd_fav_odds\")) if tip.get(\"predicted_2nd_fav_odds\") is not None else None\n                    ))\n                    already_logged.add(rid) # Avoid duplicates within the same batch\n\n            if to_insert:\n                with conn:\n                    conn.executemany(\"\"\"\n                        INSERT OR IGNORE INTO tips (\n                            race_id, venue, race_number, discipline, start_time, report_date,\n                            is_goldmine, gap12, top_five, selection_number, selection_name, predicted_2nd_fav_odds\n                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", to_insert)\n                self.logger.info(\"Hot tips batch logged\", count=len(to_insert))\n\n        await self._run_in_executor(_log)\n\n    async def get_unverified_tips(self, lookback_hours: int = 48) -> List[Dict[str, Any]]:\n        \"\"\"Returns tips that haven't been audited yet but have likely finished.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _get():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN)\n            cutoff = (now - timedelta(hours=lookback_hours)).isoformat()\n\n            cursor = conn.execute(\n                \"\"\"SELECT * FROM tips\n                   WHERE audit_completed = 0\n                   AND report_date > ?\n                   AND start_time < ?\"\"\",\n                (cutoff, now.isoformat())\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def get_recent_tips(self, limit: int = 20) -> List[Dict[str, Any]]:\n        \"\"\"Returns the most recent tips regardless of audit status, ordered by discovery time.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _get():\n            # Use ID DESC to show most recently discovered tips first\n            cursor = self._get_conn().execute(\n                \"SELECT * FROM tips ORDER BY id DESC LIMIT ?\",\n                (limit,)\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def update_audit_result(self, race_id: str, outcome: Dict[str, Any]):\n        \"\"\"Updates a single tip with its audit outcome.\"\"\"\n        if not self._initialized: await self.initialize()\n\n        def _update():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"\"\"\n                    UPDATE tips SET\n                        audit_completed = 1,\n                        verdict = ?,\n                        net_profit = ?,\n                        selection_position = ?,\n                        actual_top_5 = ?,\n                        actual_2nd_fav_odds = ?,\n                        trifecta_payout = ?,\n                        trifecta_combination = ?,\n                        superfecta_payout = ?,\n                        superfecta_combination = ?,\n                        top1_place_payout = ?,\n                        top2_place_payout = ?,\n                        audit_timestamp = ?\n                    WHERE id = (\n                        SELECT id FROM tips\n                        WHERE race_id = ? AND audit_completed = 0\n                        LIMIT 1\n                    )\n                \"\"\", (\n                    outcome.get(\"verdict\"), outcome.get(\"net_profit\"),\n                    outcome.get(\"selection_position\"), outcome.get(\"actual_top_5\"),\n                    outcome.get(\"actual_2nd_fav_odds\"), outcome.get(\"trifecta_payout\"),\n                    outcome.get(\"trifecta_combination\"),\n                    outcome.get(\"superfecta_payout\"),\n                    outcome.get(\"superfecta_combination\"),\n                    outcome.get(\"top1_place_payout\"),\n                    outcome.get(\"top2_place_payout\"),\n                    datetime.now(EASTERN).isoformat(),\n                    race_id\n                ))\n        await self._run_in_executor(_update)\n\n    async def update_audit_results_batch(self, outcomes: List[Tuple[str, Dict[str, Any]]]):\n        \"\"\"Updates multiple tips with their audit outcomes in a single transaction.\"\"\"\n        if not outcomes: return\n        if not self._initialized: await self.initialize()\n\n        def _update():\n            conn = self._get_conn()\n            with conn:\n                for race_id, outcome in outcomes:\n                    conn.execute(\"\"\"\n                        UPDATE tips SET\n                            audit_completed = 1,\n                            verdict = ?,\n                            net_profit = ?,\n                            selection_position = ?,\n                            actual_top_5 = ?,\n                            actual_2nd_fav_odds = ?,\n                            trifecta_payout = ?,\n                            trifecta_combination = ?,\n                            superfecta_payout = ?,\n                            superfecta_combination = ?,\n                            top1_place_payout = ?,\n                            top2_place_payout = ?,\n                            audit_timestamp = ?\n                        WHERE id = (\n                            SELECT id FROM tips\n                            WHERE race_id = ? AND audit_completed = 0\n                            LIMIT 1\n                        )\n                    \"\"\", (\n                        outcome.get(\"verdict\"), outcome.get(\"net_profit\"),\n                        outcome.get(\"selection_position\"), outcome.get(\"actual_top_5\"),\n                        outcome.get(\"actual_2nd_fav_odds\"), outcome.get(\"trifecta_payout\"),\n                        outcome.get(\"trifecta_combination\"),\n                        outcome.get(\"superfecta_payout\"),\n                        outcome.get(\"superfecta_combination\"),\n                        outcome.get(\"top1_place_payout\"),\n                        outcome.get(\"top2_place_payout\"),\n                        outcome.get(\"audit_timestamp\"),\n                        race_id\n                    ))\n        await self._run_in_executor(_update)\n\n    async def get_all_audited_tips(self) -> List[Dict[str, Any]]:\n        \"\"\"Returns all audited tips for reporting.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _get():\n            cursor = self._get_conn().execute(\n                \"SELECT * FROM tips WHERE audit_completed = 1 ORDER BY start_time DESC\"\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def get_recent_audited_goldmines(self, limit: int = 15) -> List[Dict[str, Any]]:\n        \"\"\"Returns recent successfully audited goldmine tips.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _get():\n            cursor = self._get_conn().execute(\n                \"SELECT * FROM tips WHERE audit_completed = 1 AND is_goldmine = 1 ORDER BY start_time DESC LIMIT ?\",\n                (limit,)\n            )\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def clear_all_tips(self):\n        \"\"\"Wipes all records from the tips table.\"\"\"\n        if not self._initialized: await self.initialize()\n        def _clear():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"DELETE FROM tips\")\n            conn.execute(\"VACUUM\")\n            self.logger.info(\"Database cleared (all tips deleted)\")\n        await self._run_in_executor(_clear)\n\n    async def migrate_from_json(self, json_path: str = \"hot_tips_db.json\"):\n        \"\"\"Migrates data from existing JSON file to SQLite with detailed error logging.\"\"\"\n        path = Path(json_path)\n        if not path.exists(): return\n        try:\n            with open(path, \"r\") as f:\n                data = json.load(f)\n            if not isinstance(data, list): return\n            self.logger.info(\"Migrating data from JSON\", count=len(data))\n            if not self._initialized: await self.initialize()\n\n            def _migrate():\n                conn = self._get_conn()\n                success_count = 0\n                for entry in data:\n                    try:\n                        with conn:\n                            conn.execute(\"\"\"\n                                INSERT OR IGNORE INTO tips (\n                                    race_id, venue, race_number, start_time, report_date,\n                                    is_goldmine, gap12, top_five, selection_number,\n                                    audit_completed, verdict, net_profit, selection_position,\n                                    actual_top_5, actual_2nd_fav_odds, trifecta_payout,\n                                    trifecta_combination, superfecta_payout,\n                                    superfecta_combination, top1_place_payout,\n                                    top2_place_payout, audit_timestamp\n                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                            \"\"\", (\n                                entry.get(\"race_id\"), entry.get(\"venue\"), entry.get(\"race_number\"),\n                                entry.get(\"start_time\"), entry.get(\"report_date\"),\n                                1 if entry.get(\"is_goldmine\") else 0, str(entry.get(\"1Gap2\", 0.0)),\n                                entry.get(\"top_five\"), entry.get(\"selection_number\"),\n                                1 if entry.get(\"audit_completed\") else 0, entry.get(\"verdict\"),\n                                entry.get(\"net_profit\"), entry.get(\"selection_position\"),\n                                entry.get(\"actual_top_5\"), entry.get(\"actual_2nd_fav_odds\"),\n                                entry.get(\"trifecta_payout\"), entry.get(\"trifecta_combination\"),\n                                entry.get(\"superfecta_payout\"), entry.get(\"superfecta_combination\"),\n                                entry.get(\"top1_place_payout\"), entry.get(\"top2_place_payout\"),\n                                entry.get(\"audit_timestamp\")\n                            ))\n                        success_count += 1\n                    except Exception as e:\n                        self.logger.error(\"Failed to migrate entry\", race_id=entry.get(\"race_id\"), error=str(e))\n                return success_count\n\n            count = await self._run_in_executor(_migrate)\n            self.logger.info(\"Migration complete\", successful=count)\n        except Exception as e:\n            self.logger.error(\"Migration failed\", error=str(e))\n\n    async def close(self):\n        def _close():\n            if self._conn:\n                self._conn.close()\n                self._conn = None\n\n        await self._run_in_executor(_close)\n        self._executor.shutdown(wait=True)\n\n\nclass HotTipsTracker:\n    \"\"\"Logs reported opportunities to a SQLite database.\"\"\"\n    def __init__(self, db_path: Optional[str] = None):\n        self.db = FortunaDB(db_path) if db_path else FortunaDB()\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    async def log_tips(self, races: List[Race]):\n        if not races:\n            return\n\n        await self.db.initialize()\n        now = datetime.now(EASTERN)\n        report_date = now.isoformat()\n        new_tips = []\n\n        # Strict future cutoff to prevent leakage (Never log more than 20 mins ahead)\n        future_limit = now + timedelta(minutes=45)\n\n        for r in races:\n            # Only store \"Best Bets\" (Goldmine, BET NOW, or You Might Like)\n            # These are marked in metadata by the analyzer.\n            if not r.metadata.get('is_best_bet') and not r.metadata.get('is_goldmine'):\n                continue\n\n            # Trustworthiness Airlock Safeguard (Council of Superbrains Directive)\n            active_runners = [run for run in r.runners if not run.scratched]\n            total_active = len(active_runners)\n\n            # Ensure trustworthy odds exist before logging (Memory Directive Fix)\n            if r.metadata.get('predicted_2nd_fav_odds') is None:\n                continue\n\n            if total_active > 0:\n                trustworthy_count = sum(1 for run in active_runners if run.metadata.get(\"odds_source_trustworthy\"))\n                trust_ratio = trustworthy_count / total_active\n                if trust_ratio < 0.5:\n                    self.logger.warning(\"Rejecting race with low trust_ratio for DB logging\", venue=r.venue, race=r.race_number, trust_ratio=round(trust_ratio, 2))\n                    continue\n\n            st = r.start_time\n            if isinstance(st, str):\n                try: st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except Exception: continue\n            if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n            # Reject races too far in the future\n            if st > future_limit or st < now - timedelta(minutes=10):\n                self.logger.debug(\"Rejecting far-future race\", venue=r.venue, start_time=st)\n                continue\n\n            is_goldmine = r.metadata.get('is_goldmine', False)\n            gap12 = r.metadata.get('1Gap2', 0.0)\n\n            tip_data = {\n                \"report_date\": report_date,\n                \"race_id\": r.id,\n                \"venue\": r.venue,\n                \"race_number\": r.race_number,\n                \"start_time\": r.start_time.isoformat() if isinstance(r.start_time, datetime) else str(r.start_time),\n                \"is_goldmine\": is_goldmine,\n                \"1Gap2\": gap12,\n                \"discipline\": r.discipline,\n                \"top_five\": r.top_five_numbers,\n                \"selection_number\": r.metadata.get('selection_number'),\n                \"selection_name\": r.metadata.get('selection_name'),\n                \"predicted_2nd_fav_odds\": r.metadata.get('predicted_2nd_fav_odds')\n            }\n            new_tips.append(tip_data)\n\n        try:\n            await self.db.log_tips(new_tips)\n            self.logger.info(\"Hot tips processed\", count=len(new_tips))\n        except Exception as e:\n            self.logger.error(\"Failed to log hot tips\", error=str(e))\n\n\n# ----------------------------------------\n# MONITOR LOGIC\n# ----------------------------------------\n#!/usr/bin/env python3\n\"\"\"\nFortuna Favorite-to-Place Betting Monitor\n=========================================\n\nThis script monitors racing data from multiple adapters and identifies\nbetting opportunities based on:\n1. Second favorite odds >= 4.0 decimal\n2. Races under 120 minutes to post (MTP)\n3. Superfecta availability preferred\n\nUsage:\n    python favorite_to_place_monitor.py [--date YYYY-MM-DD] [--refresh-interval 30]\n\"\"\"\n\n@dataclass\nclass RaceSummary:\n    \"\"\"Summary of a single race for display.\"\"\"\n    discipline: str  # T/H/G\n    track: str\n    race_number: int\n    field_size: int\n    superfecta_offered: bool\n    adapter: str\n    start_time: datetime\n    mtp: Optional[int] = None  # Minutes to post\n    second_fav_odds: Optional[float] = None\n    second_fav_name: Optional[str] = None\n    selection_number: Optional[int] = None\n    favorite_odds: Optional[float] = None\n    favorite_name: Optional[str] = None\n    top_five_numbers: Optional[str] = None\n    gap12: float = 0.0\n    is_goldmine: bool = False\n    is_best_bet: bool = False\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        return {\n            \"discipline\": self.discipline,\n            \"track\": self.track,\n            \"race_number\": self.race_number,\n            \"field_size\": self.field_size,\n            \"superfecta_offered\": self.superfecta_offered,\n            \"adapter\": self.adapter,\n            \"start_time\": self.start_time.isoformat(),\n            \"mtp\": self.mtp,\n            \"second_fav_odds\": self.second_fav_odds,\n            \"second_fav_name\": self.second_fav_name,\n            \"selection_number\": self.selection_number,\n            \"favorite_odds\": self.favorite_odds,\n            \"favorite_name\": self.favorite_name,\n            \"top_five_numbers\": self.top_five_numbers,\n            \"gap12\": self.gap12,\n            \"is_goldmine\": self.is_goldmine,\n            \"is_best_bet\": self.is_best_bet,\n        }\n\n\ndef get_discovery_adapter_classes() -> List[Type[BaseAdapterV3]]:\n    \"\"\"Returns all non-abstract discovery adapter classes.\"\"\"\n    def get_all_subclasses(cls):\n        return set(cls.__subclasses__()).union(\n            [s for c in cls.__subclasses__() for s in get_all_subclasses(c)]\n        )\n\n    return [\n        c for c in get_all_subclasses(BaseAdapterV3)\n        if not getattr(c, \"__abstractmethods__\", None)\n        and getattr(c, \"ADAPTER_TYPE\", \"discovery\") == \"discovery\"\n    ]\n\n\nclass FavoriteToPlaceMonitor:\n    \"\"\"Monitor for favorite-to-place betting opportunities.\"\"\"\n\n    def __init__(self, target_dates: Optional[List[str]] = None, refresh_interval: int = 30, config: Optional[Dict] = None):\n        \"\"\"\n        Initialize monitor.\n\n        Args:\n            target_dates: Dates to fetch races for (YYYY-MM-DD), defaults to today + tomorrow\n            refresh_interval: Seconds between refreshes for BET NOW list\n        \"\"\"\n        if target_dates:\n            self.target_dates = target_dates\n        else:\n            today = datetime.now(EASTERN)\n            tomorrow = today + timedelta(days=1)\n            self.target_dates = [today.strftime(\"%Y-%m-%d\"), tomorrow.strftime(\"%Y-%m-%d\")]\n\n        self.refresh_interval = refresh_interval\n        self.config = config or {}\n        self.all_races: List[RaceSummary] = []\n        self.adapters: List = []\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self.tracker = HotTipsTracker()\n\n    async def initialize_adapters(self, adapter_names: Optional[List[str]] = None):\n        \"\"\"Initialize all adapters, optionally filtered by name.\"\"\"\n        all_discovery_classes = get_discovery_adapter_classes()\n\n        classes_to_init = all_discovery_classes\n        if adapter_names:\n            classes_to_init = [c for c in all_discovery_classes if c.__name__ in adapter_names or getattr(c, \"SOURCE_NAME\", \"\") in adapter_names]\n\n        self.logger.info(\"Initializing adapters\", count=len(classes_to_init))\n\n        for adapter_class in classes_to_init:\n            try:\n                adapter = adapter_class(config={\"region\": self.config.get(\"region\")})\n                self.adapters.append(adapter)\n                self.logger.debug(\"Adapter initialized\", adapter=adapter_class.__name__)\n            except Exception as e:\n                self.logger.error(\"Adapter initialization failed\", adapter=adapter_class.__name__, error=str(e))\n\n        self.logger.info(\"Adapters initialization complete\", initialized=len(self.adapters))\n\n    async def fetch_all_races(self) -> List[Tuple[Race, str]]:\n        \"\"\"Fetch races from all adapters.\"\"\"\n        self.logger.info(\"Fetching races\", dates=self.target_dates)\n\n        all_races_with_adapters = []\n\n        # Run fetches in parallel for speed\n        async def fetch_one(adapter, date_str):\n            name = adapter.__class__.__name__\n            try:\n                races = await adapter.get_races(date_str)\n                self.logger.info(\"Fetch complete\", adapter=name, date=date_str, count=len(races))\n                return [(r, name) for r in races]\n            except Exception as e:\n                self.logger.error(\"Fetch failed\", adapter=name, date=date_str, error=str(e))\n                return []\n\n        fetch_tasks = []\n        for d in self.target_dates:\n            for a in self.adapters:\n                fetch_tasks.append(fetch_one(a, d))\n\n        results = await asyncio.gather(*fetch_tasks)\n        for r_list in results:\n            all_races_with_adapters.extend(r_list)\n\n        self.logger.info(\"Total races fetched\", total=len(all_races_with_adapters))\n        return all_races_with_adapters\n\n    def _get_discipline_code(self, race: Race) -> str:\n        \"\"\"Get discipline code (T/H/G).\"\"\"\n        if not race.discipline:\n            return \"T\"\n\n        d = race.discipline.lower()\n        if \"harness\" in d or \"standardbred\" in d: return \"H\"\n        if \"greyhound\" in d or \"dog\" in d: return \"G\"\n        return \"T\"\n\n    def _calculate_field_size(self, race: Race) -> int:\n        \"\"\"Calculate active field size.\"\"\"\n        return len([r for r in race.runners if not r.scratched])\n\n    def _has_superfecta(self, race: Race) -> bool:\n        \"\"\"Check if race offers Superfecta.\"\"\"\n        ab = race.available_bets or []\n        # Support metadata fallback if field not populated\n        if not ab and hasattr(race, 'metadata'):\n            ab = race.metadata.get('available_bets', [])\n        return \"Superfecta\" in ab\n\n    def _get_top_runners(self, race: Race, limit: int = 5) -> List[Runner]:\n        \"\"\"Get top runners by odds, sorted lowest first.\"\"\"\n        # Get active runners with valid odds\n        r_with_odds = []\n        for r in race.runners:\n            if r.scratched:\n                continue\n            # Refresh odds to avoid stale metadata in continuous monitor mode\n            wo = _get_best_win_odds(r)\n            if wo is not None and wo > 1.0:\n                # Update runner object with fresh odds for downstream summaries\n                r.win_odds = float(wo)\n                # Store the Decimal odds directly for sorting to avoid conversion\n                r_with_odds.append((r, wo))\n\n        if not r_with_odds:\n            return []\n\n        # Sort by odds (lowest first)\n        sorted_r = sorted(r_with_odds, key=lambda x: x[1])\n        return [x[0] for x in sorted_r[:limit]]\n\n    def _calculate_mtp(self, start_time: Optional[datetime]) -> int:\n        \"\"\"Calculate minutes to post. Returns -9999 if start_time is None.\"\"\"\n        if not start_time: return -9999\n        now = now_eastern()\n        # Use ensure_eastern to handle naive or other timezones correctly\n        st = ensure_eastern(start_time)\n        delta = st - now\n        return int(delta.total_seconds() / 60)\n\n    def _get_top_n_runners(self, race: Race, n: int = 5) -> str:\n        \"\"\"Get top N runners by win odds.\"\"\"\n        top_runners = self._get_top_runners(race, limit=n)\n        return \", \".join([str(r.number) if r.number is not None else \"?\" for r in top_runners])\n\n    def _create_race_summary(self, race: Race, adapter_name: str) -> RaceSummary:\n        \"\"\"Create a RaceSummary from a Race object.\"\"\"\n        top_runners = self._get_top_runners(race, limit=5)\n        favorite = top_runners[0] if len(top_runners) >= 1 else None\n        second_fav = top_runners[1] if len(top_runners) >= 2 else None\n\n        gap12 = 0.0\n        if favorite and second_fav and favorite.win_odds and second_fav.win_odds:\n            gap12 = round(second_fav.win_odds - favorite.win_odds, 2)\n\n        return RaceSummary(\n            discipline=self._get_discipline_code(race),\n            track=normalize_venue_name(race.venue),\n            race_number=race.race_number,\n            field_size=self._calculate_field_size(race),\n            superfecta_offered=self._has_superfecta(race),\n            adapter=adapter_name,\n            start_time=race.start_time,\n            mtp=self._calculate_mtp(race.start_time),\n            second_fav_odds=second_fav.win_odds if second_fav else None,\n            second_fav_name=second_fav.name if second_fav else None,\n            selection_number=second_fav.number if second_fav else None,\n            favorite_odds=favorite.win_odds if favorite else None,\n            favorite_name=favorite.name if favorite else None,\n            top_five_numbers=self._get_top_n_runners(race, 5),\n            gap12=gap12,\n            is_goldmine=race.metadata.get('is_goldmine', False),\n            is_best_bet=race.metadata.get('is_best_bet', False)\n        )\n\n    async def build_race_summaries(self, races_with_adapters: List[Tuple[Race, str]], window_hours: Optional[int] = 12):\n        \"\"\"Build and deduplicate summary list, with optional time window filtering.\"\"\"\n        race_map = {}\n        now = datetime.now(EASTERN)\n        cutoff = now + timedelta(hours=window_hours) if window_hours else None\n\n        for race, adapter_name in races_with_adapters:\n            try:\n                # Time window filtering\n                st = race.start_time\n                if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n                # Time window filtering removed to ensure all unique races are counted\n\n                summary = self._create_race_summary(race, adapter_name)\n                # Stable key: Canonical Venue + Race Number + Date\n                canonical_venue = get_canonical_venue(summary.track)\n                date_str = summary.start_time.strftime('%Y%m%d') if summary.start_time else \"Unknown\"\n                key = f\"{canonical_venue}|{summary.race_number}|{date_str}\"\n\n                if key not in race_map:\n                    race_map[key] = summary\n                else:\n                    existing = race_map[key]\n                    # Prefer the one with valid second favorite odds\n                    if summary.second_fav_odds and not existing.second_fav_odds:\n                        race_map[key] = summary\n                    # Or prefer more detailed available bets\n                    elif summary.superfecta_offered and not existing.superfecta_offered:\n                        race_map[key] = summary\n            except Exception: pass\n\n        unique_summaries = list(race_map.values())\n        self.all_races = sorted(unique_summaries, key=lambda x: x.start_time)\n\n        # GPT5 Improvement: Keep all races within window for analysis, not just one per track.\n        # Window broadened to 18 hours (News Mode)\n        timing_window_summaries = []\n        now = datetime.now(EASTERN)\n        for summary in unique_summaries:\n            st = summary.start_time\n            if st.tzinfo is None: st = st.replace(tzinfo=EASTERN)\n\n            # Calculate Minutes to Post\n            diff = st - now\n            mtp = diff.total_seconds() / 60\n\n            # Broaden window to 18 hours to ensure yield for \"News\"\n            if -45 < mtp <= 1080: # 18 hours\n                timing_window_summaries.append(summary)\n\n        self.golden_zone_races = timing_window_summaries\n        if not self.golden_zone_races:\n            self.logger.warning(\"\ud83d\udd2d Monitor found 0 races in the Broadened Window (-45m to 18h)\", total_unique=len(unique_summaries))\n\n    def print_full_list(self):\n        \"\"\"Log all fetched races.\"\"\"\n        lines = [\n            \"=\" * 120,\n            \"FULL RACE LIST\".center(120),\n            \"=\" * 120,\n            f\"{'DISC':<5} {'TRACK':<25} {'R#':<4} {'FIELD':<6} {'SUPER':<6} {'ADAPTER':<25} {'START TIME':<20}\",\n            \"-\" * 120\n        ]\n        for r in sorted(self.all_races, key=lambda x: (x.discipline, x.track, x.race_number)):\n            superfecta = \"Yes\" if r.superfecta_offered else \"No\"\n            # Display time in Eastern with ET suffix\n            st = r.start_time.strftime(\"%Y-%m-%d %H:%M ET\") if r.start_time else \"Unknown\"\n            lines.append(f\"{r.discipline:<5} {r.track[:24]:<25} {r.race_number:<4} {r.field_size:<6} {superfecta:<6} {r.adapter[:24]:<25} {st:<20}\")\n        lines.append(\"-\" * 120)\n        lines.append(f\"Total races: {len(self.all_races)}\")\n        self.logger.info(\"\\n\".join(lines))\n\n    def get_bet_now_races(self) -> List[RaceSummary]:\n        \"\"\"Get races meeting BET NOW criteria.\"\"\"\n        # 1. MTP <= 120 (Broadened for yield)\n        # 2. 2nd Fav Odds >= 4.0\n        # 3. Field size <= 11 (User Directive)\n        # 4. Gap > 0.25 (User Directive)\n        bet_now = [\n            r for r in self.golden_zone_races\n            if r.mtp is not None and -10 < r.mtp <= 120\n            and r.second_fav_odds is not None and r.second_fav_odds >= 4.0\n            and r.field_size <= 11\n            and r.gap12 > 0.25\n        ]\n        # Sort by Superfecta desc, then MTP asc\n        bet_now.sort(key=lambda r: (not r.superfecta_offered, r.mtp))\n        return bet_now\n\n    def get_you_might_like_races(self) -> List[RaceSummary]:\n        \"\"\"Get 'You Might Like' races with relaxed criteria.\"\"\"\n        # Criteria: Not in BET NOW, but -10 < MTP <= 240 (4h) and 2nd Fav Odds >= 3.0\n        # and field size <= 11 and Gap > 0.25\n        bet_now_keys = {(r.track, r.race_number) for r in self.get_bet_now_races()}\n        yml = [\n            r for r in self.golden_zone_races\n            if r.mtp is not None and -10 < r.mtp <= 240\n            and r.second_fav_odds is not None and r.second_fav_odds >= 3.0\n            and r.field_size <= 11\n            and r.gap12 > 0.25\n            and (r.track, r.race_number) not in bet_now_keys\n        ]\n        # Sort by MTP asc\n        yml.sort(key=lambda r: r.mtp)\n        return yml[:5]  # Limit to top 5 recommendations\n\n    async def print_bet_now_list(self):\n        \"\"\"Log filtered BET NOW list and recent audited goldmine results.\"\"\"\n        bet_now = self.get_bet_now_races()\n        lines = [\n            \"=\" * 140,\n            \"\ud83c\udfaf BET NOW - FAVORITE TO PLACE OPPORTUNITIES\".center(140),\n            \"=\" * 140,\n            f\"Updated: {datetime.now(EASTERN).strftime('%Y-%m-%d %H:%M:%S')} ET\",\n            \"Criteria: -10 < MTP <= 120 minutes AND 2nd Favorite Odds >= 4.0\",\n            \"-\" * 140\n        ]\n        if not bet_now:\n            lines.append(\"\u23f3 No races currently meet BET NOW criteria.\")\n            yml = self.get_you_might_like_races()\n            if yml:\n                lines.extend([\n                    \"=\" * 160,\n                    \"\ud83c\udf1f YOU MIGHT LIKE - NEAR-MISS OPPORTUNITIES\".center(160),\n                    \"=\" * 160,\n                    f\"{'SUPER':<6} {'MTP':<5} {'DISC':<5} {'TRACK':<20} {'R#':<4} {'FIELD':<6} {'ODDS':<20} {'TOP 5':<20}\",\n                    \"-\" * 160\n                ])\n                for r in yml:\n                    sup = \"\u2705\" if r.superfecta_offered else \"\u274c\"\n                    fo = f\"{r.favorite_odds:.2f}\" if r.favorite_odds else \"N/A\"\n                    so = f\"{r.second_fav_odds:.2f}\" if r.second_fav_odds else \"N/A\"\n                    top5 = r.top_five_numbers or \"N/A\"\n                    # Leading zero alignment (Memory Directive Fix)\n                    m_str = f\"{r.mtp:02d}\" if 0 <= r.mtp < 10 else str(r.mtp)\n                    lines.append(f\"{sup:<6} {m_str:<5} {r.discipline:<5} {r.track[:19]:<20} {r.race_number:<4} {r.field_size:<6}  ~ {fo}, {so:<15} [{top5}]\")\n                lines.append(\"-\" * 160)\n            self.logger.info(\"\\n\".join(lines))\n            return\n\n        lines.extend([\n            f\"{'SUPER':<6} {'MTP':<5} {'DISC':<5} {'TRACK':<20} {'R#':<4} {'FIELD':<6} {'ODDS':<20} {'TOP 5':<20}\",\n            \"-\" * 160\n        ])\n        for r in bet_now:\n            sup = \"\u2705\" if r.superfecta_offered else \"\u274c\"\n            fo = f\"{r.favorite_odds:.2f}\" if r.favorite_odds else \"N/A\"\n            so = f\"{r.second_fav_odds:.2f}\" if r.second_fav_odds else \"N/A\"\n            top5 = r.top_five_numbers or \"N/A\"\n            m_str = f\"{r.mtp:02d}\" if 0 <= r.mtp < 10 else str(r.mtp)\n            lines.append(f\"{sup:<6} {m_str:<5} {r.discipline:<5} {r.track[:19]:<20} {r.race_number:<4} {r.field_size:<6}  ~ {fo}, {so:<15} [{top5}]\")\n        lines.extend([\"-\" * 160, f\"Total opportunities: {len(bet_now)}\"])\n        self.logger.info(\"\\n\".join(lines))\n\n        # Include recent audited results to provide proof of system performance\n        history = await self.tracker.db.get_recent_audited_goldmines(limit=10)\n        if history:\n            historical_report = generate_historical_goldmine_report(history)\n            self.logger.info(historical_report)\n\n    def save_to_json(self, filename: str = \"race_data.json\"):\n        \"\"\"Export to JSON.\"\"\"\n        bn = self.get_bet_now_races()\n        yml = self.get_you_might_like_races()\n\n        if not bn:\n            self.logger.warning(\"\ud83d\udd2d Monitor found 0 BET NOW opportunities\", total_checked=len(self.golden_zone_races))\n            # Structured telemetry for monitoring\n            structlog.get_logger(\"FortunaTelemetry\").warning(\"empty_bet_now_list\", golden_zone_count=len(self.golden_zone_races))\n            # Create an indicator file for downstream monitoring (GPT5 Improvement)\n            try:\n                Path(\"monitor_empty.alert\").write_text(datetime.now(EASTERN).isoformat())\n            except Exception: pass\n        else:\n            # Clear alert if it exists\n            try:\n                alert_file = Path(\"monitor_empty.alert\")\n                if alert_file.exists(): alert_file.unlink()\n            except Exception: pass\n\n        data = {\n            \"generated_at\": datetime.now(EASTERN).isoformat(),\n            \"target_dates\": self.target_dates,\n            \"total_races\": len(self.all_races),\n            \"bet_now_count\": len(bn),\n            \"you_might_like_count\": len(yml),\n            \"all_races\": [r.to_dict() for r in self.all_races],\n            \"bet_now_races\": [r.to_dict() for r in bn],\n            \"you_might_like_races\": [r.to_dict() for r in yml],\n        }\n        try:\n            # Ensure parent directory exists (GPT5 Improvement)\n            Path(filename).parent.mkdir(parents=True, exist_ok=True)\n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:\n            self.logger.error(\"failed_saving_race_data\", path=filename, error=str(e))\n\n        # Persistent history log\n        self._append_to_history(bn + yml)\n\n    def _append_to_history(self, races: List[RaceSummary]):\n        \"\"\"Append races to persistent history for future result matching.\"\"\"\n        if not races: return\n        history_file = \"prediction_history.jsonl\"\n        timestamp = datetime.now(EASTERN).isoformat()\n        try:\n            with open(history_file, 'a') as f:\n                for r in races:\n                    record = r.to_dict()\n                    record[\"logged_at\"] = timestamp\n                    f.write(json.dumps(record) + \"\\n\")\n        except Exception as e:\n            self.logger.error(\"History logging failed\", error=str(e))\n\n    async def run_once(self, loaded_races: Optional[List[Race]] = None, adapter_names: Optional[List[str]] = None):\n        try:\n            if loaded_races is not None:\n                self.logger.info(\"Using loaded races\", count=len(loaded_races))\n                # Map to (Race, AdapterName) tuple expected by build_race_summaries\n                raw = [(r, r.source) for r in loaded_races]\n            else:\n                await self.initialize_adapters(adapter_names=adapter_names)\n                raw = await self.fetch_all_races()\n\n            await self.build_race_summaries(raw, window_hours=12) # Use 12h window for monitor\n            self.print_full_list()\n            await self.print_bet_now_list()\n            self.save_to_json()\n        finally:\n            for a in self.adapters: await a.shutdown()\n            await GlobalResourceManager.cleanup()\n\n    async def run_continuous(self):\n        await self.initialize_adapters()\n        raw = await self.fetch_all_races()\n        await self.build_race_summaries(raw, window_hours=12)\n        self.print_full_list()\n        try:\n            for _ in range(1000): # Iteration limit to prevent potential hangs\n                for r in self.all_races: r.mtp = self._calculate_mtp(r.start_time)\n                await self.print_bet_now_list()\n                self.save_to_json()\n                await asyncio.sleep(self.refresh_interval)\n        except KeyboardInterrupt:\n            self.logger.info(\"Stopped by user\")\n        finally:\n            for a in self.adapters: await a.shutdown()\n            await GlobalResourceManager.cleanup()\n\n\n\n\n\n# ----------------------------------------\n# EXPANDED ADAPTERS\n# ----------------------------------------\n# python_service/adapters/oddschecker_adapter.py\n\n\n\n\n\nclass OddscheckerAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Oddschecker is heavily protected by Cloudflare; Playwright with high timeout and network idle\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=120,\n            network_idle=True\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs: Any) -> Any:\n        # Playwright doesn't use impersonate but SmartFetcher handles it now\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.oddschecker.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        self._save_debug_html(index_response.text, f\"oddschecker_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Find all links to individual race pages\n        metadata = []\n\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        # Group by track to pick \"next\" race\n        track_map = defaultdict(list)\n\n        # Broaden selectors for race links\n        for selector in [\"a.race-time-link[href]\", \"a[href*='/horse-racing/'][href*='/20']\", \".rf__link\"]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\")\n                if href and not href.endswith(\"/horse-racing\"):\n                    # Ensure absolute URL\n                    full_url = href if href.startswith(\"http\") else f\"{self.BASE_URL}{href}\"\n\n                    # Extract track from URL if possible, or use parent\n                    # URL usually /horse-racing/venue/date/time\n                    parts = full_url.split(\"/\")\n                    if len(parts) >= 6:\n                        track = parts[4]\n                        txt = node_text(a) # Time is often in text\n                        track_map[track].append({\"url\": full_url, \"time_txt\": txt})\n\n        for track, races in track_map.items():\n            for r in races:\n                if re.match(r\"\\d{1,2}:\\d{2}\", r[\"time_txt\"]):\n                    try:\n                        rt = datetime.strptime(r[\"time_txt\"], \"%H:%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        # Broaden window to capture multiple races (Memory Directive Fix)\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n\n                        metadata.append(r[\"url\"])\n                    except Exception: pass\n\n        if not metadata:\n            self.logger.warning(\"No metadata found\", context=\"Oddschecker Index Parsing\", url=index_url)\n            return None\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in metadata]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                race = self._parse_race_page(parser, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        track_name_node = parser.css_first(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.text(strip=True)\n\n        race_time_node = parser.css_first(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = parser.css_first(\"a.race-time-link.active\")\n        race_number = 0\n        if active_link:\n            all_links = parser.css(\"a.race-time-link\")\n            try:\n                for i, link in enumerate(all_links):\n                    if link.html == active_link.html:\n                        race_number = i + 1\n                        break\n            except Exception:\n                pass\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in parser.css(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.text(strip=True)\n\n            odds_node = row.css_first(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.text(strip=True)\n\n            number_node = row.css_first(\"td.runner-number\")\n            number = 0\n            if number_node:\n                num_txt = \"\".join(filter(str.isdigit, number_node.text(strip=True)))\n                if num_txt:\n                    number = int(num_txt)\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds_dict[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n\n# python_service/adapters/timeform_adapter.py\n\n\n\n\n\nclass TimeformAdapter(JSONParsingMixin, BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3 and standardized on selectolax.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        self._semaphore = asyncio.Semaphore(5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Timeform often blocks basic requests; Playwright is robust\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            network_idle=True\n        )\n\n    def _get_headers(self) -> dict:\n        headers = self._get_browser_headers(host=\"www.timeform.com\")\n        headers.update({\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n        })\n        return headers\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"timeform_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Updated selector for race links\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        track_map = defaultdict(list)\n        # Broaden selectors for Timeform race links\n        for selector in [\"a[href*='/racecards/']\", \".rf__link\", \"a.rf-meeting-race__time\", \".rp-meetingItem__race__time\"]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\")\n                if href and \"/racecards/\" in href and not href.endswith(\"/racecards\"):\n                    # URL usually: /horse-racing/racecards/venue/date/time/...\n                    # or: /racecards/venue/date/time\n                    parts = href.split(\"/\")\n                    # Handle both relative and absolute-ish paths\n                    track = \"unknown\"\n                    for i, p in enumerate(parts):\n                        if p == \"racecards\" and i + 1 < len(parts):\n                            track = parts[i+1]\n                            break\n\n                    txt = node_text(a)\n                    track_map[track].append({\"url\": href, \"time_txt\": txt})\n\n        links = []\n        for track, races in track_map.items():\n            for r in races:\n                # Timeform often uses HH:MM in text\n                time_match = re.search(r\"(\\d{1,2}:\\d{2})\", r[\"time_txt\"])\n                if time_match:\n                    try:\n                        rt = datetime.strptime(time_match.group(1), \"%H:%M\").replace(\n                            year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                        )\n                        # Broaden window to capture multiple races (Memory Directive Fix)\n                        diff = (rt - now_site).total_seconds() / 60\n                        if not (-45 < diff <= 1080):\n                            continue\n\n                        full_url = r[\"url\"] if r[\"url\"].startswith(\"http\") else f\"{self.BASE_URL}{r['url']}\"\n                        links.append(full_url)\n                    except Exception: pass\n\n        if not links:\n            self.logger.warning(\"No metadata found\", context=\"Timeform Index Parsing\", url=index_url)\n            return None\n\n        async def fetch_single_html(url_path: str):\n            async with self._semaphore:\n                await asyncio.sleep(0.5)\n                response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n                return (url_path, response.text) if response else (url_path, \"\")\n\n        self.logger.info(f\"Found {len(links)} race links on Timeform\")\n        tasks = [fetch_single_html(link) for link in links]\n        results = await asyncio.gather(*tasks)\n        return {\"pages\": [r for r in results if r[1]], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for url_path, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n\n                # Extract via JSON-LD if possible\n                venue = \"\"\n                start_time = None\n                scripts = self._parse_all_jsons_from_scripts(parser, 'script[type=\"application/ld+json\"]', context=\"Betfair Index\")\n                for data in scripts:\n                    if data.get(\"@type\") == \"Event\":\n                        venue = normalize_venue_name(data.get(\"location\", {}).get(\"name\", \"\"))\n                        if sd := data.get(\"startDate\"):\n                            # 2026-01-28T14:32:00\n                            start_time = datetime.fromisoformat(sd.split('+')[0])\n                        break\n\n                if not venue:\n                    # Fallback to title\n                    title = parser.css_first(\"title\")\n                    if title:\n                        # 14:32 DUNDALK | Races 28 January 2026 ...\n                        match = re.search(r'(\\d{1,2}:\\d{2})\\s+([^|]+)', title.text())\n                        if match:\n                            time_str = match.group(1)\n                            venue = normalize_venue_name(match.group(2).strip())\n                            start_time = datetime.combine(race_date, datetime.strptime(time_str, \"%H:%M\").time())\n\n                if not venue or not start_time:\n                    continue\n\n                # Betting Forecast Parsing\n                forecast_map = {}\n                verdict_section = parser.css_first(\"section.rp-verdict\")\n                if verdict_section:\n                    forecast_text = clean_text(verdict_section.text())\n                    if \"Betting Forecast :\" in forecast_text:\n                        # \"Betting Forecast : 15/8 2.87 Spring Is Here, 3/1 4 This Guy, ...\"\n                        after_forecast = forecast_text.split(\"Betting Forecast :\")[1]\n                        # Split by comma\n                        parts = after_forecast.split(',')\n                        for part in parts:\n                            # Match odds and then name\n                            # Odds can be fractional space decimal\n                            m = re.search(r'(\\d+/\\d+|EVENS)\\s+([\\d\\.]+)?\\s*(.+)', part.strip())\n                            if m:\n                                odds_str = m.group(1)\n                                name = clean_text(m.group(3))\n                                forecast_map[name.lower()] = odds_str\n\n                # Runners\n                runners = []\n                # Use tbody as the main container for each runner\n                for row in parser.css('tbody.rp-horse-row'):\n                    if runner := self._parse_runner(row, forecast_map):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                # Race number from URL or sequence\n                race_number = 0\n                num_match = re.search(r'/(\\d+)/([^/]+)$', url_path)\n                # .../1432/207/1/view... -> the '1' is the race number\n                url_parts = url_path.split('/')\n                if len(url_parts) >= 10:\n                    try: race_number = int(url_parts[9])\n                    except Exception: pass\n\n                race = Race(\n                    id=f\"tf_{venue.lower().replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except Exception as e:\n                self.logger.warning(f\"Error parsing Timeform race: {e}\")\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Node, forecast_map: dict = None) -> Optional[Runner]:\n        \"\"\"Parses a single runner from a table row node.\"\"\"\n        try:\n            name_node = row.css_first(\"a.rp-horse\") or row.css_first(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            number = 0\n            num_attr = row.attributes.get(\"data-entrynumber\")\n            if num_attr:\n                try:\n                    val = int(num_attr)\n                    if val <= 40: number = val\n                except Exception:\n                    pass\n\n            if not number:\n                num_node = row.css_first(\".rp-entry-number\") or row.css_first(\"span.rp-horseTable_horse-number\")\n                if num_node:\n                    num_text = clean_text(num_node.text()).strip(\"()\")\n                    num_match = re.search(r\"\\d+\", num_text)\n                    if num_match:\n                        val = int(num_match.group())\n                        if val <= 40: number = val\n\n            win_odds = None\n            if forecast_map:\n                win_odds = parse_odds_to_decimal(forecast_map.get(name.lower()))\n\n            # Try to find live odds button if available (old selector)\n            if not win_odds:\n                odds_tag = row.css_first(\"button.rp-bet-placer-btn__odds\")\n                if odds_tag:\n                    win_odds = parse_odds_to_decimal(clean_text(odds_tag.text()))\n\n            odds_data = {}\n            if odds_val := create_odds_data(self.source_name, win_odds):\n                odds_data[self.source_name] = odds_val\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            return None\n\n# python_service/adapters/racingpost_adapter.py\n\n\n\n\nclass RacingPostAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # RacingPost has strong anti-bot measures. Playwright with stealth is usually the best bet.\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=90,\n            block_resources=False,\n            network_idle=True\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date, including international.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        # RacingPost international URL sometimes varies\n        intl_urls = [\n            f\"/racecards/international/{date}\",\n            f\"/racecards/{date}/international\",\n            \"/racecards/international\"\n        ]\n\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n\n        intl_response = None\n        for url in intl_urls:\n            resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n            if resp and resp.status == 200:\n                intl_response = resp\n                break\n\n        race_card_urls = []\n        try:\n            target_date = datetime.strptime(date, \"%Y-%m-%d\").date()\n        except Exception:\n            target_date = datetime.now(EASTERN).date()\n\n        site_tz = ZoneInfo(\"Europe/London\")\n        now_site = datetime.now(site_tz)\n\n        if index_response and index_response.text:\n            self._save_debug_html(index_response.text, f\"racingpost_index_{date}\")\n            index_parser = HTMLParser(index_response.text)\n\n            # Broaden window to capture multiple races (Memory Directive Fix)\n            meetings = index_parser.css('.rp-raceCourse__panel') or index_parser.css('.RC-meetingItem') or index_parser.css('.rp-meetingItem') or index_parser.css('.RC-courseCards')\n            for meeting in meetings:\n                # Broaden a tag selectors to catch new Racing Post structures\n                for link in meeting.css('a[data-test-selector^=\"RC-meetingItem__link_race\"], a.rp-raceCourse__panel__race__time, a.rp-meetingItem__race__time, a.RC-meetingItem__race__time, a.RC-meetingItem__link, a[href*=\"/racecards/\"]'):\n                    href = link.attributes.get(\"href\", \"\")\n                    if not href or \"/results/\" in href:\n                        continue\n\n                    txt = clean_text(node_text(link))\n                    time_match = re.search(r\"(\\d{1,2}:\\d{2})\", txt)\n                    if time_match:\n                        try:\n                            time_str = time_match.group(1)\n                            tm = datetime.strptime(time_str, \"%H:%M\")\n                            if tm.hour < 9:\n                                tm = tm.replace(hour=tm.hour + 12)\n\n                            rt = tm.replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n                        except Exception: pass\n\n                    race_card_urls.append(href)\n\n        elif index_response:\n            self.logger.warning(\"Unexpected status\", status=index_response.status, url=index_url)\n\n        if intl_response and intl_response.text:\n            self._save_debug_html(intl_response.text, f\"racingpost_intl_index_{date}\")\n            intl_parser = HTMLParser(intl_response.text)\n\n            meetings = intl_parser.css('.rp-raceCourse__panel') or intl_parser.css('.RC-meetingItem') or intl_parser.css('.rp-meetingItem') or intl_parser.css('.RC-courseCards')\n            for meeting in meetings:\n                for link in meeting.css('a[data-test-selector^=\"RC-meetingItem__link_race\"], a.rp-raceCourse__panel__race__time, a.rp-meetingItem__race__time, a.RC-meetingItem__race__time, a.RC-meetingItem__link, a[href*=\"/racecards/\"]'):\n                    href = link.attributes.get(\"href\", \"\")\n                    if not href or \"/results/\" in href:\n                        continue\n\n                    txt = clean_text(node_text(link))\n                    time_match = re.search(r\"(\\d{1,2}:\\d{2})\", txt)\n                    if time_match:\n                        try:\n                            time_str = time_match.group(1)\n                            tm = datetime.strptime(time_str, \"%H:%M\")\n                            if tm.hour < 9:\n                                tm = tm.replace(hour=tm.hour + 12)\n\n                            rt = tm.replace(\n                                year=target_date.year, month=target_date.month, day=target_date.day, tzinfo=site_tz\n                            )\n                            diff = (rt - now_site).total_seconds() / 60\n                            if not (-45 < diff <= 1080):\n                                continue\n                        except Exception: pass\n\n                    race_card_urls.append(href)\n        elif intl_response:\n            self.logger.warning(\"Unexpected status\", status=intl_response.status, url=intl_url)\n\n        if not race_card_urls:\n            self.logger.warning(\"Standard RacingPost link discovery failed, trying aggressive fallback\", date=date)\n            if index_response and index_response.text:\n                index_parser = HTMLParser(index_response.text)\n                for a in index_parser.css('a[href*=\"/racecards/\"]'):\n                    href = a.attributes.get(\"href\", \"\")\n                    if re.search(r\"/\\d+/.*/\\d{4}-\\d{2}-\\d{2}/\\d+\", href):\n                        race_card_urls.append(href)\n\n            if intl_response and intl_response.text:\n                intl_parser = HTMLParser(intl_response.text)\n                for a in intl_parser.css('a[href*=\"/racecards/\"]'):\n                    href = a.attributes.get(\"href\", \"\")\n                    if re.search(r\"/\\d+/.*/\\d{4}-\\d{2}-\\d{2}/\\d+\", href):\n                        race_card_urls.append(href)\n\n        if not race_card_urls:\n            self.logger.warning(\"Failed to fetch RacingPost racecard links\", date=date)\n            return None\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(\"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n                if not venue_node:\n                    continue\n                venue_raw = venue_node.text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n\n                race_time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n                if not race_time_node:\n                    continue\n                race_time_str = race_time_node.text(strip=True)\n\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            if runner := self._parse_runner(node):\n                runners.append(runner)\n        return runners\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n            name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n            odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n            if not all([number_node, name_node, odds_node]):\n                return None\n\n            number_str = clean_text(number_node.text())\n            number = 0\n            if number_str:\n                num_txt = \"\".join(filter(str.isdigit, number_str))\n                if num_txt:\n                    val = int(num_txt)\n                    if val <= 40: number = val\n            name = clean_text(name_node.text())\n            odds_str = clean_text(odds_node.text())\n            scratched = \"NR\" in odds_str.upper() or not odds_str\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if odds_data := create_odds_data(self.source_name, win_odds):\n                    odds[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError):\n            self.logger.warning(\"Could not parse RacingPost runner, skipping.\", exc_info=True)\n            return None\n\n\nclass RacingPostToteAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Tote dividends and results from Racing Post.\n    \"\"\"\n    ADAPTER_TYPE = \"results\"\n    SOURCE_NAME = \"RacingPostTote\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            timeout=45\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        url = f\"/results/{date}\"\n        resp = await self.make_request(\"GET\", url, headers=self._get_headers())\n        if not resp or not resp.text:\n            return None\n\n        self._save_debug_snapshot(resp.text, f\"rp_tote_results_{date}\")\n        parser = HTMLParser(resp.text)\n\n        # Extract links to individual race results\n        links = set()\n        selectors = [\n            'a[data-test-selector=\"RC-meetingItem__link_race\"]',\n            'a[href*=\"/results/\"]',\n            '.ui-link.rp-raceCourse__panel__race__time',\n            'a.rp-raceCourse__panel__race__time'\n        ]\n        target_venues = getattr(self, \"target_venues\", None)\n        for s in selectors:\n            for a in parser.css(s):\n                href = a.attributes.get(\"href\")\n                if href:\n                    # Filter by venue\n                    if target_venues:\n                        match_found = False\n                        for v in target_venues:\n                            if v in href.lower().replace(\"-\", \"\"):\n                                match_found = True\n                                break\n                        if not match_found:\n                            v_text = get_canonical_venue(node_text(a))\n                            if v_text in target_venues:\n                                match_found = True\n                        if not match_found:\n                            continue\n\n                    # Broaden regex to match various RP result link patterns (Memory Directive Fix)\n                    if re.search(r\"/results/.*?\\d{5,}\", href) or \\\n                       re.search(r\"/results/\\d+/\", href) or \\\n                       re.search(r\"/\\d{4}-\\d{2}-\\d{2}/\", href) or \\\n                       len(href.split(\"/\")) >= 4:\n                        links.add(href if href.startswith(\"http\") else f\"{self.BASE_URL}{href}\")\n\n        async def fetch_result_page(link):\n            r = await self.make_request(\"GET\", link, headers=self._get_headers())\n            return (link, r.text if r else \"\")\n\n        tasks = [fetch_result_page(link) for link in links]\n        pages = await asyncio.gather(*tasks)\n        return {\"pages\": pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        races = []\n        date_str = raw_data[\"date\"]\n\n        for link, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n                race = self._parse_result_page(parser, date_str, link)\n                if race:\n                    races.append(race)\n            except Exception as e:\n                self.logger.warning(\"Failed to parse RP result page\", link=link, error=str(e))\n\n        return races\n\n    def _parse_result_page(self, parser: HTMLParser, date_str: str, url: str) -> Optional[Race]:\n        venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n        if not venue_node: return None\n        venue = normalize_venue_name(venue_node.text(strip=True))\n\n        time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n        if not time_node: return None\n        time_str = time_node.text(strip=True)\n\n        try:\n            start_time = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M\").replace(tzinfo=EASTERN)\n        except Exception:\n            return None\n\n        # Extract dividends\n        dividends = {}\n        tote_container = parser.css_first('div[data-test-selector=\"RC-toteReturns\"]')\n        if not tote_container:\n             # Try alternate selector\n             tote_container = parser.css_first('.rp-toteReturns')\n\n        if tote_container:\n            for row in (tote_container.css('div.rp-toteReturns__row') or tote_container.css('.rp-toteReturns__row')):\n                try:\n                    label_node = row.css_first('div.rp-toteReturns__label') or row.css_first('.rp-toteReturns__label')\n                    val_node = row.css_first('div.rp-toteReturns__value') or row.css_first('.rp-toteReturns__value')\n                    if label_node and val_node:\n                        label = clean_text(label_node.text())\n                        value = clean_text(val_node.text())\n                        if label and value:\n                            dividends[label] = value\n                except Exception as e:\n                    self.logger.debug(\"Failed parsing RP tote row\", error=str(e))\n\n\n\n        # Extract runners (finishers)\n        runners = []\n        for row in parser.css('div[data-test-selector=\"RC-resultRunner\"]'):\n            name_node = row.css_first('a[data-test-selector=\"RC-resultRunnerName\"]')\n            if not name_node: continue\n            name = clean_text(name_node.text())\n            pos_node = row.css_first('span.rp-resultRunner__position')\n            pos = clean_text(pos_node.text()) if pos_node else \"?\"\n\n            # Try to find saddle number\n            number = 0\n            num_node = row.css_first(\".rp-resultRunner__saddleClothNo\")\n            if num_node:\n                try: number = int(clean_text(num_node.text()))\n                except Exception: pass\n\n            runners.append(Runner(\n                name=name,\n                number=number,\n                metadata={\"position\": pos}\n            ))\n\n        # Derive race number from header or navigation\n        race_num = 1\n        # Priority 1: Navigation bar active time (most reliable on RP)\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        found_in_nav = False\n        for i, link in enumerate(time_links):\n            cls = link.attributes.get(\"class\", \"\")\n            if \"active\" in cls or \"rp-raceTimeCourseName__time\" in cls:\n                race_num = i + 1\n                found_in_nav = True\n                break\n\n        if not found_in_nav:\n            # Priority 2: Text search for \"Race X\"\n            race_num_match = re.search(r'Race\\s+(\\d+)', parser.text())\n            if race_num_match:\n                race_num = int(race_num_match.group(1))\n\n        race = Race(\n            id=f\"rp_tote_{get_canonical_venue(venue)}_{date_str.replace('-', '')}_R{race_num}\",\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n            metadata={\"dividends\": dividends, \"url\": url}\n        )\n        return race\n\n# ----------------------------------------\n# MASTER ORCHESTRATOR\n# ----------------------------------------\n\nasync def run_discovery(\n    target_dates: List[str],\n    window_hours: Optional[int] = 8,\n    loaded_races: Optional[List[Race]] = None,\n    adapter_names: Optional[List[str]] = None,\n    save_path: Optional[str] = None,\n    fetch_only: bool = False,\n    live_dashboard: bool = False,\n    track_odds: bool = False,\n    region: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None\n):\n    logger = structlog.get_logger(\"run_discovery\")\n    logger.info(\"Running Discovery\", dates=target_dates, window_hours=window_hours)\n\n    try:\n        now = datetime.now(EASTERN)\n        cutoff = now + timedelta(hours=window_hours) if window_hours else None\n\n        all_races_raw = []\n        harvest_summary = {}\n\n        # Pre-populate harvest_summary based on region/filter for visibility\n        target_region = region or DEFAULT_REGION\n        target_set = USA_DISCOVERY_ADAPTERS if target_region == \"USA\" else INT_DISCOVERY_ADAPTERS\n\n        # Determine which adapters should be visible in the harvest summary\n        if adapter_names:\n            visible_adapters = [n for n in adapter_names if n in target_set]\n        else:\n            visible_adapters = list(target_set)\n\n        for adapter_name in visible_adapters:\n            harvest_summary[adapter_name] = {\"count\": 0, \"max_odds\": 0.0, \"trust_ratio\": 0.0}\n\n        if loaded_races is not None:\n            logger.info(\"Using loaded races\", count=len(loaded_races))\n            all_races_raw = loaded_races\n            adapters = []\n            # Ensure harvest files exist even for loaded runs (Memory Directive Fix)\n            try:\n                if not os.path.exists(\"discovery_harvest.json\"):\n                    with open(\"discovery_harvest.json\", \"w\") as f:\n                        json.dump(harvest_summary, f)\n            except Exception: pass\n        else:\n            # Auto-discover discovery adapter classes\n            adapter_classes = get_discovery_adapter_classes()\n\n            if adapter_names:\n                adapter_classes = [c for c in adapter_classes if c.__name__ in adapter_names or getattr(c, \"SOURCE_NAME\", \"\") in adapter_names]\n\n            # Load historical performance scores to prioritize adapters\n            db = FortunaDB()\n            adapter_scores = await db.get_adapter_scores(days=30)\n\n            # Prioritize adapters by score (descending)\n            adapter_classes = sorted(\n                adapter_classes,\n                key=lambda c: adapter_scores.get(getattr(c, \"SOURCE_NAME\", c.__name__), 0),\n                reverse=True\n            )\n\n            adapters = []\n            for cls in adapter_classes:\n                try:\n                    adapters.append(cls(config={\"region\": region}))\n                except Exception as e:\n                    logger.error(\"Failed to initialize adapter\", adapter=cls.__name__, error=str(e))\n\n            try:\n                async def fetch_one(a, date_str):\n                    try:\n                        races = await a.get_races(date_str)\n                        return a.source_name, races\n                    except Exception as e:\n                        logger.error(\"Error fetching from adapter\", adapter=a.source_name, date=date_str, error=str(e))\n                        return a.source_name, []\n\n                fetch_tasks = []\n                for d in target_dates:\n                    for a in adapters:\n                        fetch_tasks.append(fetch_one(a, d))\n\n                results = await asyncio.gather(*fetch_tasks)\n                for adapter_name, r_list in results:\n                    all_races_raw.extend(r_list)\n\n                    # Track count and MaxOdds (Proxy for successful odds fetching)\n                    m_odds = 0.0\n                    for r in r_list:\n                        for run in r.runners:\n                            if run.win_odds and run.win_odds > m_odds:\n                                m_odds = float(run.win_odds)\n\n                    if adapter_name not in harvest_summary:\n                        harvest_summary[adapter_name] = {\"count\": 0, \"max_odds\": 0.0}\n\n                    harvest_summary[adapter_name][\"count\"] += len(r_list)\n                    if m_odds > harvest_summary[adapter_name][\"max_odds\"]:\n                        harvest_summary[adapter_name][\"max_odds\"] = m_odds\n\n                    # Find the adapter instance to extract its trust_ratio\n                    matching_adapter = next((a for a in adapters if a.source_name == adapter_name), None)\n                    if matching_adapter:\n                        harvest_summary[adapter_name][\"trust_ratio\"] = max(\n                            harvest_summary[adapter_name].get(\"trust_ratio\", 0.0),\n                            getattr(matching_adapter, \"trust_ratio\", 0.0)\n                        )\n\n                logger.info(\"Fetched total races\", count=len(all_races_raw))\n            finally:\n                # Save discovery harvest summary for GHA reporting and DB persistence\n                try:\n                    # Only create if it doesn't exist or we have data\n                    if harvest_summary or not os.path.exists(\"discovery_harvest.json\"):\n                        with open(\"discovery_harvest.json\", \"w\") as f:\n                            json.dump(harvest_summary, f)\n\n                    if harvest_summary:\n                        await db.log_harvest(harvest_summary, region=region)\n                except Exception: pass\n\n                # Shutdown adapters\n                for a in adapters:\n                    try: await a.close()\n                    except Exception: pass\n\n        # Apply time window filter if requested to avoid overloading\n        # Initial time window filtering removed to ensure all unique races are tracked for reporting\n\n        if not all_races_raw:\n            logger.error(\"No races fetched from any adapter. Discovery aborted.\")\n            if save_path:\n                try:\n                    with open(save_path, \"w\") as f:\n                        json.dump([], f)\n                    logger.info(\"Saved empty race list to file\", path=save_path)\n                except Exception as e:\n                    logger.error(\"Failed to save empty race list\", error=str(e))\n            return\n        \n        # Deduplicate\n        race_map = {}\n        for race in all_races_raw:\n            canonical_venue = get_canonical_venue(race.venue)\n            # Use Canonical Venue + Race Number + Date + Discipline as stable key\n            st = race.start_time\n            if isinstance(st, str):\n                try:\n                    st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except (ValueError, TypeError):\n                    pass\n\n            date_str = st.strftime('%Y%m%d') if hasattr(st, 'strftime') else \"Unknown\"\n            # Removing discipline from key to allow better merging across adapters\n            key = f\"{canonical_venue}|{race.race_number}|{date_str}\"\n            \n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing = race_map[key]\n                # Merge runners/odds\n                for nr in race.runners:\n                    # Match by number OR name (if numbers are missing)\n                    er = next((r for r in existing.runners if (r.number != 0 and r.number == nr.number) or (r.name.lower() == nr.name.lower())), None)\n                    if er:\n                        er.odds.update(nr.odds)\n                        if not er.win_odds and nr.win_odds:\n                            er.win_odds = nr.win_odds\n                        if not er.number and nr.number:\n                            er.number = nr.number\n                    else:\n                        existing.runners.append(nr)\n\n                # Update source\n                sources = set((existing.source or \"\").split(\", \"))\n                sources.add(race.source or \"Unknown\")\n                existing.source = \", \".join(sorted(list(filter(None, sources))))\n\n        unique_races = list(race_map.values())\n        logger.info(\"Unique races identified\", count=len(unique_races))\n\n        # GPT5 Improvement: Keep all races within window for analysis, not just one per track.\n        # Window broadened to 18 hours to match grid cutoff (News Mode)\n        timing_window_races = []\n        now = datetime.now(EASTERN)\n        for race in unique_races:\n            st = race.start_time\n            if isinstance(st, str):\n                try:\n                    st = datetime.fromisoformat(st.replace('Z', '+00:00'))\n                except (ValueError, TypeError):\n                    continue\n            if st.tzinfo is None:\n                st = st.replace(tzinfo=EASTERN)\n\n            # Calculate Minutes to Post\n            diff = st - now\n            mtp = diff.total_seconds() / 60\n\n            # Broaden window to 18 hours to ensure yield for \"News\"\n            if -45 < mtp <= 1080: # 18 hours = 1080 mins\n                timing_window_races.append(race)\n                if mtp <= 45:\n                    logger.info(f\"  \ud83d\udcb0 Found Gold Candidate: {race.venue} R{race.race_number} ({mtp:.1f} MTP)\")\n                else:\n                    logger.debug(f\"  \ud83d\udd2d Found Upcoming Candidate: {race.venue} R{race.race_number} ({mtp:.1f} MTP)\")\n\n        golden_zone_races = timing_window_races\n        if not golden_zone_races:\n            logger.warning(\"\ud83d\udd2d No races found in the broadened window (-45m to 18h).\")\n\n        logger.info(\"Total unique races available for analysis\", count=len(unique_races))\n\n        # Save raw fetched/merged races if requested (Save EVERYTHING unique)\n        if save_path:\n            try:\n                with open(save_path, \"w\") as f:\n                    json.dump([r.model_dump(mode='json') for r in unique_races], f, indent=4)\n                logger.info(\"Saved all unique races to file\", path=save_path)\n            except Exception as e:\n                logger.error(\"Failed to save races\", error=str(e))\n\n        if fetch_only:\n            logger.info(\"Fetch-only mode active. Skipping analysis and reporting.\")\n            return\n\n        # Analyze ALL unique races to ensure Grid is populated with Top 5 info (News Mode)\n        analyzer = SimplySuccessAnalyzer(config=config)\n        result = analyzer.qualify_races(unique_races)\n        qualified = result.get(\"races\", [])\n\n        # Generate Grid & Goldmine (Grid uses unique_races for the broader context)\n        grid = generate_summary_grid(qualified, all_races=unique_races)\n        logger.info(\"Summary Grid Generated\")\n\n        # Generate Field Matrix for all unique races\n        field_matrix = generate_field_matrix(unique_races)\n        logger.info(\"Field Matrix Generated\")\n\n        # Log Hot Tips & Fetch recent historical results for the report\n        tracker = HotTipsTracker()\n        await tracker.log_tips(qualified)\n\n        historical_goldmines = await tracker.db.get_recent_audited_goldmines(limit=15)\n        historical_report = generate_historical_goldmine_report(historical_goldmines)\n\n        gm_report = generate_goldmine_report(qualified, all_races=unique_races)\n        if historical_report:\n            gm_report += \"\\n\" + historical_report\n\n        # NEW: Dashboard and Live Tracking\n        goldmines = [r for r in qualified if get_field(r, 'metadata', {}).get('is_goldmine')]\n\n        # Calculate today's stats for dashboard\n        recent_tips = await tracker.db.get_recent_tips(limit=100)\n        today_str = datetime.now(EASTERN).strftime(\"%Y-%m-%d\")\n        today_tips = [t for t in recent_tips if t.get(\"report_date\", \"\").startswith(today_str)]\n\n        cashed = sum(1 for t in today_tips if t.get(\"verdict\") == \"CASHED\")\n        total_tips = len(today_tips)\n        profit = sum((t.get(\"net_profit\") or 0.0) for t in today_tips)\n\n        stats = {\n            \"tips\": total_tips,\n            \"cashed\": cashed,\n            \"profit\": profit\n        }\n\n        # Generate friendly HTML report\n        try:\n            html_content = await generate_friendly_html_report(qualified, stats)\n            html_path = Path(\"fortuna_report.html\")\n            html_path.write_text(html_content, encoding=\"utf-8\")\n            logger.info(\"Friendly HTML report generated\", path=str(html_path))\n\n            # Launch the report if running as a portable app (not in GHA)\n            if not os.getenv(\"GITHUB_ACTIONS\"):\n                try:\n                    # Use absolute path for reliable opening\n                    abs_path = html_path.absolute()\n                    if sys.platform == \"win32\":\n                        os.startfile(abs_path)\n                    else:\n                        webbrowser.open(f\"file://{abs_path}\")\n                except Exception as e:\n                    logger.warning(\"Failed to automatically launch report\", error=str(e))\n        except Exception as e:\n            logger.error(\"Failed to generate HTML report\", error=str(e))\n\n        if live_dashboard:\n            try:\n                from rich.live import Live\n                from rich.console import Console\n                # Check if our custom dashboard exists\n                try:\n                    from dashboard import FortunaDashboard\n                    dash = FortunaDashboard()\n                    dash.update(goldmines, stats)\n\n                    # Start odds tracker if requested\n                    if track_odds:\n                        try:\n                            from odds_tracker import LiveOddsTracker\n                            adapter_classes = get_discovery_adapter_classes()\n                            odds_tracker = LiveOddsTracker(goldmines, adapter_classes)\n                            asyncio.create_task(odds_tracker.start_tracking())\n                        except ImportError:\n                            logger.warning(\"LiveOddsTracker not available\")\n\n                    await dash.run_live()\n                except (ImportError, Exception) as e:\n                    logger.warning(f\"Rich dashboard component missing or failed: {e}\")\n                    # Fallback to simple rich display if possible\n                    console = Console()\n                    console.print(\"\\n\" + grid + \"\\n\")\n            except ImportError:\n                logger.warning(\"Rich library not available, falling back to static display\")\n                print(\"\\n\" + grid + \"\\n\")\n        else:\n            # Fallback to static print\n            try:\n                from dashboard import print_dashboard\n                print_dashboard(goldmines, stats)\n            except Exception as e:\n                # Silently fallback to standard print if dashboard fails\n                pass\n\n            print(\"\\n\" + grid + \"\\n\")\n            if historical_report:\n                print(\"\\n\" + historical_report + \"\\n\")\n\n        # Always save reports to files (GPT5 Improvement: Defensive guards)\n        try:\n            with open(\"summary_grid.txt\", \"w\", encoding='utf-8') as f: f.write(grid)\n            with open(\"field_matrix.txt\", \"w\", encoding='utf-8') as f: f.write(field_matrix)\n            with open(\"goldmine_report.txt\", \"w\", encoding='utf-8') as f: f.write(gm_report)\n        except Exception as e:\n            logger.error(\"failed_saving_text_reports\", error=str(e))\n\n        # Save qualified races to JSON\n        report_data = {\n            \"races\": [r.model_dump(mode='json') for r in qualified],\n            \"analysis_metadata\": result.get(\"criteria\", {}),\n            \"timestamp\": datetime.now(EASTERN).isoformat(),\n        }\n        try:\n            with open(\"qualified_races.json\", \"w\", encoding='utf-8') as f:\n                json.dump(report_data, f, indent=4)\n        except Exception as e:\n            logger.error(\"failed_saving_qualified_races\", error=str(e))\n\n        # NEW: Write GHA Job Summary\n        if 'GITHUB_STEP_SUMMARY' in os.environ:\n            try:\n                predictions_md = format_predictions_section(qualified)\n                # We need a db instance for format_proof_section\n                proof_md = await format_proof_section(tracker.db)\n                harvest_md = build_harvest_table(harvest_summary, \"\ud83d\udef0\ufe0f Discovery Harvest Performance\")\n                artifacts_md = format_artifact_links()\n                write_job_summary(predictions_md, harvest_md, proof_md, artifacts_md)\n                logger.info(\"GHA Job Summary written\")\n            except Exception as e:\n                logger.error(\"Failed to write GHA summary\", error=str(e))\n\n    finally:\n        await GlobalResourceManager.cleanup()\nasync def start_desktop_app():\n    \"\"\"Starts a FastAPI server and opens a webview window for the Fortuna Dashboard.\"\"\"\n    try:\n        import uvicorn\n        from fastapi import FastAPI\n        from fastapi.responses import HTMLResponse\n        import webview\n        import threading\n        import time\n    except ImportError as e:\n        print(f\"GUI dependencies missing: {e}. Install with 'pip install fastapi uvicorn pywebview'\")\n        return\n\n    app = FastAPI(title=\"Fortuna Desktop Intelligence\")\n\n    @app.get(\"/\", response_class=HTMLResponse)\n    async def get_dashboard():\n        # Retrieve latest Goldmines from the database\n        db = FortunaDB()\n        try:\n            async with db.get_connection() as conn:\n                try:\n                    async with conn.execute(\n                        \"SELECT venue, race_number, selection_number, predicted_2nd_fav_odds, start_time \"\n                        \"FROM tips ORDER BY id DESC LIMIT 50\"\n                    ) as cursor:\n                        tips = await cursor.fetchall()\n                except Exception as e:\n                    print(f\"DB query failed: {e}\")\n                    tips = []\n        except Exception as e:\n            print(f\"Failed to connect to database: {e}\")\n            tips = []\n\n        tips_html = \"\".join([\n            f\"<tr><td>{t[4]}</td><td>{t[0]}</td><td>R{t[1]}</td><td>#{t[2]}</td><td>{t[3]}</td></tr>\"\n            for t in tips\n        ])\n\n        return f\"\"\"\n        <html>\n            <head>\n                <title>Fortuna Intelligence Desktop</title>\n                <style>\n                    body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #0f172a; color: #f8fafc; padding: 30px; }}\n                    .container {{ max-width: 1200px; margin: auto; }}\n                    h1 {{ color: #fbbf24; border-bottom: 2px solid #fbbf24; padding-bottom: 10px; text-transform: uppercase; letter-spacing: 2px; }}\n                    table {{ width: 100%; border-collapse: collapse; margin-top: 20px; background: #1e293b; border-radius: 8px; overflow: hidden; }}\n                    th, td {{ padding: 15px; text-align: left; border-bottom: 1px solid #334155; }}\n                    th {{ background: #334155; color: #fbbf24; }}\n                    tr:hover {{ background: #475569; }}\n                    .footer {{ margin-top: 30px; font-size: 0.8em; color: #94a3b8; text-align: center; }}\n                    .btn {{ display: inline-block; background: #fbbf24; color: #0f172a; padding: 10px 20px; border-radius: 5px; text-decoration: none; font-weight: bold; margin-bottom: 20px; }}\n                </style>\n                <script>\n                    setTimeout(() => {{ location.reload(); }}, 30000);\n                </script>\n            </head>\n            <body>\n                <div class=\"container\">\n                    <h1>Fortuna Intelligence Dashboard</h1>\n                    <p>Monitoring global racing markets for Goldmine opportunities...</p>\n                    <a href=\"/\" class=\"btn\">REFRESH NOW</a>\n                    <table>\n                        <thead>\n                            <tr><th>Time Discovered</th><th>Venue</th><th>Race</th><th>Selection</th><th>Odds</th></tr>\n                        </thead>\n                        <tbody>\n                            {tips_html or \"<tr><td colspan='5'>No opportunities found yet. Run discovery to populate the database.</td></tr>\"}\n                        </tbody>\n                    </table>\n                    <div class=\"footer\">Fortuna Intelligence Monolith - Sci-Fi Future Edition - Auto-refreshing every 30s</div>\n                </div>\n            </body>\n        </html>\n        \"\"\"\n\n    def run_server():\n        uvicorn.run(app, host=\"127.0.0.1\", port=8013, log_level=\"error\")\n\n    # Start FastAPI in a background thread\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n\n    # Wait a moment for server to initialize\n    time.sleep(2.0)\n\n    # Create and start the webview window if server is up\n    if server_thread.is_alive():\n        print(\"Launching Fortuna Desktop Window...\")\n        webview.create_window('Fortuna Intelligence Desktop', 'http://127.0.0.1:8013', width=1300, height=900)\n        webview.start()\n    else:\n        print(\"\u26a0\ufe0f Error: GUI Server failed to start.\")\n\nasync def ensure_browsers():\n    \"\"\"Ensure browser dependencies are available for scraping.\"\"\"\n    try:\n        # Check if playwright is installed and has a chromium binary\n        from playwright.async_api import async_playwright\n        async with async_playwright() as p:\n            try:\n                # We try to launch a headless browser to verify installation\n                browser = await p.chromium.launch(headless=True)\n                await browser.close()\n                return True\n            except Exception as e:\n                structlog.get_logger().debug(\"Playwright launch failed during verification\", error=str(e))\n    except ImportError:\n        structlog.get_logger().debug(\"Playwright not imported\")\n\n    if is_frozen():\n        print(\"\u2501\" * 60)\n        print(\"\u26a0\ufe0f  PLAYWRIGHT NOT DETECTED IN MONOLITH\")\n        print(\"\u2501\" * 60)\n        print(\"Playwright is required for some adapters but cannot be auto-installed in EXE mode.\")\n        print(\"\\nStandard HTTP-based adapters will still function.\")\n        print(\"\u2501\" * 60)\n        return False\n\n    structlog.get_logger().info(\"Installing browser dependencies (Playwright Chromium)...\")\n    try:\n        # Run installation in a separate process to avoid blocking the loop too much\n        # We explicitly don't use 'pip install playwright' here if possible because it might conflict\n        # but for local non-frozen runs it's a helpful fallback.\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"playwright==1.49.1\"], check=True, capture_output=True, text=True)\n        subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True, capture_output=True, text=True)\n        structlog.get_logger().info(\"Browser dependencies installed successfully.\")\n        return True\n    except subprocess.CalledProcessError as e:\n        structlog.get_logger().error(\n            \"Failed to install browsers\",\n            error=str(e),\n            returncode=e.returncode,\n            stdout=e.stdout,\n            stderr=e.stderr\n        )\n        return False\n    except Exception as e:\n        structlog.get_logger().error(\"Unexpected error installing browsers\", error=str(e))\n        return False\n\nasync def main_all_in_one():\n    # Configure logging at the start of main\n    structlog.configure(\n        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO)\n    )\n    # Ensure DB path env is set if passed via argument or already in environment\n    # Actually, we should probably add a --db-path arg here too for parity with analytics\n    config = load_config()\n    logger = structlog.get_logger(\"main\")\n    parser = argparse.ArgumentParser(description=\"Fortuna All-In-One - Professional Racing Intelligence\")\n    parser.add_argument(\"--date\", type=str, help=\"Target date (YYYY-MM-DD)\")\n    parser.add_argument(\"--hours\", type=int, default=8, help=\"Discovery time window in hours (default: 8)\")\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"Run in monitor mode\")\n    parser.add_argument(\"--once\", action=\"store_true\", help=\"Run monitor once\")\n    parser.add_argument(\"--region\", type=str, choices=[\"USA\", \"INT\", \"GLOBAL\"], help=\"Filter by region (USA, INT or GLOBAL)\")\n    parser.add_argument(\"--include\", type=str, help=\"Comma-separated adapter names to include\")\n    parser.add_argument(\"--save\", type=str, help=\"Save races to JSON file\")\n    parser.add_argument(\"--load\", type=str, help=\"Load races from JSON file(s), comma-separated\")\n    parser.add_argument(\"--fetch-only\", action=\"store_true\", help=\"Only fetch and save data, skip analysis and reporting\")\n    parser.add_argument(\"--db-path\", type=str, help=\"Path to tip history database\")\n    parser.add_argument(\"--clear-db\", action=\"store_true\", help=\"Clear all tips from the database and exit\")\n    parser.add_argument(\"--gui\", action=\"store_true\", help=\"Start the Fortuna Desktop GUI\")\n    parser.add_argument(\"--live-dashboard\", action=\"store_true\", help=\"Show live updating terminal dashboard\")\n    parser.add_argument(\"--track-odds\", action=\"store_true\", help=\"Monitor live odds and send notifications\")\n    parser.add_argument(\"--status\", action=\"store_true\", help=\"Show application status card and latest metrics\")\n    parser.add_argument(\"--show-log\", action=\"store_true\", help=\"Print recent fetch/audit highlights\")\n    parser.add_argument(\"--quick-help\", action=\"store_true\", help=\"Show friendly onboarding guide\")\n    parser.add_argument(\"--open-dashboard\", action=\"store_true\", help=\"Open the HTML intelligence report in browser\")\n    args = parser.parse_args()\n\n    if args.quick_help:\n        print_quick_help()\n        return\n\n    if args.status:\n        print_status_card(config)\n        return\n\n    if args.show_log:\n        await print_recent_logs()\n        return\n\n    if args.open_dashboard:\n        open_report_in_browser()\n        return\n\n    if args.db_path:\n        os.environ[\"FORTUNA_DB_PATH\"] = args.db_path\n\n    if args.quick_help:\n        print_quick_help()\n        return\n\n    if args.status:\n        print_status_card(config)\n        return\n\n    if args.show_log:\n        await print_recent_logs()\n        return\n\n    if args.open_dashboard:\n        open_report_in_browser()\n        return\n\n    # Print status card for all normal runs\n    print_status_card(config)\n\n    if args.gui:\n        # Start GUI. It runs its own event loop for the webview.\n        await ensure_browsers()\n        await start_desktop_app()\n        return\n\n    if args.clear_db:\n        db = FortunaDB()\n        await db.clear_all_tips()\n        await db.close()\n        print(\"Database cleared successfully.\")\n        return\n\n    adapter_filter = [n.strip() for n in args.include.split(\",\")] if args.include else None\n\n    # Use default region if not specified\n    if not args.region:\n        args.region = config.get(\"region\", {}).get(\"default\", DEFAULT_REGION)\n        structlog.get_logger().info(\"Using default region\", region=args.region)\n\n    # Region-based adapter filtering\n    if args.region:\n        if args.region == \"USA\":\n            target_set = USA_DISCOVERY_ADAPTERS\n        elif args.region == \"INT\":\n            target_set = INT_DISCOVERY_ADAPTERS\n        else:\n            target_set = GLOBAL_DISCOVERY_ADAPTERS\n\n        if adapter_filter:\n            adapter_filter = [n for n in adapter_filter if n in target_set]\n        else:\n            adapter_filter = list(target_set)\n\n        # Special case: TwinSpires needs to know its region internally if it's not filtered out\n        # We can pass the region via config if we were creating adapters manually,\n        # but here we use names.\n        # Actually, I updated TwinSpiresAdapter to check self.config.get(\"region\").\n        # I need to ensure the adapter gets this config.\n\n    loaded_races = None\n    if args.load:\n        loaded_races = []\n        for path in args.load.split(\",\"):\n            path = path.strip()\n            if not os.path.exists(path):\n                print(f\"Warning: File not found: {path}\")\n                logger.warning(\"Race data file not found\", path=path)\n                continue\n            try:\n                with open(path, \"r\") as f:\n                    data = json.load(f)\n                    loaded_races.extend([Race.model_validate(r) for r in data])\n            except Exception as e:\n                print(f\"Error loading {path}: {e}\")\n                logger.error(\"Failed to load race data\", path=path, error=str(e), exc_info=True)\n\n    if args.date:\n        target_dates = [args.date]\n    else:\n        now = datetime.now(EASTERN)\n        future = now + timedelta(hours=args.hours)\n\n        target_dates = [now.strftime(\"%Y-%m-%d\")]\n        if future.date() > now.date():\n            target_dates.append(future.strftime(\"%Y-%m-%d\"))\n\n    if args.monitor:\n        await ensure_browsers()\n        monitor = FavoriteToPlaceMonitor(target_dates=target_dates)\n        # Pass region config to monitor\n        monitor.config[\"region\"] = args.region\n        if args.once:\n            await monitor.run_once(loaded_races=loaded_races, adapter_names=adapter_filter)\n            if config.get(\"ui\", {}).get(\"auto_open_report\", True) and not os.getenv(\"GITHUB_ACTIONS\"):\n                open_report_in_browser()\n        else:\n            await monitor.run_continuous() # Continuous mode doesn't support load/filter yet for simplicity\n    else:\n        await ensure_browsers()\n        await run_discovery(\n            target_dates,\n            window_hours=args.hours,\n            loaded_races=loaded_races,\n            adapter_names=adapter_filter,\n            save_path=args.save,\n            fetch_only=args.fetch_only,\n            live_dashboard=args.live_dashboard,\n            track_odds=args.track_odds,\n            region=args.region, # Pass region to run_discovery\n            config=config\n        )\n        # Post-run UI enhancements (Council of Superbrains Directive)\n        if config.get(\"ui\", {}).get(\"auto_open_report\", True) and not os.getenv(\"GITHUB_ACTIONS\"):\n            open_report_in_browser()\n\nif __name__ == \"__main__\":\n    if os.getenv(\"DEBUG_SNAPSHOTS\"):\n        os.makedirs(\"debug_snapshots\", exist_ok=True)\n    \n    # Windows Selector Event Loop Policy Fix (Project Hardening)\n    if sys.platform == 'win32':\n        try:\n            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n        except AttributeError:\n            pass # Policy not available on this version\n\n    try:\n        asyncio.run(main_all_in_one())\n    except KeyboardInterrupt:\n        pass\n", "web_service/backend/models.py": "# python_service/models.py\n\nfrom datetime import datetime, date, timezone\nfrom decimal import Decimal\nimport re\nfrom typing import Annotated, Any, Callable, Dict, List, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, WrapSerializer, model_validator\n\n\ndef decimal_serializer(value: Decimal, handler: Callable[[Decimal], Any]) -> Any:\n    \"\"\"Custom serializer for Decimal to float conversion.\"\"\"\n    return float(value)\n\n\nJsonDecimal = Annotated[Decimal, WrapSerializer(decimal_serializer, when_used=\"json\")]\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[JsonDecimal] = None\n    place: Optional[JsonDecimal] = None\n    show: Optional[JsonDecimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = Field(default_factory=dict)\n    win_odds: Optional[float] = Field(None, alias=\"winOdds\")\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n    metadata: Dict[str, Any] = {}\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n    is_error_placeholder: bool = Field(False, alias=\"isErrorPlaceholder\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    top_five_numbers: Optional[str] = Field(None, alias=\"topFiveNumbers\")\n    metadata: Dict[str, Any] = {}\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AdapterError(FortunaBaseModel):\n    adapter_name: str = Field(..., alias=\"adapterName\")\n    error_message: str = Field(..., alias=\"errorMessage\")\n    attempted_url: Optional[str] = Field(None, alias=\"attemptedUrl\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    race_date: Optional[date] = Field(None, alias=\"date\")\n    races: List[Race]\n    errors: List[AdapterError]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n    metadata: Dict[str, Any] = {}\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n\n\nclass ManualParseRequest(FortunaBaseModel):\n    adapter_name: str\n    html_content: str = Field(..., max_length=5_000_000)  # ~5MB limit\n\n\n# --- Analytics Models ---\n\ndef get_canonical_venue(venue: str) -> str:\n    \"\"\"Normalize venue name for matching.\"\"\"\n    if not venue:\n        return \"\"\n    canonical = re.sub(r'\\s*\\([^)]*\\)\\s*', '', venue)\n    canonical = re.sub(r'[^a-zA-Z0-9]', '', canonical).lower()\n    return canonical\n\n\ndef parse_position(pos_str: Optional[str]) -> Optional[int]:\n    \"\"\"'1st' -> 1, '2/12' -> 2, 'W' -> 1, etc.\"\"\"\n    if not pos_str:\n        return None\n    s = str(pos_str).upper().strip()\n    direct = {\n        \"W\": 1, \"1\": 1, \"1ST\": 1,\n        \"P\": 2, \"2\": 2, \"2ND\": 2,\n        \"S\": 3, \"3\": 3, \"3RD\": 3,\n        \"4\": 4, \"4TH\": 4,\n        \"5\": 5, \"5TH\": 5,\n    }\n    if s in direct:\n        return direct[s]\n    m = re.search(r\"^(\\d+)\", s)\n    return int(m.group(1)) if m else None\n\n\nclass ResultRunner(Runner):\n    \"\"\"Extended runner with result information.\"\"\"\n    position: Optional[str] = None\n    position_numeric: Optional[int] = None\n    final_win_odds: Optional[float] = None\n    win_payout: Optional[float] = None\n    place_payout: Optional[float] = None\n    show_payout: Optional[float] = None\n\n    @model_validator(mode=\"after\")\n    def compute_position_numeric(self) -> \"ResultRunner\":\n        if self.position and self.position_numeric is None:\n            self.position_numeric = parse_position(self.position)\n        return self\n\n\nclass ResultRace(Race):\n    \"\"\"Race with full result data.\"\"\"\n    runners: List[ResultRunner] = Field(default_factory=list)\n    official_dividends: Dict[str, float] = Field(default_factory=dict)\n    discipline: Optional[str] = None\n    chart_url: Optional[str] = None\n    is_fully_parsed: bool = False\n\n    # Exotic bet payouts\n    trifecta_payout: Optional[float] = None\n    trifecta_cost: float = 1.00\n    trifecta_combination: Optional[str] = None\n    exacta_payout: Optional[float] = None\n    exacta_combination: Optional[str] = None\n    superfecta_payout: Optional[float] = None\n    superfecta_combination: Optional[str] = None\n\n    @property\n    def canonical_key(self) -> str:\n        d = self.start_time.strftime(\"%Y%m%d\")\n        t = self.start_time.strftime(\"%H%M\")\n        disc = (self.discipline or \"T\")[:1].upper()\n        return f\"{get_canonical_venue(self.venue)}|{self.race_number}|{d}|{t}|{disc}\"\n\n    @property\n    def relaxed_key(self) -> str:\n        d = self.start_time.strftime(\"%Y%m%d\")\n        disc = (self.discipline or \"T\")[:1].upper()\n        return f\"{get_canonical_venue(self.venue)}|{self.race_number}|{d}|{disc}\"\n\n    def get_top_finishers(self, n: int = 5) -> List[ResultRunner]:\n        ranked = [r for r in self.runners if r.position_numeric is not None]\n        ranked.sort(key=lambda r: r.position_numeric)\n        return ranked[:n]\n\n\nclass AuditResult(FortunaBaseModel):\n    \"\"\"Result of auditing a tip against actual race results.\"\"\"\n    tip_id: str\n    venue: str\n    race_number: int\n    verdict: str  # CASHED, BURNED, VOID, PENDING\n    net_profit: float = 0.0\n    selection_number: Optional[int] = None\n    selection_position: Optional[int] = None\n    actual_top_5: str = \"\"\n    actual_2nd_fav_odds: Optional[float] = None\n    trifecta_payout: Optional[float] = None\n    trifecta_combination: Optional[str] = None\n    superfecta_payout: Optional[float] = None\n    superfecta_combination: Optional[str] = None\n    top1_place_payout: Optional[float] = None\n    top2_place_payout: Optional[float] = None\n    audit_timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n", "web_service/backend/core/auditor.py": "# web_service/backend/core/auditor.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport structlog\n\nfrom ..models import ResultRace, ResultRunner, get_canonical_venue\nfrom .database import FortunaDB, EASTERN\n\nSTANDARD_BET = 2.00\n\ndef get_places_paid(field_size: int) -> int:\n    if field_size <= 4:\n        return 1  # win only\n    if field_size <= 7:\n        return 2  # top 2\n    return 3      # top 3\n\nclass AuditorEngine:\n    \"\"\"Matches predicted tips against actual race results using SQLite storage.\"\"\"\n\n    def __init__(self, db_path: Optional[str] = None) -> None:\n        self.db = FortunaDB(db_path)\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        await self.close()\n\n    async def close(self) -> None:\n        await self.db.close()\n\n    # -- data access -------------------------------------------------------\n\n    async def get_unverified_tips(self, lookback_hours: int = 48) -> List[Dict[str, Any]]:\n        return await self.db.get_unverified_tips(lookback_hours)\n\n    async def get_all_audited_tips(self) -> List[Dict[str, Any]]:\n        return await self.db.get_all_audited_tips()\n\n    async def get_recent_tips(self, limit: int = 20) -> List[Dict[str, Any]]:\n        return await self.db.get_recent_tips(limit)\n\n    # -- audit pipeline ----------------------------------------------------\n\n    async def audit_races(\n        self,\n        results: List[ResultRace],\n        unverified: Optional[List[Dict[str, Any]]] = None,\n    ) -> List[Dict[str, Any]]:\n        results_map = self._build_results_map(results)\n        \n        if unverified is None:\n            unverified = await self.get_unverified_tips()\n\n        audited: List[Dict[str, Any]] = []\n        outcomes_to_batch: List[Tuple[str, Dict[str, Any]]] = []\n\n        for tip in unverified:\n            try:\n                race_id = tip.get(\"race_id\")\n                if not race_id:\n                    continue\n\n                tip_key = self._tip_canonical_key(tip)\n                if not tip_key:\n                    continue\n\n                result = self._match_tip_to_result(tip_key, results_map, race_id)\n                if not result:\n                    continue\n\n                outcome = self._evaluate_tip(tip, result)\n                outcomes_to_batch.append((race_id, outcome))\n                audited.append({**tip, **outcome, \"audit_completed\": True})\n\n            except Exception as exc:\n                self.logger.error(\"Error during audit\", tip_id=tip.get(\"race_id\"), error=str(exc))\n\n        if outcomes_to_batch:\n            self.logger.info(\"Updating audit results\", count=len(outcomes_to_batch))\n            await self.db.update_audit_results_batch(outcomes_to_batch)\n\n        return audited\n\n    @staticmethod\n    def _build_results_map(results: List[ResultRace]) -> Dict[str, ResultRace]:\n        mapping: Dict[str, ResultRace] = {}\n        for r in results:\n            mapping[r.canonical_key] = r\n            if r.relaxed_key != r.canonical_key:\n                if r.relaxed_key not in mapping:\n                    mapping[r.relaxed_key] = r\n        return mapping\n\n    def _match_tip_to_result(\n        self,\n        tip_key: str,\n        results_map: Dict[str, ResultRace],\n        race_id: str,\n    ) -> Optional[ResultRace]:\n        # Exact match\n        result = results_map.get(tip_key)\n        if result:\n            return result\n\n        parts = tip_key.split(\"|\")\n\n        # Fallback 1: drop time (keep discipline)\n        if len(parts) >= 5:\n            relaxed = f\"{parts[0]}|{parts[1]}|{parts[2]}|{parts[4]}\"\n            result = results_map.get(relaxed)\n            if result:\n                return result\n\n        # Fallback 2: drop discipline (keep time)\n        if len(parts) >= 4:\n            prefix = \"|\".join(parts[:4])\n            matches = [obj for key, obj in results_map.items() if key.startswith(prefix)]\n            if matches:\n                return matches[0]\n\n        return None\n\n    @staticmethod\n    def _tip_canonical_key(tip: Dict[str, Any]) -> Optional[str]:\n        venue = tip.get(\"venue\")\n        race_number = tip.get(\"race_number\")\n        start_raw = tip.get(\"start_time\")\n        disc = (tip.get(\"discipline\") or \"T\")[:1].upper()\n\n        if not all([venue, race_number, start_raw]):\n            return None\n        try:\n            st = datetime.fromisoformat(str(start_raw).replace(\"Z\", \"+00:00\"))\n            return (\n                f\"{get_canonical_venue(venue)}\"\n                f\"|{race_number}\"\n                f\"|{st.strftime('%Y%m%d')}\"\n                f\"|{st.strftime('%H%M')}\"\n                f\"|{disc}\"\n            )\n        except (ValueError, TypeError):\n            return None\n\n    def _evaluate_tip(self, tip: Dict[str, Any], result: ResultRace) -> Dict[str, Any]:\n        selection_num = self._extract_selection_number(tip)\n        selection_name = tip.get(\"selection_name\")\n\n        top_finishers = result.get_top_finishers(5)\n        actual_top_5 = [str(r.number) for r in top_finishers]\n\n        top1_place = top_finishers[0].place_payout if len(top_finishers) >= 1 else None\n        top2_place = top_finishers[1].place_payout if len(top_finishers) >= 2 else None\n\n        actual_2nd_fav_odds = self._find_actual_2nd_fav_odds(result)\n\n        # Find our selection in result runners\n        sel_result = self._find_selection_runner(result, selection_num, selection_name)\n\n        verdict, profit = self._compute_verdict(sel_result, result)\n\n        return {\n            \"actual_top_5\": \", \".join(actual_top_5),\n            \"actual_2nd_fav_odds\": actual_2nd_fav_odds,\n            \"verdict\": verdict,\n            \"net_profit\": round(profit, 2),\n            \"selection_position\": sel_result.position_numeric if sel_result else None,\n            \"audit_timestamp\": datetime.now(EASTERN).isoformat(),\n            \"trifecta_payout\": result.trifecta_payout,\n            \"trifecta_combination\": result.trifecta_combination,\n            \"superfecta_payout\": result.superfecta_payout,\n            \"superfecta_combination\": result.superfecta_combination,\n            \"top1_place_payout\": top1_place,\n            \"top2_place_payout\": top2_place,\n        }\n\n    @staticmethod\n    def _find_actual_2nd_fav_odds(result: ResultRace) -> Optional[float]:\n        runners_list = sorted(\n            (r for r in result.runners if r.final_win_odds and r.final_win_odds > 0 and not r.scratched),\n            key=lambda r: r.final_win_odds,\n        )\n        if len(runners_list) < 2:\n            return None\n        fav_odds = runners_list[0].final_win_odds\n        higher = [r for r in runners_list if r.final_win_odds > fav_odds]\n        return higher[0].final_win_odds if higher else None\n\n    @staticmethod\n    def _find_selection_runner(\n        result: ResultRace,\n        number: Optional[int],\n        name: Optional[str],\n    ) -> Optional[ResultRunner]:\n        if number is not None:\n            by_num = next((r for r in result.runners if r.number == number), None)\n            if by_num: return by_num\n        if name:\n            return next((r for r in result.runners if r.name.lower() == name.lower()), None)\n        return None\n\n    @staticmethod\n    def _compute_verdict(sel: Optional[ResultRunner], result: ResultRace) -> Tuple[str, float]:\n        if sel is None:\n            return \"VOID\", 0.0\n        if sel.position_numeric is None:\n            return \"BURNED\", -STANDARD_BET\n\n        active = [r for r in result.runners if not r.scratched]\n        places_paid = get_places_paid(len(active))\n\n        if sel.position_numeric > places_paid:\n            return \"BURNED\", -STANDARD_BET\n\n        # CASHED \u2014 calculate profit\n        if sel.place_payout and sel.place_payout > 0:\n            return \"CASHED\", sel.place_payout - STANDARD_BET\n\n        # Heuristic fallback if payout missing\n        odds = sel.final_win_odds or 2.75\n        place_roi = max(0.1, (odds - 1.0) / 5.0)\n        return \"CASHED_ESTIMATED\", place_roi * STANDARD_BET\n\n    @staticmethod\n    def _extract_selection_number(tip: Dict[str, Any]) -> Optional[int]:\n        sel = tip.get(\"selection_number\")\n        if sel is not None:\n            try:\n                return int(sel)\n            except (ValueError, TypeError): pass\n        top_five = tip.get(\"top_five\", \"\")\n        if top_five:\n            first = str(top_five).split(\",\")[0].strip()\n            try:\n                return int(first)\n            except (ValueError, TypeError): pass\n        return None\n", "web_service/backend/core/database.py": "import asyncio\nimport json\nimport os\nimport sqlite3\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nfrom zoneinfo import ZoneInfo\n\nimport structlog\n\nEASTERN = ZoneInfo(\"America/New_York\")\n\ndef is_frozen() -> bool:\n    import sys\n    return getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS')\n\ndef get_db_path() -> str:\n    import sys\n    if os.environ.get(\"FORTUNA_DB_PATH\"):\n        return os.environ.get(\"FORTUNA_DB_PATH\")\n    \n    if is_frozen() and sys.platform == \"win32\":\n        appdata = os.getenv('APPDATA')\n        if appdata:\n            db_dir = Path(appdata) / \"Fortuna\"\n            db_dir.mkdir(parents=True, exist_ok=True)\n            return str(db_dir / \"fortuna.db\")\n    return \"fortuna.db\"\n\ndef ensure_eastern(dt: datetime) -> datetime:\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=EASTERN)\n    return dt.astimezone(EASTERN)\n\nclass FortunaDB:\n    \"\"\"\n    Thread-safe SQLite backend for Fortuna using the standard library.\n    Handles persistence for tips, predictions, and audit outcomes.\n    \"\"\"\n    def __init__(self, db_path: Optional[str] = None):\n        self.db_path = db_path or get_db_path()\n        self._executor = ThreadPoolExecutor(max_workers=1)\n        self._conn = None\n        self._conn_lock = threading.Lock()\n\n        self._initialized = False\n        self.logger = structlog.get_logger(self.__class__.__name__)\n\n    def _get_conn(self):\n        with self._conn_lock:\n            if not self._conn:\n                self._conn = sqlite3.connect(self.db_path, check_same_thread=False)\n            self._conn.row_factory = sqlite3.Row\n            # Enable WAL mode for better concurrency\n            self._conn.execute(\"PRAGMA journal_mode=WAL\")\n        return self._conn\n\n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Returns an async context manager for a database connection.\"\"\"\n        try:\n            import aiosqlite\n        except ImportError:\n            self.logger.error(\"aiosqlite not installed. Async database features will fail.\")\n            raise\n\n        async with aiosqlite.connect(self.db_path) as conn:\n            conn.row_factory = aiosqlite.Row\n            yield conn\n\n    async def _run_in_executor(self, func, *args):\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(self._executor, func, *args)\n\n    async def initialize(self):\n        \"\"\"Creates the database schema if it doesn't exist.\"\"\"\n        if self._initialized: return\n\n        def _init():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS schema_version (\n                        version INTEGER PRIMARY KEY,\n                        applied_at TEXT NOT NULL\n                    )\n                \"\"\")\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS harvest_logs (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        timestamp TEXT NOT NULL,\n                        region TEXT,\n                        adapter_name TEXT NOT NULL,\n                        race_count INTEGER NOT NULL,\n                        max_odds REAL\n                    )\n                \"\"\")\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS tips (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        race_id TEXT NOT NULL,\n                        venue TEXT NOT NULL,\n                        race_number INTEGER NOT NULL,\n                        discipline TEXT,\n                        start_time TEXT NOT NULL,\n                        report_date TEXT NOT NULL,\n                        is_goldmine INTEGER NOT NULL,\n                        gap12 TEXT,\n                        top_five TEXT,\n                        selection_number INTEGER,\n                        selection_name TEXT,\n                        audit_completed INTEGER DEFAULT 0,\n                        verdict TEXT,\n                        net_profit REAL,\n                        selection_position INTEGER,\n                        actual_top_5 TEXT,\n                        actual_2nd_fav_odds REAL,\n                        trifecta_payout REAL,\n                        trifecta_combination TEXT,\n                        superfecta_payout REAL,\n                        superfecta_combination TEXT,\n                        top1_place_payout REAL,\n                        top2_place_payout REAL,\n                        predicted_2nd_fav_odds REAL,\n                        audit_timestamp TEXT\n                    )\n                \"\"\")\n                # Cleanup potential duplicates\n                try:\n                    conn.execute(\"DROP INDEX IF EXISTS idx_race_report\")\n                    conn.execute(\"\"\"\n                        DELETE FROM tips\n                        WHERE id NOT IN (\n                            SELECT MAX(id)\n                            FROM tips\n                            GROUP BY race_id\n                        )\n                    \"\"\")\n                    conn.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS idx_race_id ON tips (race_id)\")\n                except Exception as e:\n                    self.logger.error(\"Failed to cleanup or create unique index\", error=str(e))\n                \n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_audit_time ON tips (audit_completed, start_time)\")\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_venue ON tips (venue)\")\n                conn.execute(\"CREATE INDEX IF NOT EXISTS idx_discipline ON tips (discipline)\")\n\n                # Add missing columns\n                cursor = conn.execute(\"PRAGMA table_info(tips)\")\n                columns = [column[1] for column in cursor.fetchall()]\n                for col in [\"superfecta_payout\", \"superfecta_combination\", \"top1_place_payout\", \"top2_place_payout\", \n                            \"discipline\", \"predicted_2nd_fav_odds\", \"actual_2nd_fav_odds\", \"selection_name\"]:\n                    if col not in columns:\n                        conn.execute(f\"ALTER TABLE tips ADD COLUMN {col} {'REAL' if 'payout' in col or 'odds' in col else 'TEXT'}\")\n\n        await self._run_in_executor(_init)\n\n        def _get_version():\n            cursor = self._get_conn().execute(\"SELECT MAX(version) FROM schema_version\")\n            row = cursor.fetchone()\n            return row[0] if row and row[0] is not None else 0\n\n        current_version = await self._run_in_executor(_get_version)\n\n        if current_version < 2:\n            await self.migrate_utc_to_eastern()\n            def _update_version():\n                with self._get_conn() as conn:\n                    conn.execute(\"INSERT OR REPLACE INTO schema_version (version, applied_at) VALUES (2, ?)\", (datetime.now(EASTERN).isoformat(),))\n            await self._run_in_executor(_update_version)\n\n        self._initialized = True\n        self.logger.info(\"Database initialized\", path=self.db_path)\n\n    async def migrate_utc_to_eastern(self) -> None:\n        \"\"\"Migrates existing database records from UTC to US Eastern Time.\"\"\"\n        def _migrate():\n            conn = self._get_conn()\n            cursor = conn.execute(\"\"\"\n                SELECT id, start_time, report_date, audit_timestamp FROM tips\n                WHERE start_time LIKE '%+00:00' OR start_time LIKE '%Z'\n                OR report_date LIKE '%+00:00' OR report_date LIKE '%Z'\n                OR audit_timestamp LIKE '%+00:00' OR audit_timestamp LIKE '%Z'\n            \"\"\")\n            rows = cursor.fetchall()\n            if not rows: return\n            for row in rows:\n                updates = {}\n                for col in [\"start_time\", \"report_date\", \"audit_timestamp\"]:\n                    if col not in row.keys(): continue\n                    val = row[col]\n                    if val:\n                        try:\n                            dt = datetime.fromisoformat(val.replace(\"Z\", \"+00:00\"))\n                            updates[col] = ensure_eastern(dt).isoformat()\n                        except Exception: pass\n                if updates:\n                    set_clause = \", \".join([f\"{k} = ?\" for k in updates.keys()])\n                    conn.execute(f\"UPDATE tips SET {set_clause} WHERE id = ?\", (*updates.values(), row[\"id\"]))\n        await self._run_in_executor(_migrate)\n\n    async def log_harvest(self, harvest_summary: Dict[str, Any], region: Optional[str] = None):\n        if not self._initialized: await self.initialize()\n        def _log():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN).isoformat()\n            to_insert = []\n            for adapter, data in harvest_summary.items():\n                count = data.get(\"count\", 0) if isinstance(data, dict) else data\n                max_odds = data.get(\"max_odds\", 0.0) if isinstance(data, dict) else 0.0\n                to_insert.append((now, region, adapter, count, max_odds))\n            if to_insert:\n                with conn:\n                    conn.executemany(\"INSERT INTO harvest_logs (timestamp, region, adapter_name, race_count, max_odds) VALUES (?, ?, ?, ?, ?)\", to_insert)\n        await self._run_in_executor(_log)\n\n    async def log_tips(self, tips: List[Dict[str, Any]]):\n        if not self._initialized: await self.initialize()\n        def _log():\n            conn = self._get_conn()\n            race_ids = [t.get(\"race_id\") for t in tips if t.get(\"race_id\")]\n            if not race_ids: return\n            placeholders = \",\".join([\"?\"] * len(race_ids))\n            cursor = conn.execute(f\"SELECT race_id FROM tips WHERE race_id IN ({placeholders})\", (*race_ids,))\n            already_logged = {row[\"race_id\"] for row in cursor.fetchall()}\n            to_insert = []\n            for tip in tips:\n                rid = tip.get(\"race_id\")\n                if rid and rid not in already_logged:\n                    to_insert.append((\n                        rid, tip.get(\"venue\"), tip.get(\"race_number\"),\n                        tip.get(\"discipline\"), tip.get(\"start_time\"), tip.get(\"report_date\") or datetime.now(EASTERN).isoformat(),\n                        1 if tip.get(\"is_goldmine\") else 0, str(tip.get(\"1Gap2\", 0.0)),\n                        tip.get(\"top_five\"), tip.get(\"selection_number\"), tip.get(\"selection_name\"),\n                        float(tip.get(\"predicted_2nd_fav_odds\")) if tip.get(\"predicted_2nd_fav_odds\") is not None else None\n                    ))\n                    already_logged.add(rid)\n            if to_insert:\n                with conn:\n                    conn.executemany(\"\"\"\n                        INSERT OR IGNORE INTO tips (\n                            race_id, venue, race_number, discipline, start_time, report_date,\n                            is_goldmine, gap12, top_five, selection_number, selection_name, predicted_2nd_fav_odds\n                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", to_insert)\n        await self._run_in_executor(_log)\n\n    async def get_unverified_tips(self, lookback_hours: int = 48) -> List[Dict[str, Any]]:\n        if not self._initialized: await self.initialize()\n        def _get():\n            conn = self._get_conn()\n            now = datetime.now(EASTERN)\n            cutoff = (now - timedelta(hours=lookback_hours)).isoformat()\n            cursor = conn.execute(\"SELECT * FROM tips WHERE audit_completed = 0 AND report_date > ? AND start_time < ?\", (cutoff, now.isoformat()))\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def get_all_audited_tips(self) -> List[Dict[str, Any]]:\n        if not self._initialized: await self.initialize()\n        def _get():\n            cursor = self._get_conn().execute(\"SELECT * FROM tips WHERE audit_completed = 1 ORDER BY start_time DESC\")\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def get_recent_tips(self, limit: int = 20) -> List[Dict[str, Any]]:\n        if not self._initialized: await self.initialize()\n        def _get():\n            cursor = self._get_conn().execute(\"SELECT * FROM tips ORDER BY id DESC LIMIT ?\", (limit,))\n            return [dict(row) for row in cursor.fetchall()]\n        return await self._run_in_executor(_get)\n\n    async def update_audit_result(self, race_id: str, outcome: Dict[str, Any]):\n        if not self._initialized: await self.initialize()\n        def _update():\n            conn = self._get_conn()\n            with conn:\n                conn.execute(\"\"\"\n                    UPDATE tips SET\n                        audit_completed = 1, verdict = ?, net_profit = ?, selection_position = ?,\n                        actual_top_5 = ?, actual_2nd_fav_odds = ?, trifecta_payout = ?, trifecta_combination = ?,\n                        superfecta_payout = ?, superfecta_combination = ?, top1_place_payout = ?, top2_place_payout = ?,\n                        audit_timestamp = ?\n                    WHERE id = (SELECT id FROM tips WHERE race_id = ? AND audit_completed = 0 LIMIT 1)\n                \"\"\", (\n                    outcome.get(\"verdict\"), outcome.get(\"net_profit\"), outcome.get(\"selection_position\"),\n                    outcome.get(\"actual_top_5\"), outcome.get(\"actual_2nd_fav_odds\"), outcome.get(\"trifecta_payout\"),\n                    outcome.get(\"trifecta_combination\"), outcome.get(\"superfecta_payout\"), outcome.get(\"superfecta_combination\"),\n                    outcome.get(\"top1_place_payout\"), outcome.get(\"top2_place_payout\"), datetime.now(EASTERN).isoformat(), race_id\n                ))\n        await self._run_in_executor(_update)\n\n    async def update_audit_results_batch(self, outcomes: List[Tuple[str, Dict[str, Any]]]):\n        if not outcomes: return\n        if not self._initialized: await self.initialize()\n        def _update():\n            conn = self._get_conn()\n            with conn:\n                for race_id, outcome in outcomes:\n                    conn.execute(\"\"\"\n                        UPDATE tips SET\n                            audit_completed = 1, verdict = ?, net_profit = ?, selection_position = ?,\n                            actual_top_5 = ?, actual_2nd_fav_odds = ?, trifecta_payout = ?, trifecta_combination = ?,\n                            superfecta_payout = ?, superfecta_combination = ?, top1_place_payout = ?, top2_place_payout = ?,\n                            audit_timestamp = ?\n                        WHERE id = (SELECT id FROM tips WHERE race_id = ? AND audit_completed = 0 LIMIT 1)\n                    \"\"\", (\n                        outcome.get(\"verdict\"), outcome.get(\"net_profit\"), outcome.get(\"selection_position\"),\n                        outcome.get(\"actual_top_5\"), outcome.get(\"actual_2nd_fav_odds\"), outcome.get(\"trifecta_payout\"),\n                        outcome.get(\"trifecta_combination\"), outcome.get(\"superfecta_payout\"), outcome.get(\"superfecta_combination\"),\n                        outcome.get(\"top1_place_payout\"), outcome.get(\"top2_place_payout\"), outcome.get(\"audit_timestamp\") or datetime.now(EASTERN).isoformat(), race_id\n                    ))\n        await self._run_in_executor(_update)\n\n    async def close(self) -> None:\n        def _close():\n            with self._conn_lock:\n                if self._conn:\n                    self._conn.close()\n                    self._conn = None\n        await self._run_in_executor(_close)\n        self._executor.shutdown(wait=True)\n\n    async def migrate_from_json(self, json_path: str = \"hot_tips_db.json\"):\n        path = Path(json_path)\n        if not path.exists(): return\n        try:\n            with open(path, \"r\") as f:\n                data = json.load(f)\n            if not isinstance(data, list): return\n            if not self._initialized: await self.initialize()\n            def _migrate():\n                conn = self._get_conn()\n                for entry in data:\n                    with conn:\n                        conn.execute(\"\"\"\n                            INSERT OR IGNORE INTO tips (\n                                race_id, venue, race_number, discipline, start_time, report_date,\n                                is_goldmine, audit_completed, verdict, net_profit\n                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                        \"\"\", (\n                            entry.get(\"race_id\"), entry.get(\"venue\"), entry.get(\"race_number\"),\n                            entry.get(\"discipline\"), entry.get(\"start_time\"), entry.get(\"report_date\"),\n                            1 if entry.get(\"is_goldmine\") else 0, 1 if entry.get(\"verdict\") else 0,\n                            entry.get(\"verdict\"), entry.get(\"net_profit\")\n                        ))\n            await self._run_in_executor(_migrate)\n        except Exception as e:\n            self.logger.error(\"JSON migration failed\", error=str(e))\n", "web_service/backend/adapters/results/base.py": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport re\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, Final, List, Optional, Set, Tuple, Type\nfrom zoneinfo import ZoneInfo\n\nimport structlog\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..base_adapter_v3 import BaseAdapterV3\nfrom ..mixins import BrowserHeadersMixin, DebugMixin, RacePageFetcherMixin\nfrom ...models import ResultRace, ResultRunner\nfrom ...utils.text import normalize_venue_name, clean_text\nfrom ...utils.odds import parse_odds_to_decimal\nfrom ...core.smart_fetcher import FetchStrategy, BrowserEngine\n\nEASTERN = ZoneInfo(\"America/New_York\")\n\ndef parse_currency_value(value_str: str) -> float:\n    \"\"\"'$1,234.56' -> 1234.56\"\"\"\n    if not value_str:\n        return 0.0\n    try:\n        raw = str(value_str).strip()\n        # Allow standard currency symbols and codes (GBP, EUR, USD, ZAR)\n        if re.search(r\"[^\\d.,$\u00a3\u20ac\\sA-Z]\", raw):\n            return 0.0\n\n        if \",\" in raw and \".\" in raw:\n            if raw.rfind(\",\") > raw.rfind(\".\"):\n                # European style: 1.234,56\n                cleaned = raw.replace(\".\", \"\").replace(\",\", \".\")\n            else:\n                # US style: 1,234.56\n                cleaned = raw.replace(\",\", \"\")\n        elif \",\" in raw and \".\" not in raw and re.search(r\",\\d{2}$\", raw):\n            # European format: 12,34\n            cleaned = raw.replace(\",\", \".\")\n        else:\n            cleaned = raw.replace(\",\", \"\")\n\n        cleaned = re.sub(r\"[^\\d.]\", \"\", cleaned)\n        return float(cleaned) if cleaned else 0.0\n    except (ValueError, TypeError):\n        return 0.0\n\n\ndef parse_fractional_odds(text: str) -> float:\n    \"\"\"'5/2' -> 3.5, '2.5' -> 2.5, anything else -> 0.0.\"\"\"\n    val = parse_odds_to_decimal(text)\n    return float(val) if val is not None else 0.0\n\n\ndef build_start_time(\n    date_str: str,\n    time_str: Optional[str] = None,\n    *,\n    tz: ZoneInfo = EASTERN,\n) -> datetime:\n    \"\"\"Build a tz-aware datetime from YYYY-MM-DD + optional HH:MM.\"\"\"\n    try:\n        base = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        structlog.get_logger(\"build_start_time\").warning(\"unparseable_date\", date=date_str)\n        base = datetime.now(tz)\n    hour, minute = 12, 0\n    if time_str:\n        try:\n            parts = time_str.strip().split(\":\")\n            hour, minute = int(parts[0]), int(parts[1])\n        except (ValueError, IndexError):\n            structlog.get_logger(\"build_start_time\").warning(\"malformed_time_string\", time=time_str)\n            pass\n    return base.replace(hour=hour, minute=minute, tzinfo=tz)\n\n\ndef find_nested_value(\n    obj: Any,\n    key_fragment: str,\n    *,\n    _depth: int = 0,\n    _max_depth: int = 20,\n) -> Optional[float]:\n    \"\"\"Recursively search dicts/lists for a key containing key_fragment\n    whose value is numeric. Depth-guarded.\"\"\"\n    if _depth > _max_depth:\n        return None\n    frag = key_fragment.lower()\n    if isinstance(obj, dict):\n        for k, v in obj.items():\n            if frag in k.lower() and isinstance(v, (int, float, str)):\n                parsed = (\n                    parse_currency_value(str(v))\n                    if isinstance(v, str)\n                    else float(v)\n                )\n                if parsed:\n                    return parsed\n            found = find_nested_value(\n                v, key_fragment, _depth=_depth + 1, _max_depth=_max_depth,\n            )\n            if found is not None:\n                return found\n    elif isinstance(obj, (list, tuple)):\n        for item in obj:\n            found = find_nested_value(\n                item, key_fragment, _depth=_depth + 1, _max_depth=_max_depth,\n            )\n            if found is not None:\n                return found\n    return None\n\n\n_BET_ALIASES: Final[Dict[str, List[str]]] = {\n    \"superfecta\": [\"superfecta\", \"first 4\", \"first four\"],\n    \"trifecta\":   [\"trifecta\", \"tricast\"],\n    \"exacta\":     [\"exacta\", \"forecast\"],\n}\n\n\ndef extract_exotic_payouts(\n    tables: list[Node],\n) -> Dict[str, Tuple[Optional[float], Optional[str]]]:\n    \"\"\"Scan tables for exotic-bet dividend rows.\"\"\"\n    results: Dict[str, Tuple[Optional[float], Optional[str]]] = {}\n    for table in tables:\n        text = table.text().lower()\n        for bet_type, aliases in _BET_ALIASES.items():\n            if bet_type in results:\n                continue\n            if not any(a in text for a in aliases):\n                continue\n            for row in table.css(\"tr\"):\n                row_text = row.text().lower()\n                if not any(a in row_text for a in aliases):\n                    continue\n                cols = row.css(\"td\")\n                combo: Optional[str] = None\n                payout = 0.0\n                if len(cols) >= 3:\n                    combo  = clean_text(cols[1].text())\n                    payout = parse_currency_value(cols[2].text())\n                elif len(cols) >= 2:\n                    combo  = clean_text(cols[0].text())\n                    payout = parse_currency_value(cols[1].text())\n                if payout > 0:\n                    results[bet_type] = (payout, combo)\n                    break\n    return results\n\n\ndef _extract_race_number_from_text(\n    parser: HTMLParser,\n    url: str = \"\",\n) -> Optional[int]:\n    \"\"\"Best-effort race-number from page text or URL.\"\"\"\n    m = re.search(r\"Race\\s+(\\d+)\", parser.text(), re.I)\n    if m:\n        return int(m.group(1))\n    m = re.search(r\"/R(\\d+)(?:[/?#]|$)\", url)\n    if m:\n        return int(m.group(1))\n    return None\n\n\nclass PageFetchingResultsAdapter(\n    BrowserHeadersMixin,\n    DebugMixin,\n    RacePageFetcherMixin,\n    BaseAdapterV3,\n):\n    \"\"\"Common base for results adapters.\"\"\"\n\n    ADAPTER_TYPE: Final[str] = \"results\"\n\n    _BLOCK_SIGNATURES = [\n        \"pardon our interruption\",\n        \"checking your browser\",\n        \"cloudflare\",\n        \"access denied\",\n        \"captcha\",\n        \"please verify\",\n    ]\n\n    def _check_for_block(self, html: str, url: str) -> bool:\n        lower = html.lower()\n        for sig in self._BLOCK_SIGNATURES:\n            if sig in lower and len(html) < 10000:\n                self.logger.error(\n                    \"BOT BLOCKED\",\n                    source=self.source_name,\n                    url=url,\n                    signature=sig,\n                    html_length=len(html),\n                )\n                return True\n        return False\n\n    # -- subclass must set -------------------------------------------------\n    SOURCE_NAME: str\n    BASE_URL: str\n    HOST: str\n\n    # -- subclass may override ---------------------------------------------\n    TIMEOUT: int = 60\n    IMPERSONATE: Optional[str] = None\n\n    def __init__(self, **kwargs: Any) -> None:\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            **kwargs,\n        )\n        self._target_venues: Optional[Set[str]] = None\n\n    @property\n    def target_venues(self) -> Optional[Set[str]]:\n        return self._target_venues\n\n    @target_venues.setter\n    def target_venues(self, value: Optional[Set[str]]) -> None:\n        self._target_venues = value\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CURL_CFFI,\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=self.TIMEOUT,\n        )\n\n    async def make_request(\n        self, method: str, url: str, **kwargs: Any,\n    ) -> Any:\n        if self.IMPERSONATE:\n            kwargs.setdefault(\"impersonate\", self.IMPERSONATE)\n        return await super().make_request(method, url, **kwargs)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=self.HOST)\n\n    def _validate_and_parse_races(self, raw_data: Any) -> List[ResultRace]:\n        return self._parse_races(raw_data)\n\n    async def _fetch_data(\n        self, date_str: str,\n    ) -> Optional[Dict[str, Any]]:\n        links = await self._discover_result_links(date_str)\n        if not links:\n            self.logger.warning(\n                \"No result links found\",\n                source=self.source_name,\n                date=date_str,\n            )\n            return None\n        return await self._fetch_link_pages(links, date_str)\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        raise NotImplementedError\n\n    async def _fetch_link_pages(\n        self, links: Set[str], date_str: str,\n    ) -> Optional[Dict[str, Any]]:\n        absolute = list(dict.fromkeys(\n            lnk if lnk.startswith(\"http\") else f\"{self.BASE_URL}{lnk}\"\n            for lnk in links\n        ))\n        self.logger.info(\n            \"Fetching result pages\",\n            source=self.source_name,\n            count=len(absolute),\n        )\n        metadata = [{\"url\": u, \"race_number\": 0} for u in absolute]\n        pages = await self._fetch_race_pages_concurrent(\n            metadata, self._get_headers(),\n        )\n        return {\"pages\": pages, \"date\": date_str}\n\n    def _parse_races(self, raw_data: Any) -> List[ResultRace]:\n        if not raw_data:\n            return []\n        date_str = raw_data.get(\n            \"date\", datetime.now(EASTERN).strftime(\"%Y-%m-%d\"),\n        )\n        races: List[ResultRace] = []\n        for item in raw_data.get(\"pages\", []):\n            html = item.get(\"html\") if isinstance(item, dict) else None\n            url  = item.get(\"url\", \"\") if isinstance(item, dict) else \"\"\n            if not html:\n                continue\n            \n            if self._check_for_block(html, url):\n                continue\n\n            try:\n                races.extend(self._parse_page(html, date_str, url))\n            except Exception as exc:\n                self.logger.warning(\n                    \"Failed to parse result page\",\n                    source=self.source_name,\n                    url=url,\n                    error=str(exc),\n                )\n        return races\n\n    def _parse_page(\n        self, html: str, date_str: str, url: str,\n    ) -> List[ResultRace]:\n        race = self._parse_race_page(html, date_str, url)\n        return [race] if race else []\n\n    def _parse_race_page(\n        self, html: str, date_str: str, _url: str,\n    ) -> Optional[ResultRace]:\n        raise NotImplementedError\n\n    def _venue_matches(self, text: str, href: str = \"\") -> bool:\n        if not self.target_venues:\n            return True\n\n        from ...models import get_canonical_venue\n        canon_text = get_canonical_venue(text)\n        if canon_text != \"\" and canon_text in self.target_venues:\n            return True\n\n        href_clean = href.lower().replace(\"-\", \"\").replace(\"_\", \"\")\n        for v in self.target_venues:\n            if v and v in href_clean:\n                return True\n\n        return False\n\n    def _make_race_id(\n        self,\n        prefix: str,\n        venue: str,\n        date_str: str,\n        race_num: int,\n    ) -> str:\n        from ...models import get_canonical_venue\n        canon = get_canonical_venue(venue)\n        return f\"{prefix}_{canon}_{date_str.replace('-', '')}_R{race_num}\"\n", "web_service/backend/adapters/results/equibase_results_adapter.py": "from typing import Any, Dict, List, Optional, Tuple, Set\nfrom datetime import datetime, timezone\nimport re\nimport asyncio\nfrom selectolax.parser import HTMLParser, Node\n\nfrom .base import (\n    PageFetchingResultsAdapter,\n    extract_exotic_payouts,\n    parse_currency_value,\n    build_start_time,\n    EASTERN\n)\nfrom ...models import ResultRace, ResultRunner\nfrom ...utils.text import normalize_venue_name, clean_text\nfrom ...utils.odds import parse_odds_to_decimal\nfrom ...core.smart_fetcher import FetchStrategy, BrowserEngine\n\n\nclass EquibaseResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Equibase summary charts \u2014 primary US thoroughbred results source.\"\"\"\n\n    SOURCE_NAME = \"EquibaseResults\"\n    BASE_URL    = \"https://www.equibase.com\"\n    HOST        = \"www.equibase.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 60\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        # Equibase uses Instart Logic / Imperva; PLAYWRIGHT_LEGACY with network_idle is robust\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT, # Changed from PLAYWRIGHT_LEGACY to PLAYWRIGHT as per current project availability\n            enable_js=True,\n            stealth_mode=\"camouflage\",\n            timeout=self.TIMEOUT,\n        )\n\n    def _get_headers(self) -> dict:\n        return {\"Referer\": \"https://www.equibase.com/\"}\n\n    async def _discover_result_links(self, date_str: str) -> set:\n        try:\n            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=date_str)\n            return set()\n\n        index_urls = [\n            f\"/static/chart/summary/index.html?SAP=TN\",\n            f\"/static/chart/summary/index.html?date={dt.strftime('%m/%d/%Y')}\",\n            f\"/static/chart/summary/{dt.strftime('%m%d%y')}sum.html\",\n            f\"/static/chart/summary/{dt.strftime('%Y%m%d')}sum.html\",\n        ]\n\n        resp = None\n        for url in index_urls:\n            # Try multiple impersonations to bypass Imperva/Cloudflare\n            for imp in [\"chrome120\", \"chrome110\", \"safari15_5\"]:\n                try:\n                    resp = await self.make_request(\n                        \"GET\", url, headers=self._get_headers(), impersonate=imp\n                    )\n                    if (\n                        resp and resp.text\n                        and len(resp.text) > 2000 # Increased threshold for real content\n                        and \"Pardon Our Interruption\" not in resp.text\n                        and \"<table\" in resp.text.lower() # Verify presence of data tables\n                    ):\n                        break\n                    else:\n                        resp = None\n                except Exception:\n                    continue\n            if resp:\n                break\n\n        if not resp or not resp.text:\n            self.logger.warning(\"No response from Equibase index\", date=date_str)\n            return set()\n\n        self._save_debug_snapshot(resp.text, f\"eqb_results_index_{date_str}\")\n        initial_links = self._extract_track_links(resp.text, dt)\n\n        # Resolve any RaceCardIndex links to actual sum.html files\n        resolved_links = set()\n        index_links = [ln for ln in initial_links if \"RaceCardIndex\" in ln]\n        sum_links = [ln for ln in initial_links if \"RaceCardIndex\" not in ln]\n\n        resolved_links.update(sum_links)\n\n        if index_links:\n            self.logger.info(\"Resolving track indices\", count=len(index_links))\n            metadata = [{\"url\": ln, \"race_number\": 0} for ln in index_links]\n            index_pages = await self._fetch_race_pages_concurrent(\n                metadata, self._get_headers(),\n            )\n            for p in index_pages:\n                html = p.get(\"html\")\n                if not html: continue\n                # Extract all sum.html links from this track index\n                date_short = dt.strftime(\"%m%d%y\")\n                for m in re.findall(r'href=\"([^\"]+)\"', html):\n                    normalised = m.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n                    if date_short in normalised and \"sum.html\" in normalised:\n                        resolved_links.add(self._normalise_eqb_link(normalised))\n                for m in re.findall(r'\"URL\":\"([^\"]+)\"', html):\n                    normalised = m.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n                    if date_short in normalised and \"sum.html\" in normalised:\n                        resolved_links.add(self._normalise_eqb_link(normalised))\n\n        return resolved_links\n\n    def _extract_track_links(self, html: str, dt: datetime) -> set:\n        \"\"\"Pull track-summary URLs from the index page.\"\"\"\n        parser = HTMLParser(html)\n        raw_links: set = set()\n        date_short = dt.strftime(\"%m%d%y\")\n\n        # Source 1 \u2014 inline JSON in <script> tags\n        for url_match in re.findall(r'\"URL\":\"([^\"]+)\"', html):\n            normalised = url_match.replace(\"\\\\/\", \"/\").replace(\"\\\\\", \"/\")\n            if date_short in normalised and (\n                \"sum.html\" in normalised or \"EQB.html\" in normalised or \"RaceCardIndex\" in normalised\n            ):\n                raw_links.add(normalised)\n\n        # Source 2 \u2014 <a> tags matching known patterns\n        selectors_and_patterns = [\n            ('table.display a[href*=\"sum.html\"]', None),\n            ('a[href*=\"/static/chart/summary/\"]', lambda h: \"index.html\" not in h and \"calendar.html\" not in h),\n            (\"a\", lambda h: (\n                re.search(r\"[A-Z]{3}\\d{6}(?:sum|EQB)\\.html\", h)\n                or (date_short in h and (\"sum.html\" in h.lower() or \"eqb.html\" in h.lower()))\n            ) and \"index.html\" not in h and \"calendar.html\" not in h),\n        ]\n        for selector, extra_filter in selectors_and_patterns:\n            for a in parser.css(selector):\n                href = (a.attributes.get(\"href\") or \"\").replace(\"\\\\\", \"/\")\n                if not href:\n                    continue\n                if extra_filter and not extra_filter(href):\n                    continue\n                if not self._venue_matches(a.text(), href):\n                    continue\n                raw_links.add(href)\n\n        if not raw_links:\n            self.logger.warning(\"No track links found in index\", date=str(dt.date()))\n            return set()\n\n        self.logger.info(\"Track links extracted\", count=len(raw_links))\n        return {self._normalise_eqb_link(lnk) for lnk in raw_links}\n\n    def _normalise_eqb_link(self, link: str) -> str:\n        \"\"\"Turn a relative Equibase link into an absolute URL.\"\"\"\n        if link.startswith(\"http\"):\n            return link\n        path = link.lstrip(\"/\")\n        if \"static/chart/summary/\" not in path:\n            if path.startswith(\"../\"):\n                path = \"static/chart/\" + path.replace(\"../\", \"\")\n            elif not path.startswith(\"static/\"):\n                path = f\"static/chart/summary/{path}\"\n        path = re.sub(r\"/+\", \"/\", path)\n        return f\"{self.BASE_URL}/{path}\"\n\n    # -- multi-race page parsing -------------------------------------------\n\n    def _parse_page(\n        self, html: str, date_str: str, url: str,\n    ) -> List[ResultRace]:\n        \"\"\"A track summary page contains multiple race tables.\"\"\"\n        parser = HTMLParser(html)\n\n        # Venue from page header\n        track_node = parser.css_first(\"h3\") or parser.css_first(\"h2\")\n        if not track_node:\n            self.logger.debug(\"No track header found\", url=url)\n            return []\n        venue = normalize_venue_name(track_node.text(strip=True))\n        if not venue:\n            return []\n\n        # Identify race tables and their indices among ALL tables\n        all_tables = parser.css(\"table\")\n        indexed_race_tables: List[Tuple[int, Node]] = []\n        for i, table in enumerate(all_tables):\n            header = table.css_first(\"thead tr th\")\n            if header and \"Race\" in header.text():\n                indexed_race_tables.append((i, table))\n\n        races: List[ResultRace] = []\n        for j, (idx, race_table) in enumerate(indexed_race_tables):\n            try:\n                # Dividend tables sit between this race and the next\n                next_idx = (\n                    indexed_race_tables[j + 1][0]\n                    if j + 1 < len(indexed_race_tables)\n                    else len(all_tables)\n                )\n                dividend_tables = all_tables[idx + 1 : next_idx]\n                exotics = extract_exotic_payouts(dividend_tables)\n\n                race = self._parse_race_table(\n                    race_table, venue, date_str, exotics,\n                )\n                if race:\n                    races.append(race)\n            except Exception as exc:\n                self.logger.debug(\n                    \"Failed to parse race table\", error=str(exc),\n                )\n        return races\n\n    def _parse_race_table(\n        self,\n        table: Node,\n        venue: str,\n        date_str: str,\n        exotics: Dict[str, Tuple[Optional[float], Optional[str]]],\n    ) -> Optional[ResultRace]:\n        header = table.css_first(\"thead tr th\")\n        if not header:\n            return None\n        header_text = header.text()\n\n        race_match = re.search(r\"Race\\s+(\\d+)\", header_text)\n        if not race_match:\n            return None\n        race_num = int(race_match.group(1))\n\n        # Start time from header or fallback\n        start_time = self._parse_header_time(header_text, date_str)\n\n        runners = [\n            r for row in table.css(\"tbody tr\")\n            if (r := self._parse_runner_row(row)) is not None\n        ]\n        if not runners:\n            return None\n\n        tri = exotics.get(\"trifecta\", (None, None))\n        exa = exotics.get(\"exacta\", (None, None))\n        sup = exotics.get(\"superfecta\", (None, None))\n\n        return ResultRace(\n            id=self._make_race_id(\"eqb_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n            is_fully_parsed=True,\n            trifecta_payout=tri[0],\n            trifecta_combination=tri[1],\n            exacta_payout=exa[0],\n            exacta_combination=exa[1],\n            superfecta_payout=sup[0],\n            superfecta_combination=sup[1],\n        )\n\n    @staticmethod\n    def _parse_header_time(header_text: str, date_str: str) -> datetime:\n        m = re.search(r\"(\\d{1,2}:\\d{2})\\s*([APM]{2})\", header_text, re.I)\n        if m:\n            try:\n                t = datetime.strptime(\n                    f\"{m.group(1)} {m.group(2).upper()}\", \"%I:%M %p\",\n                ).time()\n                d = datetime.strptime(date_str, \"%Y-%m-%d\")\n                return datetime.combine(d, t).replace(tzinfo=EASTERN)\n            except ValueError:\n                pass\n        return build_start_time(date_str)\n\n    def _parse_runner_row(self, row: Node) -> Optional[ResultRunner]:\n        try:\n            cols = row.css(\"td\")\n            if len(cols) < 3:\n                return None\n\n            name = clean_text(cols[2].text())\n            if not name or name.upper() in (\"HORSE\", \"NAME\", \"RUNNER\"):\n                return None\n\n            pos_text = clean_text(cols[0].text())\n            num_text = clean_text(cols[1].text())\n            number = int(num_text) if num_text.isdigit() else 0\n\n            odds_text = (\n                clean_text(cols[3].text()) if len(cols) > 3 else \"\"\n            )\n            final_odds = parse_odds_to_decimal(odds_text)\n\n            win_pay = place_pay = show_pay = 0.0\n            if len(cols) >= 7:\n                win_pay   = parse_currency_value(cols[4].text())\n                place_pay = parse_currency_value(cols[5].text())\n                show_pay  = parse_currency_value(cols[6].text())\n\n            return ResultRunner(\n                name=name,\n                number=number,\n                position=pos_text,\n                final_win_odds=final_odds,\n                win_payout=win_pay,\n                place_payout=place_pay,\n                show_payout=show_pay,\n            )\n        except Exception as exc:\n            self.logger.warning(\n                \"Failed parsing runner row\", error=str(exc),\n            )\n            return None\n", "web_service/backend/adapters/results/racing_post_results_adapter.py": "from typing import Any, Dict, List, Optional, Set, Tuple\nfrom datetime import datetime, timezone\nimport re\nimport asyncio\nfrom selectolax.parser import HTMLParser, Node\n\nfrom .base import (\n    PageFetchingResultsAdapter,\n    parse_currency_value,\n    parse_fractional_odds,\n    build_start_time,\n    _BET_ALIASES,\n    _extract_race_number_from_text\n)\nfrom ...models import ResultRace, ResultRunner\nfrom ...utils.text import normalize_venue_name, clean_text\nfrom ...core.smart_fetcher import FetchStrategy, BrowserEngine\nimport fortuna\n\n\nclass RacingPostResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Racing Post results \u2014 UK / IRE thoroughbred and jumps.\"\"\"\n\n    SOURCE_NAME = \"RacingPostResults\"\n    BASE_URL    = \"https://www.racingpost.com\"\n    HOST        = \"www.racingpost.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 60\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        strategy = super()._configure_fetch_strategy()\n        # RacingPost is JS-heavy and has strong bot detection; keep CURL_CFFI primary but ensure fallback (Project Hardening)\n        strategy.primary_engine = BrowserEngine.CURL_CFFI\n        return strategy\n\n    # -- link discovery ----------------------------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        resp = await self.make_request(\n            \"GET\", f\"/results/{date_str}\", headers=self._get_headers(),\n        )\n        if not resp or not resp.text:\n            return set()\n        \n        if self._check_for_block(resp.text, f\"/results/{date_str}\"):\n            return set()\n\n        self._save_debug_snapshot(resp.text, f\"rp_results_index_{date_str}\")\n        parser = HTMLParser(resp.text)\n        return self._extract_rp_links(parser)\n\n    def _extract_rp_links(self, parser: HTMLParser) -> set:\n        links: set = set()\n\n        _SELECTORS = [\n            'a[data-test-selector=\"RC-meetingItem__link_race\"]',\n            'a[href*=\"/results/\"]',\n            \".ui-link.rp-raceCourse__panel__race__time\",\n            \"a.rp-raceCourse__panel__race__time\",\n            \".rp-raceCourse__panel__race__time a\",\n            \".RC-meetingItem__link\",\n        ]\n        for selector in _SELECTORS:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\", \"\")\n                if not href:\n                    continue\n                if not self._venue_matches(a.text(), href):\n                    continue\n                if self._is_rp_race_link(href):\n                    links.add(href)\n\n        # Last-resort fallback\n        if not links:\n            for a in parser.css('a[href*=\"/results/\"]'):\n                href = a.attributes.get(\"href\", \"\")\n                if len(href.split(\"/\")) >= 3:\n                    links.add(href)\n\n        return links\n\n    @staticmethod\n    def _is_rp_race_link(href: str) -> bool:\n        return bool(\n            re.search(r\"/results/.*?\\d{5,}\", href)\n            or re.search(r\"/results/\\d+/\", href)\n            or re.search(r\"/\\d{4}-\\d{2}-\\d{2}/\", href)\n            or (\"/results/\" in href and len(href.split(\"/\")) >= 4)\n        )\n\n    # -- single-race page parsing ------------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, _url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        venue_node = parser.css_first(\".rp-raceTimeCourseName__course\")\n        if not venue_node:\n            return None\n        venue = normalize_venue_name(venue_node.text(strip=True))\n\n        dividends = self._parse_tote_dividends(parser)\n        trifecta_pay, trifecta_combo = self._exotic_from_dividends(\n            dividends, \"trifecta\",\n        )\n        superfecta_pay, superfecta_combo = self._exotic_from_dividends(\n            dividends, \"superfecta\",\n        )\n\n        race_num = self._extract_rp_race_number(parser)\n\n        runners = self._parse_rp_runners(parser, dividends)\n        if not runners:\n            return None\n\n        return ResultRace(\n            id=self._make_race_id(\"rp_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=build_start_time(date_str),\n            runners=runners,\n            source=self.SOURCE_NAME,\n            trifecta_payout=trifecta_pay,\n            trifecta_combination=trifecta_combo,\n            superfecta_payout=superfecta_pay,\n            superfecta_combination=superfecta_combo,\n            official_dividends={\n                k: parse_currency_value(v) for k, v in dividends.items()\n            },\n        )\n\n    # -- RP-specific helpers -----------------------------------------------\n\n    @staticmethod\n    def _parse_tote_dividends(parser: HTMLParser) -> Dict[str, str]:\n        \"\"\"Extract label\u2192value pairs from the Tote Returns panel.\"\"\"\n        container = (\n            parser.css_first('div[data-test-selector=\"RC-toteReturns\"]')\n            or parser.css_first(\".rp-toteReturns\")\n        )\n        if not container:\n            return {}\n\n        dividends: Dict[str, str] = {}\n        for row in (\n            container.css(\"div.rp-toteReturns__row\")\n            or container.css(\".rp-toteReturns__row\")\n        ):\n            label_node = (\n                row.css_first(\"div.rp-toteReturns__label\")\n                or row.css_first(\".rp-toteReturns__label\")\n            )\n            val_node = (\n                row.css_first(\"div.rp-toteReturns__value\")\n                or row.css_first(\".rp-toteReturns__value\")\n            )\n            if label_node and val_node:\n                label = clean_text(label_node.text())\n                value = clean_text(val_node.text())\n                if label and value:\n                    dividends[label] = value\n        return dividends\n\n    @staticmethod\n    def _exotic_from_dividends(\n        dividends: Dict[str, str],\n        bet_type: str,\n    ) -> Tuple[Optional[float], Optional[str]]:\n        aliases = _BET_ALIASES.get(bet_type, [bet_type])\n        for label, val in dividends.items():\n            if any(a in label.lower() for a in aliases):\n                payout = parse_currency_value(val)\n                combo = (\n                    val.split(\"\u00a3\")[-1].strip() if \"\u00a3\" in val else None\n                )\n                return payout, combo\n        return None, None\n\n    @staticmethod\n    def _extract_rp_race_number(parser: HTMLParser) -> int:\n        # Priority 1 \u2014 navigation bar active item\n        for i, link in enumerate(\n            parser.css('a[data-test-selector=\"RC-raceTime\"]'),\n        ):\n            cls = link.attributes.get(\"class\", \"\")\n            if \"active\" in cls or \"rp-raceTimeCourseName__time\" in cls:\n                return i + 1\n        # Priority 2 \u2014 text fallback\n        return _extract_race_number_from_text(parser) or 1\n\n    def _parse_rp_runners(\n        self,\n        parser: HTMLParser,\n        dividends: Dict[str, str],\n    ) -> List[ResultRunner]:\n        runners: List[ResultRunner] = []\n        for row in parser.css(\".rp-horseTable__table__row\"):\n            try:\n                name_node = row.css_first(\".rp-horseTable__horse__name\")\n                if not name_node:\n                    continue\n                name = clean_text(name_node.text())\n\n                pos_node = row.css_first(\".rp-horseTable__pos__number\")\n                pos = (\n                    clean_text(pos_node.text()) if pos_node else None\n                )\n\n                num_node = row.css_first(\".rp-horseTable__saddleClothNo\")\n                number = 0\n                if num_node:\n                    try:\n                        number = int(clean_text(num_node.text()))\n                    except ValueError:\n                        pass\n\n                # Place payout from dividends map\n                place_payout: Optional[float] = None\n                for lbl, val in dividends.items():\n                    if (\n                        \"place\" in lbl.lower()\n                        and name.lower() in lbl.lower()\n                    ):\n                        place_payout = parse_currency_value(val)\n                        break\n\n                sp_node = row.css_first(\".rp-horseTable__horse__sp\")\n                final_odds = 0.0\n                if sp_node:\n                    final_odds = parse_fractional_odds(\n                        clean_text(sp_node.text()),\n                    )\n\n                runners.append(ResultRunner(\n                    name=name,\n                    number=number,\n                    position=pos,\n                    place_payout=place_payout,\n                    final_win_odds=final_odds,\n                ))\n            except Exception:\n                continue\n        return runners\n", "web_service/backend/adapters/results/at_the_races_results_adapter.py": "from typing import Any, Dict, List, Optional, Set, Tuple\nfrom datetime import datetime, timezone\nimport re\nimport asyncio\nfrom selectolax.parser import HTMLParser, Node\n\nfrom .base import (\n    PageFetchingResultsAdapter,\n    extract_exotic_payouts,\n    parse_currency_value,\n    parse_fractional_odds,\n    build_start_time\n)\nfrom ...models import ResultRace, ResultRunner\nfrom ...utils.text import normalize_venue_name, clean_text\nfrom ...core.smart_fetcher import FetchStrategy, BrowserEngine\nimport fortuna\n\n\nclass AtTheRacesResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"At The Races results \u2014 UK / IRE.\"\"\"\n\n    SOURCE_NAME = \"AtTheRacesResults\"\n    BASE_URL    = \"https://www.attheraces.com\"\n    HOST        = \"www.attheraces.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 60\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        strategy = super()._configure_fetch_strategy()\n        # ATR uses Cloudflare; keep CURL_CFFI primary but ensure fallback (Project Hardening)\n        strategy.primary_engine = fortuna.BrowserEngine.CURL_CFFI\n        return strategy\n\n    # -- link discovery (multi-URL index) ----------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> set:\n        dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n        index_urls = [\n            f\"/results/{date_str}\",\n            f\"/results/{dt.strftime('%d-%B-%Y')}\",\n            f\"/results/international/{date_str}\",\n            f\"/results/international/{dt.strftime('%d-%B-%Y')}\",\n        ]\n\n        links: set = set()\n        for url in index_urls:\n            try:\n                resp = await self.make_request(\n                    \"GET\", url, headers=self._get_headers(),\n                )\n                if not resp or not resp.text:\n                    continue\n                self._save_debug_snapshot(\n                    resp.text,\n                    f\"atr_index_{date_str}_{url.replace('/', '_')}\",\n                )\n                links.update(self._extract_atr_links(resp.text))\n            except Exception as exc:\n                self.logger.debug(\n                    \"ATR index fetch failed\", url=url, error=str(exc),\n                )\n\n        return links\n\n    def _extract_atr_links(self, html: str) -> set:\n        parser = HTMLParser(html)\n        links: set = set()\n        # Broad selectors for all possible result links (Council of Superbrains Directive)\n        for selector in [\n            \"a[href*='/results/']\",\n            \"a[data-test-selector*='result']\",\n            \".meeting-summary a\",\n            \".p-results__item a\",\n            \".p-meetings__item a\",\n            \".p-results-meeting a\",\n        ]:\n            for a in parser.css(selector):\n                href = a.attributes.get(\"href\", \"\")\n                if not href:\n                    continue\n                if not self._venue_matches(a.text(), href):\n                    continue\n                if self._is_atr_race_link(href):\n                    links.add(\n                        href if href.startswith(\"http\")\n                        else f\"{self.BASE_URL}{href}\"\n                    )\n        return links\n\n    @staticmethod\n    def _is_atr_race_link(href: str) -> bool:\n        return bool(\n            re.search(r\"/results/.*?/\\d{4}\", href)\n            or re.search(r\"/results/\\d{2}-.*?-\\d{4}/\", href)\n            or re.search(r\"/results/.*?/\\d+$\", href)\n            or (\"/results/\" in href and len(href.split(\"/\")) >= 4)\n        )\n\n    # -- single-race page parsing ------------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        venue = self._extract_atr_venue(parser)\n        if not venue:\n            return None\n\n        race_num = 1\n        url_match = re.search(r\"/R(\\d+)$\", url)\n        if url_match:\n            race_num = int(url_match.group(1))\n\n        runners = self._parse_atr_runners(parser)\n\n        # Dividends \u2014 use the shared exotic extractor on the\n        # dedicated dividends table, then enrich with place payouts\n        div_table = parser.css_first(\".result-racecard__dividends-table\")\n        exotics: Dict[str, Tuple[Optional[float], Optional[str]]] = {}\n        if div_table:\n            exotics = extract_exotic_payouts([div_table])\n            self._map_place_payouts(div_table, runners)\n\n        if not runners:\n            return None\n\n        tri = exotics.get(\"trifecta\", (None, None))\n        sup = exotics.get(\"superfecta\", (None, None))\n\n        return ResultRace(\n            id=self._make_race_id(\"atr_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=build_start_time(date_str),\n            runners=runners,\n            trifecta_payout=tri[0],\n            trifecta_combination=tri[1],\n            superfecta_payout=sup[0],\n            superfecta_combination=sup[1],\n            source=self.SOURCE_NAME,\n        )\n\n    # -- ATR-specific helpers ----------------------------------------------\n\n    @staticmethod\n    def _extract_atr_venue(parser: HTMLParser) -> Optional[str]:\n        header = (\n            parser.css_first(\".race-header__details--primary\")\n            or parser.css_first(\".racecard-header\")\n            or parser.css_first(\".race-header\")\n        )\n        if not header:\n            return None\n        venue_node = (\n            header.css_first(\"h2\")\n            or header.css_first(\"h1\")\n            or header.css_first(\".track-name\")\n        )\n        if not venue_node:\n            return None\n        return normalize_venue_name(venue_node.text(strip=True))\n\n    def _parse_atr_runners(\n        self, parser: HTMLParser,\n    ) -> List[ResultRunner]:\n        rows = (\n            parser.css(\".result-racecard__row\")\n            or parser.css(\".card-cell--horse\")\n            or parser.css(\"atr-result-horse\")\n            or parser.css(\"div[class*='RacecardResultItem']\")\n            or parser.css(\".p-results__item\")\n        )\n\n        runners: List[ResultRunner] = []\n        for row in rows:\n            try:\n                name_node = (\n                    row.css_first(\".result-racecard__horse-name a\")\n                    or row.css_first(\".horse-name a\")\n                    or row.css_first(\"a[href*='/horse/']\")\n                    or row.css_first(\"[class*='HorseName']\")\n                )\n                if not name_node:\n                    continue\n                name = clean_text(name_node.text())\n\n                pos_node = (\n                    row.css_first(\".result-racecard__pos\")\n                    or row.css_first(\".pos\")\n                    or row.css_first(\".position\")\n                    or row.css_first(\"[class*='Position']\")\n                )\n                pos = (\n                    clean_text(pos_node.text()) if pos_node else None\n                )\n\n                num_node = row.css_first(\n                    \".result-racecard__saddle-cloth\",\n                )\n                number = 0\n                if num_node:\n                    try:\n                        number = int(clean_text(num_node.text()))\n                    except ValueError:\n                        pass\n\n                odds_node = row.css_first(\".result-racecard__odds\")\n                final_odds = 0.0\n                if odds_node:\n                    final_odds = parse_fractional_odds(\n                        clean_text(odds_node.text()),\n                    )\n\n                runners.append(ResultRunner(\n                    name=name,\n                    number=number,\n                    position=pos,\n                    final_win_odds=final_odds,\n                ))\n            except Exception:\n                continue\n        return runners\n\n    @staticmethod\n    def _map_place_payouts(\n        div_table: Node,\n        runners: List[ResultRunner],\n    ) -> None:\n        \"\"\"Enrich runners with place payouts from the dividends table.\"\"\"\n        for row in div_table.css(\"tr\"):\n            try:\n                row_text = row.text().lower()\n                if \"place\" not in row_text:\n                    continue\n                cols = row.css(\"td\")\n                if len(cols) < 2:\n                    continue\n                p_name = clean_text(\n                    cols[0].text().replace(\"Place\", \"\").strip(),\n                )\n                p_val = parse_currency_value(cols[1].text())\n                for runner in runners:\n                    if (\n                        runner.name.lower() in p_name.lower()\n                        or p_name.lower() in runner.name.lower()\n                    ):\n                        runner.place_payout = p_val\n            except Exception:\n                continue\n", "web_service/backend/adapters/results/sporting_life_results_adapter.py": "from typing import Any, Dict, List, Optional, Set, Tuple\nfrom datetime import datetime, timezone\nimport re\nimport json\nfrom selectolax.parser import HTMLParser, Node\n\nfrom .base import (\n    PageFetchingResultsAdapter,\n    parse_currency_value,\n    parse_fractional_odds,\n    build_start_time,\n    find_nested_value\n)\nfrom ...models import ResultRace, ResultRunner\nfrom ...utils.text import normalize_venue_name, clean_text\nfrom ...core.smart_fetcher import FetchStrategy, BrowserEngine\nimport fortuna\n\n\nclass SportingLifeResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Sporting Life results (UK / IRE / International).\"\"\"\n\n    SOURCE_NAME = \"SportingLifeResults\"\n    BASE_URL    = \"https://www.sportinglife.com\"\n    HOST        = \"www.sportinglife.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 45\n\n    def _configure_fetch_strategy(self) -> fortuna.FetchStrategy:\n        strategy = super()._configure_fetch_strategy()\n        # SportingLife is JS-heavy; keep CURL_CFFI primary but ensure fallback (Project Hardening)\n        strategy.primary_engine = fortuna.BrowserEngine.CURL_CFFI\n        return strategy\n\n    # -- link discovery ----------------------------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        resp = await self.make_request(\n            \"GET\",\n            f\"/racing/results/{date_str}\",\n            headers=self._get_headers(),\n        )\n        if not resp or not resp.text:\n            return set()\n        self._save_debug_snapshot(resp.text, f\"sl_results_index_{date_str}\")\n        return self._extract_sl_links(resp.text)\n\n    def _extract_sl_links(self, html: str) -> set:\n        links: set = set()\n        for a in HTMLParser(html).css(\"a[href*='/racing/results/']\"):\n            href = a.attributes.get(\"href\", \"\")\n            if not href: continue\n            if not self._venue_matches(a.text(), href):\n                continue\n            # /racing/results/2026-02-04/ludlow/901676/race-name\n            if re.search(r\"/results/\\d{4}-\\d{2}-\\d{2}/.+/\\d+/\", href):\n                links.add(href)\n        return links\n\n    # -- page parsing (two strategies) -------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        # Strategy 1 \u2014 Next.js JSON payload (most reliable)\n        script = parser.css_first(\"script#__NEXT_DATA__\")\n        if script:\n            race = self._parse_from_next_data(script.text(), date_str)\n            if race:\n                return race\n\n        # Strategy 2 \u2014 HTML scrape fallback\n        return self._parse_from_html(parser, date_str)\n\n    # -- Strategy 1: JSON -------------------------------------------------\n\n    def _parse_from_next_data(\n        self, script_text: str, date_str: str,\n    ) -> Optional[ResultRace]:\n        try:\n            data = json.loads(script_text)\n        except json.JSONDecodeError as exc:\n            self.logger.debug(\"Invalid __NEXT_DATA__\", error=str(exc))\n            return None\n\n        race_data = (\n            data.get(\"props\", {}).get(\"pageProps\", {}).get(\"race\", {})\n        )\n        if not race_data:\n            return None\n\n        summary    = race_data.get(\"race_summary\", {})\n        venue      = fortuna.normalize_venue_name(\n            summary.get(\"course_name\", \"Unknown\"),\n        )\n        race_num   = (\n            race_data.get(\"race_number\")\n            or summary.get(\"race_number\")\n            or 1\n        )\n        date_val   = summary.get(\"date\", date_str)\n        start_time = build_start_time(date_val, summary.get(\"time\"))\n\n        runners = self._runners_from_json(race_data)\n        if not runners:\n            return None\n\n        # Exotic payouts via recursive search\n        trifecta_pay   = find_nested_value(race_data, \"trifecta\")\n        superfecta_pay = find_nested_value(race_data, \"superfecta\")\n\n        # Place payouts from CSV field\n        self._apply_place_payouts_from_csv(\n            race_data.get(\"place_win\", \"\"), runners,\n        )\n\n        return ResultRace(\n            id=self._make_race_id(\"sl_res\", venue, date_val, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            trifecta_payout=trifecta_pay,\n            superfecta_payout=superfecta_pay,\n            source=self.SOURCE_NAME,\n        )\n\n    @staticmethod\n    def _runners_from_json(race_data: dict) -> List[ResultRunner]:\n        \"\"\"Extract runners from rides (result pages) or runners.\"\"\"\n        items = race_data.get(\"rides\") or race_data.get(\"runners\", [])\n        runners: List[ResultRunner] = []\n        for item in items:\n            horse = item.get(\"horse\", {})\n            name  = horse.get(\"name\") or item.get(\"name\")\n            if not name:\n                continue\n            sp_raw = (\n                item.get(\"starting_price\")\n                or item.get(\"sp\")\n                or item.get(\"betting\", {}).get(\"current_odds\", \"\")\n            )\n            runners.append(ResultRunner(\n                name=name,\n                number=(\n                    item.get(\"cloth_number\")\n                    or item.get(\"saddle_cloth_number\", 0)\n                ),\n                position=str(item.get(\"finish_position\", item.get(\"position\", \"\"))),\n                final_win_odds=parse_fractional_odds(str(sp_raw)),\n            ))\n        return runners\n\n    @staticmethod\n    def _apply_place_payouts_from_csv(\n        place_csv: str,\n        runners: List[ResultRunner],\n    ) -> None:\n        \"\"\"Map comma-separated place payouts to runners by finishing position.\"\"\"\n        if not isinstance(place_csv, str) or not place_csv:\n            return\n        pays = [parse_currency_value(p) for p in place_csv.split(\",\")]\n        for runner in runners:\n            pos = runner.position_numeric\n            if pos and 1 <= pos <= len(pays):\n                runner.place_payout = pays[pos - 1]\n\n    # -- Strategy 2: HTML fallback ----------------------------------------\n\n    def _parse_from_html(\n        self, parser: HTMLParser, date_str: str,\n    ) -> Optional[ResultRace]:\n        header = parser.css_first(\"h1\")\n        if not header:\n            return None\n\n        match = re.match(\n            r\"(\\d{1,2}:\\d{2})\\s+(.+)\\s+Result\",\n            clean_text(header.text()),\n        )\n        if not match:\n            return None\n\n        time_str   = match.group(1)\n        venue      = normalize_venue_name(match.group(2))\n        start_time = build_start_time(date_str, time_str)\n\n        runners: List[ResultRunner] = []\n        for row in parser.css(\n            'div[class*=\"ResultRunner__StyledResultRunnerWrapper\"]',\n        ):\n            name_node = row.css_first(\n                'a[class*=\"ResultRunner__StyledHorseName\"]',\n            )\n            if not name_node:\n                continue\n            pos_node = row.css_first(\n                'div[class*=\"ResultRunner__StyledRunnerPositionContainer\"]',\n            )\n            runners.append(ResultRunner(\n                name=clean_text(name_node.text()),\n                number=0,\n                position=(\n                    clean_text(pos_node.text()) if pos_node else None\n                ),\n            ))\n\n        if not runners:\n            return None\n\n        return ResultRace(\n            id=self._make_race_id(\"sl_res\", venue, date_str, 1),\n            venue=venue,\n            race_number=1,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n", "web_service/backend/adapters/results/sky_sports_results_adapter.py": "from typing import Any, Dict, List, Optional, Set, Tuple\nfrom datetime import datetime, timezone\nimport re\nfrom selectolax.parser import HTMLParser, Node\n\nfrom .base import (\n    PageFetchingResultsAdapter,\n    extract_exotic_payouts,\n    parse_fractional_odds,\n    build_start_time,\n    _extract_race_number_from_text\n)\nfrom ...models import ResultRace, ResultRunner\nfrom ...utils.text import normalize_venue_name, clean_text\nfrom ...core.smart_fetcher import FetchStrategy, BrowserEngine\nimport fortuna\n\n\nclass SkySportsResultsAdapter(PageFetchingResultsAdapter):\n    \"\"\"Sky Sports Racing results (UK / IRE).\"\"\"\n\n    SOURCE_NAME = \"SkySportsResults\"\n    BASE_URL    = \"https://www.skysports.com\"\n    HOST        = \"www.skysports.com\"\n    IMPERSONATE = \"chrome120\"\n    TIMEOUT     = 45\n\n    # -- link discovery ----------------------------------------------------\n\n    async def _discover_result_links(self, date_str: str) -> Set[str]:\n        try:\n            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n            url_dates = [dt.strftime(\"%d-%m-%Y\")]\n        except ValueError:\n            url_dates = [date_str]\n\n        links: set = set()\n        for url_date in url_dates:\n            try:\n                resp = await self.make_request(\n                    \"GET\", f\"/racing/results/{url_date}\", headers=self._get_headers(),\n                )\n                if not resp or not resp.text:\n                    continue\n                self._save_debug_snapshot(resp.text, f\"sky_results_index_{url_date}\")\n                parser = HTMLParser(resp.text)\n                links.update(self._extract_sky_links(parser, date_str, url_date))\n            except Exception as exc:\n                self.logger.debug(\"Sky index fetch failed\", url_date=url_date, error=str(exc))\n\n        return links\n\n    def _extract_sky_links(self, parser: HTMLParser, date_str: str, url_date: str) -> set:\n        links: set = set()\n        # Broad selectors for SkySports results (Council of Superbrains Directive)\n        for a in (parser.css(\"a[href*='/racing/results/']\") + parser.css(\"a[href*='/full-result/']\")):\n            href = a.attributes.get(\"href\", \"\")\n            if not href: continue\n            if not self._venue_matches(a.text(), href):\n                continue\n\n            # Match various result path patterns\n            has_race_path = any(\n                p in href for p in (\"/full-result/\", \"/race-result/\", \"/results/full-result/\")\n            ) or re.search(r\"/\\d{6,}/\", href)\n\n            # Check if link belongs to requested date or is generally a result link\n            has_date = date_str in href or url_date in href or re.search(r\"/\\d{6,}/\", href)\n\n            if has_race_path and has_date:\n                links.add(href)\n        return links\n\n    # -- page parsing ------------------------------------------------------\n\n    def _parse_race_page(\n        self, html: str, date_str: str, url: str,\n    ) -> Optional[ResultRace]:\n        parser = HTMLParser(html)\n\n        header = parser.css_first(\".sdc-site-racing-header__name\")\n        if not header:\n            return None\n\n        match = re.match(\n            r\"(\\d{1,2}:\\d{2})\\s+(.+)\",\n            clean_text(header.text()),\n        )\n        if not match:\n            return None\n\n        time_str   = match.group(1)\n        venue      = normalize_venue_name(match.group(2))\n        start_time = build_start_time(date_str, time_str)\n\n        runners = self._parse_sky_runners(parser)\n        if not runners:\n            return None\n\n        exotics  = extract_exotic_payouts(parser.css(\"table\"))\n        race_num = self._extract_sky_race_number(parser, url)\n\n        tri = exotics.get(\"trifecta\", (None, None))\n        sup = exotics.get(\"superfecta\", (None, None))\n\n        return ResultRace(\n            id=self._make_race_id(\"sky_res\", venue, date_str, race_num),\n            venue=venue,\n            race_number=race_num,\n            start_time=start_time,\n            runners=runners,\n            trifecta_payout=tri[0],\n            superfecta_payout=sup[0],\n            source=self.SOURCE_NAME,\n        )\n\n    @staticmethod\n    def _parse_sky_runners(parser: HTMLParser) -> List[ResultRunner]:\n        runners: List[ResultRunner] = []\n        for row in parser.css(\".sdc-site-racing-card__item\"):\n            name_node = row.css_first(\".sdc-site-racing-card__name\")\n            if not name_node:\n                continue\n\n            pos_node    = row.css_first(\".sdc-site-racing-card__position\")\n            number_node = row.css_first(\".sdc-site-racing-card__number\")\n            odds_node   = row.css_first(\".sdc-site-racing-card__odds\")\n\n            number = 0\n            if number_node:\n                try:\n                    number = int(re.sub(r\"\\D\", \"\", number_node.text()))\n                except (ValueError, TypeError):\n                    pass\n\n            runners.append(ResultRunner(\n                name=clean_text(name_node.text()),\n                number=number,\n                position=(\n                    clean_text(pos_node.text()) if pos_node else None\n                ),\n                final_win_odds=parse_fractional_odds(\n                    clean_text(odds_node.text()) if odds_node else \"\",\n                ),\n            ))\n        return runners\n\n    @staticmethod\n    def _extract_sky_race_number(parser: HTMLParser, url: str) -> int:\n        \"\"\"Try navigation index, then URL ID, then text fallback.\"\"\"\n        url_match = re.search(r\"/(\\d+)/\", url)\n        if url_match:\n            nav_links = parser.css(\"a[href*='/racing/results/']\")\n            for i, link in enumerate(nav_links):\n                if url_match.group(0) in (\n                    link.attributes.get(\"href\") or \"\"\n                ):\n                    return i + 1\n\n        return _extract_race_number_from_text(parser, url) or 1\n", "web_service/backend/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta, timezone\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Type\nimport re\nimport structlog\nfrom zoneinfo import ZoneInfo\n\nfrom .models import Race, Runner, get_canonical_venue\nfrom .utils.text import normalize_venue_name\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept (ImportError, RuntimeError):\n    winsound = None\n\nEASTERN = ZoneInfo(\"America/New_York\")\nDEFAULT_ODDS_FALLBACK = 2.75\n\nlog = structlog.get_logger(__name__)\n\n\ndef is_placeholder_odds(value: Optional[Decimal]) -> bool:\n    \"\"\"Detects if odds value is a known placeholder or default.\"\"\"\n    if value is None:\n        return True\n    try:\n        val_float = round(float(value), 2)\n        return val_float in {2.75}\n    except (ValueError, TypeError):\n        return True\n\n\ndef is_valid_odds(odds: Any) -> bool:\n    if odds is None: return False\n    try:\n        odds_float = float(odds)\n        if not (1.01 <= odds_float < 1000.0):\n            return False\n        return not is_placeholder_odds(Decimal(str(odds_float)))\n    except Exception: return False\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        if runner.win_odds and is_valid_odds(runner.win_odds):\n            return Decimal(str(runner.win_odds))\n\n    valid_odds = []\n    for source_data in runner.odds.values():\n        if isinstance(source_data, dict):\n            win = source_data.get('win')\n        elif hasattr(source_data, 'win'):\n            win = source_data.win\n        else:\n            win = source_data\n\n        if is_valid_odds(win):\n            valid_odds.append(Decimal(str(win)))\n\n    if valid_odds:\n        return min(valid_odds)\n\n    if runner.win_odds and is_valid_odds(runner.win_odds):\n        return Decimal(str(runner.win_odds))\n\n    return None\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None, **kwargs):\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self.config = config or {}\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(\n        self,\n        max_field_size: int = 14,\n        min_favorite_odds: float = 0.01,\n        min_second_favorite_odds: float = 0.01,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        # Timing check: Only audit races within a reasonable window around post time\n        st = race.start_time\n        if st.tzinfo is None:\n            # For tests/naive data, compare against naive now\n            now = datetime.now()\n        else:\n            # For production/aware data, compare against Eastern now\n            now = datetime.now(EASTERN)\n            if st.tzinfo != EASTERN:\n                st = st.astimezone(EASTERN)\n\n        past_cutoff = now - timedelta(minutes=45)\n        future_cutoff = now + timedelta(minutes=120)\n\n        if st < past_cutoff or st > future_cutoff:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        qualified_races = []\n        TRUSTWORTHY_RATIO_MIN = self.config.get(\"analysis\", {}).get(\"trustworthy_ratio_min\", 0.7)\n\n        for race in races:\n            if not self.is_race_qualified(race):\n                continue\n\n            active_runners = [r for r in race.runners if not r.scratched]\n            total_active = len(active_runners)\n\n            if total_active > 0:\n                trustworthy_count = sum(1 for r in active_runners if r.metadata.get(\"odds_source_trustworthy\", True))\n                if trustworthy_count / total_active < TRUSTWORTHY_RATIO_MIN:\n                    continue\n\n            score = self._evaluate_race(race)\n            if score > 0:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        return {\"criteria\": criteria, \"races\": qualified_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < Decimal(\"2.0\")\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            return 0.0\n\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n        \n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass TinyFieldTrifectaAnalyzer(TrifectaAnalyzer):\n    \"\"\"A specialized TrifectaAnalyzer that only considers races with 6 or fewer runners.\"\"\"\n\n    def __init__(self, **kwargs):\n        # Override the max_field_size to 6 for \"tiny field\" analysis\n        # Set low odds thresholds to \"let them through\" as per user request\n        super().__init__(max_field_size=6, min_favorite_odds=0.01, min_second_favorite_odds=0.01, **kwargs)\n\n    @property\n    def name(self) -> str:\n        return \"tiny_field_trifecta_analyzer\"\n\n\nclass SimplySuccessAnalyzer(BaseAnalyzer):\n    \"\"\"An analyzer that qualifies every race to show maximum successes (HTTP 200).\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"simply_success\"\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Returns races with a perfect score, applying global timing and chalk filters.\"\"\"\n        qualified = []\n        TRUSTWORTHY_RATIO_MIN = self.config.get(\"analysis\", {}).get(\"trustworthy_ratio_min\", 0.7)\n\n        for race in races:\n            active_runners = [r for r in race.runners if not r.scratched]\n            total_active = len(active_runners)\n\n            if total_active > 0:\n                trustworthy_count = sum(1 for r in active_runners if r.metadata.get(\"odds_source_trustworthy\", True))\n                if trustworthy_count / total_active < TRUSTWORTHY_RATIO_MIN:\n                    continue\n\n            all_odds = []\n            for runner in active_runners:\n                odds = _get_best_win_odds(runner)\n                if odds is not None:\n                    runner.win_odds = float(odds)\n                    all_odds.append(odds)\n\n            all_odds.sort()\n\n            if len(all_odds) >= 3 and len(set(all_odds)) == 1:\n                continue\n\n            if len(active_runners) < 2:\n                continue\n\n            valid_r_with_odds = sorted(\n                [(r, Decimal(str(r.win_odds))) for r in active_runners if r.win_odds is not None],\n                key=lambda x: x[1]\n            )\n            race.top_five_numbers = \", \".join([str(r[0].number or '?') for r in valid_r_with_odds[:5]])\n\n            is_goldmine = False\n            is_best_bet = False\n            gap12 = 0.0\n\n            if len(all_odds) >= 2:\n                fav, sec = all_odds[0], all_odds[1]\n                gap12 = round(float(sec - fav), 2)\n                \n                if gap12 > 0.25:\n                    if len(active_runners) <= 11 and sec >= Decimal(\"4.5\"):\n                        is_goldmine = True\n                    if len(active_runners) <= 11 and sec >= Decimal(\"3.5\"):\n                        is_best_bet = True\n\n                race.metadata['predicted_2nd_fav_odds'] = float(sec)\n                sec_fav = valid_r_with_odds[1][0]\n                race.metadata['selection_number'] = sec_fav.number\n                race.metadata['selection_name'] = sec_fav.name\n\n            race.metadata['is_goldmine'] = is_goldmine\n            race.metadata['is_best_bet'] = is_best_bet\n            race.metadata['1Gap2'] = gap12\n            race.qualification_score = 100.0\n            qualified.append(race)\n\n        return {\n            \"criteria\": {\n                \"mode\": \"simply_success\",\n                \"timing_filter\": \"45m_past_to_120m_future\",\n                \"chalk_filter\": \"disabled\",\n                \"goldmine_threshold\": 4.5\n            },\n            \"races\": qualified\n        }\n\n\ndef get_track_category(races_at_track: List[Any]) -> str:\n    \"\"\"Categorize the track as T (Thoroughbred), H (Harness), or G (Greyhounds).\"\"\"\n    if not races_at_track:\n        return 'T'\n\n    has_large_field = False\n    for r in races_at_track:\n        runners = r.runners\n        active_runners = len([run for run in runners if not run.scratched])\n        if active_runners > 7:\n            has_large_field = True\n            break\n\n    for race in races_at_track:\n        source = race.source or \"\"\n        race_id = (race.id or \"\").lower()\n        discipline = race.discipline or \"\"\n\n        if discipline == \"Harness\" or '_h' in race_id: return 'H'\n        if (discipline == \"Greyhound\" or '_g' in race_id) and not has_large_field:\n            return 'G'\n\n        source_lower = source.lower()\n        if (\"greyhound\" in source_lower or source in [\"GBGB\", \"Greyhound\", \"AtTheRacesGreyhound\"]) and not has_large_field:\n            return 'G'\n        if source in [\"USTrotting\", \"StandardbredCanada\", \"Harness\"] or any(kw in source_lower for kw in ['harness', 'standardbred', 'trot', 'pace']):\n            return 'H'\n\n    return 'T'\n\n\ndef generate_fortuna_fives(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate the FORTUNA FIVES appendix.\"\"\"\n    lines = [\"\", \"\", \"FORTUNA FIVES\", \"-------------\"]\n    fives = []\n    for race in (all_races or races):\n        runners = race.runners\n        field_size = len([r for r in runners if not r.scratched])\n        if field_size == 5:\n            fives.append(race)\n\n    if not fives:\n        lines.append(\"No qualifying races.\")\n        return \"\\n\".join(lines)\n\n    track_odds_sums = defaultdict(float)\n    track_odds_counts = defaultdict(int)\n    stats_races = all_races if all_races is not None else races\n    for race in stats_races:\n        v = race.venue\n        track = normalize_venue_name(v)\n        for runner in race.runners:\n            win_odds = runner.win_odds\n            if not runner.scratched and win_odds:\n                track_odds_sums[track] += float(win_odds)\n                track_odds_counts[track] += 1\n\n    track_avgs = {}\n    for track, total in track_odds_sums.items():\n        count = track_odds_counts[track]\n        if count > 0:\n            track_avgs[track] = str(int(total / count))\n\n    track_to_nums = defaultdict(list)\n    for r in fives:\n        v = r.venue\n        if v:\n            track_to_nums[normalize_venue_name(v)].append(r.race_number)\n\n    for track in sorted(track_to_nums.keys()):\n        nums = sorted(list(set(track_to_nums[track])))\n        avg_str = f\" [{track_avgs[track]}]\" if track in track_avgs else \"\"\n        lines.append(f\"{track}{avg_str}: {', '.join(map(str, nums))}\")\n\n    return \"\\n\".join(lines)\n\n\ndef generate_goldmines(races: List[Any], all_races: Optional[List[Any]] = None) -> str:\n    \"\"\"Generate the GOLDMINE RACES appendix, filtered to Superfecta races.\"\"\"\n    lines = [\"\", \"\", \"GOLDMINE RACES\", \"--------------\"]\n\n    track_categories = {}\n    source_races_for_cat = all_races if all_races is not None else races\n    races_by_track = defaultdict(list)\n    for r in source_races_for_cat:\n        v = r.venue\n        track = normalize_venue_name(v)\n        races_by_track[track].append(r)\n    for track, tr_races in races_by_track.items():\n        track_categories[track] = get_track_category(tr_races)\n\n    def is_superfecta_effective(r):\n        available_bets = r.available_bets or []\n        metadata_bets = r.metadata.get('available_bets', [])\n        if 'Superfecta' in available_bets or 'Superfecta' in metadata_bets:\n            return True\n\n        track = normalize_venue_name(r.venue)\n        cat = track_categories.get(track, 'T')\n        runners = r.runners\n        field_size = len([run for run in runners if not run.scratched])\n        if cat == 'T' and field_size >= 6:\n            return True\n        return False\n\n    goldmines = [r for r in races if r.metadata.get('is_goldmine') and is_superfecta_effective(r)]\n\n    if not goldmines:\n        lines.append(\"No qualifying races.\")\n        return \"\\n\".join(lines)\n\n    track_to_nums = defaultdict(list)\n    for r in goldmines:\n        v = r.venue\n        if v:\n            track = normalize_venue_name(v)\n            track_to_nums[track].append(r.race_number)\n\n    cat_map = {'T': 3, 'H': 2, 'G': 1}\n    formatted_tracks = []\n    for track in track_to_nums.keys():\n        cat = track_categories.get(track, 'T')\n        display_name = f\"{cat}~{track}\"\n        formatted_tracks.append((cat, track, display_name))\n\n    formatted_tracks.sort(key=lambda x: (-cat_map.get(x[0], 0), x[1]))\n\n    for cat, track, display_name in formatted_tracks:\n        nums = sorted(list(set(track_to_nums[track])))\n        lines.append(f\"{display_name}: {', '.join(map(str, nums))}\")\n    return \"\\n\".join(lines)\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self.config = config or {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        self.register_analyzer(\"tiny_field_trifecta\", TinyFieldTrifectaAnalyzer)\n        self.register_analyzer(\"simply_success\", SimplySuccessAnalyzer)\n        log.info(\n            \"AnalyzerEngine discovered plugins\",\n            available_analyzers=list(self.analyzers.keys()),\n        )\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(config=self.config, **kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).resolve().parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n        self.enabled = winsound is not None\n\n    def play(self, sound_type: str):\n        if not self.enabled:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        # Using a simple check for DesktopNotifier as in fortuna.py\n        try:\n            from notifications import DesktopNotifier\n            self.notifier = DesktopNotifier()\n        except ImportError:\n            self.notifier = None\n            \n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n        self.notifications_enabled = self.notifier is not None\n        if not self.notifications_enabled:\n            log.debug(\"Native notifications disabled (platform not supported or library missing)\")\n\n    def notify_qualified_race(self, race):\n        if race.id in self.notified_races:\n            return\n\n        # Always log the high-value opportunity regardless of notification setting\n        log.info(\n            \"High-value opportunity identified\",\n            venue=race.venue,\n            race=race.race_number,\n            score=race.qualification_score\n        )\n\n        if not self.notifications_enabled or self.notifier is None:\n            return\n\n        title = \"\ud83d\udc0e High-Value Opportunity!\"\n        message = f\"{race.venue} - Race {race.race_number}\\nScore: {race.qualification_score:.0f}%\\nPost Time: {race.start_time.strftime('%I:%M %p')}\"\n\n        try:\n            self.notifier.send(\n                title=title,\n                message=message,\n                urgency=\"high\" if race.qualification_score >= 80 else \"normal\"\n            )\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            log.error(\"Failed to send notification\", error=str(e))\n"}