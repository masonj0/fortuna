{
  "web_service/backend/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for Xpressbet API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://api.xpressbet.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not getattr(config, \"XPRESSBET_API_KEY\", None):\n            # Many adapters are skipped if not configured, this is standard.\n            self.api_key = None\n        else:\n            self.api_key = config.XPRESSBET_API_KEY\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw meetings data from the Xpressbet API.\"\"\"\n        if not self.api_key:\n            self.logger.warning(\"Xpressbet API key not configured, skipping fetch.\")\n            return None\n\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(\"GET\", f\"meetings?date={date}\", headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            return []\n\n        all_races = []\n        for meeting in raw_data.get(\"meetings\", []):\n            venue = meeting.get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, venue):\n                        all_races.append(race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.error(\n                        \"Error parsing Xpressbet race\",\n                        race_id=race_data.get(\"id\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], venue: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"id\")\n        race_number = race_data.get(\"number\")\n        start_time_str = race_data.get(\"startTime\")\n\n        if not all([race_id, race_number, start_time_str]):\n            return None\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            name = runner_data.get(\"name\")\n            number = runner_data.get(\"number\")\n            if not all([name, number]):\n                continue\n\n            runners.append(\n                Runner(\n                    name=name,\n                    number=number,\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds={},\n                )\n            )\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            start_time = datetime.now()\n\n        return Race(\n            id=f\"xb_{race_id}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\n\n\nclass EquibaseAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            block_resources=True,\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.equibase.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        # Try different possible index URLs for Equibase\n        index_urls = [\n            f\"/entries/{date}\",\n            f\"/static/entry/index.html\",\n            f\"/static/entry/{date}/index.html\",\n        ]\n\n        index_response = None\n        for url in index_urls:\n            try:\n                self.logger.info(f\"Trying Equibase index: {url}\")\n                index_response = await self.make_request(\"GET\", url, headers=self._get_headers())\n                if index_response and index_response.text and len(index_response.text) > 1000:\n                    break\n            except Exception:\n                continue\n\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Equibase index page\")\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"equibase_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # More robust race link detection\n        race_links = []\n        for a in parser.css(\"a\"):\n            href = a.attributes.get(\"href\", \"\")\n            if \"/static/entry/\" in href or \"entry-race-level\" in a.attributes.get(\"class\", \"\"):\n                 race_links.append(href)\n\n        race_links = list(set(race_links))\n\n        semaphore = asyncio.Semaphore(5)\n\n        async def fetch_single_html(race_url: str):\n            async with semaphore:\n                try:\n                    response = await self.make_request(\"GET\", race_url, headers=self._get_headers())\n                    return response.text if response else \"\"\n                except Exception as e:\n                    self.logger.warning(\"Failed to fetch race page\", url=race_url, error=str(e))\n                    return \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": [p for p in html_pages if p], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first(\"div.track-information strong\")\n                if not venue_node:\n                    continue\n                venue = clean_text(venue_node.text())\n\n                race_number_node = parser.css_first(\"div.race-information strong\")\n                if not race_number_node:\n                    continue\n                race_number_text = race_number_node.text().replace(\"Race\", \"\").strip()\n                if not race_number_text.isdigit():\n                    continue\n                race_number = int(race_number_text)\n\n                post_time_node = parser.css_first(\"p.post-time span\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    if runner := self._parse_runner(node):\n                        runners.append(runner)\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first(\"td:nth-child(1)\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            name_node = node.css_first(\"td:nth-child(3)\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            odds_node = node.css_first(\"td:nth-child(10)\")\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if odds_data := create_odds_data(self.source_name, win_odds):\n                    odds[self.source_name] = odds_data\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError, IndexError):\n            self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n            return None\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        try:\n            # Handle formats like \"12:30 PM ET\" or just \"12:30 PM\"\n            parts = time_str.replace(\"Post Time:\", \"\").strip().split(\" \")\n            if len(parts) >= 2:\n                time_part = f\"{parts[0]} {parts[1]}\"\n                dt_str = f\"{date_str} {time_part}\"\n                return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        except Exception:\n            self.logger.warning(f\"Failed to parse post time: {time_str}\")\n\n        # Fallback to a safe default\n        return datetime.now()\n",
  "web_service/backend/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairExchange\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for a given date.\"\"\"\n        if not await self._authenticate(self.http_client):\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
  "web_service/backend/adapters/at_the_races_greyhound_adapter.py": "# python_service/adapters/at_the_races_greyhound_adapter.py\n\"\"\"Adapter for AtTheRaces Greyhound racing.\"\"\"\n\nimport asyncio\nimport json\nimport re\nimport html\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass AtTheRacesGreyhoundAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for greyhounds.attheraces.com.\n\n    This site is a modern SPA with data embedded in JSON-LD and HTML attributes.\n    Uses simple HTTP requests as the data is available in the initial HTML.\n    \"\"\"\n\n    SOURCE_NAME = \"AtTheRacesGreyhound\"\n    BASE_URL = \"https://greyhounds.attheraces.com\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n        self._semaphore = asyncio.Semaphore(5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"Greyhounds ATR is a modern site but data is in HTML - HTTPX works.\"\"\"\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX, enable_js=False)\n\n    def _get_headers(self) -> dict:\n        \"\"\"Get headers for ATR Greyhound requests.\"\"\"\n        return self._get_browser_headers(\n            host=\"greyhounds.attheraces.com\",\n            referer=\"https://greyhounds.attheraces.com/racecards\",\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetch greyhound race pages for a given date.\"\"\"\n        # The index page contains JSON-LD with all races for today\n        index_url = f\"/racecards/{date}\" if date else \"/racecards\"\n\n        try:\n            index_response = await self.make_request(\n                \"GET\", index_url, headers=self._get_headers()\n            )\n        except Exception as e:\n            self.logger.error(\n                \"Failed to fetch AtTheRaces Greyhound index page\", url=index_url, error=str(e)\n            )\n            return None\n\n        if not index_response:\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"atr_grey_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        links = self._extract_links_from_json_ld(parser)\n\n        if not links:\n            self.logger.warning(\"No greyhound race links found on index page\", date=date)\n            return None\n\n        self.logger.info(f\"Found {len(links)} greyhound race links for {date}\")\n\n        pages = await self._fetch_race_pages(links)\n        return {\"pages\": pages, \"date\": date}\n\n    def _extract_links_from_json_ld(self, parser: HTMLParser) -> List[str]:\n        \"\"\"Extract race links from application/ld+json script tags.\"\"\"\n        links = []\n        for script in parser.css('script[type=\"application/ld+json\"]'):\n            try:\n                data = json.loads(script.text())\n                # Handle both single object and @graph array\n                items = data.get(\"@graph\", [data]) if isinstance(data, dict) else []\n                for item in items:\n                    if item.get(\"@type\") == \"SportsEvent\":\n                        location = item.get(\"location\")\n                        if isinstance(location, list):\n                            for loc in location:\n                                if url := loc.get(\"url\"):\n                                    links.append(url)\n                        elif isinstance(location, dict):\n                            if url := location.get(\"url\"):\n                                links.append(url)\n            except (json.JSONDecodeError, TypeError):\n                continue\n        return list(set(links))\n\n    async def _fetch_race_pages(self, links: List[str]) -> List[tuple]:\n        \"\"\"Fetch all race pages concurrently with semaphore limit.\"\"\"\n        async def fetch_single(url_path: str):\n            async with self._semaphore:\n                try:\n                    # Small delay to be polite\n                    await asyncio.sleep(0.5)\n                    response = await self.make_request(\n                        \"GET\", url_path, headers=self._get_headers()\n                    )\n                    return (url_path, response.text) if response else (url_path, \"\")\n                except Exception as e:\n                    self.logger.warning(f\"Failed to fetch {url_path}: {e}\")\n                    return (url_path, \"\")\n\n        tasks = [fetch_single(link) for link in links]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        return [\n            page for page in results\n            if not isinstance(page, Exception) and page and page[1]\n        ]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parse greyhound race pages into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date_str = raw_data[\"date\"]\n        try:\n            race_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n        except ValueError:\n            race_date = datetime.now().date()\n\n        races = []\n        for url_path, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n\n            try:\n                if race := self._parse_single_race(html_content, url_path, race_date):\n                    races.append(race)\n            except Exception as e:\n                self.logger.warning(\"Error parsing greyhound race\", url=url_path, error=str(e))\n                continue\n\n        return races\n\n    def _parse_single_race(self, html_content: str, url_path: str, race_date) -> Optional[Race]:\n        \"\"\"Parse a single greyhound race from HTML content.\"\"\"\n        parser = HTMLParser(html_content)\n\n        # Data is embedded in <page-content :items=\"...\"> or :modules=\"...\"\n        page_content = parser.css_first(\"page-content\")\n        if not page_content:\n            return None\n\n        items_raw = page_content.attributes.get(\":items\") or page_content.attributes.get(\":modules\")\n        if not items_raw:\n            return None\n\n        try:\n            modules = json.loads(html.unescape(items_raw))\n        except json.JSONDecodeError:\n            return None\n\n        # Extract race info and runners\n        venue = \"\"\n        race_time_str = \"\"\n        runners = []\n        odds_map = {}\n\n        # Usually first module is RacecardHero or similar\n        for module in modules:\n            m_type = module.get(\"type\")\n            m_data = module.get(\"data\", {})\n\n            if m_type == \"RacecardHero\":\n                venue = normalize_venue_name(m_data.get(\"track\", \"\"))\n                race_time_str = m_data.get(\"time\", \"\")\n\n            if m_type == \"OddsGrid\":\n                odds_grid = m_data.get(\"oddsGrid\", {})\n\n                # Build odds map: greyhoundId -> decimal odds\n                partners = odds_grid.get(\"partners\", {})\n                # Check premium partners first (usually contains bet365, etc.)\n                all_partners = []\n                if isinstance(partners, dict):\n                    for p_list in partners.values():\n                        if isinstance(p_list, list):\n                            all_partners.extend(p_list)\n                elif isinstance(partners, list):\n                    all_partners = partners\n\n                for partner in all_partners:\n                    p_odds = partner.get(\"odds\", [])\n                    for o in p_odds:\n                        g_id = o.get(\"betParams\", {}).get(\"greyhoundId\")\n                        price = o.get(\"value\", {}).get(\"decimal\")\n                        if g_id and price and g_id not in odds_map:\n                            try:\n                                odds_map[str(g_id)] = float(price)\n                            except ValueError:\n                                pass\n\n                # Extract runner basic info (trap, name, id)\n                traps = odds_grid.get(\"traps\", [])\n                for t in traps:\n                    trap_num = t.get(\"trap\")\n                    name = clean_text(t.get(\"name\", \"\"))\n                    href = t.get(\"href\", \"\")\n                    # Extract ID from href: /stats-hub/greyhound/20431/tommys-dolly\n                    g_id_match = re.search(r'/greyhound/(\\d+)', href)\n                    g_id = g_id_match.group(1) if g_id_match else None\n\n                    win_odds = odds_map.get(str(g_id)) if g_id else None\n\n                    odds_data = {}\n                    if odds_val := create_odds_data(self.source_name, win_odds):\n                        odds_data[self.source_name] = odds_val\n\n                    runners.append(Runner(\n                        number=trap_num or 0,\n                        name=name,\n                        odds=odds_data\n                    ))\n\n        if not venue or not runners:\n            # Fallback for venue/time from URL if Hero missing\n            # /racecard/GB/doncaster/28-January-2026/1433\n            url_parts = url_path.split('/')\n            if len(url_parts) >= 5:\n                venue = normalize_venue_name(url_parts[3])\n                race_time_str = url_parts[-1]\n\n        if not venue or not runners:\n            return None\n\n        try:\n            # Handle time format HHmm or HH:mm\n            if \":\" not in race_time_str and len(race_time_str) == 4:\n                race_time_str = f\"{race_time_str[:2]}:{race_time_str[2:]}\"\n\n            start_time = datetime.combine(\n                race_date,\n                datetime.strptime(race_time_str, \"%H:%M\").time()\n            )\n        except (ValueError, TypeError):\n            return None\n\n        race_id = f\"atrg_{venue.lower().replace(' ', '')}_{start_time:%Y%m%d_%H%M}\"\n\n        return Race(\n            id=race_id,\n            venue=venue,\n            race_number=0, # Greyhound cards usually don't have a simple race number in the same way\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for PointsBet Greyhound API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/sports/greyhound-racing/events/by-date/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw events data from the PointsBet API.\"\"\"\n        response = await self.make_request(\"GET\", date)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw events data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        all_races = []\n        for event in raw_data:\n            try:\n                if race := self._parse_race(event):\n                    all_races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.error(\n                    \"Error parsing PointsBet greyhound event\",\n                    event_id=event.get(\"eventId\"),\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race(self, event: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single event object from the API response.\"\"\"\n        event_id = event.get(\"eventId\")\n        venue = event.get(\"venueName\")\n        race_number = event.get(\"raceNumber\")\n        start_time_str = event.get(\"startsAt\")\n\n        if not all([event_id, venue, race_number, start_time_str]):\n            return None\n\n        runners = []\n        for runner_data in event.get(\"runners\", []):\n            name = runner_data.get(\"name\")\n            number = runner_data.get(\"saddleNumber\")\n            if not all([name, number]):\n                continue\n\n            runners.append(\n                Runner(\n                    name=name,\n                    number=number,\n                    scratched=runner_data.get(\"isScratched\", False),\n                    odds={},\n                )\n            )\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            start_time = datetime.now()\n\n        return Race(\n            id=f\"pbg_{event_id}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/brisnet_adapter.py": "# python_service/adapters/brisnet_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom dateutil.parser import parse as parse_time\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\n\n\nclass BrisnetAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for brisnet.com, migrated to BaseAdapterV3 with enhanced track detection.\n    \"\"\"\n\n    SOURCE_NAME = \"Brisnet\"\n    BASE_URL = \"https://www.brisnet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            timeout=30,\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.brisnet.com\")\n\n    async def _fetch_track_links(self) -> List[str]:\n        \"\"\"Fetches the list of active track links from the Brisnet index page.\"\"\"\n        url = \"/cgi-bin/intoday.cgi\"\n        response = await self.make_request(\"GET\", url, headers=self._get_headers())\n        if not response or not response.text:\n            return []\n\n        parser = HTMLParser(response.text)\n        # Find links that look like track entries (briswatch.cgi links)\n        links = []\n        for a in parser.css(\"a[href*='briswatch.cgi']\"):\n            href = a.attributes.get(\"href\")\n            if href:\n                # Ensure it's an absolute URL or relative to base\n                if not href.startswith(\"http\"):\n                    href = href if href.startswith(\"/\") else f\"/{href}\"\n                links.append(href)\n\n        return list(set(links))\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetches the raw HTML for all active tracks for the given date.\"\"\"\n        track_links = await self._fetch_track_links()\n        if not track_links:\n            self.logger.warning(\"No active tracks found on Brisnet index page.\")\n            return None\n\n        semaphore = asyncio.Semaphore(5)\n\n        async def fetch_track_page(url: str):\n            async with semaphore:\n                try:\n                    response = await self.make_request(\"GET\", url, headers=self._get_headers())\n                    if response:\n                        self._save_debug_html(response.text, f\"brisnet_{url.split('/')[-1]}_{date}\")\n                    return response.text if response else \"\"\n                except Exception as e:\n                    self.logger.warning(\"Failed to fetch track page\", url=url, error=str(e))\n                    return \"\"\n\n        tasks = [fetch_track_page(link) for link in track_links]\n        pages = await asyncio.gather(*tasks)\n\n        return {\n            \"pages\": [p for p in pages if p],\n            \"date\": date\n        }\n\n    def _parse_races(self, raw_data: Optional[dict]) -> List[Race]:\n        \"\"\"Parses the raw HTML pages into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        race_date = raw_data[\"date\"]\n        all_races = []\n\n        for html in raw_data[\"pages\"]:\n            parser = HTMLParser(html)\n\n            venue_node = parser.css_first(\"header h1\")\n            venue = \"Unknown\"\n            if venue_node:\n                venue_text = venue_node.text().split(\" - \")[0]\n                venue = normalize_venue_name(venue_text)\n\n            for race_section in parser.css(\"section.race\"):\n                try:\n                    race_number_str = race_section.attributes.get(\"data-racenumber\")\n                    if not race_number_str or not race_number_str.isdigit():\n                        continue\n                    race_number = int(race_number_str)\n\n                    post_time_node = race_section.css_first(\".race-title span\")\n                    if not post_time_node:\n                        continue\n                    post_time_str = post_time_node.text().replace(\"Post Time: \", \"\").strip()\n\n                    try:\n                        start_time = parse_time(f\"{race_date} {post_time_str}\")\n                    except (ValueError, TypeError):\n                        start_time = datetime.now()\n\n                    runners = []\n                    for row in race_section.css(\"tbody tr\"):\n                        classes = row.attributes.get(\"class\", \"\")\n                        if classes and \"scratched\" in classes.lower():\n                            continue\n\n                        cells = row.css(\"td\")\n                        if len(cells) < 3:\n                            continue\n\n                        number_text = cells[0].text().strip()\n                        number_digits = \"\".join(filter(str.isdigit, number_text))\n                        number = int(number_digits) if number_digits else 0\n\n                        name = cells[1].text().strip()\n                        odds_str = cells[2].text().strip()\n\n                        win_odds = parse_odds_to_decimal(odds_str)\n                        odds = {}\n                        if odds_data := create_odds_data(self.source_name, win_odds):\n                            odds[self.source_name] = odds_data\n\n                        runners.append(Runner(number=number, name=name, odds=odds))\n\n                    if not runners:\n                        continue\n\n                    race = Race(\n                        id=f\"brisnet_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                        field_size=len(runners),\n                    )\n                    all_races.append(race)\n                except Exception:\n                    self.logger.warning(\"Failed to parse a race on Brisnet, skipping.\", exc_info=True)\n                    continue\n\n        return all_races\n",
  "web_service/backend/adapters/__init__.py": "# python_service/adapters/__init__.py\n\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .at_the_races_greyhound_adapter import AtTheRacesGreyhoundAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .brisnet_adapter import BrisnetAdapter\nfrom .equibase_adapter import EquibaseAdapter\nfrom .fanduel_adapter import FanDuelAdapter\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .oddschecker_adapter import OddscheckerAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .racingpost_adapter import RacingPostAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .stubs import (\n    HorseRacingNationAdapter,\n    NYRABetsAdapter,\n    PuntersAdapter,\n    RacingTVAdapter,\n    TabAdapter,\n    TemplateAdapter,\n)\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\nfrom .twinspires_adapter import TwinSpiresAdapter\nfrom .universal_adapter import UniversalAdapter\nfrom .xpressbet_adapter import XpressbetAdapter\n\n__all__ = [\n    \"AtTheRacesAdapter\",\n    \"AtTheRacesGreyhoundAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairDataScientistAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"BrisnetAdapter\",\n    \"EquibaseAdapter\",\n    \"FanDuelAdapter\",\n    \"GbgbApiAdapter\",\n    \"GreyhoundAdapter\",\n    \"HarnessAdapter\",\n    \"HorseRacingNationAdapter\",\n    \"NYRABetsAdapter\",\n    \"OddscheckerAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"PuntersAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"RacingPostAdapter\",\n    \"RacingTVAdapter\",\n    \"SportingLifeAdapter\",\n    \"TabAdapter\",\n    \"TemplateAdapter\",\n    \"TheRacingApiAdapter\",\n    \"TimeformAdapter\",\n    \"TVGAdapter\",\n    \"TwinSpiresAdapter\",\n    \"UniversalAdapter\",\n    \"XpressbetAdapter\",\n]\n",
  "web_service/backend/adapters/harness_adapter.py": "# python_service/adapters/harness_adapter.py\n\"\"\"Adapter for US harness racing (USTA).\"\"\"\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\nfrom zoneinfo import ZoneInfo\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass HarnessAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US harness racing data.\"\"\"\n\n    SOURCE_NAME = \"USTrotting\"\n    BASE_URL = \"https://data.ustrotting.com/api/racenet/racing/\"\n\n    # Eastern timezone is standard for US racing\n    TIMEZONE = ZoneInfo(\"America/New_York\")\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetch harness races for a given date.\"\"\"\n        response = await self.make_request(\"GET\", f\"card/{date}\")\n        if not response:\n            return None\n        return {\"data\": response.json(), \"date\": date}\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parse card data into Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        data = raw_data.get(\"data\", {})\n        meetings = data.get(\"meetings\", [])\n\n        if not meetings:\n            self.logger.warning(\"No meetings found in harness data response.\")\n            return []\n\n        races = []\n        date = raw_data.get(\"date\")\n\n        for meeting in meetings:\n            track_name = meeting.get(\"track\", {}).get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name, date):\n                        races.append(race)\n                except Exception:\n                    self.logger.warning(\n                        \"Failed to parse harness race\",\n                        race_data=race_data,\n                        exc_info=True,\n                    )\n\n        return races\n\n    def _parse_race(\n        self, race_data: dict, track_name: str, date: str\n    ) -> Optional[Race]:\n        \"\"\"Parse a single race from USTA API.\"\"\"\n        race_number = race_data.get(\"raceNumber\")\n        post_time_str = race_data.get(\"postTime\")\n\n        if not all([race_number, post_time_str]):\n            return None\n\n        start_time = self._parse_post_time(date, post_time_str)\n        runners = self._parse_runners(race_data.get(\"runners\", []))\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"ust_{track_name.lower().replace(' ', '')}_{date}_{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runners(self, runners_data: List[dict]) -> List[Runner]:\n        \"\"\"Parse runner data into Runner objects.\"\"\"\n        runners = []\n\n        for runner_data in runners_data:\n            if runner_data.get(\"scratched\", False):\n                continue\n\n            odds_str = runner_data.get(\"morningLineOdds\", \"\")\n            # Handle odds like \"5\" -> \"5/1\"\n            if \"/\" not in odds_str and odds_str.isdigit():\n                odds_str = f\"{odds_str}/1\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds[self.source_name] = odds_data\n\n            runners.append(\n                Runner(\n                    number=runner_data.get(\"postPosition\", 0),\n                    name=runner_data.get(\"horse\", {}).get(\"name\", \"Unknown Horse\"),\n                    odds=odds,\n                    scratched=False,\n                )\n            )\n\n        return runners\n\n    def _parse_post_time(self, date: str, post_time: str) -> datetime:\n        \"\"\"Parse time string into timezone-aware datetime.\"\"\"\n        dt_str = f\"{date} {post_time}\"\n        naive_dt = datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        return naive_dt.replace(tzinfo=self.TIMEZONE)\n",
  "web_service/backend/adapters/gbgb_api_adapter.py": "# python_service/adapters/gbgb_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass GbgbApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Greyhound Board of Great Britain API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"GBGB\"\n    BASE_URL = \"https://api.gbgb.org.uk/api/\"\n\n    def __init__(self, config=None, session=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw meeting data from the GBGB API.\"\"\"\n        endpoint = f\"results/meeting/{date}\"\n        response = await self.make_request(\"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, meetings_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw meeting data into a list of Race objects.\"\"\"\n        if not meetings_data:\n            return []\n\n        all_races = []\n        for meeting in meetings_data:\n            track_name = meeting.get(\"trackName\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name):\n                        all_races.append(race)\n                except (KeyError, TypeError):\n                    self.logger.error(\n                        \"Error parsing GBGB race\",\n                        race_id=race_data.get(\"raceId\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], track_name: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"raceId\")\n        race_number = race_data.get(\"raceNumber\")\n        race_time = race_data.get(\"raceTime\")\n\n        if not all([race_id, race_number, race_time]):\n            return None\n\n        return Race(\n            id=f\"gbgb_{race_id}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(race_time.replace(\"Z\", \"+00:00\")),\n            runners=self._parse_runners(race_data.get(\"traps\", [])),\n            source=self.source_name,\n            race_name=race_data.get(\"raceTitle\"),\n            distance=f\"{race_data.get('raceDistance')}m\",\n        )\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                trap_number = runner_data.get(\"trapNumber\")\n                dog_name = runner_data.get(\"dogName\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                sp = runner_data.get(\"sp\")\n                if sp:\n                    win_odds = parse_odds_to_decimal(sp)\n                    if odds_data_val := create_odds_data(self.source_name, win_odds):\n                        odds_data[self.source_name] = odds_data_val\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Error parsing GBGB runner, skipping.\",\n                    runner_name=runner_data.get(\"dogName\"),\n                )\n                continue\n        return runners\n",
  "web_service/backend/adapters/the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for TheRacingAPI.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not getattr(config, \"THE_RACING_API_KEY\", None):\n            raise AdapterConfigError(self.SOURCE_NAME, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw race data from TheRacingAPI.\"\"\"\n        params = {\"apiKey\": self.api_key, \"date\": date}\n        response = await self.make_request(\"GET\", \"racecards/free\", params=params)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw JSON data into a list of Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"racecards\"), list):\n            self.logger.warning(\"No 'racecards' in TheRacingAPI response or invalid format.\")\n            return []\n\n        all_races = []\n        for race_summary in raw_data.get(\"racecards\", []):\n            try:\n                if race := self._parse_single_race(race_summary):\n                    all_races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_summary.get(\"race_id\"),\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_single_race(self, race_data: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"race_id\")\n        venue = race_data.get(\"course\")\n        race_number = race_data.get(\"race_number\")\n        start_time_str = race_data.get(\"off_time\")\n\n        if not all([race_id, venue, race_number, start_time_str]):\n            return None\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            name = runner_data.get(\"horse\")\n            number = runner_data.get(\"saddle_cloth\")\n            if not all([name, number]):\n                continue\n\n            odds = {}\n            # TheRacingAPI sometimes provides odds in various formats\n            # This is a simplified placeholder\n\n            runners.append(\n                Runner(\n                    name=name,\n                    number=number,\n                    scratched=runner_data.get(\"non_runner\", False),\n                    odds=odds,\n                )\n            )\n\n        if not runners:\n            return None\n\n        # Start time parsing depends on format from API\n        try:\n            start_time = datetime.fromisoformat(start_time_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            start_time = datetime.now()\n\n        return Race(\n            id=f\"tra_{race_id}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/racing_and_sports_adapter.py": "# python_service/adapters/racing_and_sports_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSports\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n        if not getattr(config, \"RACING_AND_SPORTS_TOKEN\", None):\n            raise AdapterConfigError(\n                self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\"\n            )\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetch horse racing meetings from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            \"GET\", \"v1/racing/meetings\", headers=headers, params=params\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parse meetings data into Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\n                \"No 'meetings' in RacingAndSports response or invalid format.\"\n            )\n            return []\n\n        races = []\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed := self._parse_race(meeting, race_summary):\n                        races.append(parsed)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse race\",\n                        venue=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return races\n\n    def _parse_race(\n        self, meeting: Dict[str, Any], race: Dict[str, Any]\n    ) -> Optional[Race]:\n        \"\"\"Parse a single race from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str)\n        except (ValueError, TypeError):\n            self.logger.warning(\n                \"Invalid start time\", start_time_str=start_time_str, race_id=race_id\n            )\n            return None\n\n        return Race(\n            id=f\"ras_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/base_stub_adapter.py": "# python_service/adapters/base_stub_adapter.py\n\"\"\"Base class for non-functional stub adapters.\"\"\"\n\nfrom abc import ABC\nfrom typing import Any, List\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BaseStubAdapter(BaseAdapterV3, ABC):\n    \"\"\"\n    Base class for adapters that are not yet implemented.\n\n    Subclasses only need to define SOURCE_NAME and BASE_URL.\n    \"\"\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Stub implementation - logs warning and returns None.\"\"\"\n        self.logger.warning(\n            \"Adapter is a non-functional stub\",\n            adapter=self.source_name,\n            message=\"This adapter has not been implemented and will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Stub implementation - returns empty list.\"\"\"\n        return []\n",
  "web_service/backend/adapters/twinspires_adapter.py": "\"\"\"\nTwinSpires Racing Adapter - Production Implementation\n\nUses Scrapling's AsyncStealthySession for anti-bot bypass with:\n- Persistent session pooling\n- Exponential backoff retry logic\n- Comprehensive selector strategies\n- Detailed diagnostics for debugging\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport re\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom scrapling.parser import Selector\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .constants import MAX_VALID_ODDS\nfrom .mixins import DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\nlogger = logging.getLogger(__name__)\n\n\nclass TwinSpiresAdapter(DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Production adapter for TwinSpires racing data.\n\n    Features:\n    - StealthySession with automatic Playwright fallback\n    - Exponential backoff retry logic\n    - Comprehensive selector strategies\n    - Debug HTML capture for failure analysis\n    \"\"\"\n\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    # Selector strategies - ordered by reliability\n    RACE_CONTAINER_SELECTORS = [\n        'div[class*=\"RaceCard\"]',\n        'div[class*=\"race-card\"]',\n        'div[data-testid*=\"race\"]',\n        'div[data-race-id]',\n        'section[class*=\"race\"]',\n        'article[class*=\"race\"]',\n        '.race-container',\n        '[data-race]',\n        # Broader fallbacks\n        'div[class*=\"card\"][class*=\"race\" i]',\n        'div[class*=\"event\"]',\n    ]\n\n    TRACK_NAME_SELECTORS = [\n        '[class*=\"track-name\"]',\n        '[class*=\"trackName\"]',\n        '[data-track-name]',\n        'h2[class*=\"track\"]',\n        'h3[class*=\"track\"]',\n        '.track-title',\n        '[class*=\"venue\"]',\n    ]\n\n    RACE_NUMBER_SELECTORS = [\n        '[class*=\"race-number\"]',\n        '[class*=\"raceNumber\"]',\n        '[class*=\"race-num\"]',\n        '[data-race-number]',\n        'span[class*=\"number\"]',\n    ]\n\n    POST_TIME_SELECTORS = [\n        'time[datetime]',\n        '[class*=\"post-time\"]',\n        '[class*=\"postTime\"]',\n        '[class*=\"mtp\"]',  # Minutes to post\n        '[data-post-time]',\n        '[class*=\"race-time\"]',\n    ]\n\n    RUNNER_ROW_SELECTORS = [\n        'tr[class*=\"runner\"]',\n        'div[class*=\"runner\"]',\n        'li[class*=\"runner\"]',\n        '[data-runner-id]',\n        'div[class*=\"horse-row\"]',\n        'tr[class*=\"horse\"]',\n        'div[class*=\"entry\"]',\n        '.runner-row',\n        '.horse-entry',\n    ]\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config,\n            enable_cache=True,\n            cache_ttl=180.0,\n            rate_limit=1.5  # Slightly more conservative\n        )\n        self.attempted_url: Optional[str] = None\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        TwinSpires has strong anti-bot protections.\n        Using CAMOUFLAGE stealth mode and blocking non-essential resources.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.CAMOUFOX,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            block_resources=True,\n            max_retries=3,\n            timeout=45,\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetch race data from TwinSpires for given date, including multiple disciplines.\n        \"\"\"\n        self.logger.info(f\"Fetching TwinSpires multi-discipline races for {date}\")\n\n        all_races_data = []\n\n        # We explicitly target each discipline to \"force\" all three as requested\n        disciplines = [\"thoroughbred\", \"harness\", \"greyhound\"]\n\n        for disc in disciplines:\n            url = f\"{self.BASE_URL}/bet/todays-races/{disc}\"\n            self.attempted_url = url\n            self.logger.info(f\"Fetching discipline: {disc} from {url}\")\n\n            try:\n                response = await self.make_request(\n                    \"GET\",\n                    url,\n                    network_idle=True,\n                    wait_selector='div[class*=\"race\"], [class*=\"RaceCard\"], [class*=\"track\"]',\n                )\n                if response and response.status == 200:\n                    self._save_debug_snapshot(response.text, f'twinspires_{disc}_{date}')\n                    disc_races = self._extract_races_from_page(response, date)\n                    if disc_races:\n                        # Tag them with discipline to help _detect_discipline if needed\n                        for r in disc_races:\n                            r[\"assigned_discipline\"] = disc.capitalize()\n                        all_races_data.extend(disc_races)\n                        self.logger.info(f\"Extracted {len(disc_races)} {disc} races\")\n            except Exception as e:\n                self.logger.warning(f\"Failed to fetch {disc} races: {e}\")\n\n        # If direct discipline links failed or returned nothing, try the general timeline view\n        if not all_races_data:\n            url = f\"{self.BASE_URL}/bet/todays-races/time\"\n            self.logger.info(f\"Falling back to timeline view: {url}\")\n            try:\n                response = await self.make_request(\"GET\", url, network_idle=True)\n                if response and response.status == 200:\n                    all_races_data = self._extract_races_from_page(response, date)\n            except Exception as e:\n                self.logger.warning(f\"Fallback failed: {e}\")\n\n        if all_races_data:\n            return {\n                \"races\": all_races_data,\n                \"date\": date,\n                \"source\": self.source_name,\n            }\n\n        return None\n\n    def _extract_races_from_page(self, response, date: str) -> List[dict]:\n        \"\"\"\n        Extract race information from page response.\n\n        Uses multiple selector strategies with fallback.\n        \"\"\"\n        races_data = []\n        page = Selector(response.text)\n\n        # Try each selector pattern\n        race_elements = []\n        selector_used = None\n\n        for selector in self.RACE_CONTAINER_SELECTORS:\n            try:\n                elements = page.css(selector)\n                if elements and len(elements) > 0:\n                    # Verify these look like race containers\n                    sample = elements[0]\n                    sample_text = str(sample.html) if hasattr(sample, 'html') else str(sample)\n\n                    # Quick sanity check - should have some race-like content\n                    if any(kw in sample_text.lower() for kw in ['race', 'post', 'horse', 'runner', 'odds']):\n                        race_elements = elements\n                        selector_used = selector\n                        break\n            except Exception as e:\n                self.logger.debug(f\"Selector '{selector}' failed: {e}\")\n                continue\n\n        if race_elements:\n            self.logger.info(f\"Found {len(race_elements)} race containers using: '{selector_used}'\")\n        else:\n            self.logger.warning(\"No race containers found with any selector\")\n            # Return full page for further analysis\n            return [{\n                \"html\": response.text,\n                \"track\": \"Unknown\",\n                \"race_number\": 0,\n                \"date\": date,\n                \"full_page\": True,\n            }]\n\n        # Extract data from each race element\n        for i, race_elem in enumerate(race_elements, 1):\n            try:\n                race_data = self._extract_single_race_data(race_elem, i, date)\n                if race_data:\n                    races_data.append(race_data)\n            except Exception as e:\n                self.logger.warning(f\"Failed to extract race {i}: {e}\")\n                continue\n\n        return races_data\n\n    def _extract_single_race_data(self, race_elem, default_num: int, date: str) -> Optional[dict]:\n        \"\"\"Extract data from a single race element.\"\"\"\n        try:\n            # Get HTML string\n            html = str(race_elem.html) if hasattr(race_elem, 'html') else str(race_elem)\n\n            # Extract track name\n            track_name = self._find_with_selectors(race_elem, self.TRACK_NAME_SELECTORS)\n            if not track_name:\n                track_name = f\"Track {default_num}\"\n\n            # Extract race number\n            race_num_text = self._find_with_selectors(race_elem, self.RACE_NUMBER_SELECTORS)\n            race_number = default_num\n            if race_num_text:\n                digits = ''.join(filter(str.isdigit, race_num_text))\n                if digits:\n                    race_number = int(digits)\n\n            # Extract post time\n            post_time_text = self._find_with_selectors(race_elem, self.POST_TIME_SELECTORS)\n\n            return {\n                \"html\": html,\n                \"track\": track_name.strip(),\n                \"race_number\": race_number,\n                \"post_time_text\": post_time_text,\n                \"date\": date,\n                \"full_page\": False,\n            }\n\n        except Exception as e:\n            self.logger.debug(f\"Extract single race error: {e}\")\n            return None\n\n    def _find_with_selectors(self, element, selectors: List[str]) -> Optional[str]:\n        \"\"\"Try multiple selectors and return first matching text.\"\"\"\n        for selector in selectors:\n            try:\n                found = element.css_first(selector)\n                if found:\n                    text = found.text.strip() if hasattr(found, 'text') else str(found).strip()\n                    if text:\n                        return text\n            except Exception:\n                continue\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parse extracted race data into Race objects.\"\"\"\n        if not raw_data or \"races\" not in raw_data:\n            self.logger.warning(\"No races data to parse\")\n            return []\n\n        races_list = raw_data[\"races\"]\n        date_str = raw_data.get(\"date\", datetime.now().strftime(\"%Y-%m-%d\"))\n\n        self.logger.info(f\"Parsing {len(races_list)} races\")\n\n        parsed_races = []\n\n        for race_data in races_list:\n            try:\n                race = self._parse_single_race(race_data, date_str)\n                if race and race.runners:\n                    parsed_races.append(race)\n                    self.logger.debug(\n                        f\"Parsed race\",\n                        track=race.venue,\n                        race=race.race_number,\n                        runners=len(race.runners)\n                    )\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to parse race\",\n                    track=race_data.get(\"track\"),\n                    error=str(e),\n                    exc_info=True\n                )\n                continue\n\n        self.logger.info(f\"Successfully parsed {len(parsed_races)} races with runners\")\n        return parsed_races\n\n    def _parse_single_race(self, race_data: dict, date_str: str) -> Optional[Race]:\n        \"\"\"Parse a single race from extracted data.\"\"\"\n        html_content = race_data.get(\"html\", \"\")\n        if not html_content:\n            return None\n\n        page = Selector(html_content)\n\n        track_name = race_data.get(\"track\", \"Unknown\")\n        race_number = race_data.get(\"race_number\", 1)\n\n        # Parse start time\n        start_time = self._parse_post_time(\n            race_data.get(\"post_time_text\"),\n            page,\n            date_str\n        )\n\n        # Parse runners\n        runners = self._parse_runners(page)\n\n        # Generate race ID\n        track_id = re.sub(r'[^a-z0-9]', '', track_name.lower())\n        date_compact = date_str.replace('-', '')\n\n        # Determine discipline\n        discipline = race_data.get(\"assigned_discipline\") or self._detect_discipline(page, html_content)\n\n        disc_suffix = discipline[0].lower() if discipline else \"t\"\n        race_id = f\"ts_{track_id}_{date_compact}_{disc_suffix}{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            discipline=discipline,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_post_time(\n        self,\n        time_text: Optional[str],\n        page,\n        date_str: str\n    ) -> Optional[datetime]:\n        \"\"\"Parse post time from text or page elements.\"\"\"\n        base_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n        # Try provided time text first\n        if time_text:\n            parsed = self._parse_time_string(time_text, base_date)\n            if parsed:\n                return parsed\n\n        # Try finding time in page\n        for selector in self.POST_TIME_SELECTORS:\n            elem = page.css_first(selector)\n            if not elem:\n                continue\n\n            # Check datetime attribute\n            dt_attr = elem.attrib.get('datetime') if hasattr(elem, 'attrib') else None\n            if dt_attr:\n                try:\n                    return datetime.fromisoformat(dt_attr.replace('Z', '+00:00'))\n                except ValueError:\n                    pass\n\n            # Try text content\n            text = elem.text.strip() if hasattr(elem, 'text') else str(elem).strip()\n            parsed = self._parse_time_string(text, base_date)\n            if parsed:\n                return parsed\n\n        # Default to now + 1 hour if nothing found\n        self.logger.debug(\"Could not determine post time, using default\")\n        return datetime.combine(base_date, datetime.now().time()) + timedelta(hours=1)\n\n    def _parse_time_string(self, time_str: str, base_date) -> Optional[datetime]:\n        \"\"\"Parse various time string formats.\"\"\"\n        if not time_str:\n            return None\n\n        # Clean up string\n        time_clean = re.sub(r'\\s+(EST|EDT|CST|CDT|MST|MDT|PST|PDT|ET|PT|CT|MT)$', '', time_str, flags=re.I)\n        time_clean = time_clean.strip()\n\n        # Handle \"MTP\" (minutes to post) format\n        mtp_match = re.search(r'(\\d+)\\s*(?:min|mtp)', time_clean, re.I)\n        if mtp_match:\n            minutes = int(mtp_match.group(1))\n            return datetime.now() + timedelta(minutes=minutes)\n\n        # Try various time formats\n        formats = [\n            '%I:%M %p',      # 3:45 PM\n            '%I:%M%p',       # 3:45PM\n            '%H:%M',         # 15:45\n            '%I:%M:%S %p',   # 3:45:00 PM\n        ]\n\n        for fmt in formats:\n            try:\n                time_obj = datetime.strptime(time_clean, fmt).time()\n                return datetime.combine(base_date, time_obj)\n            except ValueError:\n                continue\n\n        return None\n\n    def _parse_runners(self, page) -> List[Runner]:\n        \"\"\"Parse runner information from race HTML.\"\"\"\n        runners = []\n\n        # Find runner elements\n        runner_elements = []\n        for selector in self.RUNNER_ROW_SELECTORS:\n            try:\n                elements = page.css(selector)\n                if elements and len(elements) > 0:\n                    runner_elements = elements\n                    self.logger.debug(f\"Found {len(elements)} runners with: {selector}\")\n                    break\n            except Exception:\n                continue\n\n        if not runner_elements:\n            self.logger.debug(\"No runner elements found\")\n            return runners\n\n        for i, elem in enumerate(runner_elements):\n            try:\n                runner = self._parse_single_runner(elem, i + 1)\n                if runner:\n                    runners.append(runner)\n            except Exception as e:\n                self.logger.debug(f\"Failed to parse runner {i + 1}: {e}\")\n                continue\n\n        return runners\n\n    def _parse_single_runner(self, elem, default_number: int) -> Optional[Runner]:\n        \"\"\"Parse a single runner element.\"\"\"\n        # Get element content\n        elem_str = str(elem.html) if hasattr(elem, 'html') else str(elem)\n        elem_lower = elem_str.lower()\n\n        # Check if scratched\n        scratched = any(s in elem_lower for s in ['scratched', 'scr', 'scratch'])\n\n        # Extract program number\n        number_selectors = [\n            '[class*=\"program\"]',\n            '[class*=\"saddle\"]',\n            '[class*=\"post\"]',\n            '[class*=\"number\"]',\n            '[data-program-number]',\n            'td:first-child',\n        ]\n\n        number = None\n        for selector in number_selectors:\n            try:\n                num_elem = elem.css_first(selector)\n                if num_elem:\n                    num_text = num_elem.text.strip() if hasattr(num_elem, 'text') else str(num_elem)\n                    digits = ''.join(filter(str.isdigit, num_text))\n                    if digits:\n                        number = int(digits)\n                        break\n            except Exception:\n                continue\n\n        if number is None:\n            number = default_number\n\n        # Extract horse name\n        name_selectors = [\n            '[class*=\"horse-name\"]',\n            '[class*=\"horseName\"]',\n            '[class*=\"runner-name\"]',\n            'a[class*=\"name\"]',\n            '[data-horse-name]',\n            'td:nth-child(2)',\n        ]\n\n        name = None\n        for selector in name_selectors:\n            try:\n                name_elem = elem.css_first(selector)\n                if name_elem:\n                    name_text = name_elem.text.strip() if hasattr(name_elem, 'text') else None\n                    if name_text and len(name_text) > 1:\n                        # Clean up name\n                        name = re.sub(r'\\([^)]*\\)', '', name_text).strip()\n                        break\n            except Exception:\n                continue\n\n        if not name:\n            return None\n\n        # Extract odds\n        odds = {}\n        if not scratched:\n            odds_selectors = [\n                '[class*=\"odds\"]',\n                '[class*=\"ml\"]',  # Morning line\n                '[class*=\"morning-line\"]',\n                '[data-odds]',\n            ]\n\n            for selector in odds_selectors:\n                try:\n                    odds_elem = elem.css_first(selector)\n                    if odds_elem:\n                        odds_text = odds_elem.text.strip() if hasattr(odds_elem, 'text') else None\n                        if odds_text and odds_text.upper() not in ['SCR', 'SCRATCHED', '--', 'N/A']:\n                            win_odds = parse_odds_to_decimal(odds_text)\n                            if odds_data := create_odds_data(self.source_name, win_odds):\n                                odds[self.source_name] = odds_data\n                                break\n                except Exception:\n                    continue\n\n        return Runner(\n            number=number,\n            name=name,\n            scratched=scratched,\n            odds=odds,\n        )\n\n    def _detect_discipline(self, page, html: str) -> str:\n        \"\"\"Detect race discipline (Thoroughbred, Harness, etc).\"\"\"\n        html_lower = html.lower()\n\n        if any(kw in html_lower for kw in ['harness', 'trotter', 'pacer', 'standardbred']):\n            return \"Harness\"\n        elif any(kw in html_lower for kw in ['quarter horse', 'quarterhorse']):\n            return \"Quarter Horse\"\n        elif any(kw in html_lower for kw in ['greyhound', 'dog']):\n            return \"Greyhound\"\n\n        # Try finding breed element\n        breed_selectors = ['[class*=\"breed\"]', '[class*=\"type\"]', '[data-breed]']\n        for selector in breed_selectors:\n            try:\n                elem = page.css_first(selector)\n                if elem:\n                    text = elem.text.strip().lower() if hasattr(elem, 'text') else ''\n                    if 'harness' in text:\n                        return \"Harness\"\n                    elif 'quarter' in text:\n                        return \"Quarter Horse\"\n            except Exception:\n                continue\n\n        return \"Thoroughbred\"\n\n    async def cleanup(self):\n        \"\"\"Cleanup resources.\"\"\"\n        await self.close()\n        self.logger.info(\"TwinSpires adapter cleaned up\")\n",
  "web_service/backend/adapters/racingpost_adapter.py": "# python_service/adapters/racingpost_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\n\nclass RacingPostAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        RacingPost has strong anti-bot measures. We need to use a full\n        browser with the highest stealth settings to avoid being blocked.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,  # Strongest stealth\n            block_resources=False,  # Load all resources to appear more human\n        )\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.racingpost.com\")\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch RacingPost index page\", url=index_url)\n            return None\n\n        self._save_debug_html(index_response.text, f\"racingpost_index_{date}\")\n\n        index_parser = HTMLParser(index_response.text)\n        links = index_parser.css('a[data-test-selector^=\"RC-meetingItem__link_race\"]')\n        race_card_urls = [link.attributes[\"href\"] for link in links]\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(\"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n                if not venue_node:\n                    continue\n                venue_raw = venue_node.text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n\n                race_time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n                if not race_time_node:\n                    continue\n                race_time_str = race_time_node.text(strip=True)\n\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            if runner := self._parse_runner(node):\n                runners.append(runner)\n        return runners\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n            name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n            odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n            if not all([number_node, name_node, odds_node]):\n                return None\n\n            number_str = clean_text(number_node.text())\n            number = int(number_str) if number_str and number_str.isdigit() else 0\n            name = clean_text(name_node.text())\n            odds_str = clean_text(odds_node.text())\n            scratched = \"NR\" in odds_str.upper() or not odds_str\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if odds_data := create_odds_data(self.source_name, win_odds):\n                    odds[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError):\n            self.logger.warning(\"Could not parse RacingPost runner, skipping.\", exc_info=True)\n            return None\n",
  "web_service/backend/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairGreyhounds\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        if not await self._authenticate(self.http_client):\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Failed to parse a Betfair Greyhound market.\",\n                    exc_info=True,\n                    market=market,\n                )\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Optional[Race]:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
  "web_service/backend/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy, StealthMode\n\n\nclass SportingLifeAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    Standardized on selectolax for performance.\n    \"\"\"\n\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        SportingLife requires JavaScript rendering to get the race links,\n        so we must use a full browser engine like Playwright.\n        Uses CAMOUFLAGE mode to bypass anti-bot.\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=StealthMode.CAMOUFLAGE,\n            block_resources=True\n        )\n\n    def _get_headers(self) -> dict:\n        \"\"\"Get browser-like headers for SportingLife.\"\"\"\n        return self._get_browser_headers(\n            host=\"www.sportinglife.com\",\n            referer=\"https://www.sportinglife.com/racing/racecards\",\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = \"/racing/racecards\"  # The dated URL is causing a 307 redirect\n        index_response = await self.make_request(\n            \"GET\",\n            index_url,\n            headers=self._get_headers(),\n            follow_redirects=True,\n        )\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"sportinglife_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        links = {\n            a.attributes[\"href\"]\n            for a in parser.css('li[class^=\"MeetingSummary__LineWrapper\"] a[href*=\"/racecard/\"]')\n            if a.attributes.get(\"href\")\n        }\n\n        if not links:\n            self.logger.warning(\"No race links found on SportingLife index page\", date=date)\n            # Try a fallback selector\n            links = {\n                a.attributes[\"href\"]\n                for a in parser.css('.meeting-summary a[href*=\"/racecard/\"]')\n                if a.attributes.get(\"href\")\n            }\n\n        if not links:\n            return None\n\n        self.logger.info(f\"Found {len(links)} race links on SportingLife\")\n\n        async def fetch_single_html(url_path: str):\n            # Pass method='GET' explicitly to ensure it works through SmartFetcher\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": [p for p in html_pages if p], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to SportingLifeAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                header = parser.css_first('h1[class*=\"RacingRacecardHeader__Title\"]')\n                if not header:\n                    self.logger.warning(\"Could not find race header in SportingLife page.\")\n                    continue\n\n                header_text = clean_text(header.text())\n                parts = header_text.split()\n                if not parts:\n                    continue\n                race_time_str = parts[0]\n                track_name = \" \".join(parts[1:])\n\n                try:\n                    start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n                except ValueError:\n                    continue\n\n                race_number = 1\n                nav_links = parser.css('a[class*=\"SubNavigation__Link\"]')\n                active_link = parser.css_first('a[class*=\"SubNavigation__Link--active\"]')\n                if active_link and nav_links:\n                    try:\n                        for idx, link in enumerate(nav_links):\n                            # Compare text content or href if html comparison is too strict\n                            if link.text().strip() == active_link.text().strip():\n                                race_number = idx + 1\n                                break\n                    except Exception:\n                        pass\n\n                runners = [r for row in parser.css('div[class*=\"RunnerCard\"]') if (r := self._parse_runner(row))]\n\n                if not runners:\n                    continue\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except Exception as e:\n                self.logger.warning(\n                    \"Error parsing a race from SportingLife\",\n                    error=str(e),\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first('a[href*=\"/racing/profiles/horse/\"]')\n            if not name_node:\n                return None\n            name = clean_text(name_node.text()).splitlines()[0].strip()\n\n            num_node = row.css_first('span[class*=\"SaddleCloth__Number\"]')\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            odds_node = row.css_first('span[class*=\"Odds__Price\"]')\n            odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = {}\n            if odds_val := create_odds_data(self.source_name, win_odds):\n                odds_data[self.source_name] = odds_val\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            return None\n",
  "web_service/backend/adapters/oddschecker_adapter.py": "# python_service/adapters/oddschecker_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass OddscheckerAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    def _get_headers(self) -> dict:\n        return self._get_browser_headers(host=\"www.oddschecker.com\")\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        self._save_debug_html(index_response.text, f\"oddschecker_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Find all links to individual race pages\n        race_links = {a.attributes[\"href\"] for a in parser.css(\"a.race-time-link[href]\") if a.attributes.get(\"href\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                race = self._parse_race_page(parser, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, parser: HTMLParser, race_date) -> Optional[Race]:\n        track_name_node = parser.css_first(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.text(strip=True)\n\n        race_time_node = parser.css_first(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = parser.css_first(\"a.race-time-link.active\")\n        race_number = 1\n        if active_link:\n            all_links = parser.css(\"a.race-time-link\")\n            try:\n                for i, link in enumerate(all_links):\n                    if link.html == active_link.html:\n                        race_number = i + 1\n                        break\n            except Exception:\n                pass\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in parser.css(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Node) -> Optional[Runner]:\n        try:\n            name_node = row.css_first(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.text(strip=True)\n\n            odds_node = row.css_first(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.text(strip=True)\n\n            number_node = row.css_first(\"td.runner-number\")\n            if not number_node or not number_node.text(strip=True).isdigit():\n                return None\n            number = int(number_node.text(strip=True))\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds_dict[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
  "web_service/backend/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for TVG API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not getattr(config, \"TVG_API_KEY\", None):\n            raise AdapterConfigError(self.SOURCE_NAME, \"TVG_API_KEY is not configured.\")\n        self.api_key = config.TVG_API_KEY\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw meetings data from the TVG API.\"\"\"\n        headers = {\"X-API-Key\": self.api_key}\n        response = await self.make_request(\"GET\", f\"meetings?date={date}\", headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in TVG response or invalid format.\")\n            return []\n\n        all_races = []\n        for meeting in raw_data.get(\"meetings\", []):\n            venue = meeting.get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, venue):\n                        all_races.append(race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.error(\n                        \"Error parsing TVG race\",\n                        race_id=race_data.get(\"id\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], venue: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"id\")\n        race_number = race_data.get(\"number\")\n        start_time_str = race_data.get(\"startTime\")\n\n        if not all([race_id, race_number, start_time_str]):\n            return None\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            name = runner_data.get(\"name\")\n            number = runner_data.get(\"number\")\n            if not all([name, number]):\n                continue\n\n            odds = {}\n            # Placeholder for TVG odds parsing\n\n            runners.append(\n                Runner(\n                    name=name,\n                    number=number,\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds,\n                )\n            )\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            start_time = datetime.now()\n\n        return Race(\n            id=f\"tvg_{race_id}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/constants.py": "# python_service/adapters/constants.py\n\"\"\"Shared constants for all adapters.\"\"\"\n\nfrom typing import Final\n\n# Odds thresholds\nMAX_VALID_ODDS: Final[float] = 999.0\nMIN_VALID_ODDS: Final[float] = 1.01\n\n# Common HTTP headers for browser-like requests\nDEFAULT_BROWSER_HEADERS: Final[dict] = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Cache-Control\": \"no-cache\",\n    \"Connection\": \"keep-alive\",\n    \"Pragma\": \"no-cache\",\n    \"Sec-Fetch-Dest\": \"document\",\n    \"Sec-Fetch-Mode\": \"navigate\",\n    \"Sec-Fetch-Site\": \"none\",\n    \"Sec-Fetch-User\": \"?1\",\n    \"Upgrade-Insecure-Requests\": \"1\",\n}\n\nCHROME_USER_AGENT: Final[str] = (\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n)\n\nCHROME_SEC_CH_UA: Final[str] = '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"'\n",
  "web_service/backend/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any, Dict, List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not getattr(config, \"GREYHOUND_API_URL\", None):\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(\"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if odds_data_val := create_odds_data(self.source_name, win_odds):\n                        odds_data[self.source_name] = odds_data_val\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
  "web_service/backend/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime, timedelta, timezone\nfrom decimal import Decimal\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Using a representative eventId as a placeholder for the discovery mechanism\n        event_id = \"38183.3\"\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(\"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            return []\n\n        races = []\n        for group in raw_data.get(\"marketGroups\", []):\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\n                            \"Failed to parse a FanDuel market\",\n                            market_id=market.get(\"marketId\"),\n                            exc_info=True,\n                        )\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\").strip()\n        if not race_number_str.isdigit():\n            return None\n        race_number = int(race_number_str)\n\n        track_name = parts[1]\n        start_time = datetime.now(timezone.utc) + timedelta(hours=race_number)\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_runner_odds = runner_data.get(\"winRunnerOdds\", {})\n                current_price = win_runner_odds.get(\"currentPrice\")\n\n                if not runner_name or not current_price:\n                    continue\n\n                numerator, denominator = map(int, current_price.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                name_parts = runner_name.split(\".\", 1)\n                if len(name_parts) < 2:\n                    continue\n\n                program_number_str = name_parts[0].strip()\n                horse_name = name_parts[1].strip()\n\n                odds = {}\n                if odds_data := create_odds_data(self.source_name, decimal_odds):\n                    odds[self.source_name] = odds_data\n\n                runners.append(\n                    Runner(\n                        name=horse_name,\n                        number=(int(program_number_str) if program_number_str.isdigit() else 0),\n                        odds=odds,\n                    )\n                )\n            except (ValueError, ZeroDivisionError, IndexError, TypeError):\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\"\"\"Adapter for attheraces.com.\"\"\"\n\nimport asyncio\nimport re\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass AtTheRacesAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for attheraces.com, migrated to BaseAdapterV3.\n\n    Standardized on selectolax for performance.\n    \"\"\"\n\n    SOURCE_NAME = \"AtTheRaces\"\n    BASE_URL = \"https://www.attheraces.com\"\n\n    # Robust selector strategies with fallbacks\n    SELECTORS = {\n        \"race_links\": [\n            'a[href^=\"/racecard/\"]',\n            'a[href*=\"/racecard/\"]',\n        ],\n        \"details_container\": [\n            \".race-header__details--primary\",\n            \"atr-racecard-race-header .container\",\n            \".racecard-header .container\",\n        ],\n        \"track_name\": [\"h2\", \"h1 a\", \"h1\"],\n        \"race_time\": [\"h2 b\", \"h1 span\", \".race-time\"],\n        \"runners\": [\".odds-grid-horse\", \"atr-horse-in-racecard\", \".horse-in-racecard\"],\n    }\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n        # Limit concurrency to avoid triggering bot protection/timeouts\n        self._semaphore = asyncio.Semaphore(5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"AtTheRaces is a simple HTML site - HTTPX is fastest.\"\"\"\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX, enable_js=False)\n\n    def _get_headers(self) -> dict:\n        \"\"\"Get headers for ATR requests.\"\"\"\n        return self._get_browser_headers(\n            host=\"www.attheraces.com\",\n            referer=\"https://www.attheraces.com/racecards\",\n        )\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"Fetch race pages for a given date.\"\"\"\n        index_url = f\"/racecards/{date}\"\n\n        try:\n            index_response = await self.make_request(\n                \"GET\", index_url, headers=self._get_headers()\n            )\n        except Exception as e:\n            self.logger.error(\n                \"Failed to fetch AtTheRaces index page\", url=index_url, error=str(e)\n            )\n            return None\n\n        if not index_response:\n            self.logger.warning(\"No response from AtTheRaces index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"atr_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        links = self._find_links_with_fallback(parser)\n\n        # Filter out links that are not actual racecards (e.g. /racecards/date)\n        # Real racecards usually have a time at the end: /racecard/Venue/Date/Time\n        filtered_links = {\n            link for link in links\n            if re.search(r'/\\d{4}$', link) or re.search(r'/\\d{1,2}$', link)\n        }\n\n        if not filtered_links:\n            self.logger.warning(\"No race links found on index page\", date=date)\n            return None\n\n        self.logger.info(f\"Found {len(filtered_links)} race links for {date}\")\n\n        pages = await self._fetch_race_pages(filtered_links)\n        self.logger.info(f\"Successfully fetched {len(pages)}/{len(filtered_links)} race pages\")\n\n        return {\"pages\": pages, \"date\": date}\n\n    def _find_links_with_fallback(self, parser: HTMLParser) -> set:\n        \"\"\"Try multiple selectors to find race links.\"\"\"\n        links = set()\n        for selector in self.SELECTORS[\"race_links\"]:\n            found = {\n                a.attributes[\"href\"]\n                for a in parser.css(selector)\n                if a.attributes.get(\"href\")\n            }\n            links.update(found)\n        return links\n\n    async def _fetch_race_pages(self, links: set) -> List[tuple]:\n        \"\"\"Fetch all race pages concurrently with semaphore limit.\"\"\"\n        async def fetch_single(url_path: str):\n            async with self._semaphore:\n                try:\n                    # Random delay to be less robotic (0.5 to 1.5 seconds)\n                    await asyncio.sleep(0.5 + (hash(url_path) % 100) / 100.0)\n                    response = await self.make_request(\n                        \"GET\", url_path, headers=self._get_headers()\n                    )\n                    return (url_path, response.text) if response else (url_path, \"\")\n                except Exception as e:\n                    self.logger.warning(f\"Failed to fetch {url_path}: {e}\")\n                    return (url_path, \"\")\n\n        tasks = [fetch_single(link) for link in links]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        return [\n            page for page in results\n            if not isinstance(page, Exception) and page and page[1]\n        ]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parse race pages into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format\", date=raw_data.get(\"date\")\n            )\n            return []\n\n        races = []\n        for url_path, html in raw_data[\"pages\"]:\n            if not html:\n                continue\n\n            try:\n                if race := self._parse_single_race(html, url_path, race_date):\n                    races.append(race)\n            except Exception as e:\n                self.logger.warning(\n                    \"Error parsing race\", url=url_path, error=str(e), exc_info=True\n                )\n                self._save_debug_snapshot(html, f\"atr_parse_error_{url_path.split('/')[-1]}\", url=url_path)\n\n        return races\n\n    def _parse_single_race(\n        self, html: str, url_path: str, race_date\n    ) -> Optional[Race]:\n        \"\"\"Parse a single race from HTML.\"\"\"\n        parser = HTMLParser(html)\n\n        details = self._find_first_match(parser, self.SELECTORS[\"details_container\"])\n        if not details:\n            return None\n\n        track_node = self._find_first_match(details, self.SELECTORS[\"track_name\"])\n        track_text = clean_text(track_node.text()) if track_node else \"\"\n\n        # New pattern: \"14:32 Dundalk (IRE) 28 Jan 2026\"\n        time_match = re.search(r'(\\d{1,2}:\\d{2})', track_text)\n        if time_match:\n            time_str = time_match.group(1)\n            track_name_raw = track_text.replace(time_str, \"\").strip()\n            # Remove date if present (e.g. \"28 Jan 2026\")\n            track_name_raw = re.sub(r'\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4}', '', track_name_raw).strip()\n            track_name = normalize_venue_name(track_name_raw)\n        else:\n            track_name = normalize_venue_name(track_text)\n            time_node = self._find_first_match(details, self.SELECTORS[\"race_time\"])\n            time_str = (\n                clean_text(time_node.text()).replace(\" ATR\", \"\")\n                if time_node else \"\"\n            )\n\n        if not track_name or not time_str:\n            return None\n\n        try:\n            start_time = datetime.combine(\n                race_date, datetime.strptime(time_str, \"%H:%M\").time()\n            )\n        except ValueError:\n            self.logger.warning(\"Invalid time format\", time_str=time_str)\n            return None\n\n        race_number = self._extract_race_number(url_path)\n        runners = self._parse_runners(parser)\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"atr_{track_name.replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _find_first_match(self, parser, selectors: List[str]):\n        \"\"\"Try selectors until one matches.\"\"\"\n        for selector in selectors:\n            if node := parser.css_first(selector):\n                return node\n        return None\n\n    def _extract_race_number(self, url_path: str) -> int:\n        \"\"\"Extract race number from URL path.\"\"\"\n        pattern = r\"/(\\d{1,2})$\"\n        if match := re.search(pattern, url_path):\n            return int(match.group(1))\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> List[Runner]:\n        \"\"\"Parse all runners from the page.\"\"\"\n        # We'll map horse IDs to best odds\n        odds_map = {}\n\n        # Look in all potential odds grid wrappers\n        for wrapper in parser.css(\".odds-grid__row-wrapper--entries\"):\n            for row in wrapper.css(\".odds-grid__row--horse\"):\n                row_id = row.attributes.get(\"id\", \"\")\n                horse_id_match = re.search(r'row-(\\d+)', row_id)\n                if not horse_id_match:\n                    continue\n                horse_id = horse_id_match.group(1)\n\n                best_price = row.attributes.get(\"data-bestprice\")\n                if best_price:\n                    try:\n                        odds_map[horse_id] = float(best_price)\n                    except ValueError:\n                        pass\n\n        runner_nodes = []\n        for selector in self.SELECTORS[\"runners\"]:\n            if nodes := parser.css(selector):\n                runner_nodes = nodes\n                break\n\n        runners = []\n        for row in runner_nodes:\n            runner = self._parse_runner(row, odds_map)\n            if runner:\n                runners.append(runner)\n\n        return runners\n\n    def _parse_runner(self, row: Node, odds_map: dict) -> Optional[Runner]:\n        \"\"\"Parse a single runner.\"\"\"\n        try:\n            # Horse name can be in several places depending on layout\n            name_node = row.css_first(\"h3\") or row.css_first('a[href*=\"/form/horse/\"]')\n            if not name_node:\n                return None\n            name = clean_text(name_node.text())\n\n            num_node = row.css_first(\".horse-in-racecard__saddle-cloth-number\") or row.css_first(\".odds-grid-horse__no\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n\n            # Try to get horse ID from link to match with odds_map\n            horse_id = None\n            horse_link = row.css_first('a[href*=\"/form/horse/\"]')\n            if horse_link:\n                href = horse_link.attributes.get(\"href\", \"\")\n                # Match digits after name and before query params\n                horse_id_match = re.search(r'/(\\d+)(\\?|$)', href)\n                if horse_id_match:\n                    horse_id = horse_id_match.group(1)\n\n            win_odds = odds_map.get(horse_id) if horse_id else None\n\n            # Fallback to old selector if not in map\n            if win_odds is None:\n                odds_node = row.css_first(\".horse-in-racecard__odds\")\n                odds_str = clean_text(odds_node.text()) if odds_node else \"\"\n                win_odds = parse_odds_to_decimal(odds_str)\n\n            odds = {}\n            if odds_data := create_odds_data(self.source_name, win_odds):\n                odds[self.source_name] = odds_data\n\n            return Runner(number=number, name=name, odds=odds)\n\n        except (AttributeError, ValueError):\n            return None\n",
  "web_service/backend/adapters/betfair_datascientist_adapter.py": "# python_service/adapters/betfair_datascientist_adapter.py\n\nfrom datetime import datetime\nfrom io import StringIO\nfrom typing import List, Optional\n\nimport pandas as pd\n\nfrom ..models import Race, Runner\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass BetfairDataScientistAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Betfair Data Scientist CSV models, migrated to BaseAdapterV3.\n    \"\"\"\n\n    ADAPTER_NAME = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str, url: str, config=None):\n        source_name = f\"{self.ADAPTER_NAME}_{model_name}\"\n        super().__init__(source_name=source_name, base_url=url, config=config)\n        self.model_name = model_name\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[StringIO]:\n        \"\"\"Fetches the raw CSV data from the Betfair Data Scientist model endpoint.\"\"\"\n        endpoint = f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n        self.logger.info(f\"Fetching data from {self.base_url}{endpoint}\")\n        response = await self.make_request(\"GET\", endpoint)\n        return StringIO(response.text) if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[StringIO]) -> List[Race]:\n        \"\"\"Parses the raw CSV data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n        try:\n            df = pd.read_csv(raw_data)\n            if df.empty:\n                self.logger.warning(\"Received empty CSV from Betfair Data Scientist.\")\n                return []\n\n            df = df.rename(\n                columns={\n                    \"meetings.races.bfExchangeMarketId\": \"market_id\",\n                    \"meetings.races.runners.bfExchangeSelectionId\": \"selection_id\",\n                    \"meetings.races.runners.ratedPrice\": \"rated_price\",\n                    \"meetings.races.raceName\": \"race_name\",\n                    \"meetings.name\": \"meeting_name\",\n                    \"meetings.races.raceNumber\": \"race_number\",\n                    \"meetings.races.runners.runnerName\": \"runner_name\",\n                    \"meetings.races.runners.clothNumber\": \"saddle_cloth\",\n                }\n            )\n            races: List[Race] = []\n            for market_id, group in df.groupby(\"market_id\"):\n                race_info = group.iloc[0]\n                runners = []\n                for _, row in group.iterrows():\n                    rated_price = row.get(\"rated_price\")\n                    odds_data = {}\n                    if pd.notna(rated_price):\n                        if odds_val := create_odds_data(self.source_name, float(rated_price)):\n                            odds_data[self.source_name] = odds_val\n\n                    runners.append(\n                        Runner(\n                            name=str(row.get(\"runner_name\", \"Unknown\")),\n                            number=int(row.get(\"saddle_cloth\", 0)),\n                            odds=odds_data,\n                        )\n                    )\n\n                race = Race(\n                    id=str(market_id),\n                    venue=normalize_venue_name(str(race_info.get(\"meeting_name\", \"\"))),\n                    race_number=int(race_info.get(\"race_number\", 0)),\n                    start_time=datetime.now(),  # Placeholder, not provided in source\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n            self.logger.info(f\"Normalized {len(races)} races from {self.model_name}.\")\n            return races\n        except (pd.errors.ParserError, KeyError) as e:\n            self.logger.error(\n                \"Failed to parse Betfair Data Scientist CSV.\",\n                exc_info=True,\n                error=str(e),\n            )\n            return []\n",
  "web_service/backend/adapters/base_adapter_v3.py": "# python_service/adapters/base_v3.py\nfrom __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport json\nimport random\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport httpx\nimport structlog\nfrom tenacity import (\n    RetryError,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom python_service.core.smart_fetcher import (\n    BrowserEngine,\n    FetchStrategy,\n    SmartFetcher,\n    StealthMode,\n)\n\nfrom ..core.exceptions import AdapterHttpError, AdapterParsingError\nfrom ..manual_override_manager import ManualOverrideManager\nfrom ..models import Race\nfrom ..validators import DataValidationPipeline\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject requests\n    HALF_OPEN = \"half_open\"  # Testing if service recovered\n\n\n@dataclass\nclass CircuitBreaker:\n    \"\"\"Thread-safe circuit breaker implementation.\"\"\"\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    half_open_max_calls: int = 3\n\n    _failure_count: int = field(default=0, repr=False)\n    _last_failure_time: float = field(default=0.0, repr=False)\n    _state: CircuitState = field(default=CircuitState.CLOSED, repr=False)\n    _half_open_calls: int = field(default=0, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    @property\n    def state(self) -> CircuitState:\n        \"\"\"Returns current state without mutation. Use check_state() for transitions.\"\"\"\n        return self._state\n\n    async def check_and_transition_state(self) -> CircuitState:\n        \"\"\"Check state and handle OPEN -> HALF_OPEN transition atomically.\"\"\"\n        async with self._lock:\n            if self._state == CircuitState.OPEN:\n                if time.monotonic() - self._last_failure_time >= self.recovery_timeout:\n                    self._state = CircuitState.HALF_OPEN\n                    self._half_open_calls = 0\n            return self._state\n\n    async def record_success(self) -> None:\n        \"\"\"Record a successful call.\"\"\"\n        async with self._lock:\n            self._failure_count = 0\n            if self._state == CircuitState.HALF_OPEN:\n                self._half_open_calls += 1\n                if self._half_open_calls >= self.half_open_max_calls:\n                    self._state = CircuitState.CLOSED\n\n    async def record_failure(self) -> None:\n        \"\"\"Record a failed call.\"\"\"\n        async with self._lock:\n            self._failure_count += 1\n            self._last_failure_time = time.monotonic()\n\n            if self._failure_count >= self.failure_threshold:\n                self._state = CircuitState.OPEN\n            elif self._state == CircuitState.HALF_OPEN:\n                self._state = CircuitState.OPEN\n\n    async def allow_request(self) -> bool:\n        \"\"\"Check if a request should be allowed.\"\"\"\n        state = await self.check_and_transition_state()\n        return state in (CircuitState.CLOSED, CircuitState.HALF_OPEN)\n\n\n@dataclass\nclass RateLimiter:\n    \"\"\"Token bucket rate limiter.\"\"\"\n    requests_per_second: float = 10.0\n    burst_size: int = 20\n\n    _tokens: float = field(default=0.0, init=False, repr=False)\n    _last_update: float = field(default=0.0, init=False, repr=False)\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n\n    def __post_init__(self) -> None:\n        self._tokens = float(self.burst_size)\n        self._last_update = time.monotonic()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire a token, waiting if necessary.\"\"\"\n        async with self._lock:\n            now = time.monotonic()\n            elapsed = now - self._last_update\n            self._tokens = min(self.burst_size, self._tokens + elapsed * self.requests_per_second)\n            self._last_update = now\n\n            if self._tokens < 1:\n                wait_time = (1 - self._tokens) / self.requests_per_second\n                await asyncio.sleep(wait_time)\n                self._tokens = 0\n            else:\n                self._tokens -= 1\n\n\n@dataclass\nclass CacheEntry:\n    \"\"\"Cache entry with TTL.\"\"\"\n    data: Any\n    created_at: float\n    ttl: float\n\n    @property\n    def is_expired(self) -> bool:\n        return time.monotonic() - self.created_at > self.ttl\n\n\nclass ResponseCache:\n    \"\"\"Simple in-memory response cache.\"\"\"\n\n    def __init__(self, default_ttl: float = 300.0, max_entries: int = 1000):\n        self.default_ttl = default_ttl\n        self.max_entries = max_entries\n        self._cache: dict[str, CacheEntry] = {}\n        self._lock = asyncio.Lock()\n\n    @staticmethod\n    def _make_key(method: str, url: str, **kwargs) -> str:\n        \"\"\"Generate a stable cache key from request parameters.\"\"\"\n        # Filter out non-hashable or irrelevant kwargs\n        cacheable_kwargs = {\n            k: v for k, v in kwargs.items()\n            if k not in ('headers', 'timeout', 'follow_redirects')\n            and isinstance(v, (str, int, float, bool, tuple, type(None)))\n        }\n        key_data = f\"{method}:{url}:{json.dumps(cacheable_kwargs, sort_keys=True, default=str)}\"\n        return hashlib.sha256(key_data.encode()).hexdigest()[:32]\n\n    async def get(self, method: str, url: str, **kwargs) -> Any | None:\n        \"\"\"Get a cached response if available and not expired.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            entry = self._cache.get(key)\n            if entry and not entry.is_expired:\n                return entry.data\n            elif entry:\n                del self._cache[key]\n        return None\n\n    async def set(self, method: str, url: str, data: Any, ttl: float | None = None, **kwargs) -> None:\n        \"\"\"Cache a response.\"\"\"\n        key = self._make_key(method, url, **kwargs)\n\n        async with self._lock:\n            # Evict old entries if cache is full\n            if len(self._cache) >= self.max_entries:\n                expired_keys = [k for k, v in self._cache.items() if v.is_expired]\n                for k in expired_keys:\n                    del self._cache[k]\n\n                # If still full, remove oldest entries\n                if len(self._cache) >= self.max_entries:\n                    oldest = sorted(self._cache.items(), key=lambda x: x[1].created_at)\n                    for k, _ in oldest[:len(self._cache) // 4]:\n                        del self._cache[k]\n\n            self._cache[key] = CacheEntry(\n                data=data,\n                created_at=time.monotonic(),\n                ttl=ttl or self.default_ttl\n            )\n\n    async def clear(self) -> None:\n        \"\"\"Clear all cached entries.\"\"\"\n        async with self._lock:\n            self._cache.clear()\n\n\n@dataclass\nclass AdapterMetrics:\n    \"\"\"Thread-safe metrics for adapter health monitoring.\"\"\"\n    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False, repr=False)\n    _total_requests: int = field(default=0, repr=False)\n    _successful_requests: int = field(default=0, repr=False)\n    _failed_requests: int = field(default=0, repr=False)\n    _total_latency_ms: float = field(default=0.0, repr=False)\n    _last_success: float | None = field(default=None, repr=False)\n    _last_failure: float | None = field(default=None, repr=False)\n    _last_error: str | None = field(default=None, repr=False)\n\n    @property\n    def total_requests(self) -> int:\n        return self._total_requests\n\n    @property\n    def success_rate(self) -> float:\n        if self._total_requests == 0:\n            return 1.0\n        return self._successful_requests / self._total_requests\n\n    @property\n    def avg_latency_ms(self) -> float:\n        if self._successful_requests == 0:\n            return 0.0\n        return self._total_latency_ms / self._successful_requests\n\n    async def record_success(self, latency_ms: float) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._successful_requests += 1\n            self._total_latency_ms += latency_ms\n            self._last_success = time.time()\n\n    async def record_failure(self, error: str) -> None:\n        async with self._lock:\n            self._total_requests += 1\n            self._failed_requests += 1\n            self._last_failure = time.time()\n            self._last_error = error\n\n    def snapshot(self) -> dict[str, Any]:\n        \"\"\"Return a point-in-time snapshot of metrics.\"\"\"\n        return {\n            \"total_requests\": self._total_requests,\n            \"successful_requests\": self._successful_requests,\n            \"failed_requests\": self._failed_requests,\n            \"success_rate\": self.success_rate,\n            \"avg_latency_ms\": self.avg_latency_ms,\n            \"last_success\": self._last_success,\n            \"last_failure\": self._last_failure,\n            \"last_error\": self._last_error,\n        }\n\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    Abstract base class for all V3 data adapters.\n\n    Features:\n    - Standardized fetch/parse pattern\n    - Retry logic with exponential backoff\n    - Circuit breaker for fault tolerance\n    - Rate limiting\n    - Response caching\n    - Comprehensive metrics\n    \"\"\"\n\n    # List of common User-Agent strings for rotation\n    USER_AGENTS = [\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0\",\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n    ]\n\n    @property\n    def DEFAULT_USER_AGENT(self) -> str:\n        \"\"\"Return a randomly selected User-Agent.\"\"\"\n        return random.choice(self.USER_AGENTS)\n\n    def __init__(\n        self,\n        source_name: str,\n        base_url: str,\n        config: Any = None,\n        timeout: int = 20,\n        enable_cache: bool = True,\n        cache_ttl: float = 300.0,\n        rate_limit: float = 10.0,\n    ):\n        self.source_name = source_name\n        self.base_url = base_url.rstrip(\"/\")\n        self.config = config\n        self.timeout = timeout\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient | None = None\n        self.manual_override_manager: ManualOverrideManager | None = None\n        self.supports_manual_override = True\n        self.attempted_url: Optional[str] = None\n        # \u2705 THESE 4 LINES MUST BE HERE (not in close()):\n        self.circuit_breaker = CircuitBreaker()\n        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)\n        self.cache = ResponseCache(default_ttl=cache_ttl) if enable_cache else None\n        self.metrics = AdapterMetrics()\n\n        # New SmartFetcher integration\n        self.fetch_strategy = self._configure_fetch_strategy()\n        self.smart_fetcher = SmartFetcher(strategy=self.fetch_strategy)\n\n    async def __aenter__(self) -> \"BaseAdapterV3\":\n        \"\"\"Async context manager entry.\"\"\"\n        if self.http_client is None:\n            self.http_client = httpx.AsyncClient(timeout=self.timeout)\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Async context manager exit with cleanup.\"\"\"\n        await self.close()\n\n    async def close(self) -> None:\n        \"\"\"Clean up resources, including the SmartFetcher.\"\"\"\n        if self.http_client:\n            await self.http_client.aclose()\n            self.http_client = None\n        if hasattr(self, \"smart_fetcher\"):\n            await self.smart_fetcher.close()\n        if self.cache:\n            await self.cache.clear()\n        self.logger.debug(\"Adapter resources cleaned up\")\n\n    def enable_manual_override(self, manager: ManualOverrideManager) -> None:\n        \"\"\"Injects the manual override manager into the adapter.\"\"\"\n        self.manual_override_manager = manager\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw data (e.g., HTML, JSON) for the given date.\n        This is the only method that should perform network operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> list[Race]:\n        \"\"\"\n        Parses the raw data retrieved by _fetch_data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> list[Race]:\n        \"\"\"\n        Orchestrates the fetch-then-parse pipeline for the adapter.\n        This public method should not be overridden by subclasses.\n        \"\"\"\n        raw_data = None\n\n        # Check for manual override data first\n        if self.manual_override_manager:\n            lookup_key = f\"{self.base_url}/racecards/{date}\"\n            manual_data = self.manual_override_manager.get_manual_data(self.source_name, lookup_key)\n            if manual_data:\n                self.logger.info(\"Using manually submitted data\", url=lookup_key)\n                raw_data = {\"pages\": [manual_data[0]], \"date\": date}\n\n        # Fetch from source if no manual data\n        if raw_data is None:\n            try:\n                raw_data = await self._fetch_data(date)\n            except AdapterHttpError as e:\n                if self.manual_override_manager and self.supports_manual_override:\n                    self.manual_override_manager.register_failure(self.source_name, e.url)\n                raise\n\n        # Parse the data\n        if raw_data is not None:\n            return self._validate_and_parse_races(raw_data)\n\n        return []\n\n    def _validate_and_parse_races(self, raw_data: Any) -> list[Race]:\n        self.attempted_url = None  # Reset for each new get_races call\n        is_valid, reason = DataValidationPipeline.validate_raw_response(self.source_name, raw_data)\n        if not is_valid:\n            raise AdapterParsingError(self.source_name, f\"Raw response validation failed: {reason}\")\n\n        try:\n            parsed_races = self._parse_races(raw_data)\n        except Exception as e:\n            self.logger.error(\"Failed to parse race data\", error=str(e), exc_info=True)\n            # Save a snapshot of the problematic data on parsing failure\n            self._save_debug_snapshot(\n                content=str(raw_data),\n                context=\"parsing_error\",\n                url=getattr(e, 'url', self.attempted_url)\n            )\n            raise AdapterParsingError(self.source_name, \"Parsing logic failed.\") from e\n\n        validated_races, warnings = DataValidationPipeline.validate_parsed_races(parsed_races)\n\n        if warnings:\n            self.logger.warning(\"Validation warnings during parsing\", warnings=warnings)\n\n        return validated_races\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        Defines the fetching strategy for this adapter. Subclasses should override\n        this method to customize fetching behavior based on the target website's\n        characteristics (e.g., anti-bot measures, JavaScript requirements).\n\n        Example Overrides:\n        - SportingLife: Needs JS rendering -> primary_engine=BrowserEngine.PLAYWRIGHT\n        - AtTheRaces: Simple HTML -> primary_engine=BrowserEngine.HTTPX\n        - RacingPost: Strong anti-bot -> stealth_mode=StealthMode.CAMOUFLAGE\n        \"\"\"\n        return FetchStrategy(\n            primary_engine=BrowserEngine.PLAYWRIGHT,\n            enable_js=True,\n            stealth_mode=StealthMode.FAST,\n            block_resources=True,\n            max_retries=3,\n            timeout=30,\n        )\n\n    async def make_request(self, method: str, url: str, **kwargs) -> httpx.Response:\n        \"\"\"\n        Performs a web request using the SmartFetcher, which intelligently\n        manages browser engines, retries, and stealth capabilities. This method\n        replaces the previous direct-httpx implementation.\n        \"\"\"\n        full_url = url if url.startswith(\"http\") else f\"{self.base_url}/{url.lstrip('/')}\"\n        self.attempted_url = full_url\n\n        try:\n            # The SmartFetcher handles caching, retries, circuit breaking, etc.\n            response = await self.smart_fetcher.fetch(full_url, method=method, **kwargs)\n\n            # Log success with rich metadata from the fetcher\n            self.logger.info(\n                \"Request successful\",\n                url=full_url,\n                status=getattr(response, \"status\", \"N/A\"),\n                size_bytes=len(getattr(response, \"text\", \"\")),\n                engine=getattr(response, \"metadata\", {}).get(\"engine_used\", \"unknown\"),\n            )\n            return response\n\n        except Exception as e:\n            # Log failure with detailed diagnostics from the fetcher\n            self.logger.error(\n                \"Request failed after all retries and engine fallbacks\",\n                url=full_url,\n                error=str(e),\n                error_type=type(e).__name__,\n                health_report=self.smart_fetcher.get_health_report(),\n            )\n\n            # Save a snapshot if we have a response body in the error\n            if hasattr(e, 'response') and hasattr(e.response, 'text'):\n                self._save_debug_snapshot(\n                    content=e.response.text,\n                    context=f\"request_failed_{getattr(e.response, 'status', 'unknown')}\",\n                    url=full_url\n                )\n\n            # Re-raise as a standard adapter error for consistent downstream handling\n            status_code = getattr(getattr(e, 'response', None), 'status', 503)\n            raise AdapterHttpError(\n                adapter_name=self.source_name, status_code=status_code, url=full_url\n            ) from e\n\n    def _should_save_debug_html(self) -> bool:\n        \"\"\"Determines if the current environment is suitable for saving debug files.\"\"\"\n        import os\n        return os.getenv(\"CI\") == \"true\" or os.getenv(\"DEBUG_MODE\") == \"true\"\n\n    def _save_debug_snapshot(self, content: str, context: str, url: str | None = None):\n        \"\"\"\n        Saves HTML or other text content to a file for debugging purposes.\n        Enhanced to include metadata and better organization.\n        \"\"\"\n        if not self._should_save_debug_html():\n            return\n\n        import os\n        import re\n        import json\n        from datetime import datetime\n\n        try:\n            debug_dir = os.path.join(\"debug-snapshots\", self.source_name.lower())\n            os.makedirs(debug_dir, exist_ok=True)\n\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n\n            # Sanitize context and URL for a safe filename\n            sanitized_context = re.sub(r'[\\\\/*?:\"<>|]', \"_\", context)\n            sanitized_url = \"\"\n            if url:\n                # Remove protocol and query params for filename\n                clean_url = re.sub(r'https?://(www\\.)?', '', url).split('?')[0]\n                # Avoid backslashes in f-string for Python < 3.12 compatibility\n                url_part = re.sub(r'[\\\\/*?:\\x22<>|]', '_', clean_url)[:60]\n                sanitized_url = f\"_{url_part}\"\n\n            base_filename = f\"{timestamp}_{sanitized_context}{sanitized_url}\"\n\n            # Save the main content (HTML/JSON)\n            content_ext = \".json\" if content.startswith((\"{\", \"[\")) else \".html\"\n            filepath = os.path.join(debug_dir, f\"{base_filename}{content_ext}\")\n\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n\n            # Save metadata for better diagnostic context\n            meta_path = os.path.join(debug_dir, f\"{base_filename}_meta.json\")\n            meta = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"adapter\": self.source_name,\n                \"url\": url or self.attempted_url,\n                \"context\": context,\n                \"engine\": getattr(self.smart_fetcher, 'last_engine', 'unknown'),\n                \"health_report\": self.smart_fetcher.get_health_report()\n            }\n            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(meta, f, indent=2)\n\n            self.logger.info(\"Saved debug snapshot and metadata\",\n                             filepath=filepath, meta_path=meta_path)\n\n            # Prune old snapshots (keep last 50)\n            self._prune_debug_snapshots(debug_dir, max_files=100)\n\n        except Exception as e:\n            self.logger.warning(\"Failed to save debug snapshot\", error=str(e))\n\n    def _prune_debug_snapshots(self, debug_dir: str, max_files: int = 100):\n        \"\"\"Keep the number of debug files under control.\"\"\"\n        import os\n        try:\n            files = [os.path.join(debug_dir, f) for f in os.listdir(debug_dir)]\n            if len(files) <= max_files:\n                return\n\n            # Sort by modification time (oldest first)\n            files.sort(key=os.path.getmtime)\n            for f in files[:-max_files]:\n                os.remove(f)\n        except Exception:\n            pass\n\n    async def health_check(self) -> dict[str, Any]:\n        \"\"\"\n        Performs a health check on the adapter.\n        Subclasses can override to add custom checks.\n        \"\"\"\n        return {\n            \"adapter_name\": self.source_name,\n            \"base_url\": self.base_url,\n            \"circuit_breaker_state\": self.circuit_breaker.state.value,\n            \"metrics\": self.metrics.snapshot(),\n        }\n\n    def get_status(self) -> dict[str, Any]:\n        \"\"\"\n        Returns a dictionary representing the adapter's current status.\n        \"\"\"\n        status = \"OK\"\n        if self.circuit_breaker.state == CircuitState.OPEN:\n            status = \"CIRCUIT_OPEN\"\n        elif self.metrics.success_rate < 0.5:\n            status = \"DEGRADED\"\n\n        return {\n            \"adapter_name\": self.source_name,\n            \"status\": status,\n            \"circuit_state\": self.circuit_breaker.state.value,\n            \"success_rate\": round(self.metrics.success_rate, 3),\n        }\n\n    async def reset(self) -> None:\n        \"\"\"Reset adapter state (cache, circuit breaker, metrics).\"\"\"\n        if self.cache:\n            await self.cache.clear()\n        self.circuit_breaker = CircuitBreaker()\n        self.metrics = AdapterMetrics()\n        self.logger.info(\"Adapter state reset\")\n",
  "web_service/backend/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nimport re\nimport json\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser, Node\n\nfrom ..models import Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .mixins import BrowserHeadersMixin, DebugMixin\nfrom .utils.odds_validator import create_odds_data\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass TimeformAdapter(BrowserHeadersMixin, DebugMixin, BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3 and standardized on selectolax.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com/horse-racing\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        self._semaphore = asyncio.Semaphore(5)\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"Timeform works with HTTPX and good headers.\"\"\"\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX, enable_js=False)\n\n    def _get_headers(self) -> dict:\n        headers = self._get_browser_headers(host=\"www.timeform.com\")\n        headers.update({\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n        })\n        return headers\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        self._save_debug_snapshot(index_response.text, f\"timeform_index_{date}\")\n\n        parser = HTMLParser(index_response.text)\n        # Updated selector for race links\n        links = {a.attributes[\"href\"] for a in parser.css(\"a[href*='/racecards/'][href*='/20']\") if a.attributes.get(\"href\") and not a.attributes.get(\"href\").endswith(\"/racecards\")}\n\n        async def fetch_single_html(url_path: str):\n            async with self._semaphore:\n                await asyncio.sleep(0.5)\n                response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n                return (url_path, response.text) if response else (url_path, \"\")\n\n        self.logger.info(f\"Found {len(links)} race links on Timeform\")\n        tasks = [fetch_single_html(link) for link in links]\n        results = await asyncio.gather(*tasks)\n        return {\"pages\": [r for r in results if r[1]], \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for url_path, html_content in raw_data[\"pages\"]:\n            if not html_content:\n                continue\n            try:\n                parser = HTMLParser(html_content)\n\n                # Extract via JSON-LD if possible\n                venue = \"\"\n                start_time = None\n                for script in parser.css('script[type=\"application/ld+json\"]'):\n                    try:\n                        data = json.loads(script.text())\n                        if data.get(\"@type\") == \"Event\":\n                            venue = normalize_venue_name(data.get(\"location\", {}).get(\"name\", \"\"))\n                            if sd := data.get(\"startDate\"):\n                                # 2026-01-28T14:32:00\n                                start_time = datetime.fromisoformat(sd.split('+')[0])\n                            break\n                    except: continue\n\n                if not venue:\n                    # Fallback to title\n                    title = parser.css_first(\"title\")\n                    if title:\n                        # 14:32 DUNDALK | Races 28 January 2026 ...\n                        match = re.search(r'(\\d{1,2}:\\d{2})\\s+([^|]+)', title.text())\n                        if match:\n                            time_str = match.group(1)\n                            venue = normalize_venue_name(match.group(2).strip())\n                            start_time = datetime.combine(race_date, datetime.strptime(time_str, \"%H:%M\").time())\n\n                if not venue or not start_time:\n                    continue\n\n                # Betting Forecast Parsing\n                forecast_map = {}\n                verdict_section = parser.css_first(\"section.rp-verdict\")\n                if verdict_section:\n                    forecast_text = clean_text(verdict_section.text())\n                    if \"Betting Forecast :\" in forecast_text:\n                        # \"Betting Forecast : 15/8 2.87 Spring Is Here, 3/1 4 This Guy, ...\"\n                        after_forecast = forecast_text.split(\"Betting Forecast :\")[1]\n                        # Split by comma\n                        parts = after_forecast.split(',')\n                        for part in parts:\n                            # Match odds and then name\n                            # Odds can be fractional space decimal\n                            m = re.search(r'(\\d+/\\d+|EVENS)\\s+([\\d\\.]+)?\\s*(.+)', part.strip())\n                            if m:\n                                odds_str = m.group(1)\n                                name = clean_text(m.group(3))\n                                forecast_map[name.lower()] = odds_str\n\n                # Runners\n                runners = []\n                # Use tbody as the main container for each runner\n                for row in parser.css('tbody.rp-horse-row'):\n                    name_node = row.css_first(\"a.rp-horse\")\n                    if not name_node: continue\n                    name = clean_text(name_node.text())\n\n                    number = 0\n                    num_attr = row.attributes.get(\"data-entrynumber\")\n                    if num_attr:\n                        try: number = int(num_attr)\n                        except: pass\n\n                    if not number:\n                        num_node = row.css_first(\".rp-entry-number\")\n                        if num_node:\n                            num_text = clean_text(num_node.text())\n                            num_match = re.search(r'^\\d+', num_text)\n                            if num_match:\n                                number = int(num_match.group())\n\n                    win_odds = None\n                    if name.lower() in forecast_map:\n                        win_odds = parse_odds_to_decimal(forecast_map[name.lower()])\n\n                    # Try to find live odds button if available (old selector)\n                    if not win_odds:\n                        odds_tag = row.css_first(\"button.rp-bet-placer-btn__odds\")\n                        if odds_tag:\n                            win_odds = parse_odds_to_decimal(clean_text(odds_tag.text()))\n\n                    odds_data = {}\n                    if odds_val := create_odds_data(self.source_name, win_odds):\n                        odds_data[self.source_name] = odds_val\n\n                    runners.append(Runner(number=number, name=name, odds=odds_data))\n\n                if not runners:\n                    continue\n\n                # Race number from URL or sequence\n                race_number = 1\n                num_match = re.search(r'/(\\d+)/([^/]+)$', url_path)\n                # .../1432/207/1/view... -> the '1' is the race number\n                url_parts = url_path.split('/')\n                if len(url_parts) >= 10:\n                    try: race_number = int(url_parts[9])\n                    except: pass\n\n                race = Race(\n                    id=f\"tf_{venue.lower().replace(' ', '')}_{start_time:%Y%m%d}_R{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except Exception as e:\n                self.logger.warning(f\"Error parsing Timeform race: {e}\")\n                continue\n        return all_races\n",
  "web_service/backend/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\"\"\"Adapter for Racing and Sports Greyhound API.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSportsGreyhound\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n        if not getattr(config, \"RACING_AND_SPORTS_TOKEN\", None):\n            raise AdapterConfigError(\n                self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\"\n            )\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetch greyhound meetings from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            \"GET\", \"v1/greyhound/meetings\", headers=headers, params=params\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parse meetings data into Race objects.\"\"\"\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\n                \"No 'meetings' in RacingAndSportsGreyhound response or invalid format.\"\n            )\n            return []\n\n        races = []\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed := self._parse_race(meeting, race_summary):\n                        races.append(parsed)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse greyhound race\",\n                        venue=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return races\n\n    def _parse_race(\n        self, meeting: Dict[str, Any], race: Dict[str, Any]\n    ) -> Optional[Race]:\n        \"\"\"Parse a single race from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        # FIX: Use dogName for greyhounds, not horseName\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"dogName\", rd.get(\"greyhoundName\", \"Unknown\")),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        try:\n            start_time = datetime.fromisoformat(start_time_str)\n        except (ValueError, TypeError):\n            self.logger.warning(\n                \"Invalid start time\", start_time_str=start_time_str, race_id=race_id\n            )\n            return None\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
  "web_service/backend/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any, List\n\nfrom selectolax.parser import HTMLParser\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    Standardized on selectolax for performance.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(\"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        parser = HTMLParser(response.text)\n        # Assuming the first step is a simple CSS selector for track links\n        track_links = [self.base_url + a.attributes[\"href\"] for a in parser.css(self.definition[\"steps\"][0][\"selector\"]) if a.attributes.get(\"href\")]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
  "web_service/backend/adapters/utils/__init__.py": "# python_service/adapters/utils/__init__.py\n\"\"\"Utilities for adapters.\"\"\"\n\nfrom .odds_validator import create_odds_data, is_valid_odds\nfrom ...utils.odds import parse_odds_to_decimal as parse_odds\n\n__all__ = [\"create_odds_data\", \"is_valid_odds\", \"parse_odds\"]\n",
  "web_service/backend/adapters/utils/odds_validator.py": "# python_service/adapters/utils/odds_validator.py\n\"\"\"Utilities for validating and processing odds data.\"\"\"\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Optional, Union\n\nfrom ..constants import MAX_VALID_ODDS, MIN_VALID_ODDS\nfrom ...models import OddsData\n\n\ndef is_valid_odds(odds: Union[float, Decimal, None]) -> bool:\n    \"\"\"\n    Check if odds value is within valid range.\n\n    Args:\n        odds: The odds value to validate\n\n    Returns:\n        True if odds are valid, False otherwise\n    \"\"\"\n    if odds is None:\n        return False\n    try:\n        odds_float = float(odds)\n        return MIN_VALID_ODDS <= odds_float < MAX_VALID_ODDS\n    except (TypeError, ValueError):\n        return False\n\n\ndef create_odds_data(\n    source_name: str,\n    win_odds: Union[float, Decimal, None],\n    place_odds: Union[float, Decimal, None] = None,\n) -> Optional[OddsData]:\n    \"\"\"\n    Create an OddsData object if odds are valid.\n\n    Args:\n        source_name: Name of the odds source\n        win_odds: Win odds value\n        place_odds: Optional place odds value\n\n    Returns:\n        OddsData object or None if odds are invalid\n    \"\"\"\n    if not is_valid_odds(win_odds):\n        return None\n\n    return OddsData(\n        win=win_odds,\n        place=place_odds if is_valid_odds(place_odds) else None,\n        source=source_name,\n        last_updated=datetime.now(),\n    )\n",
  "web_service/backend/adapters/stubs/__init__.py": "# python_service/adapters/stubs/__init__.py\n\"\"\"Non-functional stub adapters.\"\"\"\n\nfrom .horseracingnation_adapter import HorseRacingNationAdapter\nfrom .nyrabets_adapter import NYRABetsAdapter\nfrom .punters_adapter import PuntersAdapter\nfrom .racingtv_adapter import RacingTVAdapter\nfrom .tab_adapter import TabAdapter\nfrom .template_adapter import TemplateAdapter\n\n__all__ = [\n    \"HorseRacingNationAdapter\",\n    \"NYRABetsAdapter\",\n    \"PuntersAdapter\",\n    \"RacingTVAdapter\",\n    \"TabAdapter\",\n    \"TemplateAdapter\",\n]\n",
  "web_service/backend/adapters/stubs/horseracingnation_adapter.py": "# python_service/adapters/stubs/horseracingnation_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass HorseRacingNationAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for horseracingnation.com.\"\"\"\n\n    SOURCE_NAME = \"HorseRacingNation\"\n    BASE_URL = \"https://www.horseracingnation.com\"\n",
  "web_service/backend/adapters/stubs/racingtv_adapter.py": "# python_service/adapters/stubs/racingtv_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass RacingTVAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for racingtv.com.\"\"\"\n\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n",
  "web_service/backend/adapters/stubs/nyrabets_adapter.py": "# python_service/adapters/stubs/nyrabets_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass NYRABetsAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for nyrabets.com.\"\"\"\n\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n",
  "web_service/backend/adapters/stubs/tab_adapter.py": "# python_service/adapters/stubs/tab_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass TabAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for tab.com.au.\"\"\"\n\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n",
  "web_service/backend/adapters/stubs/template_adapter.py": "# python_service/adapters/stubs/template_adapter.py\n\"\"\"Template for creating new adapters.\"\"\"\n\nfrom typing import Any, List\n\nfrom python_service.core.smart_fetcher import BrowserEngine, FetchStrategy\nfrom ...models import Race\nfrom ..base_adapter_v3 import BaseAdapterV3\n\n\nclass TemplateAdapter(BaseAdapterV3):\n    \"\"\"\n    A template for creating new adapters based on BaseAdapterV3.\n\n    To create a new adapter:\n    1. Copy this file and rename it\n    2. Update SOURCE_NAME and BASE_URL\n    3. Implement _configure_fetch_strategy() based on site requirements\n    4. Implement _fetch_data() to retrieve raw data\n    5. Implement _parse_races() to convert raw data to Race objects\n    \"\"\"\n\n    SOURCE_NAME = \"[IMPLEMENT ME] Example Source\"\n    BASE_URL = \"https://api.example.com\"\n\n    def __init__(self, config=None):\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=self.BASE_URL,\n            config=config\n        )\n        # Example: self.api_key = config.EXAMPLE_API_KEY\n\n    def _configure_fetch_strategy(self) -> FetchStrategy:\n        \"\"\"\n        Configure the fetch strategy based on site requirements.\n\n        Options:\n        - BrowserEngine.HTTPX: Simple HTTP (fastest, use for APIs)\n        - BrowserEngine.PLAYWRIGHT: Full browser (for JS-heavy sites)\n\n        Additional options:\n        - enable_js: Whether to execute JavaScript\n        - stealth_mode: Level of anti-detection measures\n        - block_resources: Block images/fonts for performance\n        \"\"\"\n        return FetchStrategy(primary_engine=BrowserEngine.HTTPX)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetch raw data for the given date.\n\n        Args:\n            date: Date string in YYYY-MM-DD format\n\n        Returns:\n            Raw data (dict, list, string, etc.) or None on failure\n        \"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a template - implement _fetch_data()\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        Parse raw data into Race objects.\n\n        Args:\n            raw_data: Data returned from _fetch_data()\n\n        Returns:\n            List of Race objects\n        \"\"\"\n        return []\n",
  "web_service/backend/adapters/stubs/punters_adapter.py": "# python_service/adapters/stubs/punters_adapter.py\nfrom ..base_stub_adapter import BaseStubAdapter\n\n\nclass PuntersAdapter(BaseStubAdapter):\n    \"\"\"Stub adapter for punters.com.au.\"\"\"\n\n    SOURCE_NAME = \"Punters\"\n    BASE_URL = \"https://www.punters.com.au\"\n",
  "web_service/backend/adapters/mixins/__init__.py": "# python_service/adapters/mixins/__init__.py\nfrom .debug_mixin import DebugMixin\nfrom .headers_mixin import BrowserHeadersMixin\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n__all__ = [\"DebugMixin\", \"BrowserHeadersMixin\", \"BetfairAuthMixin\"]\n",
  "web_service/backend/adapters/mixins/headers_mixin.py": "# python_service/adapters/mixins/headers_mixin.py\n\"\"\"Mixin for generating browser-like HTTP headers.\"\"\"\n\nfrom typing import Optional\n\nfrom ..constants import (\n    CHROME_SEC_CH_UA,\n    CHROME_USER_AGENT,\n    DEFAULT_BROWSER_HEADERS,\n)\n\n\nclass BrowserHeadersMixin:\n    \"\"\"Mixin that provides browser-like HTTP headers.\"\"\"\n\n    def _get_browser_headers(\n        self,\n        host: Optional[str] = None,\n        referer: Optional[str] = None,\n        *,\n        include_sec_ch: bool = True,\n    ) -> dict:\n        \"\"\"\n        Generate browser-like headers for HTTP requests.\n\n        Args:\n            host: The Host header value\n            referer: The Referer header value\n            include_sec_ch: Whether to include sec-ch-ua headers\n\n        Returns:\n            Dictionary of HTTP headers\n        \"\"\"\n        headers = {\n            **DEFAULT_BROWSER_HEADERS,\n            \"User-Agent\": CHROME_USER_AGENT,\n        }\n\n        if host:\n            headers[\"Host\"] = host\n\n        if referer:\n            headers[\"Referer\"] = referer\n\n        if include_sec_ch:\n            headers.update({\n                \"sec-ch-ua\": CHROME_SEC_CH_UA,\n                \"sec-ch-ua-mobile\": \"?0\",\n                \"sec-ch-ua-platform\": '\"Windows\"',\n            })\n\n        return headers\n",
  "web_service/backend/adapters/mixins/betfair_auth_mixin.py": "# python_service/adapters/mixins/betfair_auth_mixin.py\n\"\"\"Betfair authentication mixin with improved error handling.\"\"\"\n\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Tuple\n\nimport httpx\nimport structlog\n\nfrom ...credentials_manager import SecureCredentialsManager\nfrom ...core.exceptions import AuthenticationError\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock: asyncio.Lock = None\n\n    # Configuration\n    AUTH_URL = \"https://identitysso.betfair.com/api/login\"\n    TOKEN_VALIDITY_HOURS = 3\n    TOKEN_REFRESH_BUFFER_MINUTES = 5\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        cls._auth_lock = asyncio.Lock()\n\n    @property\n    def _is_token_valid(self) -> bool:\n        \"\"\"Check if current token is still valid with buffer time.\"\"\"\n        if not self.session_token or not self.token_expiry:\n            return False\n        buffer = timedelta(minutes=self.TOKEN_REFRESH_BUFFER_MINUTES)\n        return self.token_expiry > (datetime.now() + buffer)\n\n    async def _authenticate(self, http_client: httpx.AsyncClient) -> bool:\n        \"\"\"\n        Authenticates with Betfair using credentials from the system's credential manager.\n\n        Returns:\n            True if authentication succeeded, False otherwise\n\n        Raises:\n            AuthenticationError: If credentials are missing\n        \"\"\"\n        async with self._auth_lock:\n            if self._is_token_valid:\n                return True\n\n            log.info(\"Attempting to authenticate with Betfair...\")\n\n            try:\n                username, password = SecureCredentialsManager.get_betfair_credentials()\n            except Exception as e:\n                log.error(\"Failed to retrieve Betfair credentials\", error=str(e))\n                raise AuthenticationError(\"Betfair\", \"Credentials not available\") from e\n\n            app_key = getattr(self.config, \"BETFAIR_APP_KEY\", None)\n            if not all([app_key, username, password]):\n                raise AuthenticationError(\n                    \"Betfair\",\n                    \"Incomplete credentials: app_key, username, or password missing\"\n                )\n\n            headers = {\n                \"X-Application\": app_key,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={username}&password={password}\"\n\n            try:\n                response = await http_client.post(\n                    self.AUTH_URL, headers=headers, content=payload, timeout=20\n                )\n                response.raise_for_status()\n                data = response.json()\n\n                if data.get(\"status\") == \"SUCCESS\":\n                    self.session_token = data.get(\"token\")\n                    self.token_expiry = datetime.now() + timedelta(\n                        hours=self.TOKEN_VALIDITY_HOURS\n                    )\n                    log.info(\"Betfair authentication successful.\")\n                    return True\n                else:\n                    log.error(\"Betfair authentication failed\", error=data.get(\"error\"))\n                    self.session_token = None\n                    self.token_expiry = None\n                    return False\n\n            except httpx.HTTPError as e:\n                log.error(\"Betfair authentication HTTP error\", error=str(e))\n                self.session_token = None\n                self.token_expiry = None\n                return False\n\n    def _get_authenticated_headers(self) -> dict:\n        \"\"\"Get headers with authentication token for API requests.\"\"\"\n        if not self.session_token:\n            raise AuthenticationError(\"Betfair\", \"No valid session token\")\n\n        return {\n            \"X-Application\": getattr(self.config, \"BETFAIR_APP_KEY\", \"\"),\n            \"X-Authentication\": self.session_token,\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n        }\n",
  "web_service/backend/adapters/mixins/debug_mixin.py": "# python_service/adapters/mixins/debug_mixin.py\n\"\"\"Mixin for debug HTML saving functionality.\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\n\nclass DebugMixin:\n    \"\"\"Mixin that provides debug HTML saving capabilities.\"\"\"\n\n    DEBUG_OUTPUT_DIR: str = \"debug_output\"\n\n    def _save_debug_html(\n        self,\n        content: str,\n        filename: str,\n        *,\n        enabled: bool = True,\n        subdirectory: Optional[str] = None,\n    ) -> Optional[Path]:\n        \"\"\"\n        Save HTML content to a debug file for CI/debugging purposes.\n\n        Args:\n            content: The HTML content to save\n            filename: Base filename (without extension)\n            enabled: Whether debug saving is enabled\n            subdirectory: Optional subdirectory within debug output dir\n\n        Returns:\n            Path to saved file, or None if saving failed/disabled\n        \"\"\"\n        if not enabled:\n            return None\n\n        try:\n            output_dir = Path(self.DEBUG_OUTPUT_DIR)\n            if subdirectory:\n                output_dir = output_dir / subdirectory\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n            filepath = output_dir / f\"{filename}.html\"\n            filepath.write_text(content, encoding=\"utf-8\")\n            log.debug(\"Saved debug HTML\", path=str(filepath), size=len(content))\n            return filepath\n        except (OSError, IOError) as e:\n            log.warning(\"Failed to save debug HTML\", filename=filename, error=str(e))\n            return None\n"
}
