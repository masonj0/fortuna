{
    ".github/actions/run-asgi-diagnostics/action.yml": "name: 'Run ASGI Import Killer Diagnostics'\ndescription: 'Runs a multi-phase diagnostic to verify Python ASGI application imports.'\n\ninputs:\n  python-version:\n    description: 'The version of Python to use.'\n    required: true\n  backend-dir:\n    description: 'The directory of the backend service to test.'\n    required: true\n  backend-module-path:\n    description: 'The Python module path for the backend service (e.g., web_service.backend).'\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n      - name: \u2699\ufe0f Setup Python (EXACT VERSION)\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ inputs.python-version }}\n\n      - name: \ud83d\udccb Capture Python Info\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          Write-Host \"Python executable: $(which python)\" -ForegroundColor Cyan\n          python --version\n          python -m site\n          python -c \"import sys; print('Prefix:', sys.prefix); print('Base prefix:', sys.base_prefix)\"\n\n      - name: \ud83d\udce5 Install Requirements (Exactly as Backend Build)\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel --quiet\n\n          Write-Host \"Installing requirements.txt...\" -ForegroundColor Cyan\n          pip install -r (Join-Path \"${{ inputs.backend-dir }}\" \"requirements.txt\") -v 2>&1 | Tee-Object \"install-requirements.log\"\n\n          if (Test-Path (Join-Path \"${{ inputs.backend-dir }}\" \"requirements-dev.txt\")) {\n            Write-Host \"Installing requirements-dev.txt...\" -ForegroundColor Cyan\n            pip install -r (Join-Path \"${{ inputs.backend-dir }}\" \"requirements-dev.txt\") -v 2>&1 | Tee-Object -Append \"install-requirements.log\"\n          }\n\n          if ($LASTEXITCODE -ne 0) {\n            Write-Host \"\u274c pip install failed\" -ForegroundColor Red\n            exit 1\n          }\n\n          Write-Host \"\u2705 All dependencies installed\" -ForegroundColor Green\n\n      - name: \ud83d\udce6 Capture Installed Packages\n        shell: pwsh\n        run: |\n          pip list | Tee-Object \"installed-packages.txt\"\n          pip freeze | Tee-Object \"pip-freeze.txt\"\n\n      - name: \ud83d\udc0d Set PYTHONPATH\n        shell: pwsh\n        run: |\n          echo \"PYTHONPATH=${{ github.workspace }}\" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n          Write-Host \"PYTHONPATH set to ${{ github.workspace }}\"\n\n      - name: \ud83e\uddea PHASE 1 System Imports\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 1 SYSTEM-LEVEL IMPORTS\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('os', 'filesystem'), ('sys', 'system'), ('json', 'serialization'),\",\n            \"    ('asyncio', 'async I/O'), ('pathlib', 'paths'), ('typing', 'type hints'),\",\n            \"    ('importlib', 'import utilities')\",\n            ']',\n            'failed = []',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:20} [{desc}]\")',\n            '    except ImportError as e:',\n            '        print(f\"\u274c {mod_name:20} ImportError: {e}\")',\n            '        failed.append(mod_name)',\n            'if failed:',\n            '    print(f\"\\n\u274c {len(failed)} system imports failed\")',\n            '    sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 1 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 2 Web Framework Core\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'import traceback',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 2 WEB FRAMEWORK CORE\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('fastapi', 'web framework'),\",\n            \"    ('uvicorn', 'ASGI server'),\",\n            \"    ('starlette', 'ASGI toolkit'),\",\n            \"    ('starlette.applications', 'ASGI app'),\",\n            \"    ('starlette.routing', 'routing'),\",\n            ']',\n            'failed = []',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except ImportError as e:',\n            '        print(f\"\u274c {mod_name:30} ImportError: {e}\")',\n            '        failed.append((mod_name, str(e)))',\n            '    except Exception as e:',\n            '        print(f\"\u26a0\ufe0f  {mod_name:30} {type(e).__name__}: {e}\")',\n            'if failed:',\n            '    print(f\"\\n\u274c {len(failed)} core framework imports failed\")',\n            '    for mod, err in failed:',\n            '        print(f\"  - {mod}\")',\n            '    sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 2 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 3 Pydantic & Data Validation\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 3 PYDANTIC & DATA VALIDATION\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('pydantic', 'validation'),\",\n            \"    ('pydantic_core', 'core'),\",\n            \"    ('pydantic_settings', 'settings'),\",\n            ']',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except Exception as e:',\n            '        print(f\"\u274c {mod_name:30} {type(e).__name__}: {e}\")',\n            '        sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 3 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 4 Async/IO Utilities\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 4 ASYNC/IO UTILITIES\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('anyio', 'async compat'),\",\n            \"    ('httpcore', 'HTTP core'),\",\n            \"    ('httpx', 'HTTP client'),\",\n            \"    ('aiosqlite', 'async DB'),\",\n            ']',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except Exception as e:',\n            '        print(f\"\u274c {mod_name:30} {type(e).__name__}: {e}\")',\n            '        sys.exit(1)',\n            'print(f\"\\n\u2705 Phase 4 complete\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) { exit 1 }\n\n      - name: \ud83e\uddea PHASE 5 Optional Dependencies (non-critical)\n        shell: pwsh\n        continue-on-error: true\n        run: |\n          $script = @(\n            'import sys',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 5 OPTIONAL DEPENDENCIES (non-critical)\")',\n            'print(\"=\"*80)',\n            'modules = [',\n            \"    ('slowapi', 'rate limiting'),\",\n            \"    ('structlog', 'logging'),\",\n            \"    ('tenacity', 'retries'),\",\n            ']',\n            'for mod_name, desc in modules:',\n            '    try:',\n            '        __import__(mod_name)',\n            '        print(f\"\u2705 {mod_name:30} [{desc}]\")',\n            '    except Exception as e:',\n            '        print(f\"\u26a0\ufe0f  {mod_name:30} not critical: {type(e).__name__}\")',\n            'print(f\"\\n\u2705 Phase 5 complete (warnings OK)\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n\n      - name: \ud83e\uddea PHASE 6 Application Directory Structure\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n\n          $script = @(\n            'import os',\n            'from pathlib import Path',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 6 APPLICATION DIRECTORY STRUCTURE\")',\n            'print(\"=\"*80)',\n            'cwd = Path.cwd()',\n            'backend_dir = cwd / \"${{ inputs.backend-dir }}\"',\n            'print(f\"\\nCurrent directory: {cwd}\")',\n            'print(f\"\\nBackend directory exists: {backend_dir.exists()}\")',\n            'if backend_dir.exists():',\n            '    print(f\"  Contents:\")',\n            '    for item in backend_dir.iterdir():',\n            '        print(f\"    - {item.name}\")',\n            '    main_py = backend_dir / \"main.py\"',\n            '    api_py = backend_dir / \"api.py\"',\n            '    print(f''\\n  main.py: {main_py.stat().st_size if main_py.exists() else \"N/A\"} bytes'')',\n            '    print(f''  api.py: {api_py.stat().st_size if api_py.exists() else \"N/A\"} bytes'')'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n\n      - name: \ud83e\uddea PHASE 7 CRITICAL - Application Module Imports\n        shell: pwsh\n        run: |\n          $script = @(\n            'import sys',\n            'import traceback',\n            'import importlib',\n            'from pathlib import Path',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"PHASE 7 APPLICATION MODULE IMPORTS (CRITICAL)\")',\n            'print(\"=\"*80)',\n            'backend_module_path = \"${{ inputs.backend-module-path }}\"',\n            'print(f\"\\n[Step 1] Dynamically importing module: {backend_module_path}\")',\n            'try:',\n            '    backend_module = importlib.import_module(backend_module_path)',\n            '    print(f\"\u2705 {backend_module_path} imported successfully\")',\n            'except Exception as e:',\n            '    print(f\"\u274c FATAL: {backend_module_path} import failed\")',\n            '    print(f\"   Error: {type(e).__name__}: {e}\")',\n            '    traceback.print_exc()',\n            '    sys.exit(1)',\n            'print(''\\\\n[Step 2] Retrieving \"app\" object from the API submodule...'')',\n            'api_module_path = f\"{backend_module_path}.api\"',\n            'try:',\n            '    api_module = importlib.import_module(api_module_path)',\n            '    app = getattr(api_module, \"app\")',\n            '    print(f\"\u2705 app object retrieved from {api_module_path}\")',\n            '    print(f\"   Type: {type(app)}\")',\n            '    print(f\"   Class: {app.__class__.__name__}\")',\n            '    print(f\"   Module: {app.__class__.__module__}\")',\n            'except (ImportError, AttributeError) as e:',\n            '    print(f\"\u274c FATAL: Could not get app object from {api_module_path}\")',\n            '    print(f\"   Error: {type(e).__name__}: {e}\")',\n            '    traceback.print_exc()',\n            '    sys.exit(1)',\n            'print(\"\\n\" + \"=\"*80)',\n            'print(\"\u2705 ALL APPLICATION IMPORTS SUCCESSFUL\")',\n            'print(\"=\"*80)',\n            'print(\"\\nThe ASGI app is fully importable.\")',\n            'print(\"Uvicorn should be able to load it successfully.\")'\n          )\n          $script | Out-File -FilePath \"diag_script.py\" -Encoding utf8\n          python diag_script.py\n          if ($LASTEXITCODE -ne 0) {\n            Write-Host \"\u274c APPLICATION IMPORT TEST FAILED\" -ForegroundColor Red\n            exit 1\n          }\n\n      - name: \ud83d\udccb Generate ASGI Diagnostic Report\n        if: always()\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $report = @()\n          $report += \"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\"\n          $report += \"\u2551              ASGI IMPORT KILLER - DIAGNOSTIC REPORT                        \u2551\"\n          $report += \"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\"\n          $report += \"\"\n          $report += \"Timestamp: $(Get-Date -Format 'o')\"\n          $report += \"Python: $(python --version)\"\n          $report += \"\"\n          $report += \"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\"\n          $result = if ($LASTEXITCODE -eq 0) { 'PASS \u2705' } else { 'FAIL \u274c' }\n          $report += \"\u2502 RESULT: $result \u2502\"\n          $report += \"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\"\n          $report += \"\"\n          $report += \"If this passed:\"\n          $report += \"  \u2705 All required dependencies are installed\"\n          $report += \"  \u2705 python_service.main is importable\"\n          $report += \"  \u2705 FastAPI app is accessible\"\n          $report += \"  \u2705 The executable should work\"\n          $report += \"  \u2705 Uvicorn WILL be able to load the app\"\n          $report += \"\"\n          $report += \"If this failed:\"\n          $report += \"  \u274c See error output above for the exact problem\"\n          $report += \"  \u274c Fix the import error in your code\"\n          $report += \"  \u274c Common issues:\"\n          $report += \"     - Missing dependency in requirements.txt\"\n          $report += \"     - Syntax error in api.py or main.py\"\n          $report += \"     - Circular import in api.py\"\n          $report += \"     - api.py imports a module that fails\"\n          $report += \"\"\n          $report += \"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\"\n          $report | Tee-Object \"asgi-diagnostic-report.txt\"\n\n      - name: \ud83d\udce4 Upload Diagnostic Artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: asgi-import-diagnostics-${{ github.run_id }}\n          path: |\n            install-requirements.log\n            installed-packages.txt\n            pip-freeze.txt\n            asgi-diagnostic-report.txt\n          retention-days: 30\n          if-no-files-found: warn\n",
    ".github/workflows/build-msi-hattrickfusion-ultimate.yml": "# System Timestamp: 2024-05-21 12:00:00\nname: \ud83c\udfa9 HatTrick Fusion (Ultimate)\non:\n  workflow_dispatch:\n  push:\n    branches: [\"main\"]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.11' # \ud83d\ude80 DOWNGRADE\n  DOTNET_VERSION: '8.0.x'\n  WIX_VERSION: '4.0.5'\n  SERVICE_PORT: '8102'\n  FRONTEND_PORT: '3000'\n  MSI_NAME: 'HatTrickFusion.msi'\n  FIREWALL_RULE: 'HatTrickFusion-Port'\n  UPGRADE_CODE: 'FA689549-366B-4C5C-A482-1132F9A34B10'\n  FORTUNA_PORT: '8102'\n  # Mock API keys for service startup\n  API_KEY: mock_key\n  TVG_API_KEY: mock\n  GREYHOUND_API_URL: http://mock\n  FORTUNA_ENV: smoke-test\n\njobs:\n  # 1. Build Frontend First (so Backend can bundle it)\n  build-frontend:\n    name: Build Frontend\n    runs-on: windows-latest\n    timeout-minutes: 30\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '**/package-lock.json'\n      - name: Cache Build Output\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: web_platform/frontend/out\n          key: ${{ runner.os }}-frontend-${{ hashFiles('**/package-lock.json', 'web_platform/frontend/**/*.js', 'web_platform/frontend/**/*.ts', 'web_platform/frontend/**/*.tsx', 'web_platform/frontend/**/*.css') }}\n          restore-keys: |\n            ${{ runner.os }}-frontend-\n      - name: Install and Build\n        if: steps.cache-frontend.outputs.cache-hit != 'true'\n        run: |\n          cd web_platform/frontend\n          npm ci --prefer-offline --no-audit --no-fund\n          npm run build\n      - name: Generate Artifact Manifest\n        shell: pwsh\n        working-directory: web_platform/frontend\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"out\"\n          $manifestPath = \"frontend-manifest.tsv\"\n          \"RelativePath`tSizeBytes`tSHA256\" | Out-File $manifestPath -Encoding utf8\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          foreach ($f in $files) {\n            $rel = $f.FullName.Substring($outDir.Path.Length).TrimStart('\\','/')\n            $hash = (Get-FileHash $f.FullName -Algorithm SHA256).Hash.Substring(0,16)\n            \"$rel`t$($f.Length)`t$hash\" | Out-File $manifestPath -Encoding utf8 -Append\n          }\n          Write-Host \"\u2705 Generated frontend artifact manifest.\"\n      - name: Upload Frontend\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build\n          path: |\n            web_platform/frontend/out\n            web_platform/frontend/frontend-manifest.tsv\n\n  # 2. Build Backend (Downloads Frontend to bundle it)\n  backend-quality:\n    name: '\ud83e\uddef Backend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 20\n    needs: build-frontend\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: web_service/backend/requirements-dev.txt\n      - name: Install Dependencies\n        run: |\n          pip install -r web_service/backend/requirements-dev.txt\n      - name: Run Pytest\n        run: |\n          pytest web_service/backend/tests\n  build-backend:\n    name: '\ud83d\udc0d Build Backend (${{ matrix.arch }})'\n    runs-on: windows-latest\n    needs: [build-frontend, backend-quality]\n    timeout-minutes: 30\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    outputs:\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/download-artifact@v4\n        with:\n          name: frontend-build\n          path: web_platform/frontend/out\n      - id: meta\n        shell: pwsh\n        run: |\n          $ver = if (\"${{ github.ref }}\" -match 'refs/tags/v(.*)') { $Matches[1] } else { \"0.0.${{ github.run_number }}\" }\n          \"semver=$ver\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          $shortSha = \"${{ github.sha }}\".Substring(0,7)\n          \"short_sha=$shortSha\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_dir=web_service/backend\" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"module_path=web_service.backend\" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          architecture: ${{ matrix.arch }}\n          cache: 'pip'\n      - name: \ud83e\uddfe Create Architecture Constraints\n        id: constraints\n        shell: pwsh\n        run: |\n          $constraintDir = \"temp-constraints\"\n          New-Item -ItemType Directory -Path $constraintDir -Force | Out-Null\n          $constraintFile = Join-Path $constraintDir \"constraint-${{ matrix.arch }}.txt\"\n          if ('${{ matrix.arch }}' -eq 'x86') {\n            # \ud83d\ude80 GPT-5 SAFE MODE\n            \"numpy==1.23.5`r`npandas==1.5.3\" | Set-Content $constraintFile\n          } else { New-Item $constraintFile -ItemType File -Force }\n          \"file=$constraintFile\" | Out-File $env:GITHUB_OUTPUT -Append\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r web_service/backend/requirements.txt -c ${{ steps.constraints.outputs.file }}\n          pip install pyinstaller==6.6.0 pywin32\n      - name: Build Backend (PyInstaller)\n        run: |\n          pyinstaller --noconfirm --onedir --clean --name fortuna-backend --hidden-import=win32timezone --hidden-import=win32serviceutil --add-data \"web_service/backend;backend\" --add-data \"web_platform/frontend/out;ui\" web_service/backend/service_entry.py\n      - name: Upload Backend\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-dist-${{ matrix.arch }}\n          path: dist/fortuna-backend/\n\n  package-msi:\n    name: '\ud83d\udcbf Package MSI (${{ matrix.arch }})'\n    runs-on: windows-latest\n    needs: build-backend\n    timeout-minutes: 30\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/download-artifact@v4\n        with:\n          name: backend-dist-${{ matrix.arch }}\n          path: staging/backend\n      # ---------------------------------------------------------\n      # \ud83d\udc89 INJECT: THE DIETICIAN (Subroutine #3)\n      # ---------------------------------------------------------\n      - name: '\u2696\ufe0f The Dietician (Size Analysis)'\n        shell: pwsh\n        run: |\n          $stagingDir = \"staging\" # Adjust if your staging folder is named differently\n          if (Test-Path $stagingDir) {\n            $size = (Get-ChildItem $stagingDir -Recurse | Measure-Object -Property Length -Sum).Sum / 1MB\n            $sizeRounded = [math]::Round($size, 2)\n            Write-Host \"\ud83d\udcca Total Payload Size: $sizeRounded MB\"\n\n            if ($size -gt 300) {\n              Write-Warning \"\u26a0\ufe0f BLOAT ALERT: Payload exceeds 300MB!\"\n            }\n          } else {\n            Write-Warning \"Staging directory not found, skipping diet check.\"\n          }\n      - name: Create Restart Service Batch Script\n        shell: pwsh\n        run: |\n          $scriptContent = \"@echo off`r`necho Requesting Admin privileges to restart FortunaWebService...`r`nnet stop FortunaWebService`r`nnet start FortunaWebService`r`necho Service Restarted.`r`npause\"\n          Set-Content -Path \"staging/backend/restart_service.bat\" -Value $scriptContent -Encoding Ascii\n          Write-Host \"\u2705 Created restart_service.bat script.\"\n      - uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n      - name: \ud83d\udcc4 Ensure WiX License Exists\n        shell: pwsh\n        run: |\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            $rtfContent = [System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String(\"e1xydGYxXGFuc2lcZGVmZjB7XGZvbnR0Ymx7XGYwIEFyaWFsO319XGYwXGZzMjQgRU5EIFVTRVIgTElDRU5TRSBBR1JFRU1FTlRccGFyXHBhciBUaGlzIGlzIGEgcGxhY2Vob2xkZXIgbGljZW5zZSBmb3IgRm9ydHVuYSBGYXVjZXQuIFBsZWFzZSByZXBsYWNlIHdpdGggYWN0dWFsIHRlcm1zLn0=\"))\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: Prepare WiX\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          Copy-Item build_wix/Product_WithService.wxs build_wix/Product.wxs -Force\n          $wxsPath = 'build_wix/Product.wxs'\n          $wxsContent = [xml](Get-Content $wxsPath -Raw)\n          $serviceControl = $wxsContent.SelectSingleNode(\"//*[local-name()='ServiceControl']\")\n          if ($serviceControl -and $serviceControl.HasAttribute(\"Start\")) {\n              $serviceControl.RemoveAttribute(\"Start\")\n              $wxsContent.Save($wxsPath)\n              Write-Host \"\u2705 Dynamically removed 'Start=install' attribute from WiX template.\"\n          }\n          if (Test-Path staging/backend/fortuna-backend.exe) {\n            Move-Item staging/backend/fortuna-backend.exe staging/backend/fortuna-webservice.exe -Force\n          }\n          $proj = @(\n            '<Project Sdk=\"WixToolset.Sdk/${{ env.WIX_VERSION }}\">',\n            '  <PropertyGroup>',\n            '    <EnableDefaultCompileItems>false</EnableDefaultCompileItems>',\n            '    <OutputType>Package</OutputType>',\n            '    <Platforms>x64;x86</Platforms>',\n            '    <DefineConstants>Version=$(Version);SourceDir=$(SourceDir);ServicePort=$(ServicePort)</DefineConstants>',\n            '  </PropertyGroup>',\n            '  <ItemGroup>',\n            '    <PackageReference Include=\"WixToolset.UI.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Firewall.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Util.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '  </ItemGroup>',\n            '  <ItemGroup>',\n            '    <Compile Include=\"Product.wxs\" />',\n            '  </ItemGroup>',\n            '</Project>'\n          )\n          Set-Content build_wix/Fortuna.wixproj ($proj -join \"`r`n\") -Encoding utf8\n      - name: Build MSI\n        working-directory: build_wix\n        run: |\n          dotnet build Fortuna.wixproj -c Release -p:Platform=${{ matrix.arch }} -p:Version=\"${{ needs.build-backend.outputs.semver }}\" -p:SourceDir=\"../staging/backend\" -p:ServicePort=\"${{ env.SERVICE_PORT }}\"\n      - name: Rename & Hash MSI\n        run: |\n          $ver = \"${{ needs.build-backend.outputs.semver }}\"\n          $sha = \"${{ needs.build-backend.outputs.short_sha }}\"\n          $releaseDir = \"build_wix/bin/${{ matrix.arch }}/Release\"\n          $msiFound = Get-ChildItem -Path $releaseDir -Filter \"*.msi\" | Select-Object -First 1\n          if (-not $msiFound) { throw \"MSI not found in $releaseDir\" }\n          $targetName = \"HatTrickFusion-${{ matrix.arch }}-${ver}-${sha}.msi\"\n          $newPath = Join-Path $releaseDir $targetName\n          Move-Item -Path $msiFound.FullName -Destination $newPath -Force\n          Write-Host \"\u2705 MSI Ready: $targetName\"\n      - name: Upload MSI\n        uses: actions/upload-artifact@v4\n        with:\n          name: hat-trick-msi-${{ matrix.arch }}\n          path: build_wix/bin/${{ matrix.arch }}/Release/*\n\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test (${{ matrix.arch }})'\n    runs-on: windows-latest\n    needs: package-msi\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: hat-trick-msi-${{ matrix.arch }}\n          path: msi-installer\n      - name: \ud83d\udd25 Smoke Test\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $ErrorActionPreference = \"Stop\"\n          $msiPath = (Get-ChildItem -Path \"msi-installer\" -Filter \"*.msi\" -Recurse | Select-Object -First 1).FullName\n          if (-not $msiPath) { throw \"MSI not found!\" }\n          Write-Host \"Found MSI at $msiPath\"\n          # 1. PRE-INSTALL CLEANUP\n          Write-Host \"Attempting pre-emptive service removal...\"\n          $service = Get-Service -Name \"FortunaWebService\" -ErrorAction SilentlyContinue\n          if ($service) {\n            sc.exe delete \"FortunaWebService\"\n            Start-Sleep -Seconds 5\n          }\n          # 2. INSTALL\n          $logFile = \"msi-install.log\"\n          $msiArgs = \"/i `\"$msiPath`\" /qn /L*v `\"$logFile`\"\"\n          Write-Host \"Running: msiexec.exe $msiArgs\"\n          $proc = Start-Process msiexec.exe -ArgumentList $msiArgs -Wait -PassThru\n          if ($proc.ExitCode -ne 0 -and $proc.ExitCode -ne 3010) {\n            Get-Content $logFile -Tail 50\n            throw \"MSI installation failed with exit code $($proc.ExitCode).\"\n          }\n          Write-Host \"\u2705 MSI Installation successful.\"\n          # 3. VERIFY & RUN\n          $progFiles = ${env:ProgramFiles}\n          if ('${{ matrix.arch }}' -eq 'x86') { $progFiles = ${env:ProgramFiles(x86)} }\n          $installDir = Join-Path $progFiles \"Fortuna Faucet Service\"\n          if (-not (Test-Path $installDir)) { throw \"Installation directory not found!\" }\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"data\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"json\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"logs\") -Force | Out-Null\n          Start-Service -Name \"Fortuna Faucet Service\"\n          Start-Sleep -Seconds 10\n          # 4. HEALTH CHECK\n          $maxRetries = 5\n          $delay = 5\n          $success = $false\n          For ($i=0; $i -lt $maxRetries; $i++) {\n            try {\n              $response = Invoke-WebRequest -Uri \"http://localhost:${{ env.SERVICE_PORT }}/health\" -UseBasicParsing\n              if ($response.StatusCode -eq 200) {\n                Write-Host \"\u2705 Health check PASSED.\"\n                $success = $true\n                break\n              }\n            } catch { Write-Host \"Attempt $($i+1) failed. Retrying in $delay seconds...\" }\n            Start-Sleep -Seconds $delay\n          }\n          if (-not $success) {\n            throw \"Health check failed after $maxRetries attempts.\"\n          }\n      # ---------------------------------------------------------\n      # \ud83d\udc89 INJECT: PAPARAZZI (Subroutine #4)\n      # ---------------------------------------------------------\n      - name: '\ud83d\udcf8 Paparazzi (Visual Proof)'\n        if: success()\n        shell: pwsh\n        run: |\n          npm install playwright\n          npx playwright install chromium\n\n          # Ensure we hit the local service\n          $url = \"http://127.0.0.1:${{ env.SERVICE_PORT }}/docs\"\n\n          $nodeScript = \"const { chromium } = require('playwright'); (async () => { const browser = await chromium.launch(); const page = await browser.newPage(); try { await page.goto('$url', { timeout: 10000 }); await page.screenshot({ path: 'proof-of-life.png', fullPage: true }); console.log('\u2705 Screenshot captured!'); } catch (e) { console.error('\u274c Failed to capture screenshot:', e); process.exit(1); } await browser.close(); })();\"\n          node -e $nodeScript\n\n      - name: \ud83d\udce4 Upload Visual Proof\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: visual-proof-${{ matrix.arch || 'x64' }}\n          path: proof-of-life.png\n      - name: \ud83e\uddf9 Stop Service\n        if: always()\n        shell: pwsh\n        run: |\n          Stop-Service -Name \"FortunaWebService\" -ErrorAction SilentlyContinue\n\n  generate-sbom:\n    name: '\ud83d\udcdc Generate SBOM (${{ matrix.arch }})'\n    runs-on: ubuntu-latest\n    needs: [build-backend]\n    timeout-minutes: 30\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: backend-dist-${{ matrix.arch }}\n          path: backend\n      - name: Create SBOM\n        uses: anchore/sbom-action@v0\n        with:\n          path: backend\n          output-file: \"sbom-${{ matrix.arch }}.spdx.json\"\n          format: spdx-json\n      - name: Upload SBOM\n        uses: actions/upload-artifact@v4\n        with:\n          name: sbom-${{ matrix.arch }}\n          path: \"sbom-${{ matrix.arch }}.spdx.json\"\n\n  release:\n    name: '\ud83d\ude80 Create Release'\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/')\n    needs: [smoke-test, generate-sbom]\n    timeout-minutes: 30\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          pattern: hat-trick-msi-*\n          path: assets\n          merge-multiple: true\n      - uses: actions/download-artifact@v4\n        with:\n          pattern: sbom-*\n          path: assets\n          merge-multiple: true\n      - name: Generate Checksums\n        run: |\n          cd assets\n          sha256sum * > SHASUMS256.txt\n      - name: Publish Release\n        uses: softprops/action-gh-release@v1\n        with:\n          files: |\n            assets/*\n          draft: false\n          prerelease: false\n",
    ".github/workflows/build-web-service-msi-jules.yml": "# System Timestamp: 2024-05-21 12:00:00\nname: \ud83d\udd2c Build Fortuna Faucet Web Service Installer (Synthesized Overkill)\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: read\n  actions: read\n  checks: read\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\ndefaults:\n  run:\n    shell: pwsh\n\nenv:\n  NODE_VERSION: '20'\n  PYTHON_VERSION: '3.11' # \ud83d\ude80 DOWNGRADE\n  DOTNET_VERSION: '8.0.x'\n  PYTHONUTF8: '1'\n  PIP_DISABLE_PIP_VERSION_CHECK: '1'\n  PIP_NO_PYTHON_VERSION_WARNING: '1'\n  NPM_CONFIG_FUND: 'false'\n  NPM_CONFIG_AUDIT: 'false'\n  FORCE_COLOR: '3'\n  FRONTEND_DIR: 'web_platform/frontend'\n  FRONTEND_BUILD_DIR: 'web_platform/frontend/out'\n  WIX_DIR: 'build_wix'\n  SERVICE_PORT: '8102'\n  HEALTH_ENDPOINT: '/health'\n  API_KEY: ${{ secrets.TEST_API_KEY }}\n  TVG_API_KEY: \"mock_key\"\n  GREYHOUND_API_URL: \"http://mock\"\n  FORTUNA_ENV: \"smoke-test\"\n  MSI_STAGING_DIR: 'build_wix/staging'\n  MSI_OUTPUT_DIR: 'dist'\n  WIX_VERSION: '4.0.5'\n\njobs:\n  path-finder:\n    name: '\ud83d\udd0e Path Finder Dynamic Backend Detection'\n    runs-on: windows-latest\n    outputs:\n      backend_dir: ${{ steps.find-path.outputs.backend_dir }}\n      backend_module_path: ${{ steps.find-path.outputs.backend_module_path }}\n      spec_file: ${{ steps.find-path.outputs.spec_file }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Detect Backend Path\n        id: find-path\n        run: |\n          Set-StrictMode -Version Latest\n          $web_service_path = \"web_service/backend\"\n          $python_service_path = \"python_service\"\n          $backend_dir = \"\"\n          $backend_module_path = \"\"\n          $spec_file = \"\"\n\n          Write-Host \"--- Path Finding Forensics ---\"\n          Write-Host \"Searching for the correct backend service directory...\"\n\n          # Test web_service path\n          $web_service_main = Test-Path (Join-Path $web_service_path \"main.py\")\n          $web_service_api = Test-Path (Join-Path $web_service_path \"api.py\")\n          $web_service_init = Test-Path (Join-Path $web_service_path \"__init__.py\")\n          Write-Host \"Checking '$web_service_path':\"\n          Write-Host \"  main.py -> $web_service_main\"\n          Write-Host \"  api.py -> $web_service_api\"\n          Write-Host \"  __init__.py -> $web_service_init\"\n\n          # Test python_service path\n          $python_service_main = Test-Path (Join-Path $python_service_path \"main.py\")\n          $python_service_api = Test-Path (Join-Path $python_service_path \"api.py\")\n          $python_service_init = Test-Path (Join-Path $python_service_path \"__init__.py\")\n          Write-Host \"Checking '$python_service_path':\"\n          Write-Host \"  main.py -> $python_service_main\"\n          Write-Host \"  api.py -> $python_service_api\"\n          Write-Host \"  __init__.py -> $python_service_init\"\n\n          if ($web_service_main -and $web_service_api -and $web_service_init) {\n            $backend_dir = $web_service_path\n            $backend_module_path = \"web_service.backend\"\n            $spec_file = \"jules.spec\"\n            Write-Host \"\u2705 Verdict: Detected 'web_service/backend' as the target.\" -ForegroundColor Green\n          } elseif ($python_service_main -and $python_service_api -and $python_service_init) {\n            $backend_dir = $python_service_path\n            $backend_module_path = \"python_service\"\n            $spec_file = \"fortuna-backend-webservice.spec\"\n            Write-Host \"\u2705 Verdict: Detected 'python_service' as the target.\" -ForegroundColor Green\n          } else {\n            Write-Host \"\u274c FATAL: Could not determine a valid backend directory. Neither 'web_service/backend' nor 'python_service' contains the required files (main.py, api.py, __init__.py).\" -ForegroundColor Red\n            exit 1\n          }\n\n          Write-Host \"--- Outputs ---\"\n          Write-Host \"backend_dir: $backend_dir\"\n          Write-Host \"backend_module_path: $backend_module_path\"\n          Write-Host \"spec_file: $spec_file\"\n\n          \"backend_dir=$backend_dir\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_module_path=$backend_module_path\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"spec_file=$spec_file\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n  system-check:\n    name: '\u2699\ufe0f System Prerequisites'\n    runs-on: windows-latest\n    timeout-minutes: 5\n    outputs:\n      disk_free_gb: ${{ steps.system.outputs.disk_gb }}\n    steps:\n      - name: Verify Build Tools\n        run: |\n          Set-StrictMode -Version Latest\n          $tools = @('dotnet', 'python', 'node', 'npm', 'git')\n          foreach ($tool in $tools) {\n            Write-Host \"Checking for $($tool)...\"\n            Get-Command $tool -ErrorAction SilentlyContinue\n            if (-not $?) {\n              Write-Host \"\u274c FATAL: Build tool '$tool' not found in PATH.\" -ForegroundColor Red\n              exit 1\n            }\n          }\n          Write-Host \"\u2705 All critical build tools are present.\" -ForegroundColor Green\n      - name: Check Disk Space\n        id: system\n        run: |\n          Set-StrictMode -Version Latest\n          $disk = Get-Volume | Where-Object { $_.DriveLetter -eq 'C' }\n          $freeGB = [math]::Round($disk.SizeRemaining / 1GB, 2)\n          if ($freeGB -lt 10) {\n            Write-Host \"\u26a0\ufe0f WARNING: Low disk space. Only $freeGB GB free (10+ GB recommended).\" -ForegroundColor Yellow\n          } else {\n            Write-Host \"\u2705 Disk space check passed ($freeGB GB free).\" -ForegroundColor Green\n          }\n          \"disk_gb=$freeGB\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n  repo-preflight:\n    name: '\ud83e\uddea Repo Preflight & Integrity'\n    runs-on: windows-latest\n    needs: [path-finder, system-check]\n    timeout-minutes: 5\n    outputs:\n      frontend_lock_hash: ${{ steps.hashes.outputs.frontend_lock_hash }}\n      backend_requirements_hash: ${{ steps.hashes.outputs.backend_requirements_hash }}\n      wix_definition_hash: ${{ steps.hashes.outputs.wix_definition_hash }}\n      semver: ${{ steps.meta.outputs.semver }}\n      short_sha: ${{ steps.meta.outputs.short_sha }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Derive Build Metadata\n        id: meta\n        run: |\n          Set-StrictMode -Version Latest\n          $ref = \"${{ github.ref }}\"\n          if ($ref -like 'refs/tags/v*') {\n            $semver = $ref -replace 'refs/tags/v', ''\n          } else {\n            $semver = \"0.0.${{ github.run_number }}\"\n          }\n          $shortSha = \"${{ github.sha }}\".Substring(0,7)\n          \"semver=$semver\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"short_sha=$shortSha\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\ud83d\udd16 Version: $semver ($shortSha)\"\n\n      - name: Validate Critical Files Exist\n        env:\n          BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n        run: |\n          Set-StrictMode -Version Latest\n          $paths = @(\n            \"${{ env.FRONTEND_DIR }}/package.json\",\n            \"${{ env.FRONTEND_DIR }}/package-lock.json\",\n            (Join-Path $env:BACKEND_DIR \"requirements.txt\"),\n            (Join-Path $env:BACKEND_DIR \"main.py\"),\n            \"${{ env.WIX_DIR }}/Product_WithService.wxs\"\n          )\n          foreach ($path in $paths) {\n            if (-not (Test-Path $path)) {\n              Write-Host \"\u274c FATAL: Required path missing: $path\" -ForegroundColor Red\n              exit 1\n            }\n          }\n          Write-Host \"\u2705 All critical files confirmed.\"\n\n      - name: Capture Integrity Hashes\n        id: hashes\n        env:\n          BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n        run: |\n          Set-StrictMode -Version Latest\n          $frontend = (Get-FileHash \"${{ env.FRONTEND_DIR }}/package-lock.json\" -Algorithm SHA256).Hash\n          $backend = (Get-FileHash (Join-Path $env:BACKEND_DIR \"requirements.txt\") -Algorithm SHA256).Hash\n          $wix = (Get-FileHash \"${{ env.WIX_DIR }}/Product_WithService.wxs\" -Algorithm SHA256).Hash\n          \"frontend_lock_hash=$frontend\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"backend_requirements_hash=$backend\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          \"wix_definition_hash=$wix\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n\n      - name: Upload Integrity Snapshot\n        uses: actions/upload-artifact@v4\n        with:\n          name: repo-preflight-${{ github.run_id }}\n          path: |\n            ${{ env.FRONTEND_DIR }}/package-lock.json\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.WIX_DIR }}/Product_WithService.wxs\n          retention-days: 3\n\n  frontend-quality:\n    name: '\ud83e\uddfc Frontend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 15\n    needs: repo-preflight\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n\n      - name: Cache Frontend Build\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          key: ${{ runner.os }}-frontend-build-${{ hashFiles('${{ env.FRONTEND_DIR }}/**') }}\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm ci --prefer-offline --no-audit --no-fund\n\n      - name: Run Lint (if defined)\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          $pkg = Get-Content package.json -Raw | ConvertFrom-Json\n          if ($pkg.scripts.PSObject.Properties.Name -contains 'lint') {\n            Write-Host \"\ud83e\uddf9 Running npm run lint\"\n            npm run lint\n          } else {\n            Write-Host \"\u2139\ufe0f No lint script defined, skipping.\"\n          }\n\n      - name: Run Tests (if defined)\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          $pkg = Get-Content package.json -Raw | ConvertFrom-Json\n          if ($pkg.scripts.PSObject.Properties.Name -contains 'test') {\n            Write-Host \"\ud83e\uddea Running npm test -- --watch=false\"\n            npm test -- --watch=false\n          } else {\n            Write-Host \"\u2139\ufe0f No test script defined, skipping.\"\n          }\n\n      - name: Security Audit (non-blocking)\n        continue-on-error: true\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm audit --audit-level=critical\n\n  backend-quality:\n    name: '\ud83e\uddef Backend Quality Gates'\n    runs-on: ubuntu-latest\n    timeout-minutes: 20\n    needs: [path-finder, repo-preflight]\n    env:\n      BACKEND_REQUIREMENTS_HASH: ${{ needs.repo-preflight.outputs.backend_requirements_hash }}\n      BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n      BACKEND_SPEC: ${{ needs.path-finder.outputs.spec_file }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n          cache-dependency-path: |\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.BACKEND_DIR }}/requirements-dev.txt\n\n      - name: Cache Backend Build\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-build-${{ hashFiles(format('{0}/**', env.BACKEND_DIR), format('{0}', env.BACKEND_SPEC)) }}\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel\n          pip install -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n          if (Test-Path (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")) {\n            pip install -r (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")\n          }\n\n      - name: Bytecode Compile (Fail Fast)\n        run: |\n          Set-StrictMode -Version Latest\n          python -m compileall -q \"${{ env.BACKEND_DIR }}\"\n\n      - name: Run Pytest (if available)\n        run: |\n          Set-StrictMode -Version Latest\n          python -c 'import importlib.util, sys; sys.exit(0 if importlib.util.find_spec(\"pytest\") else 1)'\n          if ($LASTEXITCODE -eq 0) {\n            Write-Host \"\ud83e\uddea pytest detected, running suite...\"\n            python -m pytest \"${{ env.BACKEND_DIR }}\" --maxfail=1 --disable-warnings\n          } else {\n            Write-Host \"\u2139\ufe0f pytest not installed; skipping tests.\"\n          }\n\n      - name: pip-audit (non-blocking)\n        continue-on-error: true\n        run: |\n          Set-StrictMode -Version Latest\n          pip install pip-audit\n          pip-audit -r (Join-Path $env:BACKEND_DIR \"requirements.txt\")\n\n  sbom:\n    name: '\ud83d\udcc4 SBOM Snapshot'\n    runs-on: ubuntu-latest\n    timeout-minutes: 10\n    needs: repo-preflight\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Generate SBOM (SPDX)\n        uses: anchore/sbom-action@v0\n        with:\n          output-file: sbom.spdx.json\n          format: spdx-json\n\n      - name: Upload SBOM\n        uses: actions/upload-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}\n          path: sbom.spdx.json\n          retention-days: 7\n\n  build-frontend:\n    name: '\ud83d\udce6 Build Frontend'\n    runs-on: windows-latest\n    timeout-minutes: 20\n    needs: [path-finder, repo-preflight, frontend-quality]\n    env:\n      FRONTEND_LOCK_HASH: ${{ needs.repo-preflight.outputs.frontend_lock_hash }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n          cache-dependency-path: '${{ env.FRONTEND_DIR }}/package-lock.json'\n\n      - name: Cache Frontend Build\n        id: cache-frontend\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          key: ${{ runner.os }}-frontend-build-${{ hashFiles('${{ env.FRONTEND_DIR }}/**') }}\n          restore-keys: |\n            ${{ runner.os }}-frontend-build-\n\n      - name: Prime npm Cache\n        uses: actions/cache@v4\n        with:\n          path: ~\\AppData\\Local\\npm-cache\n          key: ${{ runner.os }}-npm-${{ env.NODE_VERSION }}-${{ env.FRONTEND_LOCK_HASH }}\n          restore-keys: |\n            ${{ runner.os }}-npm-${{ env.NODE_VERSION }}-\n\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm ci --prefer-offline --no-audit --no-fund\n\n      - name: Build Frontend\n        if: steps.cache-frontend.outputs.cache-hit != 'true'\n        env:\n          NEXT_PUBLIC_API_URL: http://127.0.0.1:${{ env.SERVICE_PORT }}\n        run: |\n          Set-StrictMode -Version Latest\n          cd \"${{ env.FRONTEND_DIR }}\"\n          npm run build\n\n      - name: Report Cache Status\n        run: |\n          if ('${{ steps.cache-frontend.outputs.cache-hit }}' -eq 'true') {\n            Write-Host \"\u2705 Frontend build restored from cache.\" -ForegroundColor Green\n          } else {\n            Write-Host \"\u2139\ufe0f No cache hit. A new build was performed.\" -ForegroundColor Yellow\n          }\n\n      - name: Verify Build Output\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\"\n          if (-not (Test-Path $outDir)) {\n             Write-Host \"\u274c FATAL: Build directory not found\" -ForegroundColor Red\n             exit 1\n          }\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) {\n             Write-Host \"\u274c FATAL: Build directory empty\" -ForegroundColor Red\n             exit 1\n          }\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n      - name: Generate Artifact Manifest\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $outDir = Resolve-Path \"${{ env.FRONTEND_DIR }}/out\"\n          # Fallback for different env var names in different workflows\n          if (-not (Test-Path $outDir)) { $outDir = Resolve-Path \"${{ env.FRONTEND_BUILD_DIR }}\" }\n\n          if (-not (Test-Path $outDir)) { Write-Error \"\u274c Build failed: 'out' dir missing\"; exit 1 }\n\n          $manifestPath = \"frontend-manifest.tsv\"\n          \"RelativePath`tSizeBytes`tSHA256\" | Out-File $manifestPath -Encoding utf8\n\n          $files = Get-ChildItem -Path $outDir -Recurse -File\n          if ($files.Count -eq 0) { Write-Error \"\u274c Build failed: 'out' dir empty\"; exit 1 }\n\n          Write-Host \"\u2705 Frontend built: $($files.Count) files.\"\n\n          foreach ($f in $files) {\n            # FIX: Changed TrimStart('\\\\\\\\', '/') to TrimStart('\\\\', '/') to prevent char conversion error\n            $rel = $f.FullName.Substring($outDir.Path.Length).TrimStart('\\','/')\n            $hash = (Get-FileHash $f.FullName -Algorithm SHA256).Hash.Substring(0,16)\n            \"$rel`t$($f.Length)`t$hash\" | Out-File $manifestPath -Encoding utf8 -Append\n          }\n\n      - name: Upload Frontend Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}\n          path: ${{ env.FRONTEND_BUILD_DIR }}\n          retention-days: 3\n\n      - name: Upload Manifest\n        uses: actions/upload-artifact@v4\n        with:\n          name: frontend-manifest-${{ github.run_id }}\n          path: frontend-manifest.tsv\n          retention-days: 3\n\n  build-backend:\n    name: '\ud83d\udc0d Build Backend (${{ matrix.arch }})'\n    runs-on: windows-latest\n    timeout-minutes: 25\n    needs: [path-finder, repo-preflight, build-frontend, backend-quality]\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    env:\n      BACKEND_REQUIREMENTS_HASH: ${{ needs.repo-preflight.outputs.backend_requirements_hash }}\n      BUILD_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n      BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n      BACKEND_MODULE_PATH: ${{ needs.path-finder.outputs.backend_module_path }}\n      BACKEND_SPEC: ${{ needs.path-finder.outputs.spec_file }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n      - name: Download Frontend Artifact\n        uses: actions/download-artifact@v4\n        with:\n          name: frontend-build-${{ github.run_id }}\n          path: temp-frontend\n      - name: Cache Backend Build\n        id: cache-backend\n        uses: actions/cache@v4\n        with:\n          path: dist\n          key: ${{ runner.os }}-backend-build-${{ matrix.arch }}-${{ hashFiles(format('{0}/**', env.BACKEND_DIR), format('{0}', env.BACKEND_SPEC)) }}\n          restore-keys: |\n            ${{ runner.os }}-backend-build-${{ matrix.arch }}-\n      - name: Stage Frontend for PyInstaller\n        run: |\n          Set-StrictMode -Version Latest\n          $dest = \"staging/ui\"\n          New-Item -ItemType Directory -Path $dest -Force | Out-Null\n          Copy-Item -Path \"temp-frontend/*\" -Destination $dest -Recurse -Force\n          Write-Host \"\u2705 Frontend staged for inclusion.\"\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          architecture: ${{ matrix.arch }}\n          cache: 'pip'\n          cache-dependency-path: |\n            ${{ env.BACKEND_DIR }}/requirements.txt\n            ${{ env.BACKEND_DIR }}/requirements-dev.txt\n      - name: \ud83e\uddfe Create Architecture Constraints\n        id: constraints\n        shell: pwsh\n        run: |\n          $constraintDir = \"temp-constraints\"\n          New-Item -ItemType Directory -Path $constraintDir -Force | Out-Null\n          $constraintFile = Join-Path $constraintDir \"constraint-${{ matrix.arch }}.txt\"\n          if ('${{ matrix.arch }}' -eq 'x86') {\n            \"numpy==1.23.5`npandas==1.5.3\" | Set-Content $constraintFile\n          } else { New-Item $constraintFile -ItemType File -Force }\n          \"file=$constraintFile\" | Out-File $env:GITHUB_OUTPUT -Append\n      - name: Install Dependencies\n        run: |\n          Set-StrictMode -Version Latest\n          python -m pip install --upgrade pip setuptools wheel\n          $req_file = Join-Path $env:BACKEND_DIR \"requirements.txt\"\n          pip install -r $req_file -c ${{ steps.constraints.outputs.file }}\n          if (Test-Path (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\")) {\n            pip install -r (Join-Path $env:BACKEND_DIR \"requirements-dev.txt\") -c ${{ steps.constraints.outputs.file }}\n          }\n      - name: Create Dynamic webservice.spec for PyInstaller\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        shell: python\n        env:\n          BACKEND_DIR: ${{ needs.path-finder.outputs.backend_dir }}\n          BACKEND_MODULE_PATH: ${{ needs.path-finder.outputs.backend_module_path }}\n          FRONTEND_OUT: ${{ env.FRONTEND_DIR }}/out\n        run: |\n          import os\n          from pathlib import Path\n          bk_dir = os.environ['BACKEND_DIR'].replace('\\\\', '/')\n          mod_path = os.environ['BACKEND_MODULE_PATH']\n          frontend_out = os.environ['FRONTEND_OUT'].replace('\\\\', '/')\n          spec_file = \"jules.spec\"\n          entry = f\"{bk_dir}/service_entry.py\"\n          spec = f\"\"\"\n          # -- mode: python ; coding: utf-8 --\n          from PyInstaller.utils.hooks import collect_data_files, collect_submodules\n          block_cipher = None\n          a = Analysis(\n              ['{entry}'],\n              pathex=[],\n              binaries=[],\n              datas=collect_data_files('uvicorn') + collect_data_files('slowapi') + [('{frontend_out}', 'ui')],\n              hiddenimports=collect_submodules('{mod_path}') + ['win32timezone'],\n              hookspath=[],\n              runtime_hooks=[],\n              excludes=['tests', 'pytest'],\n              win_no_prefer_redirects=False,\n              win_private_assemblies=False,\n              cipher=block_cipher,\n              noarchive=False,\n          )\n          pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n          exe = EXE(\n              pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n              name='fortuna-backend', debug=False, bootloader_ignore_signals=False, strip=False, upx=True, console=False\n          )\n          \"\"\"\n          with open(spec_file, \"w\") as f: f.write(spec)\n      - name: Create Required Backend Directories\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        run: |\n          Set-StrictMode -Version Latest\n          New-Item -ItemType Directory -Path (Join-Path $env:BACKEND_DIR \"data\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $env:BACKEND_DIR \"json\") -Force | Out-Null\n          Write-Host \"\u2705 Created required backend directories for PyInstaller.\"\n      - name: Build with PyInstaller\n        if: steps.cache-backend.outputs.cache-hit != 'true'\n        env:\n          FORTUNA_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n        run: |\n          Set-StrictMode -Version Latest\n          pyinstaller \"${{ env.BACKEND_SPEC }}\" --clean --log-level=WARN --noconfirm\n      - name: Verify Executable\n        run: |\n          Set-StrictMode -Version Latest\n          $exePath = \"dist/fortuna-backend.exe\"\n          if (-not (Test-Path $exePath)) { throw \"\u274c FATAL: Executable not found\" }\n          $size = (Get-Item $exePath).Length / 1MB\n          if ($size -lt 10) { throw \"\u274c FATAL: Executable is suspiciously small: $($size) MB.\" }\n          Write-Host \"\u2705 Backend ready: $([math]::Round($size, 2)) MB\"\n      - name: Upload Backend Executable\n        uses: actions/upload-artifact@v4\n        with:\n          name: backend-executable-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist/fortuna-backend.exe\n          retention-days: 3\n\n  diagnose-asgi-imports:\n    name: '\ud83d\udd0d ASGI Import Killer Pre-Smoke Diagnostic'\n    runs-on: windows-latest\n    timeout-minutes: 15\n    needs: [path-finder, build-backend]\n    continue-on-error: true\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n      - name: 'Run ASGI Diagnostics'\n        uses: ./.github/actions/run-asgi-diagnostics\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          backend-dir: ${{ needs.path-finder.outputs.backend_dir }}\n          backend-module-path: ${{ needs.path-finder.outputs.backend_module_path }}\n  diagnose-runtime:\n    name: '\ud83d\udd0e Diagnose PyInstaller Runtime'\n    runs-on: windows-latest\n    timeout-minutes: 10\n    needs: [path-finder, build-backend]\n    continue-on-error: true\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    env:\n      BACKEND_MODULE_PATH: ${{ needs.path-finder.outputs.backend_module_path }}\n    steps:\n      - name: \ud83d\udce5 Download Backend Executable\n        uses: actions/download-artifact@v4\n        with:\n          name: backend-executable-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist\n      - name: \ud83d\udc0d Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          architecture: ${{ matrix.arch }}\n      - name: \ud83d\udce6 Install PyInstaller\n        run: pip install pyinstaller==6.6.0\n      - name: \ud83d\udd75\ufe0f Extract and Analyze Executable Contents\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $exePath = \"dist/fortuna-backend.exe\"\n          Write-Host \"--- Executable Analysis ---\"\n          $archiveContents = pyi-archive_viewer $exePath\n          if ($LASTEXITCODE -ne 0) { throw \"Failed to analyze executable.\" }\n          Write-Host \"--- Archive Contents ---\"\n          $archiveContents | Out-Host\n          $expectedInitFile = ($env:BACKEND_MODULE_PATH.Replace('.', '/') + '/__init__.py')\n          $found = $archiveContents | ForEach-Object { $_.Replace('\\', '/') } | Select-String -Pattern $expectedInitFile -SimpleMatch -Quiet\n          if ($found) {\n            Write-Host \"\u2705 SUCCESS: Key module file found.\" -ForegroundColor Green\n          } else {\n            Write-Error \"\u274c FAILURE: Key module file '$expectedInitFile' NOT found.\"\n            exit 1\n          }\n  smoke-test:\n    name: '\ud83d\udd2c Smoke Test (${{ matrix.arch }})'\n    runs-on: windows-latest\n    timeout-minutes: 20\n    needs: [build-backend, package-msi-service]\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ matrix.arch }}-${{ github.run_id }}\n          path: msi-installer\n      - name: \ud83d\udd25 Smoke Test\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $ErrorActionPreference = \"Stop\"\n          $msiPath = (Get-ChildItem -Path \"msi-installer\" -Filter \"*.msi\" -Recurse | Select-Object -First 1).FullName\n          if (-not $msiPath) { throw \"MSI not found!\" }\n          # 1. PRE-INSTALL CLEANUP\n          if (Get-Service -Name FortunaWebService -ErrorAction SilentlyContinue) {\n            sc.exe delete FortunaWebService 2>&1 | Out-Null\n          }\n          # 2. INSTALL\n          $proc = Start-Process msiexec.exe -ArgumentList \"/i `\"$msiPath`\" /qn /L*v msi-install.log\" -Wait -PassThru\n          if ($proc.ExitCode -ne 0 -and $proc.ExitCode -ne 3010) {\n            Get-Content msi-install.log -Tail 50\n            throw \"MSI installation failed with exit code $($proc.ExitCode).\"\n          }\n          # 3. VERIFY & RUN\n          $progFiles = ${env:ProgramFiles}\n          if ('${{ matrix.arch }}' -eq 'x86') { $progFiles = ${env:ProgramFiles(x86)} }\n          $installDir = Join-Path $progFiles \"Fortuna Faucet Service\"\n          if (-not (Test-Path $installDir)) { throw \"Installation directory not found!\" }\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"data\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"json\") -Force | Out-Null\n          New-Item -ItemType Directory -Path (Join-Path $installDir \"logs\") -Force | Out-Null\n          Start-Service -Name \"FortunaWebService\"\n          Start-Sleep -Seconds 10\n          # 4. HEALTH CHECK\n          $maxRetries = 5\n          $delay = 5\n          For ($i=0; $i -lt $maxRetries; $i++) {\n            try {\n              $response = Invoke-WebRequest -Uri \"http://localhost:${{ env.SERVICE_PORT }}/health\" -UseBasicParsing\n              if ($response.StatusCode -eq 200) {\n                Write-Host \"\u2705 Health check PASSED.\"\n                Stop-Service -Name \"FortunaWebService\"\n                exit 0\n              }\n            } catch { Write-Host \"Attempt $($i+1) failed. Retrying...\" }\n            Start-Sleep -Seconds $delay\n          }\n          throw \"Health check failed.\"\n\n\n  package-msi-service:\n    name: '\ud83d\udcbf Package Service MSI (${{ matrix.arch }})'\n    runs-on: windows-latest\n    timeout-minutes: 25\n    needs: [path-finder, repo-preflight, build-backend]\n    strategy:\n      matrix:\n        arch: [x64, x86]\n    env:\n      WIX_HASH: ${{ needs.repo-preflight.outputs.wix_definition_hash }}\n      BUILD_VERSION: ${{ needs.repo-preflight.outputs.semver }}\n      SHORT_SHA: ${{ needs.repo-preflight.outputs.short_sha }}\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n      - name: Download Backend\n        uses: actions/download-artifact@v4\n        with:\n          name: backend-executable-${{ matrix.arch }}-${{ github.run_id }}\n          path: dist\n      - name: Stage Artifacts\n        id: stage\n        run: |\n          Set-StrictMode -Version Latest\n          $staging = \"${{ env.MSI_STAGING_DIR }}\"\n          New-Item -ItemType Directory -Path $staging -Force | Out-Null\n          Move-Item -Path \"dist/fortuna-backend.exe\" -Destination \"$staging/fortuna-webservice.exe\" -Force\n          $msiName = \"Fortuna-WebService-${{ matrix.arch }}-${{ env.BUILD_VERSION }}-${{ env.SHORT_SHA }}.msi\".Replace('/', '-')\n          \"msi_name=$msiName\" | Out-File $env:GITHUB_OUTPUT -Encoding utf8 -Append\n          Write-Host \"\u2705 Staged for MSI: $msiName\"\n      - name: Create Restart Service Batch Script\n        shell: pwsh\n        run: |\n          $scriptContent = \"@echo off`r`necho Requesting Admin privileges to restart FortunaWebService...`r`nnet stop FortunaWebService`r`nnet start FortunaWebService`r`necho Service Restarted.`r`npause\"\n          Set-Content -Path \"${{ env.MSI_STAGING_DIR }}/restart_service.bat\" -Value $scriptContent -Encoding Ascii\n          Write-Host \"\u2705 Created restart_service.bat script.\"\n      - name: Setup .NET SDK\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n      - name: Cache NuGet\n        uses: actions/cache@v4\n        with:\n          path: ~/.nuget/packages\n          key: ${{ runner.os }}-nuget-${{ env.WIX_HASH }}\n      - name: \ud83d\udcc4 Ensure WiX License Exists\n        run: |\n          if (-not (Test-Path build_wix)) { New-Item -ItemType Directory -Path build_wix | Out-Null }\n          $licensePath = 'build_wix/license.rtf'\n          if (-not (Test-Path $licensePath)) {\n            Write-Host '\u26a0\ufe0f License file missing. Generating placeholder...'\n            $rtfContent = '{\\rtf1\\ansi\\deff0{\\fonttbl{\\f0 Arial;}}\\f0\\fs24 END USER LICENSE AGREEMENT\\par\\par This is a placeholder license for Fortuna Faucet. Please replace with actual terms.}'\n            Set-Content -Path $licensePath -Value $rtfContent -Encoding Ascii\n            Write-Host '\u2705 Placeholder license.rtf created.'\n          } else {\n            Write-Host '\u2705 Existing license.rtf found.'\n          }\n      - name: Prepare WiX Project\n        run: |\n          Set-StrictMode -Version Latest\n          Copy-Item \"${{ env.WIX_DIR }}/Product_WithService.wxs\" \"${{ env.WIX_DIR }}/Product.wxs\" -Force\n          $proj = @(\n            '<Project Sdk=\"WixToolset.Sdk/${{ env.WIX_VERSION }}\">',\n            '  <PropertyGroup>',\n            '    <EnableDefaultCompileItems>false</EnableDefaultCompileItems>',\n            '    <OutputType>Package</OutputType>',\n            '    <Platforms>x64;x86</Platforms>',\n            '    <DefineConstants>Version=$(Version);SourceDir=$(SourceDir);ServicePort=$(ServicePort);Platform=$(Platform)</DefineConstants>',\n            '  </PropertyGroup>',\n            '  <ItemGroup>',\n            '    <PackageReference Include=\"WixToolset.UI.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Firewall.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '    <PackageReference Include=\"WixToolset.Util.wixext\" Version=\"${{ env.WIX_VERSION }}\" />',\n            '  </ItemGroup>',\n            '  <ItemGroup>',\n            '    <Compile Include=\"Product.wxs\" />',\n            '  </ItemGroup>',\n            '</Project>'\n          )\n          Set-Content \"${{ env.WIX_DIR }}/Fortuna.wixproj\" -Value ($proj -join \"`r`n\") -Encoding utf8\n      - name: Build MSI\n        working-directory: ${{ env.WIX_DIR }}\n        run: |\n          Set-StrictMode -Version Latest\n          dotnet build Fortuna.wixproj -c Release `\n            -p:Platform=${{ matrix.arch }} `\n            -p:SourceDir=\"../${{ env.MSI_STAGING_DIR }}\" `\n            -p:Version=\"${{ env.BUILD_VERSION }}\" `\n            -p:ServicePort=\"${{ env.SERVICE_PORT }}\"\n          $releaseDir = \"bin/${{ matrix.arch }}/Release\"\n          $defaultOutput = \"$releaseDir/Fortuna.msi\"\n          $targetName = \"${{ steps.stage.outputs.msi_name }}\"\n          $targetPath = \"$releaseDir/$targetName\"\n          if (Test-Path $defaultOutput) {\n            Move-Item -Path $defaultOutput -Destination $targetPath -Force\n            Write-Host \"\u2705 Renamed MSI to $targetName\"\n          } else { throw \"\u274c Build failed: $defaultOutput not found\" }\n      - name: Upload MSI + Hash\n        uses: actions/upload-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ matrix.arch }}-${{ github.run_id }}\n          path: ${{ env.WIX_DIR }}/bin/${{ matrix.arch }}/Release/*\n          retention-days: 10\n\n  create-release:\n    name: '\ud83d\ude80 Create Release'\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/')\n    needs: package-msi-service\n    permissions:\n      contents: write\n    steps:\n      - name: Download MSI\n        uses: actions/download-artifact@v4\n        with:\n          pattern: fortuna-service-msi-*\n          merge-multiple: true\n          path: assets\n\n      - name: Download SBOM\n        uses: actions/download-artifact@v4\n        with:\n          name: sbom-${{ github.run_id }}\n          path: assets\n\n      - name: Generate Checksums\n        run: |\n          cd assets\n          ls *.msi\n          sha256sum *.msi > SHASUMS256.txt\n\n      - name: Publish Release\n        uses: softprops/action-gh-release@v2\n        with:\n          files: |\n            assets/*.msi\n            assets/*.sha256\n            assets/SHASUMS256.txt\n            assets/sbom.spdx.json\n          generate_release_notes: true\n\n  stage-release-artifacts:\n    name: '\ud83d\udce6 Stage Release Artifacts'\n    runs-on: windows-latest\n    timeout-minutes: 5\n    needs: [package-msi-service, repo-preflight]\n    steps:\n      - name: \ud83d\udce5 Download MSI Installer\n        uses: actions/download-artifact@v4\n        with:\n          name: fortuna-service-msi-${{ github.run_id }}\n          path: msi-installer\n      - name: \ud83d\ude9a Stage Final Artifact\n        shell: pwsh\n        run: |\n          Set-StrictMode -Version Latest\n          $sourceDir = \"msi-installer\"\n          $destDir = \"final-release-artifact\"\n          New-Item -ItemType Directory -Path $destDir -Force | Out-Null\n\n          robocopy $sourceDir $destDir /E\n\n          if ($LASTEXITCODE -ge 8) {\n            Write-Error \"Robocopy failed with exit code $LASTEXITCODE. This indicates a serious error.\"\n            exit 1\n          }\n\n          Write-Host \"\u2705 Robocopy completed successfully.\"\n          Get-ChildItem -Path $destDir | Write-Host\n          exit 0\n\n      - name: \ud83d\udce4 Upload Final MSI Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: Final-MSI-Artifact\n          path: final-release-artifact/\n          retention-days: 90\n",
    "PSEUDOCODE.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Comprehensive System Specification (Revised & Corrected)\n**Version:** 2.2.0\n**Last Updated:** November 7, 2025\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture Pillars\n3.  Backend Engine (Python) - Detailed\n4.  Frontend Interface (TypeScript/React) - Detailed\n5.  Electron Wrapper & Windows Integration - Detailed\n6.  Data Models & API Specification\n7.  Deployment & Automation (CI/CD)\n8.  End-to-End Workflows\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551  Unifying global horse/greyhound/harness racing intelligence   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve results via a secure, local REST API to an interactive dashboard.\n  \u2022 Operate as a professional, standalone, native Windows application.\n\nCORE TENETS:\n  \u2022 UI-First Experience: The user interface is always responsive, even during backend startup or restarts.\n  \u2022 Resilient Process Management: The backend executable's lifecycle is robustly managed, with timeouts and crash detection.\n  \u2022 Asynchronous Initialization: The backend server starts instantly, deferring heavy, blocking I/O to background threads.\n  \u2022 Secure by Design: Communication between the frontend and the privileged main process is secured via a context-aware preload script.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline.\n\nSTAKEHOLDERS:\n  \u2022 End User: Receives a professional MSI installer for a one-click, dependency-free launch.\n  \u2022 Developer: Works with clean, separated Python and TypeScript stacks, governed by this specification.\n```\n\n---\n\n## 2. ARCHITECTURE PILLARS\n\n### Pillar 1: Backend Engine (Python)\n\n```\nPYTHON_BACKEND:\n  \u251c\u2500 main.py\n  \u2502  \u2514\u2500 Entry point for PyInstaller executable; starts the Uvicorn server.\n  \u2502\n  \u251c\u2500 api.py\n  \u2502  \u2514\u2500 FastAPI application definition.\n  \u2502     \u251c\u2500 Lifespan Hook: Manages async startup/shutdown logic.\n  \u2502     \u251c\u2500 API Routes: /health, /api/status, /api/races, etc.\n  \u2502     \u2514\u2500 Dependency Injection: Provides engine and security dependencies.\n  \u2502\n  \u251c\u2500 engine.py\n  \u2502  \u2514\u2500 OddsEngine: Orchestrates all data fetching and processing.\n  \u2502\n  \u251c\u2500 adapters/\n  \u2502  \u251c\u2500 base_v3.py (Abstract Base Class for all data sources)\n  \u2502  \u2514\u2500 [20+ specific adapter implementations]\n  \u2502\n  \u251c\u2500 config.py\n  \u2502  \u2514\u2500 Pydantic settings management from .env file.\n  \u2502\n  \u2514\u2500 requirements.txt\n     \u2514\u2500 Clean, de-duplicated, and conflict-free list of all Python dependencies.\n```\n\n### Pillar 2: Frontend Interface (TypeScript/React)\n\n```\nFRONTEND:\n  \u251c\u2500 next.config.mjs\n  \u2502  \u2514\u2500 Next.js config with `output: 'export'` for 100% static generation.\n  \u2502\n  \u251c\u2500 app/page.tsx\n  \u2502  \u2514\u2500 Main application shell.\n  \u2502\n  \u251c\u2500 src/components/\n  \u2502  \u251c\u2500 LiveRaceDashboard.tsx (Main stateful component)\n  \u2502  \u2502  \u251c\u2500 Manages connection state ('connecting', 'online', 'error').\n  \u2502  \u2502  \u251c\u2500 Polls Electron main process for backend status via secure IPC.\n  \u2502  \u2502  \u2514\u2500 Fetches data from the local Python API when online.\n  \u2502  \u2502\n  \u2502  \u251c\u2500 RaceCard.tsx (Displays a single race)\n  \u2502  \u2514\u2500 StatusIndicator.tsx (Shows backend connection status)\n  \u2502\n  \u2514\u2500 src/types/\n     \u2514\u2500 racing.ts (TypeScript interfaces matching backend Pydantic models)\n```\n\n### Pillar 3: Electron Wrapper & Windows Integration\n\n```\nELECTRON_WRAPPER:\n  \u251c\u2500 main.js (Electron main process)\n  \u2502  \u251c\u2500 Creates the BrowserWindow and loads the static frontend.\n  \u2502  \u251c\u2500 Implements robust lifecycle management for the backend executable.\n  \u2502  \u251c\u2500 Provides secure IPC handlers for status checks and restarts.\n  \u2502  \u2514\u2500 Creates a system tray icon for background operation.\n  \u2502\n  \u251c\u2500 preload.js (Secure IPC Bridge)\n  \u2502  \u2514\u2500 Uses `contextBridge` to safely expose specific functions to the frontend.\n  \u2502\n  \u251c\u2500 package.json\n  \u2502  \u2514\u2500 Defines Node.js dependencies and build scripts.\n  \u2502\n  \u251c\u2500 electron-builder-config.yml\n  \u2502  \u2514\u2500 Defines the configuration for creating the final MSI installer.\n  \u2502\n  \u2514\u2500 .github/workflows/build-msi.yml\n     \u2514\u2500 GitHub Actions pipeline that automates the entire build, test, and package process.\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON) - DETAILED\n\n### 3.1 Entry Point & Server Startup (`main.py`)\n\n```pseudocode\n// This is the script executed by fortuna-backend.exe\n\nPROCEDURE Main_Python_Entry_Point\n  // Guard required for PyInstaller and multiprocessing on Windows\n  IF this script is the main entry point:\n    CALL multiprocessing.freeze_support()\n\n    // Programmatically launch the FastAPI application using Uvicorn\n    // This call blocks and runs the server until the process is terminated\n    CALL uvicorn.run(\n      app=\"python_service.api:app\",\n      host=\"0.0.0.0\",\n      port=8000\n    )\nEND PROCEDURE\n```\n\n### 3.2 Asynchronous Application Lifecycle (`api.py`)\n\n```pseudocode\n// --- Lifespan Management (The key to a non-blocking startup) ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // === ON STARTUP ===\n  LOG \"Uvicorn server is online. Starting lifespan initialization.\"\n\n  // 1. Perform immediate, non-blocking tasks\n  CONNECT to Redis cache\n\n  // 2. Defer slow, blocking tasks to a background thread\n  //    This allows the server to start accepting requests instantly.\n  SCHEDULE function \"initialize_heavy_resources(app)\" to run in a ThreadPoolExecutor\n\n  LOG \"Heavy resource initialization scheduled. Server is now responsive.\"\n\n  // 3. Yield control back to Uvicorn. The server is now live.\n  YIELD\n\n  // === ON SHUTDOWN ===\n  LOG \"Shutdown signal received.\"\n  AWAIT app.state.engine.close() // Gracefully close HTTP client connections\n  DISCONNECT from Redis\n  SHUTDOWN ThreadPoolExecutor\n\n// --- Heavy Initialization (Runs in Background) ---\nFUNCTION initialize_heavy_resources(app: FastAPI):\n  TRY\n    LOG \"Background initialization of OddsEngine has started.\"\n    settings <- get_settings_from_config()\n    engine <- create new OddsEngine(config=settings)\n    // This part is slow: it loads all ~25 adapters\n    app.state.engine <- engine\n    LOG \"Background initialization complete. OddsEngine is now available.\"\n  CATCH Exception as e:\n    LOG_CRITICAL \"Failed to initialize OddsEngine in the background.\", error=e\n    app.state.engine <- null // Ensure the app knows initialization failed\n```\n\n### 3.3 Engine Orchestration (`engine.py`)\n\n```pseudocode\nCLASS OddsEngine:\n  INIT(config):\n    self.config <- config\n    self.adapters <- [List of all adapter instances]\n    self.http_client <- httpx.AsyncClient(...)\n    self.semaphore <- asyncio.Semaphore(config.MAX_CONCURRENT_REQUESTS)\n\n    // Inject the shared, persistent HTTP client into each adapter\n    FOR adapter IN self.adapters:\n      adapter.http_client <- self.http_client\n\n  @cache_async_result(ttl_seconds=300)\n  ASYNC FUNCTION fetch_all_odds(date_str):\n    // Create a list of concurrent fetching tasks, wrapped in the semaphore\n    tasks <- [self._fetch_with_semaphore(adapter, date_str) FOR adapter in self.adapters]\n    results <- AWAIT asyncio.gather(*tasks, return_exceptions=True)\n\n    // Process results, separating successes from failures\n    all_races <- []\n    FOR result IN results:\n      IF result is a success:\n        all_races.extend(result.races)\n\n    // Deduplicate and merge races from different sources\n    deduped_races <- self._dedupe_races(all_races)\n\n    RETURN AggregatedResponse(races=deduped_races, source_statuses=...)\n```\n\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/REACT) - DETAILED\n\n### 4.1 LiveRaceDashboard Component\n\n```pseudocode\nCOMPONENT LiveRaceDashboard (client-side):\n\n  STATE:\n    races: Race[] <- []\n    backendStatus: 'connecting' | 'online' | 'error' <- 'connecting'\n    lastLogs: string[] <- []\n\n  EFFECT on mount:\n    // Use the secure API exposed by preload.js\n    IF window.electronAPI exists:\n      // Set up a listener for status updates from the main process\n      window.electronAPI.onBackendStatus((update) => {\n        setBackendStatus(update.state)\n        setLastLogs(update.logs)\n      })\n\n    // Immediately request the current status\n    window.electronAPI.getBackendStatus().then((status) => {\n      setBackendStatus(status.state)\n      setLastLogs(status.logs)\n    })\n\n    // Set up a polling interval to keep status fresh\n    interval <- setInterval(() => {\n      window.electronAPI.getBackendStatus().then((status) => {\n        setBackendStatus(status.state)\n        setLastLogs(status.logs)\n      })\n    }, 3000) // Poll every 3 seconds\n\n    CLEANUP: clearInterval(interval)\n\n  EFFECT when backendStatus changes to 'online':\n    // Trigger data fetch only when the backend is confirmed to be running\n    fetchQualifiedRaces()\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard HTTP call to the local Python server\n      response <- AWAIT fetch(\"http://127.0.0.1:8000/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API returned status ${response.status}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n\n    CATCH e:\n      // If the API call fails, update the status\n      setBackendStatus('error')\n      setLastLogs([...lastLogs, `API Fetch Error: ${e.message}`])\n\n  FUNCTION RENDER:\n    <div className=\"dashboard\">\n      <StatusIndicator status={backendStatus} />\n      <RaceFilters />\n\n      IF backendStatus === 'error':\n        <ErrorDisplay logs={lastLogs} />\n      ELSE IF backendStatus === 'connecting':\n        <LoadingSkeleton />\n      ELSE IF races.length === 0:\n        <EmptyState message=\"No races matched your filters.\" />\n      ELSE:\n        <RaceGrid races={races} />\n    </div>\n```\n\n---\n\n## 5. ELECTRON WRAPPER & WINDOWS INTEGRATION - DETAILED\n\n### 5.1 Main Process (`main.js`) - With Robust Lifecycle Management\n\n```pseudocode\nCLASS FortunaDesktopApp:\n  INIT():\n    self.mainWindow <- null\n    self.backendState <- 'stopped'\n    self.backendLogs <- []\n    self.backendProcess <- null\n\n  FUNCTION createMainWindow():\n    // ... create BrowserWindow, load static frontend ...\n\n  FUNCTION startBackend():\n    IF self.backendProcess is not null:\n      self.backendProcess.kill()\n\n    self.backendState <- 'starting'\n    self.backendLogs <- ['Attempting to start backend...']\n    self.sendBackendStatusUpdate() // Notify UI\n\n    // Get path to the packaged executable\n    exePath <- path.join(process.resourcesPath, 'fortuna-backend', 'fortuna-backend.exe')\n\n    IF file at exePath does NOT exist:\n      self.backendState <- 'error'\n      self.backendLogs.push(`FATAL: Executable not found at ${exePath}`)\n      self.sendBackendStatusUpdate()\n      dialog.showErrorBox(\"Critical Error\", \"Backend is missing. Please reinstall.\")\n      RETURN\n\n    // Spawn the process\n    self.backendProcess <- spawn(exePath, [], { stdio: ['ignore', 'pipe', 'pipe'] })\n\n    // --- CRITICAL: Resiliency Logic ---\n    startupTimeout <- setTimeout(() => {\n      IF self.backendState === 'starting':\n        self.backendState <- 'error'\n        self.backendLogs.push('Error: Backend startup timed out after 30 seconds.')\n        self.backendProcess.kill()\n        self.sendBackendStatusUpdate()\n    }, 30000) // 30-second timeout\n\n    self.backendProcess.stdout.on('data', (data) => {\n      self.backendLogs.push(data.toString())\n      // A more robust check would be a successful health check poll\n      IF data.toString().includes(\"Uvicorn running\"):\n        self.backendState <- 'online'\n        clearTimeout(startupTimeout)\n        self.sendBackendStatusUpdate()\n    })\n\n    self.backendProcess.stderr.on('data', (data) => {\n      self.backendLogs.push(`[STDERR] ${data.toString()}`)\n    })\n\n    self.backendProcess.on('exit', (code) => {\n      clearTimeout(startupTimeout)\n      IF self.backendState is not 'error': // Avoid duplicate error messages\n        self.backendState <- 'error'\n        self.backendLogs.push(`Backend process exited unexpectedly with code: ${code}`)\n        self.sendBackendStatusUpdate()\n    })\n\n  FUNCTION sendBackendStatusUpdate():\n    // Send the latest status to the frontend renderer process\n    IF self.mainWindow is not null:\n      self.mainWindow.webContents.send('backend-status-update', {\n        state: self.backendState,\n        logs: self.backendLogs.slice(-20) // Send last 20 log lines\n      })\n\n// --- IPC Handlers (Securely Defined) ---\nipcMain.handle('get-backend-status', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN null\n\n  RETURN { state: self.backendState, logs: self.backendLogs.slice(-20) }\n})\n\nipcMain.on('restart-backend', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN\n\n  self.startBackend()\n})\n```\n\n### 5.2 Preload Script (`preload.js`)\n\n```pseudocode\n// Expose a limited, secure API to the frontend renderer process\ncontextBridge.exposeInMainWorld('electronAPI', {\n  getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n  restartBackend: () => ipcRenderer.send('restart-backend'),\n  onBackendStatus: (callback) => ipcRenderer.on('backend-status-update', (_event, value) => callback(value))\n})\n```\n\n---\n\n## 6. DATA MODELS & API SPECIFICATION\n\n### 6.1 Core Data Models (Pydantic/TypeScript)\n\n```\nMODEL Race:\n  id: str (unique identifier, e.g., \"Betfair_USA_Aqueduct_2025-11-07_R1\")\n  venue: str\n  race_number: int\n  start_time: datetime\n  runners: List[Runner]\n  source: str\n\nMODEL Runner:\n  name: str\n  odds: Optional[float]\n```\n\n### 6.2 Primary API Endpoints\n\n```\nENDPOINT GET /health\n  Description: Simple health check, requires no authentication.\n  Response (200 OK): {\"status\": \"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Description: Fetches all race data, runs the Trifecta analyzer, and returns qualified races.\n  Headers:\n    - X-API-Key: (Required, not used in this local setup but good practice)\n  Query Params:\n    - max_field_size: int\n    - min_odds: float\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 7. DEPLOYMENT & AUTOMATION (CI/CD)\n\n```pseudocode\nWORKFLOW Build_MSI_Installer_on_GitHub_Actions:\n  // Phase 1: Setup\n  SETUP Node.js and Python environments\n\n  // Phase 2: Build Frontend\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY static output to /electron/web-ui-build/out\n\n  // Phase 3: Build Backend\n  RUN \"pip install -r python_service/requirements.txt\"\n  // CRITICAL: Use PyInstaller with a spec file or CLI flags that include\n  // necessary hidden imports to prevent runtime crashes.\n  // e.g., --hidden-import=keyring.backends.fail.Keyring\n  EXECUTE PyInstaller to create fortuna-backend.exe\n  PLACE executable in /electron/resources/fortuna-backend\n\n  // Phase 4: Deep Integration Test\n  START fortuna-backend.exe in the background\n  POLL http://127.0.0.1:8000/health until it responds with 200 OK or times out\n  IF timeout or crash THEN FAIL the build\n\n  // Phase 5: Package\n  RUN \"npm ci\" in /electron\n  EXECUTE \"npx electron-builder\" to create the MSI installer\n\n  // Phase 6: Publish\n  UPLOAD MSI as a build artifact\n  IF build was triggered by a git tag THEN CREATE a new GitHub Release\n```\n\n---\n\n## 8. END-TO-END WORKFLOWS\n\n### 8.1 Production Startup Workflow (Resilient)\n\n```\nWORKFLOW user_launches_application:\n  STEP 1: User executes Fortuna Faucet.exe -> Electron main.js starts.\n  STEP 2: UI appears instantly. The main process creates the BrowserWindow and loads the static index.html. The UI shows a 'connecting' state.\n  STEP 3: Backend starts asynchronously. The main process calls the robust `startBackend()` function.\n  STEP 4: `startBackend()` spawns `fortuna-backend.exe` and starts a 30-second timeout.\n  STEP 5: The frontend UI polls for status every 3 seconds via the secure `window.electronAPI.getBackendStatus()`.\n  STEP 6: The backend `.exe` starts, its `lifespan` hook runs, and the Uvicorn server comes online within seconds.\n  STEP 7: The main process detects the \"Uvicorn running\" message (or a successful health poll) and updates its internal state to 'online'. The startup timeout is cleared.\n  STEP 8: On its next poll, the frontend receives the 'online' status.\n  STEP 9: The frontend's state changes, triggering the `fetchQualifiedRaces()` API call to `localhost:8000`.\n  STEP 10: Data is returned from the now fully-initialized backend and rendered in the UI.\n\n  FAILURE SCENARIO (Backend Crash):\n  STEP 6a: The backend `.exe` crashes on startup.\n  STEP 7a: The `on('exit')` handler in `main.js` fires. The state is set to 'error' with the exit code.\n  STEP 8a: On its next poll, the frontend receives the 'error' status and relevant logs.\n  STEP 9a: The UI renders an error message and a \"Restart Backend\" button.\n```\n\n---\n*This concludes the revised and definitive blueprint for the Fortuna Faucet application.*\n\n---\n\n### 9. OPERATION: THE AUDITOR (REAL-TIME VERIFICATION)\n\n**Status:** Implemented (Python)\n**Source:** `python_service/auditor.py`\n\n#### 9.1 System Context\n*Runs as a background thread. Verifies \"Qualifier\" predictions against official results scraped from AtTheRaces.com to calculate real-time profitability.*\n\n#### 9.2 Database Schema (SQLite)\n\n```pseudocode\nTABLE audit_log:\n  race_id: TEXT PRIMARY KEY      // Format: \"VENUE-YYYYMMDD-RR\"\n  track_code: TEXT\n  race_number: INTEGER\n  predicted_horse: TEXT\n  timestamp: DATETIME\n  status: TEXT                   // 'PENDING', 'CASHED', 'BURNED'\n  official_payout: REAL          // Default: 0.00\n  net_profit: REAL               // Default: 0.00\n\n  CONSTRAINT status_check CHECK (status IN ('PENDING', 'CASHED', 'BURNED'))\n  INDEX idx_status_timestamp (status, timestamp)\n```\n\n#### 9.3 Auditor Engine Logic\n\n```pseudocode\nMODULE: Auditor_Engine\nDEPENDENCIES: httpx, beautifulsoup4, sqlite3, structlog\n\nCLASS Auditor_Engine:\n\n    PROPERTIES:\n        db_path: String\n        http_client: AsyncClient\n        TOTE_UNIT: Float (2.00)\n        TRACK_CODE_MAP: Dictionary  // Maps \"DON\" -> \"Doncaster\", etc.\n\n    FUNCTION __init__(db_path):\n        self.db_path = db_path\n        self.Initialize_Database()\n        self.http_client = NEW AsyncClient(timeout=30)\n\n    # --- Phase 1: The Snapshot ---\n    # Called by OddsEngine when a bet is placed/qualified\n    ASYNC FUNCTION Snapshot_Qualifier(venue_code, race_date, race_number, predicted_horse):\n        race_id = GENERATE_ID(venue_code, race_date, race_number)\n\n        TRY:\n            QUERY = \"\"\"\n                INSERT INTO audit_log\n                (race_id, track_code, race_number, predicted_horse, timestamp, status)\n                VALUES (?, ?, ?, ?, ?, 'PENDING')\n            \"\"\"\n            EXECUTE_SQL(self.db_path, QUERY, (race_id, venue_code, race_number, predicted_horse, NOW()))\n            LOG_INFO(\"Snapshot saved: \" + race_id)\n            RETURN TRUE\n        CATCH IntegrityError:\n            LOG_WARN(\"Race already tracked: \" + race_id)\n            RETURN FALSE\n\n    # --- Phase 2: The Fetcher (Background Loop) ---\n    ASYNC FUNCTION Run_Audit_Loop():\n        self.running = TRUE\n        WHILE self.running:\n            TRY:\n                # 1. Get Pending Races (Last 60 mins)\n                cutoff = NOW() - MINUTES(60)\n                pending_races = GET_PENDING_RACES(cutoff)\n\n                IF pending_races IS EMPTY:\n                    SLEEP(120)\n                    CONTINUE\n\n                # 2. Batch by Track\n                unique_tracks = EXTRACT_UNIQUE_TRACKS(pending_races)\n\n                FOR track IN unique_tracks:\n                    track_races = FILTER(pending_races, track)\n\n                    FOR race IN track_races:\n                        # Fetch Official Result from AtTheRaces\n                        result = AWAIT self._Fetch_Official_Result(race.track_code, race.race_number)\n\n                        IF result IS NOT NULL:\n                            self._Determine_Verdict(race, result)\n\n                        SLEEP(2) # Polite delay\n\n            CATCH Exception as e:\n                LOG_ERROR(\"Audit Loop Error: \" + e)\n\n            SLEEP(120)\n\n    # --- Phase 3: The Scraper (AtTheRaces Strategy) ---\n    ASYNC FUNCTION _Fetch_Official_Result(track_code, race_number):\n        # 1. Find the specific Race URL from the daily results page\n        race_url = AWAIT self._Find_Race_URL(track_code, race_number)\n        IF race_url IS NULL: RETURN NULL\n\n        # 2. Fetch the Race Page\n        html = AWAIT self.http_client.GET(race_url)\n\n        # 3. Parse the Table\n        result = self._Parse_AtTheRaces_Results(html, track_code, race_number)\n        RETURN result\n\n    ASYNC FUNCTION _Find_Race_URL(track_code, race_number):\n        base_url = \"https://www.attheraces.com/results\"\n        html = AWAIT self.http_client.GET(base_url)\n\n        track_name = self.TRACK_CODE_MAP[track_code]\n        track_header = FIND_ELEMENT(html, text=track_name)\n\n        IF track_header EXISTS:\n            # Select the Nth link in the track's panel\n            race_links = SELECT_ALL(track_header.parent, 'a[href*=\"/racecard/\"]')\n            IF LENGTH(race_links) >= race_number:\n                RETURN race_links[race_number - 1].href\n\n        RETURN NULL\n\n    FUNCTION _Parse_AtTheRaces_Results(html, track_code, race_number):\n        table = FIND_TABLE(html, header_contains=\"Horse\")\n        IF table IS NULL: RETURN NULL\n\n        finishers = []\n        win_payout = EXTRACT_WIN_PAYOUT(html) # Parse \"Betting returns\" table\n\n        FOR row IN table.rows:\n            position = PARSE_INT(row.cells[0])\n            horse_name = row.cells[2].text\n\n            place_payout = 0.0\n            IF position == 1:\n                place_payout = win_payout\n\n            finishers.APPEND({\n                \"name\": horse_name,\n                \"position\": position,\n                \"place_payout\": place_payout\n            })\n\n        RETURN NEW OfficialResult(finishers)\n\n    # --- Phase 4: The Verdict ---\n    FUNCTION _Determine_Verdict(prediction, official_result):\n        did_place = FALSE\n        payout = 0.00\n\n        # Check if predicted horse won (AtTheRaces basic logic)\n        FOR finisher IN official_result.finishers:\n            IF finisher.name == prediction.predicted_horse AND finisher.place_payout > 0:\n                did_place = TRUE\n                payout = finisher.place_payout\n                BREAK\n\n        IF did_place:\n            status = 'CASHED'\n            net_profit = payout - self.TOTE_UNIT\n        ELSE:\n            status = 'BURNED'\n            net_profit = -self.TOTE_UNIT\n\n        UPDATE_DB(prediction.race_id, status, payout, net_profit)\n\n    # --- Phase 5: Dashboard Metrics ---\n    FUNCTION Get_Rolling_Metrics(minutes=60):\n        cutoff = NOW() - MINUTES(minutes)\n        QUERY = \"\"\"\n            SELECT\n                COUNT(*) as total,\n                SUM(CASE WHEN status='CASHED' THEN 1 ELSE 0 END) as wins,\n                SUM(net_profit) as profit\n            FROM audit_log\n            WHERE timestamp > ? AND status != 'PENDING'\n        \"\"\"\n        stats = EXECUTE_SQL(self.db_path, QUERY, (cutoff,))\n\n        RETURN {\n            \"strike_rate\": (stats.wins / stats.total) * 100,\n            \"net_profit\": stats.profit,\n            \"volume\": stats.total\n        }\n```",
    "electron/electron-builder-config.yml": "appId: com.jules.fortunafaucet\nproductName: \"Fortuna Faucet\"\n\ndirectories:\n  output: dist\n  buildResources: assets\n\nfiles:\n  - filter:\n      - \"**/*\"\n\nextraResources:\n  - from: \"../python-service-bin\"\n    to: \"python-service-bin\"\n    filter:\n      - \"**/*\"\n  - from: \"../web_platform/frontend/out\"\n    to: \"resources/frontend\"\n    filter:\n      - \"**/*\"\n\nwin:\n  target: msi\n  icon: \"assets/icon.ico\"\n\nmsi:\n  oneClick: false\n  perMachine: true\n  runAfterFinish: true\n  # Explicitly pointing to the file ensures WiX picks it up\n  shortcutName: \"Fortuna Faucet\"\n  warningsAsErrors: false",
    "electron/install-dependencies.js": "const { execSync } = require('child_process');\nconst path = require('path');\nconst fs = require('fs');\n\n// Path to the bundled Python executable (fortuna-backend.exe)\nconst PYTHON_EXE = path.join(process.resourcesPath, 'fortuna-backend.exe');\n// Path to the Python service directory (where alembic.ini is)\nconst PYTHON_SERVICE_DIR = path.join(process.resourcesPath, 'python_service');\n\nfunction runCommand(command, cwd) {\n    console.log(`Executing: ${command} in ${cwd}`);\n    try {\n        const output = execSync(command, { cwd: cwd, encoding: 'utf-8' });\n        console.log(output);\n    } catch (error) {\n        console.error(`Command failed: ${command}`);\n        console.error(error.stderr || error.stdout || error.message);\n        throw new Error(`Post-install setup failed: ${command}`);\n    }\n}\n\nfunction setupDatabase() {\n    console.log('--- Starting Database Setup (Alembic Migrations) ---');\n    // NOTE: The bundled EXE must be able to run a command like 'alembic' or a custom script\n    // that executes the migrations. Assuming the bundled EXE can run a module.\n    // A more robust solution is to bundle a dedicated migration script.\n\n    // Assuming the bundled EXE can execute a module that runs Alembic\n    const migrationCommand = `${PYTHON_EXE} -m python_service.database.run_migrations`;\n\n    // The migration script needs access to the database URL from the config.\n    // This is a placeholder, as the config loading is complex in a frozen app.\n    // For now, we assume the bundled EXE handles config loading.\n\n    runCommand(migrationCommand, PYTHON_SERVICE_DIR);\n    console.log('--- Database Setup Complete ---');\n}\n\n// This function is called by the Electron Builder installer hook\nmodule.exports = async function() {\n    try {\n        setupDatabase();\n    } catch (e) {\n        console.error('FATAL: Post-install setup failed.', e);\n        // In a real installer, you might log this and continue, or show a user error.\n    }\n};\n",
    "fortuna-backend-electron.spec": "# -*- mode: python ; coding: utf-8 -*-\nfrom pathlib import Path\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\nblock_cipher = None\nproject_root = Path(SPECPATH).parent\n\n# Helper function to include data files\ndef include(rel_path: str, target: str, store: list):\n    absolute = project_root / rel_path\n    if absolute.exists():\n        store.append((str(absolute), target))\n    else:\n        print(f\"[spec] WARNING: Skipping missing include: {absolute}\")\n\ndatas = []\nhiddenimports = set()\n\n# Include necessary data directories\ninclude('python_service/data', 'data', datas)\ninclude('python_service/json', 'json', datas)\ninclude('python_service/adapters', 'adapters', datas)\n\n# Automatically collect submodules and data files for key libraries\ndatas += collect_data_files('uvicorn')\ndatas += collect_data_files('fastapi')\ndatas += collect_data_files('starlette')\nhiddenimports.update(collect_submodules('python_service'))\nhiddenimports.update([\n    'asyncio',\n    'asyncio.windows_events',\n    'asyncio.selector_events',\n    'uvicorn.logging',\n    'uvicorn.loops.auto',\n    'uvicorn.protocols.http.h11_impl',\n    'uvicorn.protocols.http.httptools_impl',\n    'uvicorn.protocols.websockets.wsproto_impl',\n    'uvicorn.protocols.websockets.websockets_impl',\n    'uvicorn.lifespan.on',\n    'fastapi.routing',\n    'fastapi.middleware.cors',\n    'starlette.staticfiles',\n    'starlette.middleware.cors',\n    'pydantic_core',\n    'pydantic_settings.sources',\n    'anyio._backends._asyncio',\n    'httpcore',\n    'httpx',\n    'python_multipart',\n    'numpy',\n    'pandas',\n])\n\na = Analysis(\n    ['python_service/main.py'],\n    pathex=[str(project_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=sorted(hiddenimports),\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\n# \u2622\ufe0f PYZ INJECTION: Force __init__ files into the PYZ archive as modules\n# This is the definitive fix for ModuleNotFoundError at runtime.\na.pure += [\n    ('python_service', str(project_root / 'python_service/__init__.py'), 'PYMODULE'),\n]\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    name='fortuna-backend',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    console=False,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n",
    "pytest.ini": "[pytest]\npythonpath = .\ntestpaths = tests\npython_files = test_*.py\naddopts = -v\nasyncio_mode = auto\nasyncio_default_fixture_loop_scope = function\n",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n# TEMPORARY FIX: Comment out the problematic adapter\n\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\n\n# from .betfair_datascientist_adapter import BetfairDataScientistAdapter  # DISABLED: PyInstaller NumPy issue\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\n\n__all__ = [\n    \"GbgbApiAdapter\",\n    \"TVGAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"AtTheRacesAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"SportingLifeAdapter\",\n    \"TimeformAdapter\",\n    \"HarnessAdapter\",\n    \"GreyhoundAdapter\",\n    \"TheRacingApiAdapter\",\n    # \"BetfairDataScientistAdapter\",  # DISABLED\n]\n",
    "python_service/adapters/betfair_datascientist_adapter.py": "# python_service/adapters/betfair_datascientist_adapter.py\n\nfrom datetime import datetime\nfrom io import StringIO\nfrom typing import List\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass BetfairDataScientistAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Betfair Data Scientist CSV models, migrated to BaseAdapterV3.\n    \"\"\"\n\n    ADAPTER_NAME = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str, url: str, config=None):\n        source_name = f\"{self.ADAPTER_NAME}_{model_name}\"\n        super().__init__(source_name=source_name, base_url=url, config=config)\n        self.model_name = model_name\n\n    async def _fetch_data(self, date: str) -> Optional[StringIO]:\n        \"\"\"Fetches the raw CSV data from the Betfair Data Scientist model endpoint.\"\"\"\n        endpoint = f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n        self.logger.info(f\"Fetching data from {self.base_url}{endpoint}\")\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return StringIO(response.text) if response and response.text else None\n\n    def _parse_races(self, raw_data: Optional[StringIO]) -> List[Race]:\n        \"\"\"Parses the raw CSV data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n        try:\n            df = pd.read_csv(raw_data)\n            if df.empty:\n                self.logger.warning(\"Received empty CSV from Betfair Data Scientist.\")\n                return []\n\n            df = df.rename(\n                columns={\n                    \"meetings.races.bfExchangeMarketId\": \"market_id\",\n                    \"meetings.races.runners.bfExchangeSelectionId\": \"selection_id\",\n                    \"meetings.races.runners.ratedPrice\": \"rated_price\",\n                    \"meetings.races.raceName\": \"race_name\",\n                    \"meetings.name\": \"meeting_name\",\n                    \"meetings.races.raceNumber\": \"race_number\",\n                    \"meetings.races.runners.runnerName\": \"runner_name\",\n                    \"meetings.races.runners.clothNumber\": \"saddle_cloth\",\n                }\n            )\n            races: List[Race] = []\n            for market_id, group in df.groupby(\"market_id\"):\n                race_info = group.iloc[0]\n                runners = []\n                for _, row in group.iterrows():\n                    rated_price = row.get(\"rated_price\")\n                    odds_data = {}\n                    if pd.notna(rated_price):\n                        odds_data[self.source_name] = OddsData(\n                            win=float(rated_price),\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                    runners.append(\n                        Runner(\n                            name=str(row.get(\"runner_name\", \"Unknown\")),\n                            number=int(row.get(\"saddle_cloth\", 0)),\n                            odds=odds_data,\n                        )\n                    )\n\n                race = Race(\n                    id=str(market_id),\n                    venue=normalize_venue_name(str(race_info.get(\"meeting_name\", \"\"))),\n                    race_number=int(race_info.get(\"race_number\", 0)),\n                    start_time=datetime.now(),  # Placeholder, not provided in source\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n            self.logger.info(f\"Normalized {len(races)} races from {self.model_name}.\")\n            return races\n        except (pd.errors.ParserError, KeyError) as e:\n            self.logger.error(\n                \"Failed to parse Betfair Data Scientist CSV.\",\n                exc_info=True,\n                error=str(e),\n            )\n            return []\n",
    "python_service/adapters/gbgb_api_adapter.py": "# python_service/adapters/gbgb_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GbgbApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Greyhound Board of Great Britain API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"GBGB\"\n    BASE_URL = \"https://api.gbgb.org.uk/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw meeting data from the GBGB API.\"\"\"\n        endpoint = f\"results/meeting/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, meetings_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw meeting data into a list of Race objects.\"\"\"\n        if not meetings_data:\n            return []\n\n        all_races = []\n        for meeting in meetings_data:\n            track_name = meeting.get(\"trackName\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    if race := self._parse_race(race_data, track_name):\n                        all_races.append(race)\n                except (KeyError, TypeError):\n                    self.logger.error(\n                        \"Error parsing GBGB race\",\n                        race_id=race_data.get(\"raceId\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], track_name: str) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race_data.get(\"raceId\")\n        race_number = race_data.get(\"raceNumber\")\n        race_time = race_data.get(\"raceTime\")\n\n        if not all([race_id, race_number, race_time]):\n            return None\n\n        return Race(\n            id=f\"gbgb_{race_id}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(race_time.replace(\"Z\", \"+00:00\")),\n            runners=self._parse_runners(race_data.get(\"traps\", [])),\n            source=self.source_name,\n            race_name=race_data.get(\"raceTitle\"),\n            distance=f\"{race_data.get('raceDistance')}m\",\n        )\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                trap_number = runner_data.get(\"trapNumber\")\n                dog_name = runner_data.get(\"dogName\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                sp = runner_data.get(\"sp\")\n                if sp:\n                    win_odds = parse_odds_to_decimal(sp)\n                    if win_odds and win_odds < 999:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Error parsing GBGB runner, skipping.\",\n                    runner_name=runner_data.get(\"dogName\"),\n                )\n                continue\n        return runners\n",
    "python_service/adapters/oddschecker_adapter.py": "# python_service/adapters/oddschecker_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass OddscheckerAdapter(BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        # Note: Oddschecker doesn't seem to support historical dates well in its main nav,\n        # but we build the URL as if it does for future compatibility.\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        # Find all links to individual race pages\n        race_links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to OddscheckerAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                race = self._parse_race_page(soup, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\n                    \"Error parsing a race from Oddschecker, skipping race.\",\n                    exc_info=True,\n                )\n                continue\n        return all_races\n\n    def _parse_race_page(self, soup: BeautifulSoup, race_date) -> Optional[Race]:\n        track_name_node = soup.select_one(\"h1.meeting-name\")\n        if not track_name_node:\n            return None\n        track_name = track_name_node.get_text(strip=True)\n\n        race_time_node = soup.select_one(\"span.race-time\")\n        if not race_time_node:\n            return None\n        race_time_str = race_time_node.get_text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = soup.select_one(\"a.race-time-link.active\")\n        race_number = 1\n        if active_link:\n            all_links = soup.select(\"a.race-time-link\")\n            try:\n                race_number = all_links.index(active_link) + 1\n            except ValueError:\n                pass  # Keep default race number if active link not in all links\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in soup.select(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"span.selection-name\")\n            if not name_node:\n                return None\n            name = name_node.get_text(strip=True)\n\n            odds_node = row.select_one(\"span.bet-button-odds-desktop, span.best-price\")\n            if not odds_node:\n                return None\n            odds_str = odds_node.get_text(strip=True)\n\n            number_node = row.select_one(\"td.runner-number\")\n            if not number_node or not number_node.get_text(strip=True).isdigit():\n                return None\n            number = int(number_node.get_text(strip=True))\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if win_odds and win_odds < 999:\n                odds_dict[self.source_name] = OddsData(\n                    win=win_odds, source=self.source_name, last_updated=datetime.now()\n                )\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
    "python_service/adapters/racingpost_adapter.py": "# python_service/adapters/racingpost_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\nfrom selectolax.parser import Node\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingPostAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch RacingPost index page\", url=index_url)\n            return None\n\n        index_parser = HTMLParser(index_response.text)\n        links = index_parser.css('a[data-test-selector^=\"RC-meetingItem__link_race\"]')\n        race_card_urls = [link.attributes[\"href\"] for link in links]\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(self.http_client, \"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n\n                venue_node = parser.css_first('a[data-test-selector=\"RC-course__name\"]')\n                if not venue_node:\n                    continue\n                venue_raw = venue_node.text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n\n                race_time_node = parser.css_first('span[data-test-selector=\"RC-course__time\"]')\n                if not race_time_node:\n                    continue\n                race_time_str = race_time_node.text(strip=True)\n\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            if runner := self._parse_runner(node):\n                runners.append(runner)\n        return runners\n\n    def _parse_runner(self, node: Node) -> Optional[Runner]:\n        try:\n            number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n            name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n            odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n            if not all([number_node, name_node, odds_node]):\n                return None\n\n            number_str = clean_text(number_node.text())\n            number = int(number_str) if number_str and number_str.isdigit() else 0\n            name = clean_text(name_node.text())\n            odds_str = clean_text(odds_node.text())\n            scratched = \"NR\" in odds_str.upper() or not odds_str\n\n            odds = {}\n            if not scratched:\n                win_odds = parse_odds_to_decimal(odds_str)\n                if win_odds and win_odds < 999:\n                    odds = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n\n            return Runner(number=number, name=name, odds=odds, scratched=scratched)\n        except (ValueError, AttributeError):\n            self.logger.warning(\"Could not parse RacingPost runner, skipping.\", exc_info=True)\n            return None\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for The Racing API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(self.http_client, \"GET\", endpoint, headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in TheRacingAPI response.\")\n            return []\n\n        races = []\n        for race_data in raw_data.get(\"racecards\", []):\n            try:\n                race_id = race_data.get(\"race_id\")\n                off_time = race_data.get(\"off_time\")\n                course = race_data.get(\"course\")\n                race_no = race_data.get(\"race_no\")\n\n                if not all([race_id, off_time, course, race_no]):\n                    continue\n\n                start_time = datetime.fromisoformat(off_time.replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_id}\",\n                    venue=course,\n                    race_number=race_no,\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_data.get(\"race_id\"),\n                    exc_info=True,\n                )\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                horse = runner_data.get(\"horse\")\n                if not horse:\n                    continue\n\n                odds_data = {}\n                odds_list = runner_data.get(\"odds\", [])\n                if odds_list:\n                    odds_decimal_str = odds_list[0].get(\"odds_decimal\")\n                    if odds_decimal_str:\n                        win_odds = Decimal(str(odds_decimal_str))\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=horse,\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI runner\",\n                    runner_name=runner_data.get(\"horse\"),\n                    exc_info=True,\n                )\n        return runners\n",
    "python_service/adapters/utils.py": "# python_service/adapters/utils.py\n# Compatibility shim to re-export parse_odds from the centralized location.\n\nfrom ..utils.odds import parse_odds\n\n__all__ = [\"parse_odds\"]\n",
    "python_service/core/__init__.py": "",
    "python_service/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return (\n                None,\n                \"Missing core fields (id, venue, race_number, start_time, runners)\",\n            )\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\"\n                        )\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "python_service/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"127.0.0.1\", port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "python_service/middleware/__init__.py": "",
    "python_service/port_check.py": "import socket\nimport sys\n\n\ndef is_port_in_use(port: int, host: str = \"127.0.0.1\") -> bool:\n    \"\"\"\n    Checks if a local port is already in use.\n\n    Args:\n        port: The port number to check.\n        host: The host to check (defaults to localhost).\n\n    Returns:\n        True if the port is in use, False otherwise.\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.bind((host, port))\n            return False\n        except OSError:\n            return True\n\n\ndef check_port_and_exit_if_in_use(port: int, host: str = \"127.0.0.1\"):\n    \"\"\"\n    Checks the specified port and exits the application with a user-friendly\n    message if it's already in use.\n    \"\"\"\n    # Note: A simple s.connect_ex((host, port)) == 0 is not reliable, as it can\n    # intermittently fail depending on socket states. A full bind attempt is\n    # the most robust way to check for port availability.\n    if is_port_in_use(port, host):\n        print(f\"--- FATAL ERROR ---\")\n        print(f\"Port {port} on host {host} is already in use by another application.\")\n        print(f\"Please close the other application or configure Fortuna Faucet to use a different port.\")\n        print(f\"-------------------\")\n        # Use sys.exit to ensure a clean exit, especially important for PyInstaller executables.\n        sys.exit(1)\n",
    "python_service/requirements.txt": "#\n# This file is autogenerated by pip-compile with Python 3.12\n# by the following command:\n#\n#    pip-compile --output-file=python_service/requirements.txt python_service/requirements.in\n#\naiosqlite==0.21.0\n    # via -r python_service/requirements.in\naltgraph==0.17.4\n    # via pyinstaller\nannotated-doc==0.0.4\n    # via fastapi\nannotated-types==0.7.0\n    # via pydantic\nanyio==4.11.0\n    # via\n    #   httpx\n    #   starlette\nbeautifulsoup4==4.14.2\n    # via -r python_service/requirements.in\nblack==25.11.0\n    # via -r python_service/requirements.in\nbuild==1.3.0\n    # via pip-tools\ncertifi==2025.10.5\n    # via\n    #   -r python_service/requirements.in\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==2.0.0\n    # via cryptography\ncharset-normalizer==3.4.4\n    # via requests\nclick==8.3.0\n    # via\n    #   black\n    #   pip-tools\n    #   uvicorn\ncryptography==46.0.3\n    # via\n    #   -r python_service/requirements.in\n    #   secretstorage\ndeprecated==1.3.1\n    # via limits\nfastapi==0.121.1\n    # via -r python_service/requirements.in\ngreenlet==3.2.4\n    # via sqlalchemy\nh11==0.16.0\n    # via\n    #   httpcore\n    #   uvicorn\nh2==4.3.0\n    # via httpx\nhpack==4.1.0\n    # via h2\nhttpcore==1.0.9\n    # via httpx\nhttpx[http2]==0.28.1\n    # via -r python_service/requirements.in\nhyperframe==6.1.0\n    # via h2\nidna==3.11\n    # via\n    #   anyio\n    #   httpx\n    #   requests\niniconfig==2.3.0\n    # via pytest\njaraco-classes==3.4.0\n    # via keyring\njaraco-context==6.0.1\n    # via keyring\njaraco-functools==4.3.0\n    # via keyring\njeepney==0.9.0\n    # via\n    #   keyring\n    #   secretstorage\nkeyring==25.6.0\n    # via -r python_service/requirements.in\nlimits==5.6.0\n    # via slowapi\nmore-itertools==10.8.0\n    # via\n    #   jaraco-classes\n    #   jaraco-functools\nmypy-extensions==1.1.0\n    # via black\nnumpy==2.3.4\n    # via\n    #   -r python_service/requirements.in\n    #   pandas\n    #   scipy\npackaging==25.0\n    # via\n    #   black\n    #   build\n    #   limits\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\n    #   pytest\npandas==2.3.3\n    # via -r python_service/requirements.in\npathspec==0.12.1\n    # via black\npip-tools==7.5.1\n    # via -r python_service/requirements.in\nplatformdirs==4.5.0\n    # via black\npluggy==1.6.0\n    # via pytest\npsutil==7.1.3\n    # via -r python_service/requirements.in\npsycopg2-binary==2.9.11\n    # via -r python_service/requirements.in\npycparser==2.23\n    # via cffi\npydantic==2.12.4\n    # via\n    #   fastapi\n    #   pydantic-settings\npydantic-core==2.41.5\n    # via pydantic\npydantic-settings==2.12.0\n    # via -r python_service/requirements.in\npygments==2.19.2\n    # via pytest\npyinstaller==6.6.0\n    # via -r python_service/requirements.in\npyinstaller-hooks-contrib==2025.9\n    # via pyinstaller\npyproject-hooks==1.2.0\n    # via\n    #   build\n    #   pip-tools\npytest==9.0.0\n    # via\n    #   -r python_service/requirements.in\n    #   pytest-asyncio\npytest-asyncio==1.3.0\n    # via -r python_service/requirements.in\npython-dateutil==2.9.0.post0\n    # via pandas\npython-dotenv==1.2.1\n    # via pydantic-settings\npytokens==0.3.0\n    # via black\npytz==2025.2\n    # via pandas\nredis==7.0.1\n    # via -r python_service/requirements.in\nrequests==2.32.5\n    # via -r python_service/requirements.in\nscipy==1.16.3\n    # via -r python_service/requirements.in\nsecretstorage==3.4.1\n    # via keyring\nselectolax==0.4.0\n    # via -r python_service/requirements.in\nsix==1.17.0\n    # via python-dateutil\nslowapi==0.1.9\n    # via -r python_service/requirements.in\nsniffio==1.3.1\n    # via anyio\nsoupsieve==2.8\n    # via beautifulsoup4\nsqlalchemy==2.0.44\n    # via -r python_service/requirements.in\nstarlette==0.49.3\n    # via fastapi\nstructlog==25.5.0\n    # via -r python_service/requirements.in\ntenacity==8.5.0\n    # via -r python_service/requirements.in\ntyping-extensions==4.15.0\n    # via\n    #   aiosqlite\n    #   anyio\n    #   beautifulsoup4\n    #   fastapi\n    #   limits\n    #   pydantic\n    #   pydantic-core\n    #   pytest-asyncio\n    #   sqlalchemy\n    #   starlette\n    #   typing-inspection\ntyping-inspection==0.4.2\n    # via\n    #   pydantic\n    #   pydantic-settings\ntzdata==2025.2\n    # via pandas\nurllib3>=2.6.0\n    # via\n    #   -r python_service/requirements.in\n    #   requests\nuvicorn==0.30.1\n    # via -r python_service/requirements.in\nwheel==0.45.1\n    # via\n    #   -r python_service/requirements.in\n    #   pip-tools\nwrapt==2.0.1\n    # via deprecated\n\n# The following packages are considered to be unsafe in a requirements file:\n# pip\n# setuptools\n",
    "python_service/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "scripts/convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom multiprocessing import Queue\n\n# --- Configuration ---\nMANIFEST_FILES = [\n    \"MANIFEST_PART1_BACKEND.json\",\n    \"MANIFEST_PART2_FRONTEND.json\",\n    \"MANIFEST_PART3_SUPPORT.json\",\n    \"MANIFEST_PART4_ROOT.json\",\n]\nOUTPUT_DIR = \"ReviewableJSON\"\nFILE_PROCESSING_TIMEOUT = 10\nEXCLUDED_FILES = [\"package-lock.json\"]\nMAX_FILE_SIZE_MB = 10  # Max file size in megabytes\n\n\ndef read_json_manifest(manifest_path: str) -> list[str]:\n    \"\"\"Reads a JSON manifest file and returns a list of file paths.\"\"\"\n    try:\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError):\n        return []\n\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\n\ndef convert_file_to_json_sandboxed(file_path):\n    # --- Pre-flight check: File size ---\n    try:\n        file_size = os.path.getsize(file_path)\n        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:\n            return {\"error\": f\"File exceeds {MAX_FILE_SIZE_MB}MB size limit.\"}\n    except FileNotFoundError:\n        return {\"error\": \"File not found.\"}\n    except Exception as e:\n        return {\"error\": f\"Could not check file size: {e}\"}\n\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    try:\n        if p.is_alive():\n            print(f\"    [WARNING] Process for {file_path} timed out. Attempting graceful termination...\")\n            p.terminate()\n            p.join(timeout=2)  # Give it a moment to terminate gracefully\n\n            if p.is_alive():\n                print(f\"    [ERROR] Graceful termination failed. Forcibly killing process...\")\n                p.kill()  # The ultimate \"just die\"\n                p.join()\n            return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n        if not q.empty():\n            return q.get()\n        return {\"error\": \"Unknown error in sandboxed read process.\"}\n    finally:\n        # \u2705 Properly close and flush the queue\n        try:\n            while not q.empty():\n                q.get_nowait()\n        except Exception:\n            pass\n        q.close()\n        q.join_thread()\n\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'=' * 60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'=' * 60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        paths = read_json_manifest(manifest)\n        if paths:\n            all_local_paths.extend(paths)\n            print(f\"    --> Found {len(paths)} valid file paths.\")\n        else:\n            print(f\"    [WARNING] Manifest not found or is empty: {manifest}\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        if os.path.basename(local_path) in EXCLUDED_FILES:\n            print(f\"\\n--> Skipping excluded file: {local_path}\")\n            failed_count += 1\n            continue\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + \".json\")\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Backup process complete.\")\n    print(f\"Successfully processed: {processed_count}/{len(unique_local_paths)}\")\n    print(f\"Failed/Skipped: {failed_count}\")\n    print(f\"{'=' * 60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/install_fortuna_silent.bat": "@echo off\nREM Automated deployment (no UI, minimal interaction)\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" ^\n    /qn ^\n    /l*v \"%TEMP%\\fortuna_silent_install.log\" ^\n    /norestart ^\n    ALLUSERS=1 ^\n    INSTALLSCOPE=perMachine\n\nexit /b %errorlevel%",
    "web_platform/api_gateway/src/services/DatabaseService.ts": "import sqlite3 from 'sqlite3';\nimport { open, Database } from 'sqlite';\nimport path from 'path';\n\nexport class DatabaseService {\n  private db: Database | null = null;\n\n  private async getDb(): Promise<Database> {\n    if (!this.db) {\n      const dbPath = process.env.FORTUNA_DB_PATH || path.join(process.cwd(), '../../../../shared_database/races.db');\n      this.db = await open({\n        filename: dbPath,\n        driver: sqlite3.Database\n      });\n    }\n    return this.db;\n  }\n\n  async getQualifiedRaces(): Promise<any[]> {\n    const db = await this.getDb();\n    return db.all(`SELECT * FROM qualified_races`);\n  }\n}\n",
    "web_platform/frontend/next-env.d.ts": "/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.\n",
    "web_platform/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"@tanstack/react-query\": \"^5.28.9\",\n    \"file-saver\": \"^2.0.5\",\n    \"lucide-react\": \"^0.548.0\",\n    \"next\": \"^14.2.33\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"socket.io-client\": \"^4.7.4\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"file-saver\": \"^2.0.5\",\n    \"next-pwa\": \"^5.6.0\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.3.0\",\n    \"typescript\": \"^5\"\n  }\n}\n",
    "web_platform/frontend/public/workbox-4754cb34.js": "define([\"exports\"],function(t){\"use strict\";try{self[\"workbox:core:6.5.4\"]&&_()}catch(t){}const e=(t,...e)=>{let s=t;return e.length>0&&(s+=` :: ${JSON.stringify(e)}`),s};class s extends Error{constructor(t,s){super(e(t,s)),this.name=t,this.details=s}}try{self[\"workbox:routing:6.5.4\"]&&_()}catch(t){}const n=t=>t&&\"object\"==typeof t?t:{handle:t};class r{constructor(t,e,s=\"GET\"){this.handler=n(e),this.match=t,this.method=s}setCatchHandler(t){this.catchHandler=n(t)}}class i extends r{constructor(t,e,s){super(({url:e})=>{const s=t.exec(e.href);if(s&&(e.origin===location.origin||0===s.index))return s.slice(1)},e,s)}}class a{constructor(){this.t=new Map,this.i=new Map}get routes(){return this.t}addFetchListener(){self.addEventListener(\"fetch\",t=>{const{request:e}=t,s=this.handleRequest({request:e,event:t});s&&t.respondWith(s)})}addCacheListener(){self.addEventListener(\"message\",t=>{if(t.data&&\"CACHE_URLS\"===t.data.type){const{payload:e}=t.data,s=Promise.all(e.urlsToCache.map(e=>{\"string\"==typeof e&&(e=[e]);const s=new Request(...e);return this.handleRequest({request:s,event:t})}));t.waitUntil(s),t.ports&&t.ports[0]&&s.then(()=>t.ports[0].postMessage(!0))}})}handleRequest({request:t,event:e}){const s=new URL(t.url,location.href);if(!s.protocol.startsWith(\"http\"))return;const n=s.origin===location.origin,{params:r,route:i}=this.findMatchingRoute({event:e,request:t,sameOrigin:n,url:s});let a=i&&i.handler;const o=t.method;if(!a&&this.i.has(o)&&(a=this.i.get(o)),!a)return;let c;try{c=a.handle({url:s,request:t,event:e,params:r})}catch(t){c=Promise.reject(t)}const h=i&&i.catchHandler;return c instanceof Promise&&(this.o||h)&&(c=c.catch(async n=>{if(h)try{return await h.handle({url:s,request:t,event:e,params:r})}catch(t){t instanceof Error&&(n=t)}if(this.o)return this.o.handle({url:s,request:t,event:e});throw n})),c}findMatchingRoute({url:t,sameOrigin:e,request:s,event:n}){const r=this.t.get(s.method)||[];for(const i of r){let r;const a=i.match({url:t,sameOrigin:e,request:s,event:n});if(a)return r=a,(Array.isArray(r)&&0===r.length||a.constructor===Object&&0===Object.keys(a).length||\"boolean\"==typeof a)&&(r=void 0),{route:i,params:r}}return{}}setDefaultHandler(t,e=\"GET\"){this.i.set(e,n(t))}setCatchHandler(t){this.o=n(t)}registerRoute(t){this.t.has(t.method)||this.t.set(t.method,[]),this.t.get(t.method).push(t)}unregisterRoute(t){if(!this.t.has(t.method))throw new s(\"unregister-route-but-not-found-with-method\",{method:t.method});const e=this.t.get(t.method).indexOf(t);if(!(e>-1))throw new s(\"unregister-route-route-not-registered\");this.t.get(t.method).splice(e,1)}}let o;const c=()=>(o||(o=new a,o.addFetchListener(),o.addCacheListener()),o);function h(t,e,n){let a;if(\"string\"==typeof t){const s=new URL(t,location.href);a=new r(({url:t})=>t.href===s.href,e,n)}else if(t instanceof RegExp)a=new i(t,e,n);else if(\"function\"==typeof t)a=new r(t,e,n);else{if(!(t instanceof r))throw new s(\"unsupported-route-type\",{moduleName:\"workbox-routing\",funcName:\"registerRoute\",paramName:\"capture\"});a=t}return c().registerRoute(a),a}try{self[\"workbox:strategies:6.5.4\"]&&_()}catch(t){}const u={cacheWillUpdate:async({response:t})=>200===t.status||0===t.status?t:null},l={googleAnalytics:\"googleAnalytics\",precache:\"precache-v2\",prefix:\"workbox\",runtime:\"runtime\",suffix:\"undefined\"!=typeof registration?registration.scope:\"\"},f=t=>[l.prefix,t,l.suffix].filter(t=>t&&t.length>0).join(\"-\"),w=t=>t||f(l.precache),d=t=>t||f(l.runtime);function p(t,e){const s=new URL(t);for(const t of e)s.searchParams.delete(t);return s.href}class y{constructor(){this.promise=new Promise((t,e)=>{this.resolve=t,this.reject=e})}}const g=new Set;function m(t){return\"string\"==typeof t?new Request(t):t}class v{constructor(t,e){this.h={},Object.assign(this,e),this.event=e.event,this.u=t,this.l=new y,this.p=[],this.m=[...t.plugins],this.v=new Map;for(const t of this.m)this.v.set(t,{});this.event.waitUntil(this.l.promise)}async fetch(t){const{event:e}=this;let n=m(t);if(\"navigate\"===n.mode&&e instanceof FetchEvent&&e.preloadResponse){const t=await e.preloadResponse;if(t)return t}const r=this.hasCallback(\"fetchDidFail\")?n.clone():null;try{for(const t of this.iterateCallbacks(\"requestWillFetch\"))n=await t({request:n.clone(),event:e})}catch(t){if(t instanceof Error)throw new s(\"plugin-error-request-will-fetch\",{thrownErrorMessage:t.message})}const i=n.clone();try{let t;t=await fetch(n,\"navigate\"===n.mode?void 0:this.u.fetchOptions);for(const s of this.iterateCallbacks(\"fetchDidSucceed\"))t=await s({event:e,request:i,response:t});return t}catch(t){throw r&&await this.runCallbacks(\"fetchDidFail\",{error:t,event:e,originalRequest:r.clone(),request:i.clone()}),t}}async fetchAndCachePut(t){const e=await this.fetch(t),s=e.clone();return this.waitUntil(this.cachePut(t,s)),e}async cacheMatch(t){const e=m(t);let s;const{cacheName:n,matchOptions:r}=this.u,i=await this.getCacheKey(e,\"read\"),a=Object.assign(Object.assign({},r),{cacheName:n});s=await caches.match(i,a);for(const t of this.iterateCallbacks(\"cachedResponseWillBeUsed\"))s=await t({cacheName:n,matchOptions:r,cachedResponse:s,request:i,event:this.event})||void 0;return s}async cachePut(t,e){const n=m(t);var r;await(r=0,new Promise(t=>setTimeout(t,r)));const i=await this.getCacheKey(n,\"write\");if(!e)throw new s(\"cache-put-with-no-response\",{url:(a=i.url,new URL(String(a),location.href).href.replace(new RegExp(`^${location.origin}`),\"\"))});var a;const o=await this.R(e);if(!o)return!1;const{cacheName:c,matchOptions:h}=this.u,u=await self.caches.open(c),l=this.hasCallback(\"cacheDidUpdate\"),f=l?await async function(t,e,s,n){const r=p(e.url,s);if(e.url===r)return t.match(e,n);const i=Object.assign(Object.assign({},n),{ignoreSearch:!0}),a=await t.keys(e,i);for(const e of a)if(r===p(e.url,s))return t.match(e,n)}(u,i.clone(),[\"__WB_REVISION__\"],h):null;try{await u.put(i,l?o.clone():o)}catch(t){if(t instanceof Error)throw\"QuotaExceededError\"===t.name&&await async function(){for(const t of g)await t()}(),t}for(const t of this.iterateCallbacks(\"cacheDidUpdate\"))await t({cacheName:c,oldResponse:f,newResponse:o.clone(),request:i,event:this.event});return!0}async getCacheKey(t,e){const s=`${t.url} | ${e}`;if(!this.h[s]){let n=t;for(const t of this.iterateCallbacks(\"cacheKeyWillBeUsed\"))n=m(await t({mode:e,request:n,event:this.event,params:this.params}));this.h[s]=n}return this.h[s]}hasCallback(t){for(const e of this.u.plugins)if(t in e)return!0;return!1}async runCallbacks(t,e){for(const s of this.iterateCallbacks(t))await s(e)}*iterateCallbacks(t){for(const e of this.u.plugins)if(\"function\"==typeof e[t]){const s=this.v.get(e),n=n=>{const r=Object.assign(Object.assign({},n),{state:s});return e[t](r)};yield n}}waitUntil(t){return this.p.push(t),t}async doneWaiting(){let t;for(;t=this.p.shift();)await t}destroy(){this.l.resolve(null)}async R(t){let e=t,s=!1;for(const t of this.iterateCallbacks(\"cacheWillUpdate\"))if(e=await t({request:this.request,response:e,event:this.event})||void 0,s=!0,!e)break;return s||e&&200!==e.status&&(e=void 0),e}}class R{constructor(t={}){this.cacheName=d(t.cacheName),this.plugins=t.plugins||[],this.fetchOptions=t.fetchOptions,this.matchOptions=t.matchOptions}handle(t){const[e]=this.handleAll(t);return e}handleAll(t){t instanceof FetchEvent&&(t={event:t,request:t.request});const e=t.event,s=\"string\"==typeof t.request?new Request(t.request):t.request,n=\"params\"in t?t.params:void 0,r=new v(this,{event:e,request:s,params:n}),i=this.q(r,s,e);return[i,this.D(i,r,s,e)]}async q(t,e,n){let r;await t.runCallbacks(\"handlerWillStart\",{event:n,request:e});try{if(r=await this.U(e,t),!r||\"error\"===r.type)throw new s(\"no-response\",{url:e.url})}catch(s){if(s instanceof Error)for(const i of t.iterateCallbacks(\"handlerDidError\"))if(r=await i({error:s,event:n,request:e}),r)break;if(!r)throw s}for(const s of t.iterateCallbacks(\"handlerWillRespond\"))r=await s({event:n,request:e,response:r});return r}async D(t,e,s,n){let r,i;try{r=await t}catch(i){}try{await e.runCallbacks(\"handlerDidRespond\",{event:n,request:s,response:r}),await e.doneWaiting()}catch(t){t instanceof Error&&(i=t)}if(await e.runCallbacks(\"handlerDidComplete\",{event:n,request:s,response:r,error:i}),e.destroy(),i)throw i}}function b(t){t.then(()=>{})}function q(){return q=Object.assign?Object.assign.bind():function(t){for(var e=1;e<arguments.length;e++){var s=arguments[e];for(var n in s)({}).hasOwnProperty.call(s,n)&&(t[n]=s[n])}return t},q.apply(null,arguments)}let D,U;const x=new WeakMap,L=new WeakMap,I=new WeakMap,C=new WeakMap,E=new WeakMap;let N={get(t,e,s){if(t instanceof IDBTransaction){if(\"done\"===e)return L.get(t);if(\"objectStoreNames\"===e)return t.objectStoreNames||I.get(t);if(\"store\"===e)return s.objectStoreNames[1]?void 0:s.objectStore(s.objectStoreNames[0])}return k(t[e])},set:(t,e,s)=>(t[e]=s,!0),has:(t,e)=>t instanceof IDBTransaction&&(\"done\"===e||\"store\"===e)||e in t};function O(t){return t!==IDBDatabase.prototype.transaction||\"objectStoreNames\"in IDBTransaction.prototype?(U||(U=[IDBCursor.prototype.advance,IDBCursor.prototype.continue,IDBCursor.prototype.continuePrimaryKey])).includes(t)?function(...e){return t.apply(B(this),e),k(x.get(this))}:function(...e){return k(t.apply(B(this),e))}:function(e,...s){const n=t.call(B(this),e,...s);return I.set(n,e.sort?e.sort():[e]),k(n)}}function T(t){return\"function\"==typeof t?O(t):(t instanceof IDBTransaction&&function(t){if(L.has(t))return;const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"complete\",r),t.removeEventListener(\"error\",i),t.removeEventListener(\"abort\",i)},r=()=>{e(),n()},i=()=>{s(t.error||new DOMException(\"AbortError\",\"AbortError\")),n()};t.addEventListener(\"complete\",r),t.addEventListener(\"error\",i),t.addEventListener(\"abort\",i)});L.set(t,e)}(t),e=t,(D||(D=[IDBDatabase,IDBObjectStore,IDBIndex,IDBCursor,IDBTransaction])).some(t=>e instanceof t)?new Proxy(t,N):t);var e}function k(t){if(t instanceof IDBRequest)return function(t){const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener(\"success\",r),t.removeEventListener(\"error\",i)},r=()=>{e(k(t.result)),n()},i=()=>{s(t.error),n()};t.addEventListener(\"success\",r),t.addEventListener(\"error\",i)});return e.then(e=>{e instanceof IDBCursor&&x.set(e,t)}).catch(()=>{}),E.set(e,t),e}(t);if(C.has(t))return C.get(t);const e=T(t);return e!==t&&(C.set(t,e),E.set(e,t)),e}const B=t=>E.get(t);const P=[\"get\",\"getKey\",\"getAll\",\"getAllKeys\",\"count\"],M=[\"put\",\"add\",\"delete\",\"clear\"],W=new Map;function j(t,e){if(!(t instanceof IDBDatabase)||e in t||\"string\"!=typeof e)return;if(W.get(e))return W.get(e);const s=e.replace(/FromIndex$/,\"\"),n=e!==s,r=M.includes(s);if(!(s in(n?IDBIndex:IDBObjectStore).prototype)||!r&&!P.includes(s))return;const i=async function(t,...e){const i=this.transaction(t,r?\"readwrite\":\"readonly\");let a=i.store;return n&&(a=a.index(e.shift())),(await Promise.all([a[s](...e),r&&i.done]))[0]};return W.set(e,i),i}N=(t=>q({},t,{get:(e,s,n)=>j(e,s)||t.get(e,s,n),has:(e,s)=>!!j(e,s)||t.has(e,s)}))(N);try{self[\"workbox:expiration:6.5.4\"]&&_()}catch(t){}const S=\"cache-entries\",K=t=>{const e=new URL(t,location.href);return e.hash=\"\",e.href};class A{constructor(t){this._=null,this.L=t}I(t){const e=t.createObjectStore(S,{keyPath:\"id\"});e.createIndex(\"cacheName\",\"cacheName\",{unique:!1}),e.createIndex(\"timestamp\",\"timestamp\",{unique:!1})}C(t){this.I(t),this.L&&function(t,{blocked:e}={}){const s=indexedDB.deleteDatabase(t);e&&s.addEventListener(\"blocked\",t=>e(t.oldVersion,t)),k(s).then(()=>{})}(this.L)}async setTimestamp(t,e){const s={url:t=K(t),timestamp:e,cacheName:this.L,id:this.N(t)},n=(await this.getDb()).transaction(S,\"readwrite\",{durability:\"relaxed\"});await n.store.put(s),await n.done}async getTimestamp(t){const e=await this.getDb(),s=await e.get(S,this.N(t));return null==s?void 0:s.timestamp}async expireEntries(t,e){const s=await this.getDb();let n=await s.transaction(S).store.index(\"timestamp\").openCursor(null,\"prev\");const r=[];let i=0;for(;n;){const s=n.value;s.cacheName===this.L&&(t&&s.timestamp<t||e&&i>=e?r.push(n.value):i++),n=await n.continue()}const a=[];for(const t of r)await s.delete(S,t.id),a.push(t.url);return a}N(t){return this.L+\"|\"+K(t)}async getDb(){return this._||(this._=await function(t,e,{blocked:s,upgrade:n,blocking:r,terminated:i}={}){const a=indexedDB.open(t,e),o=k(a);return n&&a.addEventListener(\"upgradeneeded\",t=>{n(k(a.result),t.oldVersion,t.newVersion,k(a.transaction),t)}),s&&a.addEventListener(\"blocked\",t=>s(t.oldVersion,t.newVersion,t)),o.then(t=>{i&&t.addEventListener(\"close\",()=>i()),r&&t.addEventListener(\"versionchange\",t=>r(t.oldVersion,t.newVersion,t))}).catch(()=>{}),o}(\"workbox-expiration\",1,{upgrade:this.C.bind(this)})),this._}}class F{constructor(t,e={}){this.O=!1,this.T=!1,this.k=e.maxEntries,this.B=e.maxAgeSeconds,this.P=e.matchOptions,this.L=t,this.M=new A(t)}async expireEntries(){if(this.O)return void(this.T=!0);this.O=!0;const t=this.B?Date.now()-1e3*this.B:0,e=await this.M.expireEntries(t,this.k),s=await self.caches.open(this.L);for(const t of e)await s.delete(t,this.P);this.O=!1,this.T&&(this.T=!1,b(this.expireEntries()))}async updateTimestamp(t){await this.M.setTimestamp(t,Date.now())}async isURLExpired(t){if(this.B){const e=await this.M.getTimestamp(t),s=Date.now()-1e3*this.B;return void 0===e||e<s}return!1}async delete(){this.T=!1,await this.M.expireEntries(1/0)}}try{self[\"workbox:range-requests:6.5.4\"]&&_()}catch(t){}async function H(t,e){try{if(206===e.status)return e;const n=t.headers.get(\"range\");if(!n)throw new s(\"no-range-header\");const r=function(t){const e=t.trim().toLowerCase();if(!e.startsWith(\"bytes=\"))throw new s(\"unit-must-be-bytes\",{normalizedRangeHeader:e});if(e.includes(\",\"))throw new s(\"single-range-only\",{normalizedRangeHeader:e});const n=/(\\d*)-(\\d*)/.exec(e);if(!n||!n[1]&&!n[2])throw new s(\"invalid-range-values\",{normalizedRangeHeader:e});return{start:\"\"===n[1]?void 0:Number(n[1]),end:\"\"===n[2]?void 0:Number(n[2])}}(n),i=await e.blob(),a=function(t,e,n){const r=t.size;if(n&&n>r||e&&e<0)throw new s(\"range-not-satisfiable\",{size:r,end:n,start:e});let i,a;return void 0!==e&&void 0!==n?(i=e,a=n+1):void 0!==e&&void 0===n?(i=e,a=r):void 0!==n&&void 0===e&&(i=r-n,a=r),{start:i,end:a}}(i,r.start,r.end),o=i.slice(a.start,a.end),c=o.size,h=new Response(o,{status:206,statusText:\"Partial Content\",headers:e.headers});return h.headers.set(\"Content-Length\",String(c)),h.headers.set(\"Content-Range\",`bytes ${a.start}-${a.end-1}/${i.size}`),h}catch(t){return new Response(\"\",{status:416,statusText:\"Range Not Satisfiable\"})}}function $(t,e){const s=e();return t.waitUntil(s),s}try{self[\"workbox:precaching:6.5.4\"]&&_()}catch(t){}function z(t){if(!t)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(\"string\"==typeof t){const e=new URL(t,location.href);return{cacheKey:e.href,url:e.href}}const{revision:e,url:n}=t;if(!n)throw new s(\"add-to-cache-list-unexpected-type\",{entry:t});if(!e){const t=new URL(n,location.href);return{cacheKey:t.href,url:t.href}}const r=new URL(n,location.href),i=new URL(n,location.href);return r.searchParams.set(\"__WB_REVISION__\",e),{cacheKey:r.href,url:i.href}}class G{constructor(){this.updatedURLs=[],this.notUpdatedURLs=[],this.handlerWillStart=async({request:t,state:e})=>{e&&(e.originalRequest=t)},this.cachedResponseWillBeUsed=async({event:t,state:e,cachedResponse:s})=>{if(\"install\"===t.type&&e&&e.originalRequest&&e.originalRequest instanceof Request){const t=e.originalRequest.url;s?this.notUpdatedURLs.push(t):this.updatedURLs.push(t)}return s}}}class V{constructor({precacheController:t}){this.cacheKeyWillBeUsed=async({request:t,params:e})=>{const s=(null==e?void 0:e.cacheKey)||this.W.getCacheKeyForURL(t.url);return s?new Request(s,{headers:t.headers}):t},this.W=t}}let J,Q;async function X(t,e){let n=null;if(t.url){n=new URL(t.url).origin}if(n!==self.location.origin)throw new s(\"cross-origin-copy-response\",{origin:n});const r=t.clone(),i={headers:new Headers(r.headers),status:r.status,statusText:r.statusText},a=e?e(i):i,o=function(){if(void 0===J){const t=new Response(\"\");if(\"body\"in t)try{new Response(t.body),J=!0}catch(t){J=!1}J=!1}return J}()?r.body:await r.blob();return new Response(o,a)}class Y extends R{constructor(t={}){t.cacheName=w(t.cacheName),super(t),this.j=!1!==t.fallbackToNetwork,this.plugins.push(Y.copyRedirectedCacheableResponsesPlugin)}async U(t,e){const s=await e.cacheMatch(t);return s||(e.event&&\"install\"===e.event.type?await this.S(t,e):await this.K(t,e))}async K(t,e){let n;const r=e.params||{};if(!this.j)throw new s(\"missing-precache-entry\",{cacheName:this.cacheName,url:t.url});{const s=r.integrity,i=t.integrity,a=!i||i===s;n=await e.fetch(new Request(t,{integrity:\"no-cors\"!==t.mode?i||s:void 0})),s&&a&&\"no-cors\"!==t.mode&&(this.A(),await e.cachePut(t,n.clone()))}return n}async S(t,e){this.A();const n=await e.fetch(t);if(!await e.cachePut(t,n.clone()))throw new s(\"bad-precaching-response\",{url:t.url,status:n.status});return n}A(){let t=null,e=0;for(const[s,n]of this.plugins.entries())n!==Y.copyRedirectedCacheableResponsesPlugin&&(n===Y.defaultPrecacheCacheabilityPlugin&&(t=s),n.cacheWillUpdate&&e++);0===e?this.plugins.push(Y.defaultPrecacheCacheabilityPlugin):e>1&&null!==t&&this.plugins.splice(t,1)}}Y.defaultPrecacheCacheabilityPlugin={cacheWillUpdate:async({response:t})=>!t||t.status>=400?null:t},Y.copyRedirectedCacheableResponsesPlugin={cacheWillUpdate:async({response:t})=>t.redirected?await X(t):t};class Z{constructor({cacheName:t,plugins:e=[],fallbackToNetwork:s=!0}={}){this.F=new Map,this.H=new Map,this.$=new Map,this.u=new Y({cacheName:w(t),plugins:[...e,new V({precacheController:this})],fallbackToNetwork:s}),this.install=this.install.bind(this),this.activate=this.activate.bind(this)}get strategy(){return this.u}precache(t){this.addToCacheList(t),this.G||(self.addEventListener(\"install\",this.install),self.addEventListener(\"activate\",this.activate),this.G=!0)}addToCacheList(t){const e=[];for(const n of t){\"string\"==typeof n?e.push(n):n&&void 0===n.revision&&e.push(n.url);const{cacheKey:t,url:r}=z(n),i=\"string\"!=typeof n&&n.revision?\"reload\":\"default\";if(this.F.has(r)&&this.F.get(r)!==t)throw new s(\"add-to-cache-list-conflicting-entries\",{firstEntry:this.F.get(r),secondEntry:t});if(\"string\"!=typeof n&&n.integrity){if(this.$.has(t)&&this.$.get(t)!==n.integrity)throw new s(\"add-to-cache-list-conflicting-integrities\",{url:r});this.$.set(t,n.integrity)}if(this.F.set(r,t),this.H.set(r,i),e.length>0){const t=`Workbox is precaching URLs without revision info: ${e.join(\", \")}\\nThis is generally NOT safe. Learn more at https://bit.ly/wb-precache`;console.warn(t)}}}install(t){return $(t,async()=>{const e=new G;this.strategy.plugins.push(e);for(const[e,s]of this.F){const n=this.$.get(s),r=this.H.get(e),i=new Request(e,{integrity:n,cache:r,credentials:\"same-origin\"});await Promise.all(this.strategy.handleAll({params:{cacheKey:s},request:i,event:t}))}const{updatedURLs:s,notUpdatedURLs:n}=e;return{updatedURLs:s,notUpdatedURLs:n}})}activate(t){return $(t,async()=>{const t=await self.caches.open(this.strategy.cacheName),e=await t.keys(),s=new Set(this.F.values()),n=[];for(const r of e)s.has(r.url)||(await t.delete(r),n.push(r.url));return{deletedURLs:n}})}getURLsToCacheKeys(){return this.F}getCachedURLs(){return[...this.F.keys()]}getCacheKeyForURL(t){const e=new URL(t,location.href);return this.F.get(e.href)}getIntegrityForCacheKey(t){return this.$.get(t)}async matchPrecache(t){const e=t instanceof Request?t.url:t,s=this.getCacheKeyForURL(e);if(s){return(await self.caches.open(this.strategy.cacheName)).match(s)}}createHandlerBoundToURL(t){const e=this.getCacheKeyForURL(t);if(!e)throw new s(\"non-precached-url\",{url:t});return s=>(s.request=new Request(t),s.params=Object.assign({cacheKey:e},s.params),this.strategy.handle(s))}}const tt=()=>(Q||(Q=new Z),Q);class et extends r{constructor(t,e){super(({request:s})=>{const n=t.getURLsToCacheKeys();for(const r of function*(t,{ignoreURLParametersMatching:e=[/^utm_/,/^fbclid$/],directoryIndex:s=\"index.html\",cleanURLs:n=!0,urlManipulation:r}={}){const i=new URL(t,location.href);i.hash=\"\",yield i.href;const a=function(t,e=[]){for(const s of[...t.searchParams.keys()])e.some(t=>t.test(s))&&t.searchParams.delete(s);return t}(i,e);if(yield a.href,s&&a.pathname.endsWith(\"/\")){const t=new URL(a.href);t.pathname+=s,yield t.href}if(n){const t=new URL(a.href);t.pathname+=\".html\",yield t.href}if(r){const t=r({url:i});for(const e of t)yield e.href}}(s.url,e)){const e=n.get(r);if(e){return{cacheKey:e,integrity:t.getIntegrityForCacheKey(e)}}}},t.strategy)}}t.CacheFirst=class extends R{async U(t,e){let n,r=await e.cacheMatch(t);if(!r)try{r=await e.fetchAndCachePut(t)}catch(t){t instanceof Error&&(n=t)}if(!r)throw new s(\"no-response\",{url:t.url,error:n});return r}},t.ExpirationPlugin=class{constructor(t={}){this.cachedResponseWillBeUsed=async({event:t,request:e,cacheName:s,cachedResponse:n})=>{if(!n)return null;const r=this.V(n),i=this.J(s);b(i.expireEntries());const a=i.updateTimestamp(e.url);if(t)try{t.waitUntil(a)}catch(t){}return r?n:null},this.cacheDidUpdate=async({cacheName:t,request:e})=>{const s=this.J(t);await s.updateTimestamp(e.url),await s.expireEntries()},this.X=t,this.B=t.maxAgeSeconds,this.Y=new Map,t.purgeOnQuotaError&&function(t){g.add(t)}(()=>this.deleteCacheAndMetadata())}J(t){if(t===d())throw new s(\"expire-custom-caches-only\");let e=this.Y.get(t);return e||(e=new F(t,this.X),this.Y.set(t,e)),e}V(t){if(!this.B)return!0;const e=this.Z(t);if(null===e)return!0;return e>=Date.now()-1e3*this.B}Z(t){if(!t.headers.has(\"date\"))return null;const e=t.headers.get(\"date\"),s=new Date(e).getTime();return isNaN(s)?null:s}async deleteCacheAndMetadata(){for(const[t,e]of this.Y)await self.caches.delete(t),await e.delete();this.Y=new Map}},t.NetworkFirst=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u),this.tt=t.networkTimeoutSeconds||0}async U(t,e){const n=[],r=[];let i;if(this.tt){const{id:s,promise:a}=this.et({request:t,logs:n,handler:e});i=s,r.push(a)}const a=this.st({timeoutId:i,request:t,logs:n,handler:e});r.push(a);const o=await e.waitUntil((async()=>await e.waitUntil(Promise.race(r))||await a)());if(!o)throw new s(\"no-response\",{url:t.url});return o}et({request:t,logs:e,handler:s}){let n;return{promise:new Promise(e=>{n=setTimeout(async()=>{e(await s.cacheMatch(t))},1e3*this.tt)}),id:n}}async st({timeoutId:t,request:e,logs:s,handler:n}){let r,i;try{i=await n.fetchAndCachePut(e)}catch(t){t instanceof Error&&(r=t)}return t&&clearTimeout(t),!r&&i||(i=await n.cacheMatch(e)),i}},t.RangeRequestsPlugin=class{constructor(){this.cachedResponseWillBeUsed=async({request:t,cachedResponse:e})=>e&&t.headers.has(\"range\")?await H(t,e):e}},t.StaleWhileRevalidate=class extends R{constructor(t={}){super(t),this.plugins.some(t=>\"cacheWillUpdate\"in t)||this.plugins.unshift(u)}async U(t,e){const n=e.fetchAndCachePut(t).catch(()=>{});e.waitUntil(n);let r,i=await e.cacheMatch(t);if(i);else try{i=await n}catch(t){t instanceof Error&&(r=t)}if(!i)throw new s(\"no-response\",{url:t.url,error:r});return i}},t.cleanupOutdatedCaches=function(){self.addEventListener(\"activate\",t=>{const e=w();t.waitUntil((async(t,e=\"-precache-\")=>{const s=(await self.caches.keys()).filter(s=>s.includes(e)&&s.includes(self.registration.scope)&&s!==t);return await Promise.all(s.map(t=>self.caches.delete(t))),s})(e).then(t=>{}))})},t.clientsClaim=function(){self.addEventListener(\"activate\",()=>self.clients.claim())},t.precacheAndRoute=function(t,e){!function(t){tt().precache(t)}(t),function(t){const e=tt();h(new et(e,t))}(e)},t.registerRoute=h});\n",
    "web_platform/frontend/src/components/ErrorDisplay.tsx": "// web_platform/frontend/src/components/ErrorDisplay.tsx\n'use client';\n\nimport React from 'react';\n\ninterface ErrorInfo {\n  message: string;\n  suggestion: string;\n  details?: string;\n}\n\ninterface ErrorDisplayProps {\n  error: ErrorInfo;\n}\n\nexport const ErrorDisplay: React.FC<ErrorDisplayProps> = ({ error }) => {\n  return (\n    <div className=\"bg-red-900/20 border border-red-500/30 text-white rounded-lg p-6 max-w-2xl mx-auto my-8\">\n      <div className=\"flex items-center mb-4\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-8 w-8 text-red-400 mr-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n          <path fillRule=\"evenodd\" d=\"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z\" clipRule=\"evenodd\" />\n        </svg>\n        <h2 className=\"text-2xl font-bold text-red-400\">An Error Occurred</h2>\n      </div>\n      <p className=\"text-lg text-slate-300 mb-2\">{error.message}</p>\n      <p className=\"text-slate-400 mb-6\">{error.suggestion}</p>\n      {error.details && (\n        <details className=\"bg-slate-800/50 rounded-lg p-4\">\n          <summary className=\"cursor-pointer text-sm text-slate-500 hover:text-white\">\n            Technical Details\n          </summary>\n          <pre className=\"text-xs text-slate-400 mt-2 p-2 bg-black/30 rounded overflow-x-auto\">\n            <code>{error.details}</code>\n          </pre>\n        </details>\n      )}\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/RaceCardSkeleton.tsx": "// web_platform/frontend/src/components/RaceCardSkeleton.tsx\nimport React from 'react';\n\nexport const RaceCardSkeleton: React.FC = () => {\n  return (\n    <div className=\"race-card-skeleton border border-gray-700 rounded-lg p-4 bg-gray-800 shadow-lg animate-pulse\">\n      {/* Skeleton Header */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <div className=\"h-7 w-28 bg-gray-700 rounded-md\"></div>\n            <div className=\"h-4 w-40 bg-gray-700 rounded-md mt-2\"></div>\n          </div>\n        </div>\n        <div className=\"h-16 w-16 bg-gray-700 rounded-full\"></div>\n      </div>\n\n      {/* Skeleton Info Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n      </div>\n\n      {/* Skeleton Runner Rows */}\n      <div className=\"space-y-2\">\n        {[...Array(3)].map((_, i) => (\n          <div key={i} className=\"runner-row rounded-md p-3\">\n            <div className=\"flex items-center justify-between\">\n              <div className=\"flex items-center gap-4 flex-1\">\n                <div className=\"w-10 h-10 rounded-full bg-gray-700\"></div>\n                <div className=\"flex flex-col space-y-2\">\n                  <div className=\"h-5 w-32 bg-gray-700 rounded-md\"></div>\n                  <div className=\"h-4 w-40 bg-gray-700 rounded-md\"></div>\n                </div>\n              </div>\n              <div className=\"text-right\">\n                <div className=\"h-6 w-16 bg-gray-700 rounded-md\"></div>\n                <div className=\"h-3 w-12 bg-gray-700 rounded-md mt-2\"></div>\n              </div>\n            </div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n};\n",
    "web_platform/frontend/src/components/Tabs.tsx": "// src/components/Tabs.tsx\n'use client';\n\nimport React, { useState } from 'react';\n\ntype Tab = {\n  label: string;\n  content: React.ReactNode;\n};\n\ntype TabsProps = {\n  tabs: Tab[];\n};\n\nexport function Tabs({ tabs }: TabsProps) {\n  const [activeTab, setActiveTab] = useState(0);\n\n  return (\n    <div>\n      <div className=\"border-b border-slate-700\">\n        <nav className=\"-mb-px flex space-x-8\" aria-label=\"Tabs\">\n          {tabs.map((tab, index) => (\n            <button\n              key={tab.label}\n              onClick={() => setActiveTab(index)}\n              className={`${\n                activeTab === index\n                  ? 'border-blue-500 text-blue-400'\n                  : 'border-transparent text-slate-400 hover:text-slate-200 hover:border-slate-500'\n              } whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm transition-colors focus:outline-none`}\n            >\n              {tab.label}\n            </button>\n          ))}\n        </nav>\n      </div>\n      <div className=\"mt-8\">{tabs[activeTab].content}</div>\n    </div>\n  );\n}\n",
    "web_platform/frontend/src/types/electron.d.ts": "// web_platform/frontend/src/types/electron.d.ts\n\n/**\n * This declaration file extends the global Window interface to include the\n * 'electronAPI' object exposed by the preload script. This provides\n * TypeScript with type information for the functions we're using for IPC.\n */\nexport {};\n\ndeclare global {\n  interface Window {\n    electronAPI?: {\n      /**\n       * Asynchronously fetches the secure API key from the main process.\n       * @returns {Promise<string|null>} A promise that resolves with the API key or null if not found.\n       */\n      getApiKey: () => Promise<string | null>;\n      /**\n       * Registers a callback for backend status updates from the main process.\n       * @param callback The function to execute. Receives an object with state and logs.\n       * @returns A function to unsubscribe the listener.\n       */\n      onBackendStatusUpdate: (callback: (status: { state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }) => void) => () => void;\n\n      /**\n       * Sends a command to the main process to restart the backend executable.\n       */\n      restartBackend: () => void;\n\n      /**\n       * Asynchronously fetches the current backend status from the main process.\n       * @returns {Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>}\n       */\n      getBackendStatus: () => Promise<{ state: 'starting' | 'running' | 'error' | 'stopped'; logs: string[] }>;\n      generateApiKey: () => Promise<string>;\n      saveApiKey: (apiKey: string) => Promise<{ success: boolean }>;\n      saveBetfairCredentials: (credentials: { appKey: string; username: string; password: string }) => Promise<{ success: boolean }>;\n      getApiPort: () => Promise<number | null>;\n    };\n  }\n}\n",
    "web_service/backend/__init__.py": "# This file makes this directory a package.\n",
    "web_service/backend/adapters/betfair_auth_mixin.py": "# python_service/adapters/betfair_auth_mixin.py\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..credentials_manager import SecureCredentialsManager\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock = asyncio.Lock()\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        \"\"\"\n        Authenticates with Betfair using credentials from the system's credential manager,\n        ensuring the session token is valid and refreshing it if necessary.\n        \"\"\"\n        async with self._auth_lock:\n            if self.session_token and self.token_expiry and self.token_expiry > (datetime.now() + timedelta(minutes=5)):\n                return\n\n            log.info(\"Attempting to authenticate with Betfair...\")\n            username, password = SecureCredentialsManager.get_betfair_credentials()\n\n            if not all([self.config.BETFAIR_APP_KEY, username, password]):\n                raise ValueError(\"Betfair credentials not fully configured in credential manager.\")\n\n            auth_url = \"https://identitysso.betfair.com/api/login\"\n            headers = {\n                \"X-Application\": self.config.BETFAIR_APP_KEY,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={username}&password={password}\"\n\n            response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n            response.raise_for_status()\n            data = response.json()\n\n            if data.get(\"status\") == \"SUCCESS\":\n                self.session_token = data.get(\"token\")\n                self.token_expiry = datetime.now() + timedelta(hours=3)\n                log.info(\"Betfair authentication successful.\")\n            else:\n                log.error(\"Betfair authentication failed\", error=data.get(\"error\"))\n                self.session_token = None  # Reset token to prevent using a stale one\n                return  # Return gracefully and let the adapter handle the lack of a token\n",
    "web_service/backend/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Note: FanDuel's API is not date-centric. Event discovery would be needed for a robust implementation.\n        # This uses a hardcoded eventId as a placeholder.\n        event_id = \"38183.3\"\n        self.logger.info(f\"Fetching races from FanDuel for event_id: {event_id}\")\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            self.logger.warning(\"FanDuel response missing 'marketGroups' key\")\n            return []\n\n        races = []\n        for group in raw_data.get(\"marketGroups\", []):\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\n                            \"Failed to parse a FanDuel market\",\n                            market=market,\n                            exc_info=True,\n                        )\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            self.logger.warning(f\"Could not parse race and track from FanDuel market name: {market_name}\")\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\").strip()\n        if not race_number_str.isdigit():\n            return None\n        race_number = int(race_number_str)\n\n        track_name = parts[1]\n\n        # Placeholder for start_time - FanDuel's market API doesn't provide it directly\n        start_time = datetime.now(timezone.utc) + timedelta(hours=race_number)\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_runner_odds = runner_data.get(\"winRunnerOdds\", {})\n                current_price = win_runner_odds.get(\"currentPrice\")\n\n                if not runner_name or not current_price:\n                    continue\n\n                numerator, denominator = map(int, current_price.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                odds = OddsData(\n                    win=decimal_odds,\n                    source=self.source_name,\n                    last_updated=datetime.now(timezone.utc),\n                )\n\n                name_parts = runner_name.split(\".\", 1)\n                if len(name_parts) < 2:\n                    continue\n                program_number_str = name_parts[0].strip()\n                horse_name = name_parts[1].strip()\n\n                runners.append(\n                    Runner(\n                        name=horse_name,\n                        number=(int(program_number_str) if program_number_str.isdigit() else 0),\n                        odds={self.source_name: odds},\n                    )\n                )\n            except (ValueError, ZeroDivisionError, IndexError, TypeError):\n                self.logger.warning(\n                    \"Could not parse FanDuel runner\",\n                    runner_data=runner_data,\n                    exc_info=True,\n                )\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/nyrabets_adapter.py": "# python_service/adapters/nyrabets_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass NYRABetsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for nyrabets.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSportsGreyhound\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw greyhound meetings data from the Racing and Sports API.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Accept\": \"application/json\",\n        }\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(\n            self.http_client,\n            \"GET\",\n            \"v1/greyhound/meetings\",\n            headers=headers,\n            params=params,\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Optional[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        all_races = []\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in RacingAndSportsGreyhound response or invalid format.\")\n            return all_races\n\n        for meeting in raw_data.get(\"meetings\", []):\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse RacingAndSportsGreyhound race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True,\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        race_number = race.get(\"raceNumber\")\n\n        if not all([race_id, start_time_str, race_number]):\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\", 0),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n            if isinstance(rd, dict) and rd.get(\"runnerNumber\")\n        ]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race_number,\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "web_service/backend/adapters/template_adapter.py": "# python_service/adapters/template_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TemplateAdapter(BaseAdapterV3):\n    \"\"\"\n    A template for creating new adapters, based on the BaseAdapterV3 pattern.\n    This adapter is a non-functional stub.\n    \"\"\"\n\n    SOURCE_NAME = \"[IMPLEMENT ME] Example Source\"\n    BASE_URL = \"https://api.example.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/cache_manager.py": "# python_service/cache_manager.py\nimport asyncio\nimport hashlib\nimport json\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self):\n        self.redis_client = None\n        self.memory_cache = {}\n        self.is_configured = False\n        log.info(\"CacheManager initialized (not connected).\")\n\n    async def connect(self, redis_url: str):\n        if self.is_configured or not REDIS_AVAILABLE or not redis_url:\n            return\n\n        try:\n            log.info(\"Attempting to connect to Redis...\", url=redis_url)\n            # Use the async version of the client\n            self.redis_client = redis.asyncio.from_url(redis_url, decode_responses=True)\n            await self.redis_client.ping()  # Verify connection asynchronously\n            self.is_configured = True\n            log.info(\"Redis cache connected successfully.\")\n        except (redis.exceptions.ConnectionError, asyncio.TimeoutError) as e:\n            log.warning(\n                \"Failed to connect to Redis. Falling back to in-memory cache.\",\n                error=str(e),\n            )\n            self.redis_client = None\n            self.is_configured = False\n\n    async def disconnect(self):\n        if self.redis_client:\n            await self.redis_client.close()\n            log.info(\"Redis connection closed.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    async def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = await self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis GET failed, falling back to memory cache.\", error=e)\n\n        entry = self.memory_cache.get(key)\n        if entry and entry.get(\"expires_at\", datetime.min) > datetime.now():\n            return entry.get(\"value\")\n        return None\n\n    async def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        try:\n            serialized = json.dumps(value, default=str)\n        except (TypeError, ValueError) as e:\n            log.error(\"Failed to serialize value for caching.\", value=value, error=str(e))\n            return\n\n        if self.redis_client:\n            try:\n                await self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except redis.exceptions.RedisError as e:\n                log.warning(\"Redis SET failed, falling back to memory cache.\", error=e)\n\n        self.memory_cache[key] = {\n            \"value\": value,\n            \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds),\n        }\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager()\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = await cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n\n            try:\n                await cache_manager.set(cache_key, result, ttl_seconds)\n            except Exception as e:\n                log.error(\"Failed to store result in cache.\", error=str(e), key=cache_key)\n\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "web_service/backend/credentials_manager.py": "# python_service/credentials_manager.py\ntry:\n    import keyring\n\n    # This check is crucial for cross-platform compatibility\n    import keyring.backends.windows\n\n    IS_WINDOWS = True\nexcept ImportError:\n    keyring = None\n    IS_WINDOWS = False\n\n\nclass SecureCredentialsManager:\n    \"\"\"Manages secrets in the system's native credential store.\"\"\"\n\n    SERVICE_NAME = \"Fortuna\"\n\n    @staticmethod\n    def save_credential(account: str, secret: str) -> bool:\n        \"\"\"Saves a secret for a given account (e.g., 'api_key', 'betfair_username').\"\"\"\n        if not IS_WINDOWS:\n            print(\"Credential storage is only supported on Windows.\")\n            return False\n        try:\n            keyring.set_password(SecureCredentialsManager.SERVICE_NAME, account, secret)\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credential for {account}: {e}\")\n            return False\n\n    @staticmethod\n    def get_credential(account: str) -> str:\n        \"\"\"Retrieves a secret for a given account.\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            return keyring.get_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credential for {account}: {e}\")\n            return None\n\n    @staticmethod\n    def get_betfair_credentials() -> tuple[str, str]:\n        \"\"\"Convenience method to retrieve both Betfair username and password.\"\"\"\n        username = SecureCredentialsManager.get_credential(\"betfair_username\")\n        password = SecureCredentialsManager.get_credential(\"betfair_password\")\n        return username, password\n\n    @staticmethod\n    def delete_credential(account: str):\n        \"\"\"Deletes a specific credential.\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception:\n            pass\n",
    "web_service/backend/fortuna_windows_service.py": "# fortuna_windows_service.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Ensure the script's directory is at the front of the path\nscript_dir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, script_dir)\n\ntry:\n    from fortuna_service import FortunaBackgroundService\nexcept ImportError as e:\n    # Log a detailed error to the Windows Event Log if the import fails\n    servicemanager.LogErrorMsg(f\"FATAL: Could not import FortunaBackgroundService. Error: {e}\")\n    sys.exit(1)  # Exit with an error code\n\n\nclass FortunaWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaV8Service\"\n    _svc_display_name_ = \"Fortuna V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.fortuna_service = FortunaBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(name)s - %(levelname)s - %(message)s\",\n            handlers=[servicemanager.LogHandler()],\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.fortuna_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        self.fortuna_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaWindowsService)\n",
    "web_service/backend/main.py": "import uvicorn\nimport sys\nimport os\nimport asyncio\nfrom multiprocessing import freeze_support\n\n# Force UTF-8 encoding for stdout and stderr, crucial for PyInstaller on Windows\nos.environ[\"PYTHONUTF8\"] = \"1\"\n\n# This is the definitive entry point for the Fortuna Faucet backend service.\n# It is designed to be compiled with PyInstaller.\n\ndef _configure_sys_path():\n    \"\"\"\n    Dynamically adds project paths to sys.path.\n    This is the robust solution to ensure that string-based imports like\n    \"web_service.backend.api:app\" work correctly when the application is\n    run from a PyInstaller executable. The `_MEIPASS` attribute is a temporary\n    directory created by PyInstaller at runtime.\n    \"\"\"\n    if getattr(sys, \"frozen\", False) and hasattr(sys, \"_MEIPASS\"):\n        # Running in a PyInstaller bundle. The project root is the _MEIPASS directory.\n        project_root = os.path.abspath(sys._MEIPASS)\n\n        # Aggressively add paths to resolve potential module lookup issues in frozen mode.\n        paths_to_add = [\n            project_root,\n            os.path.join(project_root, \"web_service\"),\n            os.path.join(project_root, \"web_service\", \"backend\"),\n        ]\n\n        # Insert paths at the beginning of sys.path in reverse order\n        # to maintain the desired precedence.\n        for path in reversed(paths_to_add):\n            if path not in sys.path:\n                sys.path.insert(0, path)\n                print(f\"INFO: Added path to sys.path: {path}\")\n\n    else:\n        # Running as a normal script. The project root is two levels up.\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n        if project_root not in sys.path:\n            sys.path.insert(0, project_root)\n            print(f\"INFO: Added project root to sys.path: {project_root}\")\n\n\ndef main():\n    \"\"\"\n    Primary entry point for the Fortuna Faucet backend application.\n    This function configures and runs the Uvicorn server.\n    \"\"\"\n    # CRITICAL: This must be called before any other application imports.\n    _configure_sys_path()\n\n    # When packaged, we need to ensure multiprocessing works correctly.\n    if getattr(sys, \"frozen\", False):\n        # CRITICAL for multiprocessing support in frozen mode on Windows.\n        freeze_support()\n\n    # Import the app object here after sys.path is configured.\n    from web_service.backend.api import app, HTTPException\n    from web_service.backend.config import get_settings\n    from fastapi.staticfiles import StaticFiles\n    from fastapi.responses import FileResponse\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n\n    settings = get_settings()\n\n    # In CI/CD, binding to 0.0.0.0 is more robust than 127.0.0.1.\n    # We will override the host setting for the smoke test environment.\n    run_host = settings.UVICORN_HOST\n    if os.environ.get(\"FORTUNA_ENV\") == \"smoke-test\":\n        run_host = \"0.0.0.0\"\n        print(f\"INFO: Smoke test environment detected. Overriding host to '{run_host}'\")\n\n    # --- Port Sanity Check ---\n    check_port_and_exit_if_in_use(settings.FORTUNA_PORT, run_host)\n\n    # --- Conditional UI Serving for Web Service Mode ---\n    # Only serve the UI if the FORTUNA_MODE environment variable is set to 'webservice'.\n    # This prevents the Electron-packaged backend from trying to serve files it doesn't have.\n    if os.environ.get(\"FORTUNA_MODE\") == \"webservice\":\n        # Define the path to the static UI files, accommodating PyInstaller's bundle.\n        if getattr(sys, \"frozen\", False):\n            # In a bundled app, the UI files are in the '_MEIPASS/ui' directory.\n            STATIC_DIR = os.path.join(sys._MEIPASS, \"ui\")\n        else:\n            # In development, they are in the frontend's output directory.\n            STATIC_DIR = os.path.abspath(\n                os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"web_platform\", \"frontend\", \"out\")\n            )\n\n        # Mount the static assets directory for CSS, JS, etc.\n        if os.path.exists(os.path.join(STATIC_DIR, \"_next\")):\n            app.mount(\"/_next\", StaticFiles(directory=os.path.join(STATIC_DIR, \"_next\")), name=\"next\")\n\n        # Serve the main index.html for any non-API path.\n        @app.get(\"/{full_path:path}\", include_in_schema=False)\n        async def serve_frontend(full_path: str):\n            if full_path.startswith(\"api/\") or full_path.startswith(\"docs\") or full_path == \"health\":\n                # This is an API route, let FastAPI handle it.\n                # A 404 will be raised naturally if no route matches.\n                return\n\n            index_path = os.path.join(STATIC_DIR, \"index.html\")\n            if os.path.exists(index_path):\n                return FileResponse(index_path)\n            else:\n                # This will only be hit if the frontend files are missing entirely.\n                raise HTTPException(\n                    status_code=404,\n                    detail=\"Frontend not found. Please build the frontend and ensure it's in the correct location.\",\n                )\n\n    # CRITICAL FIX FOR PYINSTALLER on WINDOWS: Force event loop policy\n    if sys.platform == \"win32\" and getattr(sys, 'frozen', False):\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n        print(\"[BOOT] Applied WindowsSelectorEventLoopPolicy for PyInstaller\", file=sys.stderr)\n\n    print(f\"INFO: Starting Uvicorn server...\")\n    print(f\"      APP: web_service.backend.api:app\")\n    print(f\"      HOST: {run_host}\")\n    print(f\"      PORT: {settings.FORTUNA_PORT}\")\n\n    uvicorn.run(\n        app,\n        host=run_host,\n        port=settings.FORTUNA_PORT,\n        log_level=\"info\"\n    )\n\nif __name__ == \"__main__\":\n    main()\n",
    "web_service/backend/models_v3.py": "# python_service/models_v3.py\n# Defines the data structures for the V3 adapter architecture.\n\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import List\n\n\n@dataclass\nclass NormalizedRunner:\n    runner_id: str\n    name: str\n    saddle_cloth: str\n    odds_decimal: float\n\n\n@dataclass\nclass NormalizedRace:\n    race_key: str\n    track_key: str\n    start_time_iso: str\n    race_name: str\n    runners: List[NormalizedRunner] = field(default_factory=list)\n    source_ids: List[str] = field(default_factory=list)\n",
    "web_service/backend/requirements.in": "#\n# Fortuna Faucet - High-Level Backend Dependencies\n# This is the source of truth. Run 'pip-compile' to generate requirements.txt.\n#\n\n# --- Core Application Framework (Hard Pins) ---\nfastapi\nuvicorn==0.30.1\ncryptography\n\n# --- Core Application Dependencies (Flexible) ---\ntenacity\npydantic-settings\nhttpx[http2]\nselectolax==0.4.0\nbeautifulsoup4\nslowapi\nredis\npandas\nnumpy\nscipy\naiosqlite\nSQLAlchemy\npsycopg2-binary\nstructlog\ncertifi\n\n# --- Desktop & OS Integration (Flexible) ---\npsutil\npywin32 ; sys_platform == 'win32'\nwindows-toasts ; sys_platform == 'win32'\nkeyring\npynput ; sys_platform == 'win32'\n\n# --- Development & Testing Dependencies ---\npytest\npytest-asyncio\nblack\n\n# --- Build Dependencies (Hard Pins) ---\n# THE FIX: Upgraded to 6.6.0 for official Python 3.12 support.\npyinstaller==6.6.0\nwheel\nsetuptools>=78.1.1,<81\npip-tools\nrequests>=2.32.4\nurllib3>=2.5.0\n",
    "web_service/backend/requirements.txt": "#\n# This file is autogenerated by pip-compile with Python 3.12\n# by the following command:\n#\n#    pip-compile --output-file=python_service/requirements.txt python_service/requirements.in\n#\naiosqlite==0.21.0\n    # via -r python_service/requirements.in\naltgraph==0.17.4\n    # via pyinstaller\nannotated-doc==0.0.4\n    # via fastapi\nannotated-types==0.7.0\n    # via pydantic\nanyio==4.11.0\n    # via\n    #   httpx\n    #   starlette\nbeautifulsoup4==4.14.2\n    # via -r python_service/requirements.in\nblack==25.11.0\n    # via -r python_service/requirements.in\nbuild==1.3.0\n    # via pip-tools\ncertifi==2025.10.5\n    # via\n    #   -r python_service/requirements.in\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==2.0.0\n    # via cryptography\ncharset-normalizer==3.4.4\n    # via requests\nclick==8.3.0\n    # via\n    #   black\n    #   pip-tools\n    #   uvicorn\ncryptography==46.0.3\n    # via\n    #   -r python_service/requirements.in\n    #   secretstorage\ndeprecated==1.3.1\n    # via limits\nfastapi==0.121.1\n    # via -r python_service/requirements.in\ngreenlet==3.2.4\n    # via sqlalchemy\nh11==0.16.0\n    # via\n    #   httpcore\n    #   uvicorn\nh2==4.3.0\n    # via httpx\nhpack==4.1.0\n    # via h2\nhttpcore==1.0.9\n    # via httpx\nhttpx[http2]==0.28.1\n    # via -r python_service/requirements.in\nhyperframe==6.1.0\n    # via h2\nidna==3.11\n    # via\n    #   anyio\n    #   httpx\n    #   requests\niniconfig==2.3.0\n    # via pytest\njaraco-classes==3.4.0\n    # via keyring\njaraco-context==6.0.1\n    # via keyring\njaraco-functools==4.3.0\n    # via keyring\njeepney==0.9.0\n    # via\n    #   keyring\n    #   secretstorage\nkeyring==25.6.0\n    # via -r python_service/requirements.in\nlimits==5.6.0\n    # via slowapi\nmore-itertools==10.8.0\n    # via\n    #   jaraco-classes\n    #   jaraco-functools\nmypy-extensions==1.1.0\n    # via black\nnumpy\n    # via\n    #   -r python_service/requirements.in\n    #   pandas\n    #   scipy\npackaging==25.0\n    # via\n    #   black\n    #   build\n    #   limits\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\n    #   pytest\npandas==2.3.3\n    # via -r python_service/requirements.in\npathspec==0.12.1\n    # via black\npip-tools==7.5.1\n    # via -r python_service/requirements.in\nplatformdirs==4.5.0\n    # via black\npluggy==1.6.0\n    # via pytest\npsutil==7.1.3\n    # via -r python_service/requirements.in\npsycopg2-binary==2.9.11\n    # via -r python_service/requirements.in\npycparser==2.23\n    # via cffi\npydantic==2.12.4\n    # via\n    #   fastapi\n    #   pydantic-settings\npydantic-core==2.41.5\n    # via pydantic\npydantic-settings==2.12.0\n    # via -r python_service/requirements.in\npygments==2.19.2\n    # via pytest\npyinstaller==6.6.0\n    # via -r python_service/requirements.in\npyinstaller-hooks-contrib==2025.9\n    # via pyinstaller\npyproject-hooks==1.2.0\n    # via\n    #   build\n    #   pip-tools\npytest==9.0.0\n    # via\n    #   -r python_service/requirements.in\n    #   pytest-asyncio\npytest-asyncio==1.3.0\n    # via -r python_service/requirements.in\npython-dateutil==2.9.0.post0\n    # via pandas\npython-dotenv==1.2.1\n    # via pydantic-settings\npytokens==0.3.0\n    # via black\npytz==2025.2\n    # via pandas\nredis==7.0.1\n    # via -r python_service/requirements.in\nrequests==2.32.5\n    # via -r python_service/requirements.in\nscipy==1.16.3\n    # via -r python_service/requirements.in\nsecretstorage==3.4.1\n    # via keyring\nselectolax==0.4.0\n    # via -r python_service/requirements.in\nsix==1.17.0\n    # via python-dateutil\nslowapi==0.1.9\n    # via -r python_service/requirements.in\nsniffio==1.3.1\n    # via anyio\nsoupsieve==2.8\n    # via beautifulsoup4\nsqlalchemy==2.0.44\n    # via -r python_service/requirements.in\nstarlette==0.49.3\n    # via fastapi\nstructlog==25.5.0\n    # via -r python_service/requirements.in\ntenacity==8.5.0\n    # via -r python_service/requirements.in\ntyping-extensions==4.15.0\n    # via\n    #   aiosqlite\n    #   anyio\n    #   beautifulsoup4\n    #   fastapi\n    #   limits\n    #   pydantic\n    #   pydantic-core\n    #   pytest-asyncio\n    #   sqlalchemy\n    #   starlette\n    #   typing-inspection\ntyping-inspection==0.4.2\n    # via\n    #   pydantic\n    #   pydantic-settings\ntzdata==2025.2\n    # via pandas\nurllib3>=2.6.0\n    # via\n    #   -r python_service/requirements.in\n    #   requests\nuvicorn==0.30.1\n    # via -r python_service/requirements.in\nwheel==0.45.1\n    # via\n    #   -r python_service/requirements.in\n    #   pip-tools\nwrapt==2.0.1\n    # via deprecated\n\n# The following packages are considered to be unsafe in a requirements file:\n# pip\n# setuptools\npywin32==306; sys_platform == 'win32'\n",
    "web_service/backend/tests/__init__.py": "",
    "web_service/backend/utils/text.py": "# python_service/utils/text.py\n# Centralized text and name normalization utilities\nimport re\nfrom typing import Optional\n\n\ndef clean_text(text: Optional[str]) -> Optional[str]:\n    \"\"\"Strips leading/trailing whitespace and collapses internal whitespace.\"\"\"\n    if not text:\n        return None\n    return \" \".join(text.strip().split())\n\n\ndef normalize_venue_name(name: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Normalizes a UK or Irish racecourse name to a standard format.\n    Handles common abbreviations and variations.\n    \"\"\"\n    if not name:\n        return None\n\n    # Use a temporary variable for matching, but return the properly cased name\n    cleaned_name_upper = clean_text(name).upper()\n\n    VENUE_MAP = {\n        \"ASCOT\": \"Ascot\",\n        \"AYR\": \"Ayr\",\n        \"BANGOR-ON-DEE\": \"Bangor-on-Dee\",\n        \"CATTERICK BRIDGE\": \"Catterick\",\n        \"CHELMSFORD CITY\": \"Chelmsford\",\n        \"EPSOM DOWNS\": \"Epsom\",\n        \"FONTWELL\": \"Fontwell Park\",\n        \"HAYDOCK\": \"Haydock Park\",\n        \"KEMPTON\": \"Kempton Park\",\n        \"LINGFIELD\": \"Lingfield Park\",\n        \"NEWMARKET (ROWLEY)\": \"Newmarket\",\n        \"NEWMARKET (JULY)\": \"Newmarket\",\n        \"SANDOWN\": \"Sandown Park\",\n        \"STRATFORD\": \"Stratford-on-Avon\",\n        \"YARMOUTH\": \"Great Yarmouth\",\n        \"CURRAGH\": \"Curragh\",\n        \"DOWN ROYAL\": \"Down Royal\",\n    }\n\n    # Check primary map first\n    if cleaned_name_upper in VENUE_MAP:\n        return VENUE_MAP[cleaned_name_upper]\n\n    # Handle cases where the key is the desired output but needs to be mapped from a variation\n    # e.g. CHELMSFORD maps to Chelmsford\n    # Title case the cleaned name for a sensible default\n    title_cased_name = clean_text(name).title()\n    if title_cased_name in VENUE_MAP.values():\n        return title_cased_name\n\n    # Return the title-cased cleaned name as a fallback\n    return title_cased_name\n\n\ndef normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n",
    "web_service/frontend/app/components/LiveModeToggle.tsx": "// web_platform/frontend/src/components/LiveModeToggle.tsx\n'use client';\n\nimport React from 'react';\n\ninterface LiveModeToggleProps {\n  isLive: boolean;\n  onToggle: (isLive: boolean) => void;\n  isDisabled: boolean;\n}\n\nexport const LiveModeToggle: React.FC<LiveModeToggleProps> = ({ isLive, onToggle, isDisabled }) => {\n  const handleToggle = () => {\n    if (!isDisabled) {\n      onToggle(!isLive);\n    }\n  };\n\n  return (\n    <button\n      onClick={handleToggle}\n      disabled={isDisabled}\n      className={`relative inline-flex items-center h-8 rounded-full w-32 transition-colors duration-300 ease-in-out focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-offset-slate-800 focus:ring-blue-500 ${\n        isDisabled ? 'cursor-not-allowed bg-slate-700' : 'cursor-pointer'\n      } ${isLive ? 'bg-green-600' : 'bg-slate-600'}`}\n    >\n      <span className=\"sr-only\">Toggle Live Mode</span>\n      <span\n        className={`absolute left-1 top-1 inline-block w-6 h-6 rounded-full bg-white transform transition-transform duration-300 ease-in-out ${\n          isLive ? 'translate-x-[104px]' : 'translate-x-0'\n        }`}\n      />\n      <span\n        className={`absolute left-8 transition-opacity duration-200 ease-in-out ${\n          !isLive && !isDisabled ? 'opacity-100' : 'opacity-50'\n        }`}\n      >\n        Poll\n      </span>\n      <span\n        className={`absolute right-4 transition-opacity duration-200 ease-in-out ${\n          isLive && !isDisabled ? 'opacity-100' : 'opacity-50'\n        }`}\n      >\n        \u26a1 Live\n      </span>\n    </button>\n  );\n};\n",
    "web_service/frontend/app/components/RaceFilters.tsx": "// web_platform/frontend/src/components/RaceFilters.tsx\n'use client';\n\nimport { useState, useCallback } from 'react';\nimport { Settings, RotateCcw } from 'lucide-react';\n\ninterface FilterParams {\n  maxFieldSize: number;\n  minFavoriteOdds: number;\n  minSecondFavoriteOdds: number;\n}\n\nexport interface RaceFiltersProps {\n  onParamsChange: (params: FilterParams) => void;\n  isLoading: boolean;\n  refetch: () => void;\n}\n\nconst DEFAULT_PARAMS: FilterParams = {\n  maxFieldSize: 10,\n  minFavoriteOdds: 2.5,\n  minSecondFavoriteOdds: 4.0,\n};\n\nexport function RaceFilters({ onParamsChange, isLoading, refetch }: RaceFiltersProps) {\n  const [params, setParams] = useState<FilterParams>(DEFAULT_PARAMS);\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  // Handle individual parameter changes\n  const handleChange = useCallback((key: keyof FilterParams, value: number) => {\n    setParams(prev => {\n      const updated = { ...prev, [key]: value };\n      onParamsChange(updated);\n      return updated;\n    });\n    // Debounce the refetch call\n    const timer = setTimeout(() => {\n      refetch();\n    }, 500);\n    return () => clearTimeout(timer);\n  }, [onParamsChange, refetch]);\n\n  // Reset to defaults\n  const handleReset = useCallback(() => {\n    setParams(DEFAULT_PARAMS);\n    onParamsChange(DEFAULT_PARAMS);\n    refetch();\n  }, [onParamsChange, refetch]);\n\n  return (\n    <div className=\"bg-gradient-to-r from-slate-800 to-slate-900 rounded-lg p-4 mb-6 border border-slate-700\">\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-2\">\n          <Settings className=\"w-5 h-5 text-amber-500\" />\n          <h3 className=\"text-lg font-semibold text-white\">Race Filters</h3>\n        </div>\n        <button\n          onClick={() => setIsExpanded(!isExpanded)}\n          className=\"text-sm text-slate-400 hover:text-slate-200 transition\"\n        >\n          {isExpanded ? 'Hide' : 'Show'}\n        </button>\n      </div>\n\n      {isExpanded && (\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 pt-4 border-t border-slate-700\">\n          {/* Max Field Size */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Max Field Size\n              <span className=\"text-amber-500 ml-2\">{params.maxFieldSize}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2\"\n              max=\"20\"\n              value={params.maxFieldSize}\n              onChange={(e) => handleChange('maxFieldSize', parseInt(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Filters races with larger fields</p>\n          </div>\n\n          {/* Min Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"1.5\"\n              max=\"5\"\n              step=\"0.1\"\n              value={params.minFavoriteOdds}\n              onChange={(e) => handleChange('minFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = pickier favorites</p>\n          </div>\n\n          {/* Min Second Favorite Odds */}\n          <div className=\"space-y-2\">\n            <label className=\"block text-sm font-medium text-slate-300\">\n              Min 2nd Favorite Odds\n              <span className=\"text-amber-500 ml-2\">{params.minSecondFavoriteOdds.toFixed(2)}</span>\n            </label>\n            <input\n              type=\"range\"\n              min=\"2.0\"\n              max=\"8\"\n              step=\"0.1\"\n              value={params.minSecondFavoriteOdds}\n              onChange={(e) => handleChange('minSecondFavoriteOdds', parseFloat(e.target.value))}\n              disabled={isLoading}\n              className=\"w-full accent-amber-500 cursor-pointer disabled:opacity-50\"\n            />\n            <p className=\"text-xs text-slate-500\">Higher = better odds separation</p>\n          </div>\n\n          {/* Reset Button */}\n          <div className=\"md:col-span-3 flex justify-end pt-4 border-t border-slate-700\">\n            <button\n              onClick={handleReset}\n              disabled={isLoading}\n              className=\"inline-flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 text-slate-200 rounded text-sm font-medium transition disabled:opacity-50\"\n            >\n              <RotateCcw className=\"w-4 h-4\" />\n              Reset to Defaults\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n}\n",
    "web_service/frontend/app/components/TrifectaFactors.tsx": "// TrifectaFactors.tsx - FINAL, DYNAMIC VERSION\n'use client';\nimport React from 'react';\n\ninterface TrifectaFactorsProps {\n  factorsJson: string | null;\n}\n\nexport function TrifectaFactors({ factorsJson }: TrifectaFactorsProps) {\n  if (!factorsJson) {\n    return <div className=\"text-sm text-gray-500\">No analysis factors available.</div>;\n  }\n\n  try {\n    const factors = JSON.parse(factorsJson);\n    const positiveFactors = Object.entries(factors).filter(([key, value]: [string, any]) => value.ok);\n\n    if (positiveFactors.length === 0) {\n      return <div className=\"text-sm text-gray-500\">No positive factors identified.</div>;\n    }\n\n    return (\n      <div className=\"mt-2 text-xs\">\n        <h4 className=\"font-semibold mb-1\">Key Factors:</h4>\n        <ul className=\"list-disc list-inside space-y-1\">\n          {positiveFactors.map(([key, value]: [string, any]) => (\n            <li key={key} className=\"text-gray-700\">\n              <span className=\"font-medium text-green-600\">\u2713</span> {value.reason} ({value.points > 0 ? `+${value.points}` : value.points} pts)\n            </li>\n          ))}\n        </ul>\n      </div>\n    );\n  } catch (error) {\n    console.error(\"Failed to parse trifecta factors:\", error);\n    return <div className=\"text-sm text-red-500\">Error displaying analysis factors.</div>;\n  }\n}",
    "web_service/frontend/app/lib/queryClient.ts": "// web_platform/frontend/src/lib/queryClient.ts\nimport { QueryClient } from '@tanstack/react-query';\n\nexport const queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      retry: 3,\n      staleTime: 1000 * 60 * 5, // 5 minutes\n    },\n  },\n});\n",
    "web_service/frontend/next-env.d.ts": "/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.\n",
    "web_service/frontend/public/manifest.json": "{\n  \"name\": \"Fortuna Faucet Command Deck\",\n  \"short_name\": \"Fortuna\",\n  \"description\": \"Real-time racing analysis.\",\n  \"start_url\": \"/\",\n  \"display\": \"standalone\",\n  \"background_color\": \"#1a202c\",\n  \"theme_color\": \"#1a202c\",\n  \"icons\": [\n    {\n      \"src\": \"/icons/icon-192x192.png\",\n      \"sizes\": \"192x192\",\n      \"type\": \"image/png\"\n    },\n    {\n      \"src\": \"/icons/icon-512x512.png\",\n      \"sizes\": \"512x512\",\n      \"type\": \"image/png\"\n    }\n  ]\n}\n",
    "wix/WixUI_CustomProgress.wxs": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\">\n  <Fragment>\n    <UI>\n      <!-- Override the default InstallProgress dialog -->\n      <Dialog Id=\"InstallProgressDlg\" Width=\"370\" Height=\"270\" Title=\"Fortuna Faucet Installation\" Modeless=\"yes\">\n        <Control Id=\"Title\" Type=\"Title\" X=\"20\" Y=\"6\" Width=\"330\" Height=\"18\" Text=\"Installation Progress\" />\n        <Control Id=\"BannerBitmap\" Type=\"Bitmap\" X=\"0\" Y=\"0\" Width=\"370\" Height=\"44\" TabSkip=\"no\" Text=\"WixUI_Bmp_Banner\" />\n        <Control Id=\"Back\" Type=\"PushButton\" X=\"180\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Back\" Disabled=\"yes\" />\n        <Control Id=\"Next\" Type=\"PushButton\" X=\"236\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"&amp;Next\" Disabled=\"yes\" />\n        <Control Id=\"Cancel\" Type=\"PushButton\" X=\"304\" Y=\"243\" Width=\"56\" Height=\"17\" Text=\"Cancel\" />\n\n        <Control Id=\"ActionText\" Type=\"Text\" X=\"70\" Y=\"80\" Width=\"280\" Height=\"20\" TabSkip=\"no\">\n          <Subscribe Event=\"ActionText\" Attribute=\"Text\" />\n        </Control>\n        <Control Id=\"Description\" Type=\"Text\" X=\"35\" Y=\"55\" Width=\"300\" Height=\"20\" Text=\"Please wait while the installer copies files.\" />\n\n        <!-- This is the new control to display the current filename -->\n        <Control Id=\"CurrentFileText\" Type=\"Text\" X=\"70\" Y=\"100\" Width=\"280\" Height=\"20\">\n            <Subscribe Event=\"SetProgress\" Attribute=\"Text\" />\n        </Control>\n\n        <Control Id=\"ProgressBar\" Type=\"ProgressBar\" X=\"35\" Y=\"120\" Width=\"300\" Height=\"10\" ProgressBlocks=\"yes\" Text=\"Progress\">\n          <Subscribe Event=\"SetProgress\" Attribute=\"Progress\" />\n        </Control>\n      </Dialog>\n\n      <!-- The Publish element must be a child of UI, not Dialog -->\n      <Publish Dialog=\"InstallProgressDlg\" Control=\"Cancel\" Event=\"SpawnDialog\" Value=\"CancelDlg\">1</Publish>\n    </UI>\n  </Fragment>\n</Wix>\n"
}