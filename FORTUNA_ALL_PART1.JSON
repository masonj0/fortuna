{
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.\n",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n\n# Import all adapter classes to make them available for dynamic loading.\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .brisnet_adapter import BrisnetAdapter\nfrom .drf_adapter import DRFAdapter\nfrom .equibase_adapter import EquibaseAdapter\nfrom .fanduel_adapter import FanDuelAdapter\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .horseracingnation_adapter import HorseRacingNationAdapter\nfrom .nyrabets_adapter import NYRABetsAdapter\nfrom .oddschecker_adapter import OddscheckerAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .punters_adapter import PuntersAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .racingpost_adapter import RacingPostAdapter\nfrom .racingtv_adapter import RacingTVAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .tab_adapter import TabAdapter\nfrom .template_adapter import TemplateAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\nfrom .twinspires_adapter import TwinSpiresAdapter\nfrom .universal_adapter import UniversalAdapter\nfrom .xpressbet_adapter import XpressbetAdapter\n\n# Define the public API for the adapters package, making it easy for the\n# orchestrator to discover and use them.\n__all__ = [\n    \"AtTheRacesAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairDataScientistAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"BrisnetAdapter\",\n    \"DRFAdapter\",\n    \"EquibaseAdapter\",\n    \"FanDuelAdapter\",\n    \"GbgbApiAdapter\",\n    \"GreyhoundAdapter\",\n    \"HarnessAdapter\",\n    \"HorseRacingNationAdapter\",\n    \"NYRABetsAdapter\",\n    \"OddscheckerAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"PuntersAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"RacingPostAdapter\",\n    \"RacingTVAdapter\",\n    \"SportingLifeAdapter\",\n    \"TabAdapter\",\n    \"TemplateAdapter\",\n    \"TheRacingApiAdapter\",\n    \"TimeformAdapter\",\n    \"TVGAdapter\",\n    \"TwinSpiresAdapter\",\n    \"UniversalAdapter\",\n    \"XpressbetAdapter\",\n]\n",
    "python_service/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom bs4 import BeautifulSoup, Tag\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_v3 import BaseAdapterV3\n\n\nclass AtTheRacesAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for attheraces.com, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"AtTheRaces\"\n    BASE_URL = \"https://www.attheraces.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch AtTheRaces index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        all_races = []\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format provided to AtTheRacesAdapter\", date=raw_data.get(\"date\"))\n            return []\n\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                header = soup.select_one(\"h1.heading-racecard-title\").get_text()\n                track_name_raw, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n                track_name = normalize_venue_name(track_name_raw)\n                active_link = soup.select_one(\"a.race-time-link.active\")\n                race_number = (\n                    active_link.find_parent(\"div\", \"races\").select(\"a.race-time-link\").index(active_link) + 1\n                )\n                start_time = datetime.combine(race_date, datetime.strptime(race_time, \"%H:%M\").time())\n\n                runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n                race = Race(\n                    id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\"Error parsing a race from AtTheRaces, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = clean_text(row.select_one(\"h3.horse-name a\").get_text())\n            num_str = clean_text(row.select_one(\"span.horse-number\").get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n            odds_str = clean_text(row.select_one(\"button.best-odds\").get_text())\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())}\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on AtTheRaces, skipping runner.\")\n            return None\n",
    "python_service/adapters/base_v3.py": "# python_service/adapters/base_v3.py\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import AsyncGenerator, Any, List\nimport httpx\nimport structlog\nfrom tenacity import retry, stop_after_attempt, wait_exponential, RetryError\nfrom ..core.exceptions import AdapterHttpError\nfrom ..models import Race\n\nclass BaseAdapterV3(ABC):\n    \"\"\"\n    A self-contained, architecturally superior abstract base class for data adapters.\n    It includes retry logic, a circuit breaker, and a standardized fetch/parse pattern.\n    \"\"\"\n    # Circuit Breaker settings\n    FAILURE_THRESHOLD = 3\n    COOLDOWN_PERIOD_SECONDS = 300  # 5 minutes\n\n    def __init__(self, source_name: str, base_url: str, config=None, timeout: int = 20):\n        self.source_name = source_name\n        self.base_url = base_url\n        self.config = config\n        self.timeout = timeout\n        self.logger = structlog.get_logger(adapter_name=self.source_name)\n        self.http_client: httpx.AsyncClient = None # To be assigned by the engine\n\n        # Circuit Breaker State\n        self.circuit_breaker_tripped = False\n        self.circuit_breaker_failure_count = 0\n        self.circuit_breaker_last_failure = 0\n\n    @abstractmethod\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw data (e.g., HTML, JSON) for the given date.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw data into a list of Race objects.\"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str) -> AsyncGenerator[Race, None]:\n        \"\"\"Orchestrates the fetch and parse process with circuit breaker logic.\"\"\"\n        if self.circuit_breaker_tripped:\n            if time.time() - self.circuit_breaker_last_failure < self.COOLDOWN_PERIOD_SECONDS:\n                self.logger.warning(\"Circuit breaker is tripped. Skipping fetch.\")\n                return\n            else:\n                self._reset_circuit_breaker()\n\n        try:\n            raw_data = await self._fetch_data(date)\n            if raw_data is not None:\n                parsed_races = self._parse_races(raw_data)\n                for race in parsed_races:\n                    yield race\n            self._reset_circuit_breaker() # Success\n        except Exception:\n            self.logger.error(\"get_races pipeline failed.\", exc_info=True)\n            self._handle_failure()\n\n    @retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))\n    async def make_request(\n        self, http_client: httpx.AsyncClient, method: str, url: str, **kwargs\n    ) -> httpx.Response:\n        \"\"\"Makes an HTTP request with built-in retry logic.\"\"\"\n        full_url = f\"{self.base_url.rstrip('/')}/{url.lstrip('/')}\"\n        try:\n            self.logger.info(f\"Making request: {method.upper()} {full_url}\")\n            response = await http_client.request(method, full_url, timeout=self.timeout, **kwargs)\n            response.raise_for_status()\n            return response\n        except httpx.HTTPStatusError as e:\n            raise AdapterHttpError(self.source_name, e.response.status_code, str(e.request.url)) from e\n        except (httpx.RequestError, RetryError) as e:\n            raise AdapterHttpError(self.source_name, 503, str(e)) from e\n\n    def get_status(self) -> dict:\n        return {\n            \"adapter_name\": self.source_name,\n            \"circuit_breaker_tripped\": self.circuit_breaker_tripped,\n            \"failure_count\": self.circuit_breaker_failure_count,\n            \"last_failure\": datetime.fromtimestamp(self.circuit_breaker_last_failure).isoformat() if self.circuit_breaker_last_failure else None,\n        }\n\n    def _handle_failure(self):\n        self.circuit_breaker_failure_count += 1\n        self.circuit_breaker_last_failure = time.time()\n        if self.circuit_breaker_failure_count >= self.FAILURE_THRESHOLD:\n            self.circuit_breaker_tripped = True\n            self.logger.critical(\"Circuit breaker has been tripped.\")\n\n    def _reset_circuit_breaker(self):\n        if self.circuit_breaker_tripped:\n            self.logger.info(\"Cooldown period passed. Resetting circuit breaker.\")\n        self.circuit_breaker_tripped = False\n        self.circuit_breaker_failure_count = 0\n",
    "python_service/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nfrom datetime import datetime\nfrom typing import Any, List\n\nfrom ..models import Race, Runner\nfrom .base_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairExchange\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for a given date.\"\"\"\n        await self._authenticate()\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        return await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\"from\": start_time.isoformat(), \"to\": end_time.isoformat()},\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                races.append(self._parse_race(market))\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market['marketId']\n        event = market['event']\n        start_time = datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00'))\n\n        runners = [\n            Runner(\n                number=runner.get('sortPriority', i + 1),\n                name=runner['runnerName'],\n                scratched=runner['status'] != 'ACTIVE',\n                selection_id=runner['selectionId']\n            )\n            for i, runner in enumerate(market.get('runners', []))\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get('venue', 'Unknown Venue'),\n            race_number=self._extract_race_number(market.get('marketName', '')),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        import re\n        match = re.search(r'\\bR(\\d{1,2})\\b', name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "python_service/adapters/betfair_auth_mixin.py": "# python_service/adapters/betfair_auth_mixin.py\n\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nimport httpx\nimport structlog\nfrom ..credentials_manager import SecureCredentialsManager\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock = asyncio.Lock()\n\n    async def _authenticate(self):\n        \"\"\"\n        Authenticates with Betfair using credentials from the system's credential manager,\n        ensuring the session token is valid and refreshing it if necessary.\n        \"\"\"\n        async with self._auth_lock:\n            if self.session_token and self.token_expiry and self.token_expiry > (\n                datetime.now() + timedelta(minutes=5)\n            ):\n                return\n\n            log.info(\"Attempting to authenticate with Betfair...\")\n            username, password = SecureCredentialsManager.get_betfair_credentials()\n\n            if not all([self.config.BETFAIR_APP_KEY, username, password]):\n                raise ValueError(\"Betfair credentials not fully configured in credential manager.\")\n\n            auth_url = \"https://identitysso.betfair.com/api/login\"\n            headers = {\n                \"X-Application\": self.config.BETFAIR_APP_KEY,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={username}&password={password}\"\n\n            response = await self.http_client.post(\n                auth_url, headers=headers, content=payload, timeout=20\n            )\n            response.raise_for_status()\n            data = response.json()\n\n            if data.get(\"status\") == \"SUCCESS\":\n                self.session_token = data.get(\"token\")\n                self.token_expiry = datetime.now() + timedelta(hours=3)\n                log.info(\"Betfair authentication successful.\")\n            else:\n                log.error(\"Betfair authentication failed\", error=data.get('error'))\n                raise ConnectionError(f\"Betfair authentication failed: {data.get('error')}\")\n",
    "python_service/adapters/betfair_datascientist_adapter.py": "# python_service/adapters/betfair_datascientist_adapter.py\n\nfrom datetime import datetime\nfrom io import StringIO\nfrom typing import Any, List\n\nimport pandas as pd\nfrom ..models import Race, Runner\nfrom ..utils.text import normalize_course_name\nfrom .base_v3 import BaseAdapterV3\n\n\nclass BetfairDataScientistAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Betfair Data Scientist CSV models, migrated to BaseAdapterV3.\n    \"\"\"\n    ADAPTER_NAME = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str, url: str, config=None):\n        source_name = f\"{self.ADAPTER_NAME}_{model_name}\"\n        super().__init__(source_name=source_name, base_url=url, config=config)\n        self.model_name = model_name\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw CSV data from the Betfair Data Scientist model endpoint.\"\"\"\n        endpoint = f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n        self.logger.info(f\"Fetching data from {self.base_url}{endpoint}\")\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return StringIO(response.text) if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw CSV data into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n        try:\n            df = pd.read_csv(raw_data)\n            df = df.rename(\n                columns={\n                    \"meetings.races.bfExchangeMarketId\": \"market_id\",\n                    \"meetings.races.runners.bfExchangeSelectionId\": \"selection_id\",\n                    \"meetings.races.runners.ratedPrice\": \"rated_price\",\n                    \"meetings.races.raceName\": \"race_name\",\n                    \"meetings.name\": \"meeting_name\",\n                    \"meetings.races.raceNumber\": \"race_number\",\n                    \"meetings.races.runners.runnerName\": \"runner_name\",\n                    \"meetings.races.runners.clothNumber\": \"saddle_cloth\",\n                }\n            )\n            races = []\n            for market_id, group in df.groupby(\"market_id\"):\n                race_info = group.iloc[0]\n                runners = [\n                    Runner(\n                        name=str(row.get(\"runner_name\")),\n                        number=int(row.get(\"saddle_cloth\", 0)),\n                        odds=float(row.get(\"rated_price\", 0.0)),\n                    )\n                    for _, row in group.iterrows()\n                ]\n                race = Race(\n                    id=str(market_id),\n                    venue=normalize_course_name(str(race_info.get(\"meeting_name\", \"\"))),\n                    race_number=int(race_info.get(\"race_number\", 0)),\n                    start_time=datetime.now(), # Placeholder, not provided in source\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n            self.logger.info(f\"Normalized {len(races)} races from {self.model_name}.\")\n            return races\n        except (pd.errors.ParserError, KeyError):\n            self.logger.error(\"Failed to parse Betfair Data Scientist CSV.\", exc_info=True)\n            return []\n",
    "python_service/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import Any, List\n\nfrom ..models import Race, Runner\nfrom .base_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairGreyhounds\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        await self._authenticate()\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        return await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\"from\": start_time.isoformat(), \"to\": end_time.isoformat()},\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                races.append(self._parse_race(market))\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse a Betfair Greyhound market.\", exc_info=True, market=market)\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market['marketId']\n        event = market['event']\n        start_time = datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00'))\n\n        runners = [\n            Runner(\n                number=runner.get('sortPriority', i + 1),\n                name=runner['runnerName'],\n                scratched=runner['status'] != 'ACTIVE',\n                selection_id=runner['selectionId']\n            )\n            for i, runner in enumerate(market.get('runners', []))\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get('venue', 'Unknown Venue'),\n            race_number=self._extract_race_number(market.get('marketName', '')),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r'\\bR(\\d{1,2})\\b', name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "python_service/adapters/brisnet_adapter.py": "# python_service/adapters/brisnet_adapter.py\nfrom datetime import datetime\nfrom typing import Any, List\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_v3 import BaseAdapterV3\n\n\nclass BrisnetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for brisnet.com, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"Brisnet\"\n    BASE_URL = \"https://www.brisnet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw HTML from the Brisnet race page.\"\"\"\n        # Note: Brisnet URL structure seems to require a track code, e.g., 'CD' for Churchill Downs.\n        # This implementation will need to be improved to dynamically handle different tracks.\n        # For now, it is hardcoded to Churchill Downs as a placeholder.\n        url = f\"/race/{date}/CD\"\n        response = await self.make_request(self.http_client, \"GET\", url)\n        return {\"html\": response.text, \"date\": date} if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        venue_text_node = soup.select_one(\"header h1\")\n        if not venue_text_node:\n            self.logger.warning(\"Could not find venue name on Brisnet page.\")\n            return []\n\n        venue_text = venue_text_node.text\n        venue = normalize_venue_name(venue_text.split(\" - \")[0])\n\n        races = []\n        for race_section in soup.select(\"section.race\"):\n            try:\n                race_number = int(race_section[\"data-racenumber\"])\n                post_time_str = race_section.select_one(\".race-title span\").text.replace(\"Post Time: \", \"\").strip()\n                start_time = parse(f\"{race_date} {post_time_str}\")\n\n                runners = []\n                for row in race_section.select(\"tbody tr\"):\n                    if \"scratched\" in row.get(\"class\", []):\n                        continue\n\n                    cells = row.find_all(\"td\")\n                    number = int(cells[0].text)\n                    name = cells[1].text.strip()\n                    odds_str = cells[2].text.strip()\n\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    odds = {}\n                    if win_odds:\n                        odds[self.source_name] = OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())\n\n                    runners.append(Runner(number=number, name=name, odds=odds))\n\n                race = Race(\n                    id=f\"brisnet_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                    field_size=len(runners),\n                )\n                races.append(race)\n            except (AttributeError, ValueError, IndexError):\n                self.logger.warning(\"Failed to parse a race on Brisnet, skipping.\", exc_info=True)\n                continue\n\n        return races\n",
    "python_service/adapters/drf_adapter.py": "# python_service/adapters/drf_adapter.py\nfrom datetime import datetime\nfrom typing import Any, List\nfrom bs4 import BeautifulSoup\nfrom dateutil.parser import parse\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import normalize_venue_name\nfrom .base_v3 import BaseAdapterV3\n\n\nclass DRFAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for drf.com, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"DRF\"\n    BASE_URL = \"https://www.drf.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw HTML from the DRF entries page.\"\"\"\n        url = f\"/entries/{date}/USA\"\n        response = await self.make_request(self.http_client, \"GET\", url)\n        return {\"html\": response.text, \"date\": date} if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html\"):\n            return []\n\n        html = raw_data[\"html\"]\n        race_date = raw_data[\"date\"]\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        venue_node = soup.select_one(\"div.track-info h1\")\n        if not venue_node:\n            self.logger.warning(\"Could not find venue name on DRF page.\")\n            return []\n\n        venue_text = venue_node.text\n        venue = normalize_venue_name(venue_text.split(\" - \")[0].replace(\"Entries for \", \"\"))\n\n        races = []\n        for race_entry in soup.select(\"div.race-entries\"):\n            try:\n                race_number = int(race_entry[\"data-race-number\"])\n                post_time_node = race_entry.select_one(\".post-time\")\n                if not post_time_node:\n                    continue\n                post_time_str = post_time_node.text.replace(\"Post Time: \", \"\").strip()\n                start_time = parse(f\"{race_date} {post_time_str}\")\n\n                runners = []\n                for entry in race_entry.select(\"li.entry\"):\n                    if \"scratched\" in entry.get(\"class\", []):\n                        continue\n\n                    number = int(entry.select_one(\".program-number\").text)\n                    name = entry.select_one(\".horse-name\").text\n                    odds_str = entry.select_one(\".odds\").text.replace('-', '/')\n\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    odds = {}\n                    if win_odds:\n                        odds[self.source_name] = OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())\n\n                    runners.append(Runner(number=number, name=name, odds=odds))\n\n                race = Race(\n                    id=f\"drf_{venue.replace(' ', '').lower()}_{race_date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                    field_size=len(runners),\n                )\n                races.append(race)\n            except (AttributeError, ValueError, KeyError):\n                self.logger.warning(\"Failed to parse a race on DRF, skipping.\", exc_info=True)\n                continue\n        return races\n",
    "python_service/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom selectolax.parser import HTMLParser\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_v3 import BaseAdapterV3\n\n\nclass EquibaseAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Equibase race entries, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"Equibase\"\n    BASE_URL = \"https://www.equibase.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        d = datetime.strptime(date, \"%Y-%m-%d\").date()\n        index_url = f\"/entries/Entries.cfm?ELEC_DATE={d.month}/{d.day}/{d.year}&STYLE=EQB\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Equibase index page\", url=index_url)\n            return None\n\n        parser = HTMLParser(index_response.text)\n        track_links = [\n            link.attributes['href'] for link in parser.css(\"div.track-information a\")\n            if \"race=\" not in link.attributes.get(\"href\", \"\")\n        ]\n\n        async def get_race_links_from_track(track_url: str):\n            response = await self.make_request(self.http_client, \"GET\", track_url, headers=self._get_headers())\n            if not response:\n                return []\n            parser = HTMLParser(response.text)\n            return [link.attributes['href'] for link in parser.css(\"a.program-race-link\")]\n\n        tasks = [get_race_links_from_track(link) for link in track_links]\n        results = await asyncio.gather(*tasks)\n        race_links = [f\"{self.base_url}{link}\" for sublist in results for link in sublist]\n\n        async def fetch_single_html(race_url: str):\n            response = await self.make_request(self.http_client, \"GET\", race_url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        date = raw_data[\"date\"]\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                venue = clean_text(parser.css_first(\"div.track-information strong\").text())\n                race_number = int(parser.css_first(\"div.race-information strong\").text().replace(\"Race\", \"\").strip())\n                post_time_str = parser.css_first(\"p.post-time span\").text().strip()\n                start_time = self._parse_post_time(date, post_time_str)\n\n                runners = []\n                runner_nodes = parser.css(\"table.entries-table tbody tr\")\n                for node in runner_nodes:\n                    try:\n                        number = int(node.css_first(\"td:nth-child(1)\").text(strip=True))\n                        name = clean_text(node.css_first(\"td:nth-child(3)\").text())\n                        odds_str = clean_text(node.css_first(\"td:nth-child(10)\").text())\n                        scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n                        odds = {}\n                        if not scratched:\n                            win_odds = parse_odds_to_decimal(odds_str)\n                            if win_odds and win_odds < 999:\n                                odds = {\n                                    self.source_name: OddsData(\n                                        win=win_odds, source=self.source_name, last_updated=datetime.now()\n                                    )\n                                }\n                        runners.append(Runner(number=number, name=name, odds=odds, scratched=scratched))\n                    except (ValueError, AttributeError, IndexError):\n                        self.logger.warning(\"Could not parse Equibase runner, skipping.\", exc_info=True)\n                        continue\n\n                race = Race(\n                    id=f\"eqb_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                    venue=venue,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=runners,\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse Equibase race page.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        time_part = time_str.split(\" \")[-2] + \" \" + time_str.split(\" \")[-1]\n        dt_str = f\"{date_str} {time_part}\"\n        return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime, timedelta, timezone\nfrom decimal import Decimal\nfrom typing import Any, Dict, List, Optional\n\nfrom ..models import OddsData, Race, Runner\nfrom .base_v3 import BaseAdapterV3\n\n\nclass FanDuelAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for FanDuel's private API, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"FanDuel\"\n    BASE_URL = \"https://sb-api.nj.sportsbook.fanduel.com/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market data from the FanDuel API.\"\"\"\n        # Note: FanDuel's API is not date-centric. Event discovery would be needed for a robust implementation.\n        # This uses a hardcoded eventId as a placeholder.\n        event_id = \"38183.3\"\n        self.logger.info(f\"Fetching races from FanDuel for event_id: {event_id}\")\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Dict[str, Any]) -> List[Race]:\n        \"\"\"Parses the raw API response into a list of Race objects.\"\"\"\n        if not raw_data or \"marketGroups\" not in raw_data:\n            self.logger.warning(\"FanDuel response missing 'marketGroups' key\")\n            return []\n\n        races = []\n        for group in raw_data[\"marketGroups\"]:\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        if race := self._parse_single_race(market):\n                            races.append(race)\n                    except Exception:\n                        self.logger.error(\"Failed to parse a FanDuel market\", market=market, exc_info=True)\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single market from the API response into a Race object.\"\"\"\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            self.logger.warning(f\"Could not parse race and track from FanDuel market name: {market_name}\")\n            return None\n\n        race_number_str = parts[0].replace(\"Race \", \"\")\n        track_name = parts[1]\n\n        # Placeholder for start_time - FanDuel's market API doesn't provide it directly\n        start_time = datetime.now(timezone.utc) + timedelta(hours=int(race_number_str))\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            try:\n                runner_name = runner_data.get(\"runnerName\")\n                win_odds = runner_data.get(\"winRunnerOdds\", {}).get(\"currentPrice\")\n                if not runner_name or not win_odds:\n                    continue\n\n                numerator, denominator = map(int, win_odds.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n\n                odds = OddsData(win=decimal_odds, source=self.source_name, last_updated=datetime.now(timezone.utc))\n                program_number_str = runner_name.split(\".\")[0].strip()\n\n                runners.append(Runner(\n                    name=runner_name.split(\".\")[1].strip(),\n                    number=int(program_number_str) if program_number_str.isdigit() else 0,\n                    odds={self.source_name: odds},\n                ))\n            except (ValueError, ZeroDivisionError, IndexError):\n                self.logger.warning(\"Could not parse FanDuel runner\", runner_data=runner_data, exc_info=True)\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number_str}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=int(race_number_str),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/gbgb_api_adapter.py": "# python_service/adapters/gbgb_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_v3 import BaseAdapterV3\n\n\nclass GbgbApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for the Greyhound Board of Great Britain API, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"GBGB\"\n    BASE_URL = \"https://api.gbgb.org.uk/api/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw meeting data from the GBGB API.\"\"\"\n        endpoint = f\"results/meeting/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, meetings_data: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meeting data into a list of Race objects.\"\"\"\n        if not meetings_data:\n            return []\n\n        all_races = []\n        for meeting in meetings_data:\n            track_name = meeting.get(\"trackName\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    all_races.append(self._parse_race(race_data, track_name))\n                except (KeyError, TypeError):\n                    self.logger.error(\n                        \"Error parsing GBGB race\",\n                        race_id=race_data.get(\"raceId\"),\n                        exc_info=True,\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], track_name: str) -> Race:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        return Race(\n            id=f\"gbgb_{race_data['raceId']}\",\n            venue=track_name,\n            race_number=race_data[\"raceNumber\"],\n            start_time=datetime.fromisoformat(race_data[\"raceTime\"].replace(\"Z\", \"+00:00\")),\n            runners=self._parse_runners(race_data.get(\"traps\", [])),\n            source=self.source_name,\n            race_name=race_data.get(\"raceTitle\"),\n            distance=f\"{race_data.get('raceDistance')}m\",\n        )\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                odds_data = {}\n                sp = runner_data.get(\"sp\")\n                win_odds = parse_odds_to_decimal(sp)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds, source=self.source_name, last_updated=datetime.now()\n                    )\n\n                runners.append(\n                    Runner(\n                        number=runner_data[\"trapNumber\"],\n                        name=runner_data[\"dogName\"],\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, TypeError):\n                self.logger.warning(\"Error parsing GBGB runner, skipping.\", runner_name=runner_data.get(\"dogName\"))\n                continue\n        return runners\n",
    "python_service/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any, Dict, List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData, Race, Runner\nfrom .base_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, 'GREYHOUND_API_URL') or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(source_name=self.SOURCE_NAME, base_url=config.GREYHOUND_API_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data[\"cards\"]:\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_data['race_id']}\",\n                        venue=venue,\n                        race_number=race_data[\"race_number\"],\n                        start_time=datetime.fromtimestamp(race_data[\"start_time\"]),\n                        runners=self._parse_runners(race_data[\"runners\"]),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get('race_id', 'N/A'),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds, source=self.source_name, last_updated=datetime.now()\n                        )\n\n                runners.append(Runner(\n                    number=runner_data[\"trap_number\"],\n                    name=runner_data[\"dog_name\"],\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds_data,\n                ))\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "python_service/adapters/harness_adapter.py": "# python_service/adapters/harness_adapter.py\nfrom datetime import datetime\nfrom typing import Any, List\nfrom zoneinfo import ZoneInfo\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base_v3 import BaseAdapterV3\n\n\nclass HarnessAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US harness racing data from data.ustrotting.com.\"\"\"\n\n    SOURCE_NAME = \"USTrotting\"\n    BASE_URL = \"https://data.ustrotting.com/api/racenet/racing/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the USTA API.\"\"\"\n        card_data = await self.make_request(self.http_client, \"GET\", f\"card/{date}\")\n        return {\"data\": card_data, \"date\": date} if card_data else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"data\") or not raw_data[\"data\"].get(\"meetings\"):\n            self.logger.warning(\"No meetings found in harness data response.\")\n            return []\n\n        all_races = []\n        date = raw_data[\"date\"]\n        for meeting in raw_data[\"data\"][\"meetings\"]:\n            track_name = meeting.get(\"track\", {}).get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    all_races.append(self._parse_race(race_data, track_name, date))\n                except Exception:\n                    self.logger.warning(\n                        \"Failed to parse harness race, skipping.\",\n                        race_data=race_data,\n                        exc_info=True\n                    )\n                    continue\n        return all_races\n\n    def _parse_race(self, race_data: dict, track_name: str, date: str) -> Race:\n        \"\"\"Parses a single race from the USTA API into a Race object.\"\"\"\n        race_number = race_data.get(\"raceNumber\", 0)\n        post_time_str = race_data.get(\"postTime\", \"00:00 AM\")\n        start_time = self._parse_post_time(date, post_time_str)\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            if runner_data.get(\"scratched\", False):\n                continue\n\n            odds_str = runner_data.get(\"morningLineOdds\", \"\")\n            if \"/\" not in odds_str and odds_str.isdigit():\n                odds_str = f\"{odds_str}/1\"\n\n            odds = {}\n            win_odds = parse_odds_to_decimal(odds_str)\n            if win_odds and win_odds < 999:\n                odds = {self.SOURCE_NAME: OddsData(win=win_odds, source=self.SOURCE_NAME, last_updated=datetime.now())}\n\n            runners.append(\n                Runner(\n                    number=runner_data.get(\"postPosition\", 0),\n                    name=runner_data.get(\"horse\", {}).get(\"name\", \"Unknown Horse\"),\n                    odds=odds,\n                    scratched=False,\n                )\n            )\n\n        return Race(\n            id=f\"ust_{track_name.lower().replace(' ', '')}_{date}_{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n    def _parse_post_time(self, date: str, post_time: str) -> datetime:\n        \"\"\"Parses a time string like '07:00 PM' into a timezone-aware datetime object.\"\"\"\n        dt_str = f\"{date} {post_time}\"\n        naive_dt = datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        # Assume Eastern Time for USTA data, a common standard for US racing.\n        eastern = ZoneInfo(\"America/New_York\")\n        return naive_dt.replace(tzinfo=eastern)\n",
    "python_service/adapters/horseracingnation_adapter.py": "# python_service/adapters/horseracingnation_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass HorseRacingNationAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for horseracingnation.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n    SOURCE_NAME = \"HorseRacingNation\"\n    BASE_URL = \"https://www.horseracingnation.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/nyrabets_adapter.py": "# python_service/adapters/nyrabets_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass NYRABetsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for nyrabets.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n    SOURCE_NAME = \"NYRABets\"\n    BASE_URL = \"https://nyrabets.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/oddschecker_adapter.py": "# python_service/adapters/oddschecker_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any, List, Optional\n\nfrom bs4 import BeautifulSoup, Tag\n\nfrom ..models import OddsData, Race, Runner\nfrom .base_v3 import BaseAdapterV3\nfrom ..utils.odds import parse_odds_to_decimal\n\n\nclass OddscheckerAdapter(BaseAdapterV3):\n    \"\"\"Adapter for scraping horse racing odds from Oddschecker, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"Oddschecker\"\n    BASE_URL = \"https://www.oddschecker.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date. This involves a multi-level fetch.\n        \"\"\"\n        # Note: Oddschecker doesn't seem to support historical dates well in its main nav,\n        # but we build the URL as if it does for future compatibility.\n        index_url = f\"/horse-racing/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Oddschecker index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        # Find all links to individual race pages\n        race_links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in race_links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings from different races into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format provided to OddscheckerAdapter\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                race = self._parse_race_page(soup, race_date)\n                if race:\n                    all_races.append(race)\n            except (AttributeError, IndexError, ValueError):\n                self.logger.warning(\"Error parsing a race from Oddschecker, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_race_page(self, soup: BeautifulSoup, race_date) -> Optional[Race]:\n        track_name = soup.select_one(\"h1.meeting-name\").get_text(strip=True)\n        race_time_str = soup.select_one(\"span.race-time\").get_text(strip=True)\n\n        # Heuristic to find race number from navigation\n        active_link = soup.select_one(\"a.race-time-link.active\")\n        race_number = 1\n        if active_link:\n            all_links = soup.select(\"a.race-time-link\")\n            race_number = all_links.index(active_link) + 1\n\n        start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n        runners = [runner for row in soup.select(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n\n        if not runners:\n            return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _parse_runner_row(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = row.select_one(\"span.selection-name\").get_text(strip=True)\n            odds_str = row.select_one(\"span.bet-button-odds-desktop, span.best-price\").get_text(strip=True)\n            number = int(row.select_one(\"td.runner-number\").get_text(strip=True))\n\n            if not name or not odds_str:\n                return None\n\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_dict = {}\n            if win_odds and win_odds < 999:\n                odds_dict[self.source_name] = OddsData(\n                    win=win_odds, source=self.source_name, last_updated=datetime.now()\n                )\n\n            return Runner(number=number, name=name, odds=odds_dict)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on Oddschecker, skipping runner.\")\n            return None\n",
    "python_service/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\nfrom typing import Any, List\nfrom ..models import Race, Runner, OddsData\nfrom .base_v3 import BaseAdapterV3\nfrom datetime import datetime\nfrom decimal import Decimal\n\n# NOTE: This is a hypothetical implementation based on a potential API structure.\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for the hypothetical PointsBet Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all greyhound events for a given date.\"\"\"\n        endpoint = f'sports/greyhound-racing/events/by-date/{date}'\n        response = await self.make_request(self.http_client, \"GET\", endpoint)\n        return response.json().get('events', []) if response else None\n\n    def _parse_races(self, raw_data: list) -> List[Race]:\n        \"\"\"Parses the raw event data into a list of standardized Race objects.\"\"\"\n        races = []\n        for event in raw_data:\n            try:\n                if not event.get('competitors') or not event.get('startTime'):\n                    continue\n\n                runners = []\n                for competitor in event.get('competitors', []):\n                    if competitor.get('price'):\n                        odds_val = Decimal(str(competitor['price']))\n                        odds = {\n                            self.source_name: OddsData(\n                                win=odds_val, source=self.source_name, last_updated=datetime.now()\n                            )\n                        }\n                        runner = Runner(\n                            number=competitor.get('number', 99),\n                            name=competitor.get('name', 'Unknown'),\n                            odds=odds\n                        )\n                        runners.append(runner)\n\n                if runners:\n                    race = Race(\n                        id=f'pbg_{event[\"id\"]}',\n                        venue=event.get('venue', {}).get('name', 'Unknown Venue'),\n                        start_time=datetime.fromisoformat(event['startTime']),\n                        race_number=event.get('raceNumber', 1),\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\"Failed to parse PointsBet Greyhound event.\", event=event, exc_info=True)\n                continue\n        return races\n",
    "python_service/adapters/punters_adapter.py": "# python_service/adapters/punters_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass PuntersAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for punters.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n    SOURCE_NAME = \"Punters\"\n    BASE_URL = \"https://www.punters.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/racing_and_sports_adapter.py": "# python_service/adapters/racing_and_sports_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSports\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw meetings data from the Racing and Sports API.\"\"\"\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(self.http_client, \"GET\", \"v1/racing/meetings\", headers=headers, params=params)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Dict[str, Any]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        all_races = []\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in RacingAndSports response or invalid format.\")\n            return all_races\n\n        for meeting in raw_data[\"meetings\"]:\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse RacingAndSports race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        if not race_id or not start_time_str:\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\"),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", []) if isinstance(rd, dict)\n        ]\n\n        return Race(\n            id=f\"ras_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race.get(\"raceNumber\"),\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import Race, Runner\nfrom .base_v3 import BaseAdapterV3\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for Racing and Sports Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"RacingAndSportsGreyhound\"\n    BASE_URL = \"https://api.racingandsports.com.au/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def _fetch_data(self, date: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Fetches the raw greyhound meetings data from the Racing and Sports API.\"\"\"\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n        response = await self.make_request(self.http_client, \"GET\", \"v1/greyhound/meetings\", headers=headers, params=params)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Dict[str, Any]) -> List[Race]:\n        \"\"\"Parses the raw meetings data into a list of Race objects.\"\"\"\n        all_races = []\n        if not raw_data or not isinstance(raw_data.get(\"meetings\"), list):\n            self.logger.warning(\"No 'meetings' in RacingAndSportsGreyhound response or invalid format.\")\n            return all_races\n\n        for meeting in raw_data[\"meetings\"]:\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError):\n                    self.logger.warning(\n                        \"Failed to parse RacingAndSportsGreyhound race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        exc_info=True\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        \"\"\"Parses a single race object from the API response.\"\"\"\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        if not race_id or not start_time_str:\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\"),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", []) if isinstance(rd, dict)\n        ]\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race.get(\"raceNumber\"),\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/racingpost_adapter.py": "# python_service/adapters/racingpost_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List\n\nfrom selectolax.parser import HTMLParser\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base_v3 import BaseAdapterV3\n\n\nclass RacingPostAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping Racing Post racecards, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"RacingPost\"\n    BASE_URL = \"https://www.racingpost.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url, headers=self._get_headers())\n        if not index_response:\n            self.logger.warning(\"Failed to fetch RacingPost index page\", url=index_url)\n            return None\n\n        index_parser = HTMLParser(index_response.text)\n        links = index_parser.css('a[data-test-selector^=\"RC-meetingItem__link_race\"]')\n        race_card_urls = [link.attributes['href'] for link in links]\n\n        async def fetch_single_html(url: str):\n            response = await self.make_request(self.http_client, \"GET\", url, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"html_contents\"):\n            return []\n\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            if not html:\n                continue\n            try:\n                parser = HTMLParser(html)\n                venue_raw = parser.css_first('a[data-test-selector=\"RC-course__name\"]').text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n                race_time_str = parser.css_first('span[data-test-selector=\"RC-course__time\"]').text(strip=True)\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.error(\"Failed to parse RacingPost race from HTML content.\", exc_info=True)\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            try:\n                number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n                name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n                odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n                if not all([number_node, name_node, odds_node]):\n                    continue\n\n                number_str = clean_text(number_node.text())\n                number = int(number_str) if number_str and number_str.isdigit() else 0\n                name = clean_text(name_node.text())\n                odds_str = clean_text(odds_node.text())\n                scratched = \"NR\" in odds_str.upper() or not odds_str\n\n                odds = {}\n                if not scratched:\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    if win_odds and win_odds < 999:\n                        odds = {\n                            self.source_name: OddsData(\n                                win=win_odds, source=self.source_name, last_updated=datetime.now()\n                            )\n                        }\n\n                runners.append(Runner(number=number, name=name, odds=odds, scratched=scratched))\n            except (ValueError, AttributeError):\n                self.logger.warning(\"Could not parse RacingPost runner, skipping.\", parser=parser)\n                continue\n        return runners\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/racingtv_adapter.py": "# python_service/adapters/racingtv_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass RacingTVAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping data from racingtv.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom bs4 import BeautifulSoup, Tag\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_v3 import BaseAdapterV3\n\n\nclass SportingLifeAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for sportinglife.com, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"SportingLife\"\n    BASE_URL = \"https://www.sportinglife.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        Returns a dictionary containing the HTML content and the date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch SportingLife index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format provided to SportingLifeAdapter\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                track_name = clean_text(soup.select_one(\"a.hr-race-header-course-name__link\").get_text())\n                race_time_str = clean_text(soup.select_one(\"span.hr-race-header-time__time\").get_text())\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n                race_number = (\n                    soup.select(\"a.hr-race-header-navigation-link\").index(active_link) + 1 if active_link else 1\n                )\n                runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError):\n                self.logger.warning(\"Error parsing a race from SportingLife, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = clean_text(row.select_one(\"a.hr-racing-runner-horse-name\").get_text())\n            num_str = clean_text(row.select_one(\"span.hr-racing-runner-saddle-cloth-no\").get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n            odds_str = clean_text(row.select_one(\"span.hr-racing-runner-odds\").get_text())\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())}\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            self.logger.warning(\"Failed to parse a runner on SportingLife, skipping runner.\")\n            return None\n",
    "python_service/adapters/tab_adapter.py": "# python_service/adapters/tab_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass TabAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for tab.com.au.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n    SOURCE_NAME = \"TAB\"\n    BASE_URL = \"https://www.tab.com.au\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/template_adapter.py": "# python_service/adapters/template_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass TemplateAdapter(BaseAdapterV3):\n    \"\"\"\n    A template for creating new adapters, based on the BaseAdapterV3 pattern.\n    This adapter is a non-functional stub.\n    \"\"\"\n    SOURCE_NAME = \"[IMPLEMENT ME] Example Source\"\n    BASE_URL = \"https://api.example.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/the_racing_api_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any, Dict, List\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData, Race, Runner\nfrom .base_v3 import BaseAdapterV3\n\n\nclass TheRacingApiAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for The Racing API, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"TheRacingAPI\"\n    BASE_URL = \"https://api.theracingapi.com/v1/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, date: str) -> Dict[str, Any]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(self.http_client, \"GET\", endpoint, headers=headers)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Dict[str, Any]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in TheRacingAPI response.\")\n            return []\n\n        races = []\n        for race_data in raw_data[\"racecards\"]:\n            try:\n                start_time = datetime.fromisoformat(race_data[\"off_time\"].replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_data['race_id']}\",\n                    venue=race_data[\"course\"],\n                    race_number=race_data[\"race_no\"],\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI race\",\n                    race_id=race_data.get(\"race_id\"),\n                    exc_info=True\n                )\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                odds_data = {}\n                if runner_data.get(\"odds\"):\n                    win_odds = Decimal(str(runner_data[\"odds\"][0][\"odds_decimal\"]))\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds, source=self.source_name, last_updated=datetime.now()\n                    )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=runner_data[\"horse\"],\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception:\n                self.logger.error(\n                    \"Error parsing TheRacingAPI runner\",\n                    runner_name=runner_data.get(\"horse\"),\n                    exc_info=True\n                )\n        return runners\n",
    "python_service/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\n\nfrom bs4 import BeautifulSoup, Tag\n\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/horse-racing/racecards/{date}\"\n        index_response = await self.make_request(self.http_client, \"GET\", index_url)\n        if not index_response:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(self.http_client, \"GET\", url_path)\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\"Invalid date format provided to TimeformAdapter\", date=raw_data.get(\"date\"))\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                track_name = clean_text(soup.select_one(\"h1.rp-raceTimeCourseName_name\").get_text())\n                race_time_str = clean_text(soup.select_one(\"span.rp-raceTimeCourseName_time\").get_text())\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = clean_text(row.select_one(\"a.rp-horseTable_horse-name\").get_text())\n            num_str = clean_text(row.select_one(\"span.rp-horseTable_horse-number\").get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
    "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List\n\nfrom ..core.exceptions import AdapterConfigError, AdapterParsingError\nfrom ..models import Race, Runner\nfrom ..utils.text import clean_text\nfrom .base_v3 import BaseAdapterV3\n\n\nclass TVGAdapter(BaseAdapterV3):\n    \"\"\"Adapter for fetching US racing data from the TVG API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"TVG\"\n    BASE_URL = \"https://api.tvg.com/v2/races/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(self.http_client, \"GET\", summary_url, headers=headers)\n        if not tracks_response:\n            return None\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get('tracks', []):\n            track_id = track.get('id')\n            for race in track.get('races', []):\n                race_id = race.get('id')\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(self.http_client, \"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        # Filter out exceptions and return only successful responses\n        return [resp.json() for resp in race_detail_responses if not isinstance(resp, Exception)]\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of detailed race JSON objects into Race models.\"\"\"\n        races = []\n        if not isinstance(raw_data, list):\n            self.logger.warning(\"raw_data is not a list, cannot parse TVG races.\")\n            return races\n\n        for race_detail in raw_data:\n            try:\n                races.append(self._parse_race(race_detail))\n            except AdapterParsingError:\n                self.logger.warning(\"Failed to parse TVG race detail, skipping.\", race_detail=race_detail, exc_info=True)\n        return races\n\n    def _parse_race(self, race_detail: dict) -> Race:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get('track')\n        race_info = race_detail.get('race')\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get('runners', []):\n            if runner_data.get('scratched'):\n                continue\n\n            odds = runner_data.get('odds', {})\n            current_odds = odds.get('currentPrice', {})\n            odds_str = current_odds.get('fractional') or odds.get('morningLinePrice', {}).get('fractional')\n\n            try:\n                number = int(runner_data.get('programNumber', '0').replace('A', ''))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            runners.append(Runner(\n                number=number,\n                name=clean_text(runner_data.get('name')),\n                odds=odds_str, # Odds will be parsed later in the data pipeline\n                scratched=False\n            ))\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        try:\n            start_time = datetime.fromisoformat(\n                race_info.get(\"postTime\").replace(\"Z\", \"+00:00\")\n            )\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name, f\"Could not parse post time: {race_info.get('postTime')}\"\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get('name'),\n            race_number=race_info.get('number'),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name\n        )\n",
    "python_service/adapters/twinspires_adapter.py": "# python_service/adapters/twinspires_adapter.py\nfrom datetime import datetime\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for twinspires.com.\n    This is a placeholder for a full implementation using the discovered JSON API.\n    \"\"\"\n    SOURCE_NAME = \"TwinSpires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"\n        TODO: Implement the logic to fetch data from the Twinspires JSON API.\n\n        *** JB's Discoveries for the next agent to implement ***\n\n        The goal is to reverse-engineer the site's API to get a list of all tracks\n        for a given day, then get the race card for each track, and finally the\n        runner data for each race.\n\n        1.  **Main Page (potential source for a list of all tracks):**\n            - https://www.twinspires.com/bet/todays-races/\n\n        2.  **API Endpoint for All Tracks (CRITICAL DISCOVERY):**\n            - This URL appears to provide the core data for all tracks for the day.\n            - https://www.twinspires.com/adw/todays-tracks?affid=0\n\n        3.  **Example Track URLs (these seem to load the race card data):**\n            - Greyhound: https://www.twinspires.com/adw/todays-tracks/cp1/Greyhound/races?affid=0\n            - Thoroughbred: https://www.twinspires.com/adw/todays-tracks/fl/Thoroughbred/races?affid=0\n            - Harness: https://www.twinspires.com/adw/todays-tracks/mr/Harness/races?affid=0\n\n        3.  **Example Race Card JSON (for Central Park):**\n            This data is likely fetched by one of the track URLs above. Note that this\n            JSON does NOT contain the runner (horse/greyhound) data, so another API\n            call will be needed to get the entries for each race.\n\n            ```json\n            [\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":1,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T10:36:17-04:00\",\"postTimeStamp\":1761575777000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":2,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T10:54:15-04:00\",\"postTimeStamp\":1761576855000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"537 Y\",\"distanceLong\":\"537 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":3,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T11:13:16-04:00\",\"postTimeStamp\":1761577996000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":4,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T11:32:24-04:00\",\"postTimeStamp\":1761579144000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":5,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T11:51:15-04:00\",\"postTimeStamp\":1761580275000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"537 Y\",\"distanceLong\":\"537 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":6,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T12:09:15-04:00\",\"postTimeStamp\":1761581355000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":7,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T12:28:16-04:00\",\"postTimeStamp\":1761582496000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"537 Y\",\"distanceLong\":\"537 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":8,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T12:47:20-04:00\",\"postTimeStamp\":1761583640000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":9,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T13:06:22-04:00\",\"postTimeStamp\":1761584782000,\"mtp\":0,\"status\":\"Closed\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":10,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T13:23:59-04:00\",\"postTimeStamp\":1761585839000,\"mtp\":3,\"status\":\"Open\",\"distance\":\"537 Y\",\"distanceLong\":\"537 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":true,\"hasExpertPick\":false},\n                {\"raceType\":\"\",\"purse\":\"0\",\"maxClaimPrice\":\"0\",\"grade\":\"\",\"raceNumber\":11,\"raceDate\":\"2025-10-27\",\"postTime\":\"2025-10-27T13:43:00-04:00\",\"postTimeStamp\":1761586980000,\"mtp\":99,\"status\":\"Open\",\"distance\":\"303 Y\",\"distanceLong\":\"303 YARDS\",\"ageRestrictions\":\"\",\"sexRestrictions\":\"\",\"wagers\":\"Win Place Exacta Quinella Trifecta Central Park\",\"country\":\"ENG\",\"carryover\":[],\"formattedPurse\":\"$0\",\"hasSilks\":false,\"displayRaceName\":\"\",\"displayRaceDescription\":\"\",\"surfaceConditionMap\":{},\"hasBrisPick\":false,\"currentRace\":false,\"hasExpertPick\":false}\n            ]\n            ```\n        \"\"\"\n        self.logger.info(\"Fetching data for TwinSpires\")\n        # Placeholder: returning None as the actual implementation is pending.\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        TODO: Implement the logic to parse the JSON data from the Twinspires API.\n        \"\"\"\n        if not raw_data:\n            return []\n\n        # Placeholder: returning an empty list as the actual implementation is pending.\n        return []\n\n    def get_races(self, date: str) -> List[Race]:\n        \"\"\"\n        Orchestrates the fetching and parsing of race data for a given date.\n        This method will be called by the FortunaEngine.\n        \"\"\"\n        # This is a synchronous wrapper for the async orchestrator\n        # The actual implementation will use the async methods.\n        self.logger.info(f\"Getting races for {date} from {self.SOURCE_NAME}\")\n        return []\n",
    "python_service/adapters/universal_adapter.py": "# python_service/adapters/universal_adapter.py\nimport json\nfrom typing import Any, List\n\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass UniversalAdapter(BaseAdapterV3):\n    \"\"\"\n    An adapter that executes logic from a declarative JSON definition file.\n    NOTE: This is a simplified proof-of-concept implementation.\n    \"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Executes the fetch steps defined in the JSON definition.\"\"\"\n        self.logger.info(f\"Executing Universal Adapter PoC for {self.source_name}\")\n        response = await self.make_request(self.http_client, \"GET\", self.definition[\"start_url\"])\n        if not response:\n            return None\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        # In a full implementation, we would fetch and return each track page's content.\n        # For this PoC, we are not fetching the individual track links.\n        self.logger.warning(\"UniversalAdapter is a proof-of-concept and does not fully fetch all data.\")\n        return track_links\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a proof-of-concept and does not parse any data.\"\"\"\n        return []\n",
    "python_service/adapters/utils.py": "# python_service/adapters/utils.py\n# Compatibility shim to re-export parse_odds from the centralized location.\n\nfrom ..utils.odds import parse_odds\n\n__all__ = ['parse_odds']\n",
    "python_service/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any, List\n\nfrom ..models import Race\nfrom .base_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. \"\n            \"It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "python_service/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        scored_races = []\n        for race in races:\n            # The _evaluate_race method now always returns a float score.\n            race.qualification_score = self._evaluate_race(race)\n            scored_races.append(race)\n\n        scored_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\"Universal scoring complete\", total_races_scored=len(scored_races), criteria=criteria)\n\n        for race in scored_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": scored_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n\n    def play(self, sound_type: str):\n        if not winsound:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier() if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "python_service/api.py": "# python_service/api.py\n\nimport os\nfrom contextlib import asynccontextmanager\nfrom datetime import date, datetime, timedelta\nfrom typing import List, Optional\n\nimport aiosqlite\nimport structlog\nfrom fastapi import Depends, FastAPI, HTTPException, Query, Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom typing import Callable\n\nfrom .analyzer import AnalyzerEngine\nfrom .config import get_settings\nfrom .engine import FortunaEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .middleware.error_handler import validation_exception_handler\nfrom .models import AggregatedResponse, QualifiedRacesResponse, Race, TipsheetRace\nfrom .security import verify_api_key\n\n# --- PyInstaller Explicit Imports ---\nfrom .adapters import *\n# ------------------------------------\n\nlog = structlog.get_logger()\n\n# Lifespan context manager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    configure_logging()\n    log.info(\"Server startup sequence initiated.\")\n    try:\n        settings = get_settings()\n        app.state.engine = FortunaEngine(config=settings)\n        app.state.analyzer_engine = AnalyzerEngine()\n        log.info(\"Server startup: Configuration validated and FortunaEngine initialized successfully.\")\n    except Exception as e:\n        log.critical(\"FATAL: Failed to initialize FortunaEngine during server startup.\", exc_info=True)\n        raise e\n    yield\n    if hasattr(app.state, 'engine') and app.state.engine:\n        log.info(\"Server shutdown: Closing HTTP client resources.\")\n        await app.state.engine.close()\n    log.info(\"Server shutdown sequence complete.\")\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI(\n    title=\"Fortuna Faucet API\",\n    version=\"2.1\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\"\n)\n\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\nsettings = get_settings()\napp.include_router(health_router)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"GET\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_engine(request: Request) -> FortunaEngine:\n    return request.app.state.engine\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request, engine: FortunaEngine = Depends(get_engine), _=Depends(verify_api_key)\n):\n    try:\n        return engine.get_all_adapter_statuses()\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[str] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n        pattern=\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\",\n    ),\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_favorite_odds: float = Query(2.5, ge=1.0, le=100.0),\n    min_second_favorite_odds: float = Query(4.0, ge=1.0, le=100.0),\n):\n    try:\n        date_str = race_date or datetime.now().date().strftime(\"%Y-%m-%d\")\n        aggregated_data = await engine.get_races(date_str)\n        races = [Race(**r) for r in aggregated_data.get(\"races\", [])]\n        analyzer_engine = request.app.state.analyzer_engine\n        custom_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds,\n        }\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except Exception:\n        log.error(\"Error in /api/races/qualified\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n@app.get(\"/api/races/filter-suggestions\")\nasync def get_filter_suggestions(engine: FortunaEngine = Depends(get_engine)):\n    try:\n        date_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n        aggregated = await engine.get_races(date_str)\n        if not aggregated or not aggregated.get(\"races\"):\n            return {\"suggestions\": {}}\n        field_sizes = [len(r[\"runners\"]) for r in aggregated[\"races\"]]\n        favorite_odds, second_favorite_odds = [], []\n        for race_data in aggregated[\"races\"]:\n            race = Race(**race_data)\n            runners = race.runners\n            if len(runners) >= 2:\n                odds_list = []\n                for runner in runners:\n                    if not runner.scratched and runner.odds:\n                        best_odd = min((o.win for o in runner.odds.values() if o.win is not None), default=None)\n                        if best_odd is not None:\n                            odds_list.append(float(best_odd))\n                if len(odds_list) >= 2:\n                    odds_list.sort()\n                    favorite_odds.append(odds_list[0])\n                    second_favorite_odds.append(odds_list[1])\n        return {\n            \"suggestions\": {\n                \"max_field_size\": {\"recommended\": int(sum(field_sizes) / len(field_sizes)) if field_sizes else 10},\n                \"min_favorite_odds\": {\"recommended\": 2.5},\n                \"min_second_favorite_odds\": {\"recommended\": 4.0},\n            }\n        }\n    except Exception:\n        log.error(\"Error generating filter suggestions\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Failed to generate suggestions\")\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[str] = Query(\n        default=None,\n        description=\"Date of the races in YYYY-MM-DD format. Defaults to today.\",\n        pattern=\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\",\n    ),\n    source: Optional[str] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        date_str = race_date or datetime.now().date().strftime(\"%Y-%m-%d\")\n        return await engine.get_races(date_str, source)\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\nDB_PATH = \"fortuna.db\"\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = \"SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC\"\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    return results\n\n@app.get(\"/health/legacy\", tags=[\"Health\"], summary=\"Check for Deprecated Legacy Components\")\nasync def check_legacy_files():\n    legacy_files = [\"checkmate_service.py\", \"checkmate_web/main.py\"]\n    present_files = [f for f in legacy_files if os.path.exists(f)]\n    if present_files:\n        return {\"status\": \"WARNING\", \"message\": \"Legacy files detected.\", \"detected_files\": present_files}\n    return {\"status\": \"CLEAN\", \"message\": \"No known legacy files detected.\"}\n",
    "python_service/cache_manager.py": "# python_service/cache_manager.py\nimport hashlib\nimport json\nimport os\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self, redis_url: str = None):\n        self.redis_client = None\n        self.memory_cache = {}\n        if REDIS_AVAILABLE and redis_url:\n            try:\n                self.redis_client = redis.from_url(redis_url, decode_responses=True)\n                log.info(\"Redis cache connected successfully.\")\n            except Exception as e:\n                log.warning(f\"Failed to connect to Redis: {e}. Falling back to in-memory cache.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except Exception as e:\n                log.warning(f\"Redis GET failed: {e}\")\n\n        entry = self.memory_cache.get(key)\n        if entry and entry[\"expires_at\"] > datetime.now():\n            return entry[\"value\"]\n        return None\n\n    def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        serialized = json.dumps(value, default=str)\n        if self.redis_client:\n            try:\n                self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except Exception as e:\n                log.warning(f\"Redis SET failed: {e}\")\n\n        self.memory_cache[key] = {\"value\": value, \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds)}\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager(redis_url=os.getenv(\"REDIS_URL\"))\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n            cache_manager.set(cache_key, result, ttl_seconds)\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "python_service/checkmate_service.py": "# checkmate_service.py\n# The main service runner, upgraded to the final Endgame architecture.\n\nimport json\nimport logging\nimport os\nimport sqlite3\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom .engine import EnhancedTrifectaAnalyzer\nfrom .engine import Race\nfrom .engine import Settings\nfrom .engine import SuperchargedOrchestrator\n\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            # Correctly resolve paths from the service's location\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n            schema_path = os.path.join(base_dir, \"shared_database\", \"schema.sql\")\n            web_schema_path = os.path.join(base_dir, \"shared_database\", \"web_schema.sql\")\n\n            # Read both schema files\n            with open(schema_path, \"r\") as f:\n                schema = f.read()\n            with open(web_schema_path, \"r\") as f:\n                web_schema = f.read()\n\n            # Apply both schemas in a single transaction\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.executescript(schema)\n                cursor.executescript(web_schema)\n                conn.commit()\n            self.logger.info(\"CRITICAL SUCCESS: All database schemas (base + web) applied successfully.\")\n        except Exception as e:\n            self.logger.critical(f\"FATAL: Database setup failed. Other platforms will fail. Error: {e}\", exc_info=True)\n            raise\n\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO live_races (\n                        race_id, track_name, race_number, post_time, raw_data_json,\n                        checkmate_score, qualified, trifecta_factors_json, updated_at\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        race.race_id,\n                        race.track_name,\n                        race.race_number,\n                        race.post_time,\n                        race.model_dump_json(),\n                        race.checkmate_score,\n                        race.is_qualified,\n                        race.trifecta_factors_json,\n                        datetime.now(),\n                    ),\n                )\n            for status in statuses:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO adapter_status (\n                        adapter_name, status, last_run, races_found, error_message,\n                        execution_time_ms\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        status.get(\"adapter_id\"),\n                        status.get(\"status\"),\n                        status.get(\"timestamp\"),\n                        status.get(\"races_found\"),\n                        status.get(\"error_message\"),\n                        int(status.get(\"response_time\", 0) * 1000),\n                    ),\n                )\n\n            if races or statuses:\n                cursor.execute(\n                    \"INSERT INTO events (event_type, payload) VALUES (?, ?)\",\n                    (\"RACES_UPDATED\", json.dumps({\"race_count\": len(races)})),\n                )\n\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\")\n\n\nclass CheckmateBackgroundService:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        from dotenv import load_dotenv\n\n        dotenv_path = os.path.join(os.path.dirname(__file__), \"..\", \".env\")\n        load_dotenv(dotenv_path=dotenv_path)\n\n        db_path = os.getenv(\"CHECKMATE_DB_PATH\")\n        if not db_path:\n            self.logger.critical(\"FATAL: CHECKMATE_DB_PATH environment variable not set. Service cannot start.\")\n            raise ValueError(\"CHECKMATE_DB_PATH is not configured.\")\n\n        self.logger.info(f\"Database path loaded from environment: {db_path}\")\n\n        self.settings = Settings()\n        self.db_handler = DatabaseHandler(db_path)\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\n        self.python_analyzer = EnhancedTrifectaAnalyzer(self.settings)\n        self.stop_event = threading.Event()\n        self.rust_engine_path = os.path.join(\n            os.path.dirname(__file__), \"..\", \"rust_engine\", \"target\", \"release\", \"checkmate_engine.exe\"\n        )\n\n    def _analyze_with_rust(self, races: List[Race]) -> Optional[List[Race]]:\n        self.logger.info(\"Attempting analysis with external Rust engine.\")\n        try:\n            race_data_json = json.dumps([r.model_dump() for r in races])\n            result = subprocess.run(\n                [self.rust_engine_path], input=race_data_json, capture_output=True, text=True, check=True, timeout=30\n            )\n            results_data = json.loads(result.stdout)\n            results_map = {res[\"race_id\"]: res for res in results_data}\n\n            for race in races:\n                if race.race_id in results_map:\n                    res = results_map[race.race_id]\n                    race.checkmate_score = res.get(\"checkmate_score\")\n                    race.is_qualified = res.get(\"qualified\")\n                    race.trifecta_factors_json = json.dumps(res.get(\"trifecta_factors\"))\n            return races\n        except FileNotFoundError:\n            self.logger.warning(\"Rust engine not found. Falling back to Python analyzer.\")\n            return None\n        except (subprocess.CalledProcessError, json.JSONDecodeError, subprocess.TimeoutExpired) as e:\n            self.logger.error(f\"Rust engine execution failed: {e}. Falling back to Python analyzer.\")\n            return None\n\n    def _analyze_with_python(self, races: List[Race]) -> List[Race]:\n        self.logger.info(\"Performing analysis with internal Python engine.\")\n        return [self.python_analyzer.analyze_race_advanced(race) for race in races]\n\n    def run_continuously(self, interval_seconds: int = 60):\n        self.logger.info(\"Background service thread starting continuous run.\")\n\n        while not self.stop_event.is_set():\n            try:\n                self.logger.info(\"Starting data collection and analysis cycle.\")\n                races, statuses = self.orchestrator.get_races_parallel()\n\n                analyzed_races = None\n                if os.path.exists(self.rust_engine_path):\n                    analyzed_races = self._analyze_with_rust(races)\n\n                if analyzed_races is None:  # Fallback condition\n                    analyzed_races = self._analyze_with_python(races)\n\n                if analyzed_races:  # Ensure we have something to update\n                    self.db_handler.update_races_and_status(analyzed_races, statuses)\n\n            except Exception as e:\n                self.logger.critical(f\"Unhandled exception in service loop: {e}\", exc_info=True)\n\n            self.logger.info(f\"Cycle complete. Sleeping for {interval_seconds} seconds.\")\n            self.stop_event.wait(interval_seconds)\n        self.logger.info(\"Background service run loop has terminated.\")\n\n    def start(self):\n        self.stop_event.clear()\n        self.thread = threading.Thread(target=self.run_continuously)\n        self.thread.daemon = True\n        self.thread.start()\n        self.logger.info(\"CheckmateBackgroundService started.\")\n\n    def stop(self):\n        self.stop_event.set()\n        if hasattr(self, \"thread\") and self.thread.is_alive():\n            self.thread.join(timeout=10)\n        self.logger.info(\"CheckmateBackgroundService stopped.\")\n",
    "python_service/config.py": "# python_service/config.py\nimport os\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport structlog\nfrom pydantic import model_validator\nfrom pydantic_settings import BaseSettings\n\nfrom .credentials_manager import SecureCredentialsManager\n\n# --- Encryption Setup ---\ntry:\n    from cryptography.fernet import Fernet\n    ENCRYPTION_ENABLED = True\nexcept ImportError:\n    ENCRYPTION_ENABLED = False\n\nKEY_FILE = Path('.key')\nCIPHER = None\nif ENCRYPTION_ENABLED and KEY_FILE.exists():\n    with open(KEY_FILE, 'rb') as f:\n        key = f.read()\n    CIPHER = Fernet(key)\n\ndef decrypt_value(value: Optional[str]) -> Optional[str]:\n    \"\"\"If a value is encrypted, decrypts it. Otherwise, returns it as is.\"\"\"\n    if value and value.startswith('encrypted:') and CIPHER:\n        try:\n            return CIPHER.decrypt(value[10:].encode()).decode()\n        except Exception:\n            # Return the corrupted value for debugging, but it will likely fail later\n            return value\n    return value\n\n\nclass Settings(BaseSettings):\n    API_KEY: str = \"\"\n\n    # --- API Gateway Configuration ---\n    UVICORN_HOST: str = \"127.0.0.1\"\n    UVICORN_PORT: int = 8000\n    UVICORN_RELOAD: bool = True\n\n    # --- Database Configuration ---\n    DATABASE_TYPE: str = \"sqlite\"\n    DATABASE_URL: str = \"sqlite:///./fortuna.db\"\n\n    # --- Optional Betfair Credentials ---\n    BETFAIR_APP_KEY: Optional[str] = None\n\n    # --- Caching & Performance ---\n    REDIS_URL: str = \"redis://localhost:6379\"\n    CACHE_TTL_SECONDS: int = 1800  # 30 minutes\n    MAX_CONCURRENT_REQUESTS: int = 10\n    HTTP_POOL_CONNECTIONS: int = 100\n    HTTP_POOL_MAXSIZE: int = 100\n    HTTP_MAX_KEEPALIVE: int = 50\n    DEFAULT_TIMEOUT: int = 30\n    ADAPTER_TIMEOUT: int = 20\n\n    # --- Logging ---\n    LOG_LEVEL: str = \"INFO\"\n\n    # --- Optional Adapter Keys ---\n    NEXT_PUBLIC_API_KEY: Optional[str] = None # Allow frontend key to be present in .env\n    TVG_API_KEY: Optional[str] = None\n    RACING_AND_SPORTS_TOKEN: Optional[str] = None\n    POINTSBET_API_KEY: Optional[str] = None\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    model_config = {\"env_file\": \".env\", \"case_sensitive\": True}\n\n    @model_validator(mode='after')\n    def process_settings(self) -> 'Settings':\n        \"\"\"\n        This validator runs after the initial settings are loaded from .env and\n        performs two key functions:\n        1. If API_KEY is missing, it falls back to the SecureCredentialsManager.\n        2. It decrypts any fields that were loaded from the .env file.\n        \"\"\"\n        # 1. Fallback for API_KEY\n        if not self.API_KEY:\n            self.API_KEY = SecureCredentialsManager.get_credential(\"api_key\") or \"MISSING\"\n\n        # 2. Decrypt sensitive fields\n        self.BETFAIR_APP_KEY = decrypt_value(self.BETFAIR_APP_KEY)\n\n        return self\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Loads settings and performs a proactive check for legacy paths.\"\"\"\n    log = structlog.get_logger(__name__)\n    if ENCRYPTION_ENABLED and not KEY_FILE.exists():\n        log.warning(\n            \"encryption_key_not_found\",\n            file=str(KEY_FILE),\n            recommendation=\"Run 'python manage_secrets.py' to generate a key.\",\n        )\n\n    settings = Settings()\n\n    # --- Legacy Path Detection ---\n    legacy_paths = [\"attic/\", \"checkmate_web/\", \"vba_source/\"]\n    for path in legacy_paths:\n        if os.path.exists(path):\n            log.warning(\n                \"legacy_path_detected\",\n                path=path,\n                recommendation=\"This directory is obsolete and should be removed for optimal performance and security.\"\n            )\n\n    return settings\n",
    "python_service/core/__init__.py": "",
    "python_service/core/errors.py": "# python_service/core/errors.py\nfrom enum import Enum\n\n\nclass ErrorCategory(Enum):\n    CONFIGURATION_ERROR = \"Configuration missing or invalid\"\n    NETWORK_ERROR = \"HTTP/Network request failed\"\n    PARSING_ERROR = \"Data parsing or validation unsuccessful\"\n    UNEXPECTED_ERROR = \"An unhandled exception occurred\"\n",
    "python_service/core/exceptions.py": "# python_service/core/exceptions.py\n\"\"\"\nCustom, application-specific exceptions for the Fortuna Faucet service.\n\nThis module defines a hierarchy of exception classes to provide standardized\nerror handling, particularly for the data adapter layer. Using these specific\nexceptions instead of generic ones allows for more precise error handling and\nclearer logging throughout the application.\n\"\"\"\n\nclass FortunaException(Exception):\n    \"\"\"Base class for all custom exceptions in this application.\"\"\"\n    pass\n\nclass AdapterError(FortunaException):\n    \"\"\"Base class for all adapter-related errors.\"\"\"\n    def __init__(self, adapter_name: str, message: str):\n        self.adapter_name = adapter_name\n        super().__init__(f\"[{adapter_name}] {message}\")\n\nclass AdapterRequestError(AdapterError):\n    \"\"\"Raised for general network or request-related issues.\"\"\"\n    pass\n\nclass AdapterHttpError(AdapterRequestError):\n    \"\"\"Raised for unsuccessful HTTP responses (e.g., 4xx or 5xx status codes).\"\"\"\n    def __init__(self, adapter_name: str, status_code: int, url: str):\n        self.status_code = status_code\n        self.url = url\n        message = f\"Received HTTP {status_code} from {url}\"\n        super().__init__(adapter_name, message)\n\nclass AdapterAuthError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 401/403 errors, indicating an auth failure.\"\"\"\n    pass\n\nclass AdapterRateLimitError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 429 errors, indicating a rate limit has been hit.\"\"\"\n    pass\n\nclass AdapterTimeoutError(AdapterRequestError):\n    \"\"\"Raised when a request to an external API times out.\"\"\"\n    pass\n\nclass AdapterConnectionError(AdapterRequestError):\n    \"\"\"Raised for DNS lookup failures or refused connections.\"\"\"\n    pass\n\nclass AdapterConfigError(AdapterError):\n    \"\"\"Raised when an adapter is missing necessary configuration (e.g., an API key).\"\"\"\n    pass\n\nclass AdapterParsingError(AdapterError):\n    \"\"\"Raised when an adapter fails to parse the response from an API.\"\"\"\n    pass\n",
    "python_service/credentials_manager.py": "# python_service/credentials_manager.py\nimport keyring\n\ntry:\n    # This check is crucial for cross-platform compatibility\n    import keyring.backends.windows\n    IS_WINDOWS = True\nexcept ImportError:\n    IS_WINDOWS = False\n\nclass SecureCredentialsManager:\n    \"\"\"Manages secrets in the system's native credential store.\"\"\"\n\n    SERVICE_NAME = \"Fortuna\"\n\n    @staticmethod\n    def save_credential(account: str, secret: str) -> bool:\n        \"\"\"Saves a secret for a given account (e.g., 'api_key', 'betfair_username').\"\"\"\n        if not IS_WINDOWS:\n            print(\"Credential storage is only supported on Windows.\")\n            return False\n        try:\n            keyring.set_password(SecureCredentialsManager.SERVICE_NAME, account, secret)\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credential for {account}: {e}\")\n            return False\n\n    @staticmethod\n    def get_credential(account: str) -> str:\n        \"\"\"Retrieves a secret for a given account.\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            return keyring.get_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credential for {account}: {e}\")\n            return None\n\n    @staticmethod\n    def get_betfair_credentials() -> tuple[str, str]:\n        \"\"\"Convenience method to retrieve both Betfair username and password.\"\"\"\n        username = SecureCredentialsManager.get_credential(\"betfair_username\")\n        password = SecureCredentialsManager.get_credential(\"betfair_password\")\n        return username, password\n\n    @staticmethod\n    def delete_credential(account: str):\n        \"\"\"Deletes a specific credential.\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(SecureCredentialsManager.SERVICE_NAME, account)\n        except Exception:\n            pass\n",
    "python_service/db/init.py": "# python_service/db/init.py\nimport sqlite3\nimport os\nfrom ..config import get_settings\n\ndef initialize_database():\n    \"\"\"\n    Initializes the database based on the configuration.\n    Currently supports a simple SQLite fallback for local testing.\n    \"\"\"\n    settings = get_settings()\n    db_type = getattr(settings, 'DATABASE_TYPE', 'sqlite').lower()\n\n    if db_type == 'sqlite':\n        # DATABASE_URL for sqlite will be like 'sqlite:///./fortuna.db'\n        db_path = settings.DATABASE_URL.split('///')[1]\n\n        # Ensure the directory for the database exists\n        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n\n        try:\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n\n            # The schema is based on the provided pg_schemas, adapted for SQLite\n            # This is a simplified version for demonstration.\n            cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS races (\n                id TEXT PRIMARY KEY,\n                venue TEXT NOT NULL,\n                race_number INTEGER NOT NULL,\n                start_time TEXT NOT NULL,\n                source TEXT,\n                field_size INTEGER\n            )\n            \"\"\")\n\n            cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS runners (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                race_id TEXT,\n                number INTEGER,\n                name TEXT,\n                odds REAL,\n                FOREIGN KEY (race_id) REFERENCES races (id)\n            )\n            \"\"\")\n\n            conn.commit()\n            conn.close()\n            print(\"SQLite database initialized successfully.\")\n        except sqlite3.Error as e:\n            print(f\"Error initializing SQLite database: {e}\")\n            raise\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any, Dict, List, Tuple\n\nimport httpx\nimport structlog\n\nfrom .adapters.base_v3 import BaseAdapterV3 # Import V3 base class\nfrom .adapters import * # Import all adapter classes\nfrom .cache_manager import cache_async_result\nfrom .config import get_settings\nfrom .models import AggregatedResponse, Race, Runner\n\nlog = structlog.get_logger(__name__)\n\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n\n        try:\n            self.config = config or get_settings()\n            self.logger.info(\"Configuration loaded.\")\n\n            self.logger.info(\"Initializing adapters...\")\n            self.adapters: List[BaseAdapterV3] = []\n            adapter_classes = [\n                AtTheRacesAdapter, BetfairAdapter, BetfairGreyhoundAdapter, BrisnetAdapter,\n                DRFAdapter, EquibaseAdapter, FanDuelAdapter, GbgbApiAdapter, GreyhoundAdapter,\n                HarnessAdapter, HorseRacingNationAdapter, NYRABetsAdapter, OddscheckerAdapter,\n                PuntersAdapter, RacingAndSportsAdapter, RacingAndSportsGreyhoundAdapter,\n                RacingPostAdapter, RacingTVAdapter, SportingLifeAdapter, TabAdapter,\n                TheRacingApiAdapter, TimeformAdapter, TwinSpiresAdapter, TVGAdapter,\n                XpressbetAdapter, PointsBetGreyhoundAdapter\n            ]\n\n            for adapter_cls in adapter_classes:\n                try:\n                    self.adapters.append(adapter_cls(config=self.config))\n                except Exception:\n                    self.logger.warning(f\"Failed to initialize adapter: {adapter_cls.__name__}\", exc_info=True)\n\n            # Special case for BetfairDataScientistAdapter with extra args\n            try:\n                self.adapters.append(BetfairDataScientistAdapter(\n                    model_name=\"ThoroughbredModel\",\n                    url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets\",\n                    config=self.config\n                ))\n            except Exception:\n                self.logger.warning(\"Failed to initialize adapter: BetfairDataScientistAdapter\", exc_info=True)\n\n            self.logger.info(f\"{len(self.adapters)} adapters initialized successfully.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n                max_connections=self.config.HTTP_POOL_CONNECTIONS,\n                max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            # Assign the shared client to each adapter\n            for adapter in self.adapters:\n                adapter.http_client = self.http_client\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(\n        self, adapter: BaseAdapterV3, date: str\n    ) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps a V3 adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n\n        try:\n            races = [race async for race in adapter.get_races(date)]\n            is_success = True\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True\n            )\n            error_message = str(e)\n\n        duration = (datetime.now() - start_time).total_seconds()\n\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races and reconciles odds from different sources.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        existing_runner.odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"fortuna_engine_races\")\n    async def get_races(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Fetches and aggregates race data from all configured adapters.\n        The result of this method is cached.\n        \"\"\"\n        target_adapters = self.adapters\n        if source_filter:\n            log.info(\"Applying source filter\", source=source_filter)\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        for result in results:\n            if isinstance(result, Exception):\n                log.error(\"Adapter fetch task failed\", error=result, exc_info=False)\n                continue\n\n            _adapter_name, adapter_result, _duration = result\n            source_info = adapter_result.get(\"source_info\", {})\n            source_infos.append(source_info)\n            if source_info.get(\"status\") == \"SUCCESS\":\n                all_races.extend(adapter_result.get(\"races\", []))\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n            },\n        )\n        return response_obj.model_dump()\n",
    "python_service/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return None, \"Missing core fields (id, venue, race_number, start_time, runners)\"\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\")\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "python_service/fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport structlog\n\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.config import get_settings\nfrom python_service.engine import FortunaEngine\nfrom python_service.etl import run_etl_for_yesterday\nfrom python_service.models import Race\n\nlog = structlog.get_logger(__name__)\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = FortunaEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n        try:\n            background_tasks = set() # Create a dummy set for background tasks\n            aggregated_data = await self.odds_engine.get_races(today_str, background_tasks)\n            all_races = aggregated_data.get('races', [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer('trifecta')\n            qualified_races = analyzer.qualify_races(all_races) # This now returns a sorted list with scores\n            log.info(\"Watchman: Initial target acquisition and ranking complete\", target_count=len(qualified_races))\n\n            # Log the top targets for better observability\n            for race in qualified_races[:5]:\n                log.info(\"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat()\n                )\n            return qualified_races\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        # active_targets = list(targets)\n\n        # from python_service.adapters.betfair_adapter import BetfairAdapter\n        # async with LiveOddsMonitor(betfair_adapter=BetfairAdapter(config=self.settings)) as live_monitor:\n        #     async with httpx.AsyncClient() as client:\n        #         while active_targets:\n        #             now = datetime.now(timezone.utc)\n\n        #             # Find races that are within the 5-minute monitoring window\n        #             races_to_monitor = [\n        #                 r\n        #                 for r in active_targets\n        #                 if r.start_time.replace(tzinfo=timezone.utc) > now\n        #                 and r.start_time.replace(tzinfo=timezone.utc)\n        #                 < now + timedelta(minutes=5)\n        #             ]\n\n        #             if races_to_monitor:\n        #                 for race in races_to_monitor:\n        #                     log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n        #                         race_id=race.id,\n        #                         venue=race.venue,\n        #                         score=race.qualification_score\n        #                     )\n        #                     updated_race = await live_monitor.monitor_race(race, client)\n        #                     log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n        #                     # Remove from target list to prevent re-monitoring\n        #                     active_targets = [t for t in active_targets if t.id != race.id]\n\n        #             if not active_targets:\n        #                 break # Exit loop if all targets are processed\n\n        #             await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        try:\n            initial_targets = await self.get_initial_targets()\n            if initial_targets:\n                await self.run_tactical_monitoring(initial_targets)\n            else:\n                log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n        finally:\n            await self.odds_engine.close()\n\n        # Run ETL for yesterday's data after all other operations are complete\n        try:\n            log.info(\"Starting daily ETL process for Scribe's Archives...\")\n            run_etl_for_yesterday()\n            log.info(\"Daily ETL process completed successfully.\")\n        except Exception:\n            log.error(\"Daily ETL process failed.\", exc_info=True)\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\nasync def main():\n    from python_service.logging_config import configure_logging\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "python_service/health.py": "# python_service/health.py\nfrom datetime import datetime\nfrom typing import Dict\nfrom typing import List\n\nimport psutil\nimport structlog\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\nlog = structlog.get_logger(__name__)\n\n\nclass HealthMonitor:\n    def __init__(self):\n        self.adapter_health: Dict[str, Dict] = {}\n        self.system_metrics: List[Dict] = []\n        self.max_metrics_history = 100\n\n    def record_adapter_response(self, adapter_name: str, success: bool, duration: float):\n        if adapter_name not in self.adapter_health:\n            self.adapter_health[adapter_name] = {\n                \"total_requests\": 0,\n                \"successful_requests\": 0,\n                \"failed_requests\": 0,\n                \"avg_response_time\": 0.0,\n                \"last_success\": None,\n                \"last_failure\": None,\n            }\n\n        health = self.adapter_health[adapter_name]\n        health[\"total_requests\"] += 1\n\n        if success:\n            health[\"successful_requests\"] += 1\n            health[\"last_success\"] = datetime.now().isoformat()\n        else:\n            health[\"failed_requests\"] += 1\n            health[\"last_failure\"] = datetime.now().isoformat()\n\n        health[\"avg_response_time\"] = (\n            health[\"avg_response_time\"] * (health[\"total_requests\"] - 1) + duration\n        ) / health[\"total_requests\"]\n\n    def get_system_metrics(self) -> Dict:\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage(\"/\")\n\n        metrics = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_available_gb\": round(memory.available / (1024**3), 2),\n            \"disk_percent\": disk.percent,\n            \"disk_free_gb\": round(disk.free / (1024**3), 2),\n        }\n\n        self.system_metrics.append(metrics)\n        if len(self.system_metrics) > self.max_metrics_history:\n            self.system_metrics.pop(0)\n\n        return metrics\n\n    def get_health_report(self) -> Dict:\n        system_metrics = self.get_system_metrics()\n        return {\n            \"status\": \"healthy\" if self.is_system_healthy() else \"degraded\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"system\": system_metrics,\n            \"adapters\": self.adapter_health,\n            \"metrics_history\": self.system_metrics[-10:],\n        }\n\n    def is_system_healthy(self) -> bool:\n        if not self.system_metrics:\n            return True\n        latest = self.system_metrics[-1]\n        return latest[\"cpu_percent\"] < 80 and latest[\"memory_percent\"] < 85 and latest[\"disk_percent\"] < 90\n\n\n# Global instance for the application to use\nhealth_monitor = HealthMonitor()\n\n\n@router.get(\"/health/detailed\", tags=[\"Health\"])\nasync def get_detailed_health():\n    \"\"\"Provides a comprehensive health check of the system.\"\"\"\n    return health_monitor.get_health_report()\n\n\n@router.get(\"/health\", tags=[\"Health\"])\nasync def get_basic_health():\n    \"\"\"Provides a basic health check for load balancers and uptime monitoring.\"\"\"\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n",
    "python_service/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex(('127.0.0.1', port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\nif __name__ == '__main__':\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "python_service/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "python_service/middleware/__init__.py": "",
    "python_service/middleware/error_handler.py": "# python_service/middleware/error_handler.py\n\nfrom fastapi import Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\n\n\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"Convert Pydantic validation errors to user-friendly messages.\"\"\"\n    return JSONResponse(\n        status_code=422,\n        content={\n            \"detail\": \"Invalid request parameters\",\n            \"errors\": [\n                {\n                    \"field\": error[\"loc\"][-1] if error[\"loc\"] else \"unknown\",\n                    \"message\": error[\"msg\"],\n                    \"type\": error[\"type\"],\n                }\n                for error in exc.errors()\n            ],\n        },\n    )\n",
    "python_service/minimal_service.py": "# python_service/minimal_service.py\n# This is the minimal, sanctioned Flask backend for Checkmate Solo.\n\nimport random\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom flask import Flask\nfrom flask import jsonify\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n# This enables CORS for all domains on all routes.\n# For a production environment, you would want to restrict this\n# to the domain of your frontend application.\nCORS(app)\n\n\ndef generate_mock_race(race_id: int):\n    \"\"\"Generates a single mock race with randomized data, mirroring the frontend's mock generator.\"\"\"\n    tracks = [\"Belmont Park\", \"Churchill Downs\", \"Santa Anita\", \"Keeneland\", \"Del Mar\"]\n    horses = [\n        \"Thunder Strike\",\n        \"Lightning Bolt\",\n        \"Swift Arrow\",\n        \"Golden Dream\",\n        \"Storm Chaser\",\n        \"Midnight Runner\",\n        \"Royal Flash\",\n        \"Desert Wind\",\n    ]\n\n    race_horses = []\n    for i in range(8):\n        betfair = round(2 + random.random() * 15, 2)\n        pointsbet = round(betfair * (0.9 + random.random() * 0.2), 2)\n        tvg = round(betfair * (0.85 + random.random() * 0.3), 2)\n\n        odds_values = {\"Betfair\": betfair, \"PointsBet\": pointsbet, \"TVG\": tvg}\n        best_source = min(odds_values, key=odds_values.get)\n        best_odds = odds_values[best_source]\n\n        avg_odds = (betfair + pointsbet + tvg) / 3\n        value_score = ((avg_odds - best_odds) / best_odds) * 100 if best_odds > 0 else 0\n\n        race_horses.append(\n            {\n                \"number\": i + 1,\n                \"name\": random.choice(horses),\n                \"odds\": {\n                    \"betfair\": f\"{betfair:.2f}\",\n                    \"pointsbet\": f\"{pointsbet:.2f}\",\n                    \"tvg\": f\"{tvg:.2f}\",\n                    \"best\": f\"{best_odds:.2f}\",\n                    \"best_source\": best_source,\n                },\n                \"value_score\": f\"{value_score:.1f}\",\n                \"trend\": random.choice([\"up\", \"down\"]),\n            }\n        )\n\n    race_horses.sort(key=lambda x: float(x[\"value_score\"]), reverse=True)\n\n    return {\n        \"id\": race_id,\n        \"track\": random.choice(tracks),\n        \"race_number\": random.randint(1, 10),\n        \"post_time\": (datetime.now() + timedelta(minutes=random.randint(5, 120))).isoformat(),\n        \"horses\": race_horses,\n    }\n\n\n@app.route(\"/api/races/live\", methods=[\"GET\"])\ndef get_live_races():\n    \"\"\"\n    This endpoint provides a list of live mock race data for the frontend.\n    It mimics the data structure the CheckmateSolo component expects.\n    \"\"\"\n    mock_races = [generate_mock_race(i) for i in range(5)]\n    return jsonify(mock_races)\n\n\nif __name__ == \"__main__\":\n    # The frontend component's TODO comment specifies port 8000.\n    print(\"Starting Checkmate Solo minimal backend service...\")\n    print(\"API available at http://localhost:8000/api/races/live\")\n    app.run(host=\"0.0.0.0\", port=8000, debug=False)\n",
    "python_service/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ConfigDict\nfrom pydantic import Field\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[Decimal] = None\n    place: Optional[Decimal] = None\n    show: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n",
    "python_service/models_v3.py": "# python_service/models_v3.py\n# Defines the data structures for the V3 adapter architecture.\n\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import List\n\n\n@dataclass\nclass NormalizedRunner:\n    runner_id: str\n    name: str\n    saddle_cloth: str\n    odds_decimal: float\n\n\n@dataclass\nclass NormalizedRace:\n    race_key: str\n    track_key: str\n    start_time_iso: str\n    race_name: str\n    runners: List[NormalizedRunner] = field(default_factory=list)\n    source_ids: List[str] = field(default_factory=list)\n",
    "python_service/notifications.py": "# python_service/notifications.py\n\nimport sys\n\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\ndef send_toast(title: str, message: str):\n    \"\"\"\n    Sends a desktop notification. This function is platform-aware and will only\n    attempt to send a toast on Windows. On other operating systems, it will\n    log the notification content.\n    \"\"\"\n    if sys.platform == \"win32\":\n        try:\n            from windows_toasts import Toast\n            from windows_toasts import WindowsToaster\n            toaster = WindowsToaster(title)\n            new_toast = Toast()\n            new_toast.text_fields = [message]\n            toaster.show_toast(new_toast)\n            log.info(\"Sent Windows toast notification.\", title=title, message=message)\n        except ImportError:\n            log.warning(\"windows_toasts library not found, skipping notification.\",\n                        recommendation=\"Install with: pip install windows-toasts\")\n        except Exception:\n            log.error(\"Failed to send Windows toast notification.\", exc_info=True)\n    else:\n        log.info(\"Skipping toast notification on non-Windows platform.\",\n                   platform=sys.platform, title=title, message=message)\n",
    "python_service/requirements.txt": "# Fortuna Faucet - Python Dependencies (Windows Optimized v2)\n# This is the 'golden' manifest, combining the blueprint's pinned versions with all feature requirements.\n\n# --- Core Backend (FastAPI & Async) ---\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\n\n# --- HTTP & Web Scraping ---\nhttpx==0.25.1\ntenacity==8.2.3\nrequests\n\n# --- HTML & Data Parsing ---\nselectolax\n\n# --- Logging & Configuration ---\nstructlog==23.2.0\npython-dotenv==1.0.0\n\n# --- Caching ---\nredis==5.0.1\nslowapi==0.1.9\n\n# --- Database & ETL ---\nSQLAlchemy\npsycopg2-binary\n\n# --- Monitoring & GUI ---\nmatplotlib==3.8.2\n\n# --- UI & Visualization (for utility scripts) ---\nrich\nstreamlit\npikepdf\ntabula-py\n\n# --- Windows Specific (DO NOT REMOVE MARKERS) ---\npsutil==5.9.6; sys_platform == 'win32'\npywin32==306; sys_platform == 'win32'\nwindows-toasts; sys_platform == 'win32'\n\n# --- Testing ---\npytest==7.4.3\npytest-asyncio==0.21.1\n\n# --- Development ---\nblack==23.11.0\ncryptography",
    "python_service/requirements_minimal.txt": "httpx==0.25.0\nstructlog==23.2.0\npydantic==2.5.0\nuvicorn==0.24.0\nfastapi==0.104.1\ntenacity==8.2.3\n",
    "python_service/run_api.py": "# python_service/run_api.py\n\nimport uvicorn\n\n\ndef main():\n    # This entry point is for the packaged application\n    uvicorn.run(\"python_service.api:app\", host=\"127.0.0.1\", port=8000, reload=False)\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/run_server.py": "# python_service/run_server.py\nimport uvicorn\nfrom config import get_settings\nfrom db.init import initialize_database\n\ndef main():\n    \"\"\"\n    Initializes the database and then starts the Uvicorn server programmatically.\n    This wrapper provides a reliable way for external processes (like Electron)\n    to launch the server and receive a clear success signal.\n    \"\"\"\n    # Initialize the database before starting the server\n    initialize_database()\n\n    settings = get_settings()\n\n    # This print statement is the crucial signal that the Electron process will wait for.\n    print(\"Backend ready\", flush=True)\n\n    uvicorn.run(\n        \"api:app\",\n        host=settings.UVICORN_HOST,\n        port=settings.UVICORN_PORT,\n        reload=settings.UVICORN_RELOAD,\n        log_level=\"info\",\n    )\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/security.py": "# python_service/security.py\n\nimport secrets\n\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Security\nfrom fastapi import status\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings\nfrom .config import get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\n\nasync def verify_api_key(key: str = Security(api_key_header), settings: Settings = Depends(get_settings)):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n    \"\"\"\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid or missing API Key\")\n",
    "python_service/setup_wizard_gui.py": "import tkinter as tk\nfrom tkinter import messagebox\n\n\nclass FortunaSetupWizard(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"\ud83d\udc34 Fortuna Faucet - First Time Setup\")\n        self.geometry(\"700x500\")\n        self.configure(bg='#1a1a2e')\n\n        # Step tracking\n        self.current_step = 0\n        self.settings = {}\n\n        self._create_widgets()\n\n    def _create_widgets(self):\n        \"\"\"Create multi-step wizard UI\"\"\"\n        # Header\n        header = tk.Label(\n            self,\n            text=\"Welcome to Fortuna Faucet\",\n            font=(\"Segoe UI\", 18, \"bold\"),\n            bg='#1a1a2e',\n            fg='#00ff88'\n        )\n        header.pack(pady=20)\n\n        # Step indicator\n        self.step_label = tk.Label(\n            self,\n            text=\"Step 1 of 4: Generate API Key\",\n            font=(\"Segoe UI\", 11),\n            bg='#1a1a2e',\n            fg='#ffffff'\n        )\n        self.step_label.pack(pady=10)\n\n        # Content frame (will be updated for each step)\n        self.content_frame = tk.Frame(self, bg='#1a1a2e')\n        self.content_frame.pack(fill=tk.BOTH, expand=True, padx=30, pady=20)\n\n        # Buttons\n        button_frame = tk.Frame(self, bg='#1a1a2e')\n        button_frame.pack(fill=tk.X, padx=30, pady=20)\n\n        self.prev_btn = tk.Button(\n            button_frame,\n            text=\"< Back\",\n            command=self.previous_step,\n            state=tk.DISABLED,\n            bg='#404060',\n            fg='#ffffff',\n            padx=20\n        )\n        self.prev_btn.pack(side=tk.LEFT)\n\n        self.next_btn = tk.Button(\n            button_frame,\n            text=\"Next >\",\n            command=self.next_step,\n            bg='#00ff88',\n            fg='#000000',\n            font=(\"Segoe UI\", 11, \"bold\"),\n            padx=20\n        )\n        self.next_btn.pack(side=tk.RIGHT)\n\n        # Show step 1\n        self.show_step_1()\n\n    def show_step_1(self):\n        \"\"\"Generate API Key\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83d\udd10 Secure API Key\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg='#1a1a2e',\n            fg='#ffffff'\n        ).pack(anchor=\"w\", pady=(0, 10))\n\n        tk.Label(\n            self.content_frame,\n            text=(\n                \"A secure API key will be generated and stored in Windows Credential Manager.\\n\"\n                \"No file will contain your secrets.\"\n            ),\n            wraplength=600,\n            justify=tk.LEFT,\n            bg='#1a1a2e',\n            fg='#cccccc'\n        ).pack(anchor=\"w\", pady=10)\n\n        self.api_key_display = tk.Entry(\n            self.content_frame,\n            font=(\"Courier\", 10),\n            width=60,\n            state=tk.DISABLED\n        )\n        self.api_key_display.pack(pady=10, fill=tk.X)\n\n        gen_btn = tk.Button(\n            self.content_frame,\n            text=\"\ud83d\udd04 Generate New Key\",\n            command=self.generate_api_key,\n            bg='#0f6cbd',\n            fg='#ffffff'\n        )\n        gen_btn.pack(pady=10)\n\n        self.current_step = 0\n        self.update_buttons()\n\n    def show_step_2(self):\n        \"\"\"Betfair Configuration\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83c\udfc7 Betfair Exchange (Optional)\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg='#1a1a2e',\n            fg='#ffffff'\n        ).pack(anchor=\"w\", pady=(0, 10))\n\n        tk.Label(\n            self.content_frame,\n            text=\"Optional: Add Betfair credentials for live odds monitoring.\\nLeave blank to skip.\",\n            bg='#1a1a2e',\n            fg='#cccccc'\n        ).pack(anchor=\"w\", pady=10)\n\n        # Betfair form\n        tk.Label(self.content_frame, text=\"App Key:\", bg='#1a1a2e', fg='#ffffff').pack(anchor=\"w\")\n        self.betfair_appkey = tk.Entry(self.content_frame, width=60, show=\"*\")\n        self.betfair_appkey.pack(fill=tk.X, pady=(0, 10))\n\n        tk.Label(self.content_frame, text=\"Username:\", bg='#1a1a2e', fg='#ffffff').pack(anchor=\"w\")\n        self.betfair_user = tk.Entry(self.content_frame, width=60)\n        self.betfair_user.pack(fill=tk.X, pady=(0, 10))\n\n        tk.Label(self.content_frame, text=\"Password:\", bg='#1a1a2e', fg='#ffffff').pack(anchor=\"w\")\n        self.betfair_pass = tk.Entry(self.content_frame, width=60, show=\"*\")\n        self.betfair_pass.pack(fill=tk.X, pady=(0, 10))\n\n        # Test connection button\n        test_btn = tk.Button(\n            self.content_frame,\n            text=\"\ud83e\uddea Test Connection\",\n            command=self.test_betfair_connection,\n            bg='#0f6cbd',\n            fg='#ffffff'\n        )\n        test_btn.pack(pady=10)\n\n        self.current_step = 1\n        self.update_buttons()\n\n    def show_step_3(self):\n        \"\"\"Verify Installation\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\u2713 Verifying Setup\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg='#1a1a2e',\n            fg='#00ff88'\n        ).pack(anchor=\"w\", pady=(0, 20))\n\n        checks = [\n            (\"Python 3.11+\", self.verify_python),\n            (\"Node.js installed\", self.verify_nodejs),\n            (\"pip packages ready\", self.verify_pip),\n            (\"npm packages ready\", self.verify_npm),\n        ]\n\n        for check_name, check_func in checks:\n            result = check_func()\n            status = \"\u2705\" if result else \"\u274c\"\n            color = \"#00ff88\" if result else \"#ff4444\"\n\n            label = tk.Label(\n                self.content_frame,\n                text=f\"{status} {check_name}\",\n                bg='#1a1a2e',\n                fg=color\n            )\n            label.pack(anchor=\"w\", pady=5)\n\n        self.current_step = 2\n        self.update_buttons()\n\n    def show_step_4(self):\n        \"\"\"Complete\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83c\udf89 Setup Complete!\",\n            font=(\"Segoe UI\", 14, \"bold\"),\n            bg='#1a1a2e',\n            fg='#00ff88'\n        ).pack(pady=20)\n\n        tk.Label(\n            self.content_frame,\n            text=\"Fortuna Faucet is ready to launch.\\n\\nClick 'Finish' to start the application.\",\n            wraplength=600,\n            bg='#1a1a2e',\n            fg='#ffffff'\n        ).pack(pady=20)\n\n        self.current_step = 3\n        self.next_btn.config(text=\"\u2713 Finish\", command=self.finish_setup)\n        self.update_buttons()\n\n    def _clear_content(self):\n        \"\"\"Clear content frame\"\"\"\n        for widget in self.content_frame.winfo_children():\n            widget.destroy()\n\n    def update_buttons(self):\n        \"\"\"Enable/disable navigation buttons\"\"\"\n        self.prev_btn.config(state=tk.NORMAL if self.current_step > 0 else tk.DISABLED)\n        step_titles = [\"API Key\", \"Betfair (Optional)\", \"Verification\", \"Complete\"]\n        self.step_label.config(\n            text=f\"Step {self.current_step + 1} of 4: {step_titles[self.current_step]}\"\n        )\n\n    def next_step(self):\n        if self.current_step == 0:\n            self.show_step_2()\n        elif self.current_step == 1:\n            self.show_step_3()\n        elif self.current_step == 2:\n            self.show_step_4()\n\n    def previous_step(self):\n        if self.current_step == 3:\n            self.show_step_3()\n        elif self.current_step == 2:\n            self.show_step_2()\n        elif self.current_step == 1:\n            self.show_step_1()\n\n    def generate_api_key(self):\n        import secrets\n        key = secrets.token_urlsafe(32)\n        self.settings['api_key'] = key\n        self.api_key_display.config(state=tk.NORMAL)\n        self.api_key_display.delete(0, tk.END)\n        self.api_key_display.insert(0, key)\n        self.api_key_display.config(state=tk.DISABLED)\n        messagebox.showinfo(\"Success\", \"API Key generated and stored securely!\")\n\n    def test_betfair_connection(self):\n        # Test credentials\n        messagebox.showinfo(\"Testing...\", \"Attempting to connect to Betfair...\")\n        # Actual implementation would test the API\n\n    def verify_python(self) -> bool:\n        # Check Python version\n        return True\n\n    def verify_nodejs(self) -> bool:\n        # Check Node.js installation\n        return True\n\n    def verify_pip(self) -> bool:\n        # Verify pip packages\n        return True\n\n    def verify_npm(self) -> bool:\n        # Verify npm packages\n        return True\n\n    def finish_setup(self):\n        # Save credentials to Windows Credential Manager\n        # Launch Fortuna\n        self.destroy()\n\nif __name__ == \"__main__\":\n    app = FortunaSetupWizard()\n    app.mainloop()\n",
    "python_service/utils/__init__.py": "",
    "python_service/utils/odds.py": "# Centralized odds parsing utility, created by Operation: The A+ Trifecta\nfrom decimal import Decimal\nfrom decimal import InvalidOperation\nfrom typing import Optional\nfrom typing import Union\n\n\ndef parse_odds_to_decimal(odds: Union[str, int, float, None]) -> Optional[Decimal]:\n    \"\"\"\n    Parse various odds formats to Decimal for precise financial calculations.\n    Handles fractional, decimal, and special cases ('EVS', 'SP', etc.).\n    Returns None for unparseable or invalid values.\n    \"\"\"\n    if odds is None:\n        return None\n\n    if isinstance(odds, (int, float)):\n        return Decimal(str(odds))\n\n    odds_str = str(odds).strip().upper()\n\n    SPECIAL_CASES = {\n        \"EVS\": Decimal(\"2.0\"),\n        \"EVENS\": Decimal(\"2.0\"),\n        \"SP\": None,\n        \"SCRATCHED\": None,\n        \"SCR\": None,\n        \"\": None,\n    }\n\n    if odds_str in SPECIAL_CASES:\n        return SPECIAL_CASES[odds_str]\n\n    if \"/\" in odds_str:\n        try:\n            parts = odds_str.split(\"/\")\n            if len(parts) != 2:\n                return None\n            num, den = map(Decimal, parts)\n            if den <= 0:\n                return None\n            return Decimal(\"1.0\") + (num / den)\n        except (ValueError, InvalidOperation):\n            return None\n\n    try:\n        return Decimal(odds_str)\n    except (ValueError, InvalidOperation):\n        return None\n",
    "python_service/utils/text.py": "# python_service/utils/text.py\n# Centralized text and name normalization utilities\nimport re\nfrom typing import Optional\n\n\ndef clean_text(text: Optional[str]) -> Optional[str]:\n    \"\"\"Strips leading/trailing whitespace and collapses internal whitespace.\"\"\"\n    if not text:\n        return None\n    return \" \".join(text.strip().split())\n\n\ndef normalize_venue_name(name: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Normalizes a UK or Irish racecourse name to a standard format.\n    Handles common abbreviations and variations.\n    \"\"\"\n    if not name:\n        return None\n\n    # Use a temporary variable for matching, but return the properly cased name\n    cleaned_name_upper = clean_text(name).upper()\n\n    VENUE_MAP = {\n        \"ASCOT\": \"Ascot\",\n        \"AYR\": \"Ayr\",\n        \"BANGOR-ON-DEE\": \"Bangor-on-Dee\",\n        \"CATTERICK BRIDGE\": \"Catterick\",\n        \"CHELMSFORD CITY\": \"Chelmsford\",\n        \"EPSOM DOWNS\": \"Epsom\",\n        \"FONTWELL\": \"Fontwell Park\",\n        \"HAYDOCK\": \"Haydock Park\",\n        \"KEMPTON\": \"Kempton Park\",\n        \"LINGFIELD\": \"Lingfield Park\",\n        \"NEWMARKET (ROWLEY)\": \"Newmarket\",\n        \"NEWMARKET (JULY)\": \"Newmarket\",\n        \"SANDOWN\": \"Sandown Park\",\n        \"STRATFORD\": \"Stratford-on-Avon\",\n        \"YARMOUTH\": \"Great Yarmouth\",\n        \"CURRAGH\": \"Curragh\",\n        \"DOWN ROYAL\": \"Down Royal\",\n    }\n\n    # Check primary map first\n    if cleaned_name_upper in VENUE_MAP:\n        return VENUE_MAP[cleaned_name_upper]\n\n    # Handle cases where the key is the desired output but needs to be mapped from a variation\n    # e.g. CHELMSFORD maps to Chelmsford\n    # Title case the cleaned name for a sensible default\n    title_cased_name = clean_text(name).title()\n    if title_cased_name in VENUE_MAP.values():\n        return title_cased_name\n\n    # Return the title-cased cleaned name as a fallback\n    return title_cased_name\n\n\ndef normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n",
    "python_service/windows_service_wrapper.py": "# windows_service_wrapper.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Add the service's directory to the Python path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom checkmate_service import CheckmateBackgroundService\n\n\nclass CheckmateWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"CheckmateV8Service\"\n    _svc_display_name_ = \"Checkmate V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.checkmate_service = CheckmateBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO, format=\"%(name)s - %(levelname)s - %(message)s\", handlers=[servicemanager.LogHandler()]\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.checkmate_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE, servicemanager.PYS_SERVICE_STARTED, (self._svc_name_, \"\")\n        )\n        self.main()\n\n    def main(self):\n        self.checkmate_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(CheckmateWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(CheckmateWindowsService)\n"
}