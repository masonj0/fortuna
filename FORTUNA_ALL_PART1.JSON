{
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.\n",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\n\n# from .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\n\n# Define the public API for the adapters package, making it easy for the\n# orchestrator to discover and use them.\n__all__ = [\n    \"GbgbApiAdapter\",\n    \"TVGAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"AtTheRacesAdapter\",\n    # \"PointsBetGreyhoundAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"SportingLifeAdapter\",\n    \"TimeformAdapter\",\n    \"HarnessAdapter\",\n    \"GreyhoundAdapter\",\n    \"TheRacingApiAdapter\",\n]\n",
    "python_service/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom ..utils.text import normalize_venue_name\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass AtTheRacesAdapter(BaseAdapter):\n    \"\"\"\n    Adapter for attheraces.com.\n    This adapter now follows the modern fetch/parse pattern.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"AtTheRaces\", base_url=\"https://www.attheraces.com\", config=config)\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Optional[List[str]]:\n        \"\"\"\n        Fetches the raw HTML for all race pages. This involves first getting the\n        racecard index, then fetching each individual race page concurrently.\n        \"\"\"\n        index_response = await self.make_request(http_client, \"GET\", \"/racecards\")\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.race-time-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(http_client, \"GET\", url_path)\n            return response.text\n\n        tasks = [fetch_single_html(link) for link in links]\n        return await asyncio.gather(*tasks)\n\n    def _parse_races(self, raw_data: List[str]) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        all_races = []\n        for html in raw_data:\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                header = soup.select_one(\"h1.heading-racecard-title\").get_text()\n                track_name_raw, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n                track_name = normalize_venue_name(track_name_raw)\n                active_link = soup.select_one(\"a.race-time-link.active\")\n                race_number = (\n                    active_link.find_parent(\"div\", \"races\").select(\"a.race-time-link\").index(active_link) + 1\n                )\n                start_time = datetime.strptime(f\"{datetime.now().date()} {race_time}\", \"%Y-%m-%d %H:%M\")\n                runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n                race = Race(\n                    id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, IndexError, ValueError) as e:\n                self.logger.error(\"Error parsing race from AtTheRaces\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = clean_text(row.select_one(\"h3.horse-name a\").get_text())\n            num_str = clean_text(row.select_one(\"span.horse-number\").get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n            odds_str = clean_text(row.select_one(\"button.best-odds\").get_text())\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())}\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            # If a runner can't be parsed, log it but don't fail the whole race\n            log.warning(\"Failed to parse a runner on AtTheRaces, skipping runner.\")\n            return None\n",
    "python_service/adapters/base.py": "# python_service/adapters/base.py\nimport threading\nimport time\n\nimport httpx\nimport structlog\nfrom tenacity import AsyncRetrying\nfrom tenacity import stop_after_attempt\nfrom tenacity import wait_exponential\nfrom typing import Any, List\nfrom ..models import Race\nfrom ..core.exceptions import AdapterAuthError\nfrom ..core.exceptions import AdapterConnectionError\nfrom ..core.exceptions import AdapterHttpError\nfrom ..core.exceptions import AdapterRateLimitError\nfrom ..core.exceptions import AdapterRequestError\nfrom ..core.exceptions import AdapterTimeoutError\n\n\nclass BaseAdapter:\n    \"\"\"\n    The consolidated, unified base class for all data adapters.\n\n    This class provides a standard interface and robust error handling for all adapters.\n    It includes:\n    - Tenacity-based request retries with exponential backoff.\n    - A thread-safe circuit breaker to prevent hammering failing services.\n    - Standardized error handling that raises specific custom exceptions.\n    \"\"\"\n\n    def __init__(self, source_name: str, base_url: str = \"\", config: dict = None):\n        self.source_name = source_name\n        self.base_url = base_url\n        self.config = config or {}\n        self.logger = structlog.get_logger(self.__class__.__name__)\n        self._breaker_lock = threading.Lock()\n        self.retryer = AsyncRetrying(\n            stop=stop_after_attempt(3),\n            wait=wait_exponential(multiplier=1, min=2, max=10)\n        )\n        # Circuit Breaker State\n        self.circuit_breaker_tripped = False\n        self.circuit_breaker_failure_count = 0\n        self.circuit_breaker_last_failure = 0\n        self.FAILURE_THRESHOLD = 3\n        self.COOLDOWN_PERIOD_SECONDS = 300  # 5 minutes\n\n    # --- New Modern Pattern ---\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Any:\n        \"\"\"\n        (Modern Pattern) Fetches the raw data for a given date from the source.\n        This method should only contain network-related logic.\n        \"\"\"\n        raise NotImplementedError\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"\n        (Modern Pattern) Parses the raw data into a list of Race objects.\n        This method should be a pure function with no side effects.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"\n        (Modern Pattern) Orchestrates the fetching and parsing of race data.\n        This method implements the circuit breaker and handles the pipeline.\n        Subclasses should NOT override this method.\n        \"\"\"\n        with self._breaker_lock:\n            if self.circuit_breaker_tripped:\n                if time.time() - self.circuit_breaker_last_failure > self.COOLDOWN_PERIOD_SECONDS:\n                    self.logger.info(\"Circuit breaker cooldown expired. Attempting to reset.\")\n                    self.circuit_breaker_tripped = False\n                    self.circuit_breaker_failure_count = 0\n                else:\n                    self.logger.warning(\"Circuit breaker is tripped. Skipping fetch.\")\n                    return []\n\n        try:\n            raw_data = await self._fetch_data(http_client, date)\n            if raw_data is None:\n                self.logger.warning(\"No data returned from fetch.\", adapter=self.source_name)\n                return []\n\n            parsed_races = self._parse_races(raw_data)\n\n            with self._breaker_lock:\n                if self.circuit_breaker_failure_count > 0:\n                    self.logger.info(\"Request successful. Resetting circuit breaker failure count.\")\n                    self.circuit_breaker_failure_count = 0\n\n            return parsed_races\n        except Exception as e:\n            self.logger.error(\n                \"Pipeline error in adapter.\",\n                adapter=self.source_name,\n                error=str(e),\n                exc_info=True\n            )\n            self._handle_failure()\n            return []\n\n    # --- Engine Interface ---\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"\n        Primary entry point called by the FortunaEngine.\n        Legacy adapters should override this method directly.\n        Modern adapters that implement _fetch_data and _parse_races can\n        rely on this default implementation.\n        \"\"\"\n        return await self.get_races(date, http_client)\n\n    async def make_request(\n        self, http_client: httpx.AsyncClient, method: str, url: str, **kwargs\n    ) -> httpx.Response:\n        \"\"\"\n        Makes an HTTP request with retries, circuit breaker logic, and standardized error handling.\n\n        Args:\n            http_client: An httpx.AsyncClient instance.\n            method: The HTTP method (e.g., \"GET\", \"POST\").\n            url: The URL endpoint.\n            **kwargs: Additional arguments for httpx.request.\n\n        Returns:\n            An httpx.Response object on success.\n\n        Raises:\n            AdapterHttpError: For 4xx/5xx responses.\n            AdapterTimeoutError: For request timeouts.\n            AdapterConnectionError: For connection-level errors.\n            AdapterRequestError: For other httpx request errors or a tripped circuit breaker.\n        \"\"\"\n        with self._breaker_lock:\n            if self.circuit_breaker_tripped:\n                if time.time() - self.circuit_breaker_last_failure > self.COOLDOWN_PERIOD_SECONDS:\n                    self.logger.info(\"Circuit breaker cooldown expired. Attempting to reset.\")\n                    self.circuit_breaker_tripped = False\n                    self.circuit_breaker_failure_count = 0\n                else:\n                    self.logger.warning(\"Circuit breaker is tripped. Skipping request.\")\n                    raise AdapterRequestError(self.source_name, \"Circuit breaker is tripped\")\n\n        full_url = url if url.startswith('http') else f\"{self.base_url}{url}\"\n\n        async def _make_request():\n            response = await http_client.request(method, full_url, **kwargs)\n            response.raise_for_status()\n            return response\n\n        try:\n            async for attempt in self.retryer:\n                with attempt:\n                    response = await _make_request()\n                    # On success, reset the circuit breaker failure count\n                    with self._breaker_lock:\n                        if self.circuit_breaker_failure_count > 0:\n                            self.logger.info(\"Request successful. Resetting circuit breaker failure count.\")\n                            self.circuit_breaker_failure_count = 0\n                    return response\n        except httpx.TimeoutException as e:\n            self._handle_failure()\n            self.logger.error(\"request_timeout\", url=full_url, error=str(e))\n            raise AdapterTimeoutError(self.source_name, f\"Request timed out for {full_url}\") from e\n        except httpx.ConnectError as e:\n            self._handle_failure()\n            self.logger.error(\"request_connection_error\", url=full_url, error=str(e))\n            raise AdapterConnectionError(self.source_name, f\"Connection failed for {full_url}\") from e\n        except httpx.HTTPStatusError as e:\n            self._handle_failure()\n            status = e.response.status_code\n            self.logger.error(\"http_status_error\", status_code=status, url=full_url)\n            if status in [401, 403]:\n                raise AdapterAuthError(self.source_name, status, full_url) from e\n            if status == 429:\n                raise AdapterRateLimitError(self.source_name, status, full_url) from e\n            raise AdapterHttpError(self.source_name, status, full_url) from e\n        except httpx.RequestError as e:\n            self._handle_failure()\n            self.logger.error(\"request_error\", url=full_url, error=str(e))\n            raise AdapterRequestError(self.source_name, f\"An unexpected request error occurred for {full_url}\") from e\n\n    def _handle_failure(self):\n        \"\"\"Manages the circuit breaker state on any request failure.\"\"\"\n        with self._breaker_lock:\n            self.circuit_breaker_failure_count += 1\n            self.circuit_breaker_last_failure = time.time()\n            if self.circuit_breaker_failure_count >= self.FAILURE_THRESHOLD:\n                if not self.circuit_breaker_tripped:\n                    self.circuit_breaker_tripped = True\n                    self.logger.critical(\"Circuit breaker tripped due to repeated failures.\")\n",
    "python_service/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\nfrom datetime import datetime\nfrom typing import List\n\nimport httpx\n\nfrom ..core.exceptions import AdapterAuthError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairAdapter(BetfairAuthMixin, BaseAdapter):\n    \"\"\"Adapter for fetching horse racing data from the Betfair Exchange API.\"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(\n            source_name=\"BetfairExchange\",\n            base_url=\"https://api.betfair.com/exchange/betting/rest/v1.0/\",\n            config=config,\n        )\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches the raw market catalogue for a given date and parses it.\"\"\"\n        await self._authenticate(http_client)\n        if not self.session_token:\n            raise AdapterAuthError(self.source_name, \"Authentication failed, cannot fetch data.\")\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            http_client=http_client,\n            method=\"post\",\n            url=\"listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"7\"],  # Horse Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\", \"US\", \"FR\", \"ZA\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\"from\": start_time.isoformat(), \"to\": end_time.isoformat()}\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"]\n            }\n        )\n        raw_data = response.json()\n\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                races.append(self._parse_race(market))\n            except (KeyError, TypeError) as e:\n                self.logger.warning(\"Failed to parse a Betfair market.\", exc_info=True, market=market)\n                raise AdapterParsingError(self.source_name, f\"Failed to parse market: {market.get('marketId')}\") from e\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market['marketId']\n        event = market['event']\n        start_time = datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00'))\n\n        runners = [\n            Runner(\n                number=runner.get('sortPriority', i + 1),\n                name=runner['runnerName'],\n                scratched=runner['status'] != 'ACTIVE',\n                selection_id=runner['selectionId']\n            )\n            for i, runner in enumerate(market.get('runners', []))\n        ]\n\n        return Race(\n            id=f\"bf_{market_id}\",\n            venue=event.get('venue', 'Unknown Venue'),\n            race_number=self._extract_race_number(market.get('marketName', '')),\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 1m Mdn Stks').\"\"\"\n        import re\n        match = re.search(r'\\bR(\\d{1,2})\\b', name)\n        return int(match.group(1)) if match else 0\n",
    "python_service/adapters/betfair_auth_mixin.py": "# python_service/adapters/betfair_auth_mixin.py\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\n\nclass BetfairAuthMixin:\n    \"\"\"Encapsulates Betfair authentication logic for reuse across adapters.\"\"\"\n\n    session_token: Optional[str] = None\n    token_expiry: Optional[datetime] = None\n    _auth_lock = asyncio.Lock()\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        async with self._auth_lock:\n            # Re-check token after acquiring lock\n            if self.session_token and self.token_expiry and self.token_expiry > (\n                datetime.now() + timedelta(minutes=5)\n            ):\n                return\n\n            if not all(\n                [self.app_key, self.config.BETFAIR_USERNAME, self.config.BETFAIR_PASSWORD]\n            ):\n                raise ValueError(\"Betfair credentials not fully configured.\")\n\n            auth_url = \"https://identitysso.betfair.com/api/login\"\n            headers = {\n                \"X-Application\": self.app_key,\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n            }\n            payload = f\"username={self.config.BETFAIR_USERNAME}&password={self.config.BETFAIR_PASSWORD}\"\n\n            log.info(f\"{self.__class__.__name__}: Authenticating...\")\n            response = await http_client.post(\n                auth_url, headers=headers, content=payload, timeout=20\n            )\n            response.raise_for_status()\n            data = response.json()\n            if data.get(\"status\") == \"SUCCESS\":\n                self.session_token = data.get(\"token\")\n                self.token_expiry = datetime.now() + timedelta(hours=3)\n            else:\n                raise ConnectionError(\n                    f\"Betfair authentication failed: {data.get('error')}\"\n                )\n",
    "python_service/adapters/betfair_datascientist_adapter.py": "# python_service/adapters/betfair_datascientist_adapter.py\n\nfrom datetime import datetime\nfrom io import StringIO\nfrom typing import List\n\nimport httpx\nimport pandas as pd\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.text import normalize_course_name\nfrom .base import BaseAdapter\n\n\nclass BetfairDataScientistAdapter(BaseAdapter):\n    \"\"\"\n    Adapter for the Betfair Data Scientist CSV models.\n    This adapter is instantiated dynamically by the engine for each configured model.\n    \"\"\"\n    ADAPTER_NAME = \"BetfairDataScientist\"\n\n    def __init__(self, model_name: str, url: str, config=None):\n        source_name = f\"{self.ADAPTER_NAME}_{model_name}\"\n        super().__init__(source_name=source_name, base_url=url, config=config)\n        self.model_name = model_name\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches and parses CSV data from the Betfair Data Scientist model endpoint.\"\"\"\n        endpoint = self._build_endpoint(date)\n        self.logger.info(f\"Fetching data from {self.base_url}{endpoint}\")\n\n        response = await self.make_request(http_client, \"GET\", endpoint)\n\n        try:\n            raw_data = StringIO(response.text)\n            df = pd.read_csv(raw_data)\n            df = df.rename(\n                columns={\n                    \"meetings.races.bfExchangeMarketId\": \"market_id\",\n                    \"meetings.races.runners.bfExchangeSelectionId\": \"selection_id\",\n                    \"meetings.races.runners.ratedPrice\": \"rated_price\",\n                    \"meetings.races.raceName\": \"race_name\",\n                    \"meetings.name\": \"meeting_name\",\n                    \"meetings.races.raceNumber\": \"race_number\",\n                    \"meetings.races.runners.runnerName\": \"runner_name\",\n                    \"meetings.races.runners.clothNumber\": \"saddle_cloth\",\n                }\n            )\n            races = []\n            for market_id, group in df.groupby(\"market_id\"):\n                race_info = group.iloc[0]\n                runners = [\n                    Runner(\n                        name=str(row.get(\"runner_name\")),\n                        number=int(row.get(\"saddle_cloth\", 0)),\n                        odds=float(row.get(\"rated_price\", 0.0)),\n                    )\n                    for _, row in group.iterrows()\n                ]\n                race = Race(\n                    id=str(market_id),\n                    venue=normalize_course_name(str(race_info.get(\"meeting_name\", \"\"))),\n                    race_number=int(race_info.get(\"race_number\", 0)),\n                    # Note: The CSV does not provide a start time, using current time as a placeholder.\n                    start_time=datetime.now(),\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n            self.logger.info(f\"Normalized {len(races)} races from {self.model_name}.\")\n            return races\n        except (pd.errors.ParserError, KeyError) as e:\n            self.logger.error(\"Failed to parse Betfair Data Scientist CSV.\", error=str(e))\n            raise AdapterParsingError(self.source_name, \"Failed to parse CSV response.\") from e\n\n    def _build_endpoint(self, date: str) -> str:\n        \"\"\"Constructs the query parameters for the CSV endpoint.\"\"\"\n        return f\"?date={date}&presenter=RatingsPresenter&csv=true\"\n",
    "python_service/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime\nfrom typing import List\n\nimport httpx\n\nfrom ..core.exceptions import AdapterAuthError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapter):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API.\"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(\n            source_name=\"BetfairGreyhound\",\n            base_url=\"https://api.betfair.com/exchange/betting/rest/v1.0/\",\n            config=config,\n        )\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        await self._authenticate(http_client)\n        if not self.session_token:\n            raise AdapterAuthError(self.source_name, \"Authentication failed, cannot fetch data.\")\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            http_client=http_client,\n            method=\"post\",\n            url=\"listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\"from\": start_time.isoformat(), \"to\": end_time.isoformat()}\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"]\n            }\n        )\n        raw_data = response.json()\n\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                races.append(self._parse_race(market))\n            except (KeyError, TypeError) as e:\n                self.logger.warning(\"Failed to parse a Betfair Greyhound market.\", exc_info=True, market=market)\n                raise AdapterParsingError(self.source_name, f\"Failed to parse market: {market.get('marketId')}\") from e\n        return races\n\n    def _parse_race(self, market: dict) -> Race:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market['marketId']\n        event = market['event']\n        start_time = datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00'))\n\n        runners = [\n            Runner(\n                number=runner.get('sortPriority', i + 1),\n                name=runner['runnerName'],\n                scratched=runner['status'] != 'ACTIVE',\n                selection_id=runner['selectionId']\n            )\n            for i, runner in enumerate(market.get('runners', []))\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get('venue', 'Unknown Venue'),\n            race_number=self._extract_race_number(market.get('marketName', '')),\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r'\\bR(\\d{1,2})\\b', name)\n        return int(match.group(1)) if match else 0\n",
    "python_service/adapters/brisnet_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\n\nimport httpx\nimport structlog\n\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass BrisnetAdapter(BaseAdapter):\n    \"\"\"Adapter for brisnet.com.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"Brisnet\", base_url=\"https://www.brisnet.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"BrisnetAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time, is_success=True, error_message=\"Not Implemented\")\n",
    "python_service/adapters/drf_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\n\nimport httpx\nimport structlog\n\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass DRFAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping data from drf.com.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"DRF\", base_url=\"https://www.drf.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"DRFAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time, is_success=True, error_message=\"Not Implemented\")\n",
    "python_service/adapters/equibase_adapter.py": "# python_service/adapters/equibase_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom selectolax.parser import HTMLParser\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base import BaseAdapter\n\n\nclass EquibaseAdapter(BaseAdapter):\n    \"\"\"A production-ready adapter for scraping Equibase race entries.\"\"\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=\"Equibase\", base_url=\"https://www.equibase.com\", config=config)\n\n    async def fetch_races(self, date_str: str, http_client) -> List[Race]:\n        \"\"\"\n        Fetches all US & Canadian races for a given date from equibase.com.\n        \"\"\"\n        entry_urls = await self._get_entry_urls(date_str, http_client)\n\n        tasks = [self._fetch_races_from_entry_page(url, date_str, http_client) for url in entry_urls]\n        results = await asyncio.gather(*tasks)\n\n        # Flatten the list of lists into a single list of races\n        return [race for sublist in results for race in sublist]\n\n    async def _get_entry_urls(self, date_str: str, http_client) -> list[str]:\n        \"\"\"Gets all individual track entry page URLs for a given date.\"\"\"\n        d = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n        url = f\"/entries/Entries.cfm?ELEC_DATE={d.month}/{d.day}/{d.year}&STYLE=EQB\"\n        response = await self.make_request(http_client, \"GET\", url, headers=self._get_headers())\n        parser = HTMLParser(response.text)\n        links = parser.css(\"div.track-information a\")\n        return [\n            f\"{self.base_url}{link.attributes['href']}\"\n            for link in links\n            if \"race=\" not in link.attributes.get(\"href\", \"\")\n        ]\n\n    async def _fetch_races_from_entry_page(self, url: str, date_str: str, http_client) -> List[Race]:\n        \"\"\"Fetches all race data from a single track's entry page.\"\"\"\n        response = await self.make_request(http_client, \"GET\", url, headers=self._get_headers())\n        parser = HTMLParser(response.text)\n        race_links = parser.css(\"a.program-race-link\")\n\n        tasks = [\n            self._parse_race_page(\n                f\"{self.base_url}{link.attributes['href']}\", date_str, http_client\n            )\n            for link in race_links\n        ]\n        return [race for race in await asyncio.gather(*tasks) if race]\n\n\n    async def _parse_race_page(self, url: str, date_str: str, http_client) -> Optional[Race]:\n        \"\"\"Parses a single race card page.\"\"\"\n        try:\n            response = await self.make_request(http_client, \"GET\", url, headers=self._get_headers())\n            parser = HTMLParser(response.text)\n\n            venue = clean_text(parser.css_first(\"div.track-information strong\").text())\n            race_number = int(parser.css_first(\"div.race-information strong\").text().replace(\"Race\", \"\").strip())\n            post_time_str = parser.css_first(\"p.post-time span\").text().strip()\n            start_time = self._parse_post_time(date_str, post_time_str)\n\n            runners = []\n            runner_nodes = parser.css(\"table.entries-table tbody tr\")\n            for node in runner_nodes:\n                try:\n                    number = int(node.css_first(\"td:nth-child(1)\").text(strip=True))\n                    name = clean_text(node.css_first(\"td:nth-child(3)\").text())\n                    odds_str = clean_text(node.css_first(\"td:nth-child(10)\").text())\n                    scratched = \"scratched\" in node.attributes.get(\"class\", \"\").lower()\n\n                    odds = {}\n                    if not scratched:\n                        win_odds = parse_odds_to_decimal(odds_str)\n                        if win_odds and win_odds < 999:\n                            odds = {\n                                self.source_name: OddsData(\n                                    win=win_odds, source=self.source_name, last_updated=datetime.now()\n                                )\n                            }\n\n                    runners.append(Runner(number=number, name=name, odds=odds, scratched=scratched))\n                except (ValueError, AttributeError):\n                    # Log and skip the problematic runner\n                    self.logger.warning(\"Could not parse runner, skipping.\", url=url)\n                    continue\n\n            return Race(\n                id=f\"eqb_{venue.lower().replace(' ', '')}_{date_str}_{race_number}\",\n                venue=venue,\n                race_number=race_number,\n                start_time=start_time,\n                runners=runners,\n                source=self.source_name,\n            )\n        except (AttributeError, ValueError) as e:\n            raise AdapterParsingError(self.source_name, f\"Failed to parse race page at {url}\") from e\n\n    def _parse_post_time(self, date_str: str, time_str: str) -> datetime:\n        \"\"\"Parses a time string like 'Post Time: 12:30 PM ET' into a datetime object.\"\"\"\n        time_part = time_str.split(\" \")[-2] + \" \" + time_str.split(\" \")[-1]\n        dt_str = f\"{date_str} {time_part}\"\n        return datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/fanduel_adapter.py": "# python_service/adapters/fanduel_adapter.py\n\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger()\n\n\nclass FanDuelAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching horse racing odds from FanDuel's private API.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"FanDuel\", base_url=\"https://sb-api.nj.sportsbook.fanduel.com/api/\", config=config)\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches races for a given date. Note: FanDuel API is event-based, not date-based.\"\"\"\n        # This is a placeholder for a more robust event discovery mechanism.\n        event_id = \"38183.3\"  # Example: A major race event\n\n        log.info(\"Fetching races from FanDuel\", event_id=event_id)\n\n        endpoint = f\"markets?_ak=Fh2e68s832c41d4b&eventId={event_id}\"\n        response = await self.make_request(http_client, \"GET\", endpoint)\n        data = response.json()\n\n        return self._parse_races(data)\n\n    def _parse_races(self, data: Dict[str, Any]) -> List[Race]:\n        races = []\n        if \"marketGroups\" not in data:\n            log.warning(\"FanDuel response missing 'marketGroups' key\")\n            return []\n\n        for group in data[\"marketGroups\"]:\n            if group.get(\"marketGroupName\") == \"Win\":\n                for market in group.get(\"markets\", []):\n                    try:\n                        race = self._parse_single_race(market)\n                        if race:\n                            races.append(race)\n                    except AdapterParsingError as e:\n                        log.error(\"Failed to parse a FanDuel market\", market=market, error=str(e))\n        return races\n\n    def _parse_single_race(self, market: Dict[str, Any]) -> Optional[Race]:\n        market_name = market.get(\"marketName\", \"\")\n        if not market_name.startswith(\"Race\"):\n            return None\n\n        parts = market_name.split(\" - \")\n        if len(parts) < 2:\n            raise AdapterParsingError(\n                self.source_name,\n                f\"Could not parse race and track from market name: {market_name}\",\n            )\n\n        race_number_str = parts[0].replace(\"Race \", \"\")\n        track_name = parts[1]\n\n        # Placeholder for start_time - FanDuel's market API doesn't provide it directly\n        start_time = datetime.now(timezone.utc) + timedelta(hours=int(race_number_str))\n\n        runners = []\n        for runner_data in market.get(\"runners\", []):\n            runner_name = runner_data.get(\"runnerName\")\n            win_odds = runner_data.get(\"winRunnerOdds\", {}).get(\"currentPrice\")\n            if not runner_name or not win_odds:\n                continue\n\n            try:\n                numerator, denominator = map(int, win_odds.split(\"/\"))\n                decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n            except (ValueError, ZeroDivisionError):\n                log.warning(\"Could not parse FanDuel odds\", odds_str=win_odds, runner=runner_name)\n                continue\n\n            odds = OddsData(win=decimal_odds, source=self.source_name, last_updated=datetime.now(timezone.utc))\n\n            program_number_str = runner_name.split(\".\")[0].strip()\n\n            runners.append(Runner(\n                name=runner_name.split(\".\")[1].strip(),\n                number=int(program_number_str) if program_number_str.isdigit() else None,\n                odds={self.source_name: odds},\n            ))\n\n        if not runners:\n            return None\n\n        race_id = f\"FD-{track_name.replace(' ', '')[:5].upper()}-{start_time.strftime('%Y%m%d')}-R{race_number_str}\"\n\n        return Race(\n            id=race_id,\n            venue=track_name,\n            race_number=int(race_number_str),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/gbgb_api_adapter.py": "# python_service/adapters/gbgb_api_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\nimport httpx\nimport structlog\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass GbgbApiAdapter(BaseAdapter):\n    \"\"\"\n    Adapter for the undocumented JSON API for the Greyhound Board of Great Britain.\n    This adapter now follows the modern fetch/parse pattern.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"GBGB\", base_url=\"https://api.gbgb.org.uk/api/\", config=config)\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches the raw meeting data from the GBGB API.\"\"\"\n        endpoint = f\"results/meeting/{date}\"\n        response = await self.make_request(http_client, \"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, meetings_data: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses the raw meeting data into a list of Race objects.\"\"\"\n        if not meetings_data:\n            return []\n\n        all_races = []\n        for meeting in meetings_data:\n            track_name = meeting.get(\"trackName\")\n            for race_data in meeting.get(\"races\", []):\n                try:\n                    all_races.append(self._parse_race(race_data, track_name))\n                except (KeyError, TypeError) as e:\n                    log.error(\n                        f\"{self.source_name}: Error parsing race\",\n                        race_id=race_data.get(\"raceId\"),\n                        error=str(e),\n                        exc_info=True,\n                    )\n                    # Propagate the error to be handled by the base class orchestrator\n                    raise AdapterParsingError(\n                        self.source_name,\n                        f\"Failed to parse race: {race_data.get('raceId')}\",\n                    ) from e\n        return all_races\n\n    def _parse_race(self, race_data: Dict[str, Any], track_name: str) -> Race:\n        return Race(\n            id=f\"gbgb_{race_data['raceId']}\",\n            venue=track_name,\n            race_number=race_data[\"raceNumber\"],\n            start_time=datetime.fromisoformat(race_data[\"raceTime\"].replace(\"Z\", \"+00:00\")),\n            runners=self._parse_runners(race_data.get(\"traps\", [])),\n            source=self.source_name,\n            race_name=race_data.get(\"raceTitle\"),\n            distance=f\"{race_data.get('raceDistance')}m\",\n        )\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for runner_data in runners_data:\n            try:\n                odds_data = {}\n                sp = runner_data.get(\"sp\")\n                win_odds = parse_odds_to_decimal(sp)\n                if win_odds and win_odds < 999:\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds, source=self.source_name, last_updated=datetime.now()\n                    )\n\n                runners.append(\n                    Runner(\n                        number=runner_data[\"trapNumber\"],\n                        name=runner_data[\"dogName\"],\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, TypeError):\n                log.warning(f\"{self.source_name}: Error parsing runner\", runner_name=runner_data.get(\"dogName\"))\n                # Skip runner, but don't fail the whole race\n                continue\n        return runners\n",
    "python_service/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass GreyhoundAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching Greyhound racing data. Activated by setting GREYHOUND_API_URL in .env\"\"\"\n\n    def __init__(self, config):\n        if not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.source_name, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(source_name=\"Greyhound Racing\", base_url=config.GREYHOUND_API_URL, config=config)\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches upcoming greyhound races for the specified date.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(http_client, \"GET\", endpoint)\n\n        response_json = response.json()\n        if not response_json or not response_json.get(\"cards\"):\n            log.warning(\"GreyhoundAdapter: No 'cards' in response or empty list.\")\n            return []\n\n        return self._parse_cards(response_json[\"cards\"])\n\n    def _parse_cards(self, cards: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses a list of cards and their races into Race objects.\"\"\"\n        all_races = []\n        if cards is None:\n            return all_races\n        for card in cards:\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            races_data = card.get(\"races\", [])\n            for race_data in races_data:\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_data['race_id']}\",\n                        venue=venue,\n                        race_number=race_data[\"race_number\"],\n                        start_time=datetime.fromtimestamp(race_data[\"start_time\"]),\n                        runners=self._parse_runners(race_data[\"runners\"]),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    log.error(\n                        f\"GreyhoundAdapter: Error parsing race {race_data.get('race_id', 'N/A')}\",\n                        error=str(e),\n                        race_data=race_data,\n                    )\n                    raise AdapterParsingError(\n                        self.source_name,\n                        f\"Failed to parse race: {race_data.get('race_id')}\",\n                    ) from e\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds, source=self.source_name, last_updated=datetime.now()\n                        )\n\n                runners.append(Runner(\n                    number=runner_data[\"trap_number\"],\n                    name=runner_data[\"dog_name\"],\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds_data,\n                ))\n            except (KeyError, ValidationError):\n                log.warning(\"GreyhoundAdapter: Error parsing runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "python_service/adapters/harness_adapter.py": "# python_service/adapters/harness_adapter.py\nfrom datetime import datetime\nfrom typing import AsyncGenerator\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base import BaseAdapter\n\n\nclass HarnessAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching US harness racing data from data.ustrotting.com.\"\"\"\n\n    SOURCE_NAME = \"USTrotting\"\n    BASE_URL = \"https://data.ustrotting.com/api/racenet/racing/\"\n\n    def __init__(self, config=None):\n        super().__init__(self.SOURCE_NAME, self.BASE_URL)\n\n    async def fetch_races(self, date: str) -> AsyncGenerator[Race, None]:\n        \"\"\"Fetches all harness races for a given date.\"\"\"\n        card_data = await self.make_request(method=\"get\", url=f\"{self.BASE_URL}card/{date}\")\n        if not card_data or not card_data.get(\"meetings\"):\n            return\n\n        for meeting in card_data[\"meetings\"]:\n            track_name = meeting.get(\"track\", {}).get(\"name\")\n            for race_data in meeting.get(\"races\", []):\n                yield self._parse_race(race_data, track_name, date)\n\n    def _parse_race(self, race_data: dict, track_name: str, date: str) -> Race:\n        \"\"\"Parses a single race from the USTA API into a Race object.\"\"\"\n        race_number = race_data.get(\"raceNumber\", 0)\n        post_time_str = race_data.get(\"postTime\", \"00:00 AM\")\n        start_time = self._parse_post_time(date, post_time_str)\n\n        runners = []\n        for runner_data in race_data.get(\"runners\", []):\n            odds_str = runner_data.get(\"morningLineOdds\", \"\")\n            # Ensure odds are fractional for parsing\n            if \"/\" not in odds_str and odds_str.isdigit():\n                odds_str = f\"{odds_str}/1\"\n\n            odds = {}\n            win_odds = parse_odds_to_decimal(odds_str)\n            if win_odds and win_odds < 999:\n                odds = {self.SOURCE_NAME: OddsData(win=win_odds, source=self.SOURCE_NAME, last_updated=datetime.now())}\n\n            runners.append(\n                Runner(\n                    number=runner_data.get(\"postPosition\", 0),\n                    name=runner_data.get(\"horse\", {}).get(\"name\", \"Unknown Horse\"),\n                    odds=odds,\n                    scratched=runner_data.get(\"scratched\", False),\n                )\n            )\n\n        return Race(\n            id=f\"ust_{track_name.lower().replace(' ', '')}_{date}_{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.SOURCE_NAME,\n        )\n\n    def _parse_post_time(self, date: str, post_time: str) -> datetime:\n        \"\"\"Parses a time string like '07:00 PM' into a timezone-aware datetime object.\"\"\"\n        from zoneinfo import ZoneInfo\n\n        dt_str = f\"{date} {post_time}\"\n        naive_dt = datetime.strptime(dt_str, \"%Y-%m-%d %I:%M %p\")\n        # Assume Eastern Time for USTA data, a common standard for US racing.\n        eastern = ZoneInfo(\"America/New_York\")\n        return naive_dt.replace(tzinfo=eastern)\n",
    "python_service/adapters/horseracingnation_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..models import Race\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass HorseRacingNationAdapter(BaseAdapter):\n    \"\"\"Adapter for horseracingnation.com.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"HorseRacingNation\", base_url=\"https://www.horseracingnation.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"HorseRacingNationAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {\n            \"races\": [],\n            \"source_info\": {\n                \"name\": self.source_name,\n                \"status\": \"SUCCESS\",\n                \"races_fetched\": 0,\n                \"error_message\": \"Not Implemented\",\n                \"fetch_duration\": (datetime.now() - start_time).total_seconds(),\n            },\n        }\n",
    "python_service/adapters/nyrabets_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\n\nimport httpx\nimport structlog\n\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass NYRABetsAdapter(BaseAdapter):\n    \"\"\"Adapter for nyrabets.com.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"NYRABets\", base_url=\"https://nyrabets.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"NYRABetsAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time, is_success=True, error_message=\"Not Implemented\")\n",
    "python_service/adapters/oddschecker_adapter.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Oddschecker Adapter (Canonized)\n# ==============================================================================\n# This adapter was sourced from the 'Live Odds Anthology' and has been modernized\n# to conform to the project's current BaseAdapter framework.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\n\nclass OddscheckerAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping live horse racing odds from Oddschecker.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"Oddschecker\", base_url=\"https://www.oddschecker.com\", config=config)\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        meeting_links = await self._get_all_meeting_links(http_client)\n        if not meeting_links:\n            return []\n\n        tasks = [self._fetch_single_meeting(link, http_client) for link in meeting_links]\n        races_from_all_meetings = await asyncio.gather(*tasks)\n\n        return [race for sublist in races_from_all_meetings for race in sublist if race]\n\n    async def _get_all_meeting_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        response = await self.make_request(http_client, \"GET\", \"/horse-racing\")\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        links = {self.base_url + a[\"href\"] for a in soup.select(\"a.meeting-title[href]\")}\n        return sorted(list(links))\n\n    async def _fetch_single_meeting(self, url: str, client: httpx.AsyncClient) -> List[Optional[Race]]:\n        response = await self.make_request(client, \"GET\", url.replace(self.base_url, \"\"))\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        race_links = {self.base_url + a[\"href\"] for a in soup.select(\"a.race-time-link[href]\")}\n        tasks = [self._fetch_and_parse_race_card(link, client) for link in race_links]\n        return await asyncio.gather(*tasks)\n\n    async def _fetch_and_parse_race_card(self, url: str, client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            response = await self.make_request(client, \"GET\", url.replace(self.base_url, \"\"))\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            return self._parse_race_page(soup, url)\n        except (AttributeError, IndexError, ValueError) as e:\n            log.error(\"Oddschecker failed to parse race card\", url=url, error=e)\n            raise AdapterParsingError(self.source_name, f\"Failed to parse race card at {url}\") from e\n\n    def _parse_race_page(self, soup: BeautifulSoup, url: str) -> Optional[Race]:\n        track_name = soup.select_one(\"h1.meeting-name\").get_text(strip=True)\n        race_time_str = soup.select_one(\"span.race-time\").get_text(strip=True)\n        race_number = int(url.split(\"-\")[-1]) if \"race-\" in url else 0\n\n        runners = [runner for row in soup.select(\"tr.race-card-row\") if (runner := self._parse_runner_row(row))]\n        if not runners:\n            return None\n\n        today_str = datetime.now().strftime(\"%Y-%m-%d\")\n        start_time = datetime.strptime(f\"{today_str} {race_time_str}\", \"%Y-%m-%d %H:%M\")\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{start_time.strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name\n        )\n\n    def _parse_runner_row(self, row: Tag) -> Optional[Runner]:\n        name = row.select_one(\"span.selection-name\").get_text(strip=True)\n        odds_str = row.select_one(\"span.bet-button-odds-desktop, span.best-price\").get_text(strip=True)\n        number = int(row.select_one(\"td.runner-number\").get_text(strip=True))\n\n        if not name or not odds_str:\n            return None\n\n        odds_val = parse_odds(odds_str)\n        odds_dict = {}\n        if odds_val:\n            odds_dict[self.source_name] = OddsData(\n                win=Decimal(str(odds_val)), source=self.source_name, last_updated=datetime.now()\n            )\n\n        return Runner(number=number, name=name, odds=odds_dict)\n",
    "python_service/adapters/pointsbet_greyhound_adapter.py": "from typing import List\n\nimport httpx\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\n\n# NOTE: This is a hypothetical implementation based on a potential API structure.\n\nclass PointsBetGreyhoundAdapter(BaseAdapter):\n    def __init__(self, config: dict):\n        super().__init__(source_name=\"PointsBetGreyhound\", base_url=\"https://api.pointsbet.com/api/v2/\", config=config)\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches all greyhound events for a given date from the hypothetical PointsBet API.\"\"\"\n        endpoint = f'sports/greyhound-racing/events/by-date/{date}'\n        response = await self.make_request(http_client, \"GET\", endpoint)\n        raw_data = response.json().get('events', [])\n        return self._parse_races(raw_data)\n\n    def _parse_races(self, raw_data: list) -> List[Race]:\n        \"\"\"Parses the raw event data into a list of standardized Race objects.\"\"\"\n        races = []\n        for event in raw_data:\n            if not event.get('competitors') or not event.get('startTime'):\n                continue\n\n            runners = []\n            for competitor in event.get('competitors', []):\n                if competitor.get('price'):\n                    runner = Runner(\n                        number=competitor.get('number', 99),\n                        name=competitor.get('name', 'Unknown'),\n                        odds={'win': float(competitor['price'])}\n                    )\n                    runners.append(runner)\n\n            if runners:\n                race = Race(\n                    id=f'pbg_{event[\"id\"]}',\n                    venue=event.get('venue', {}).get('name', 'Unknown Venue'),\n                    start_time=event['startTime'],\n                    race_number=event.get('raceNumber', 1),\n                    runners=runners,\n                    source=self.source_name,\n                )\n                races.append(race)\n        return races\n",
    "python_service/adapters/punters_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..models import Race\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass PuntersAdapter(BaseAdapter):\n    \"\"\"Adapter for punters.com.au.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"Punters\", base_url=\"https://www.punters.com.au\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"PuntersAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {\n            \"races\": [],\n            \"source_info\": {\n                \"name\": self.source_name,\n                \"status\": \"SUCCESS\",\n                \"races_fetched\": 0,\n                \"error_message\": \"Not Implemented\",\n                \"fetch_duration\": (datetime.now() - start_time).total_seconds(),\n            },\n        }\n",
    "python_service/adapters/racing_and_sports_adapter.py": "# python_service/adapters/racing_and_sports_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass RacingAndSportsAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"Racing and Sports\", base_url=\"https://api.racingandsports.com.au/\")\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n        meetings_url = \"v1/racing/meetings\"\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n\n        meetings_response = await self.make_request(http_client, \"GET\", meetings_url, headers=headers, params=params)\n\n        try:\n            meetings_data = meetings_response.json()\n            return self._parse_races(meetings_data)\n        except (ValueError, TypeError) as e:\n            log.error(\"RacingAndSportsAdapter: Failed to parse response JSON\", error=str(e))\n            raise AdapterParsingError(self.source_name, \"Failed to parse API response JSON.\") from e\n\n    def _parse_races(self, meetings_data: Dict[str, Any]) -> List[Race]:\n        all_races = []\n        if not meetings_data or not isinstance(meetings_data.get(\"meetings\"), list):\n            return all_races\n\n        for meeting in meetings_data[\"meetings\"]:\n            if not isinstance(meeting, dict):\n                continue\n            for race_summary in meeting.get(\"races\", []):\n                if not isinstance(race_summary, dict):\n                    continue\n                try:\n                    if parsed_race := self._parse_ras_race(meeting, race_summary):\n                        all_races.append(parsed_race)\n                except (KeyError, TypeError, ValueError) as e:\n                    log.warning(\n                        \"RacingAndSportsAdapter: Failed to parse race, skipping\",\n                        meeting=meeting.get(\"venueName\"),\n                        race_id=race_summary.get(\"raceId\"),\n                        error=str(e),\n                    )\n        return all_races\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        if not race_id or not start_time_str:\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\"),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", []) if isinstance(rd, dict)\n        ]\n\n        return Race(\n            id=f\"ras_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race.get(\"raceNumber\"),\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"Racing and Sports Greyhound\", base_url=\"https://api.racingandsports.com.au/\")\n        if not hasattr(config, \"RACING_AND_SPORTS_TOKEN\") or not config.RACING_AND_SPORTS_TOKEN:\n            raise AdapterConfigError(self.source_name, \"RACING_AND_SPORTS_TOKEN is not configured.\")\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n        meetings_url = \"v1/greyhound/meetings\"\n        params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n\n        meetings_response = await self.make_request(http_client, \"GET\", meetings_url, headers=headers, params=params)\n\n        try:\n            meetings_data = meetings_response.json()\n            if not meetings_data or not meetings_data.get(\"meetings\"):\n                log.warning(\"No greyhound meetings found in RacingAndSports response.\")\n                return []\n\n            all_races = []\n            for meeting in meetings_data[\"meetings\"]:\n                for race_summary in meeting.get(\"races\", []):\n                    try:\n                        if parsed_race := self._parse_ras_race(meeting, race_summary):\n                            all_races.append(parsed_race)\n                    except (KeyError, TypeError, ValueError) as e:\n                        log.warning(\n                            \"RacingAndSportsGreyhoundAdapter: Failed to parse race, skipping\",\n                            meeting=meeting.get(\"venueName\"),\n                            race_id=race_summary.get(\"raceId\"),\n                            error=str(e),\n                        )\n            return all_races\n        except (ValueError, TypeError) as e:\n            log.error(\"RacingAndSportsGreyhoundAdapter: Failed to parse response JSON\", error=str(e))\n            raise AdapterParsingError(self.source_name, \"Failed to parse API response JSON.\") from e\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Optional[Race]:\n        race_id = race.get(\"raceId\")\n        start_time_str = race.get(\"startTime\")\n        if not race_id or not start_time_str:\n            return None\n\n        runners = [\n            Runner(\n                number=rd.get(\"runnerNumber\"),\n                name=rd.get(\"horseName\", \"Unknown\"),\n                scratched=rd.get(\"isScratched\", False),\n            )\n            for rd in race.get(\"runners\", [])\n        ]\n\n        return Race(\n            id=f\"rasg_{race_id}\",\n            venue=meeting.get(\"venueName\", \"Unknown Venue\"),\n            race_number=race.get(\"raceNumber\"),\n            start_time=datetime.fromisoformat(start_time_str),\n            runners=runners,\n            source=self.source_name,\n        )\n",
    "python_service/adapters/racingpost_adapter.py": "# python_service/adapters/racingpost_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any, List, Optional\nimport httpx\nfrom selectolax.parser import HTMLParser\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData, Race, Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text, normalize_venue_name\nfrom .base import BaseAdapter\n\n\nclass RacingPostAdapter(BaseAdapter):\n    \"\"\"\n    A production-ready adapter for scraping Racing Post racecards.\n    This adapter now follows the modern fetch/parse pattern.\n    \"\"\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=\"RacingPost\", base_url=\"https://www.racingpost.com\", config=config)\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Any:\n        \"\"\"\n        Fetches the raw HTML content for all races on a given date.\n        This involves a two-step process: first get the index of race URLs,\n        then fetch the content of each URL concurrently.\n        \"\"\"\n        # Step 1: Get all individual race card URLs\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(http_client, \"GET\", index_url, headers=self._get_headers())\n        index_parser = HTMLParser(index_response.text)\n        links = index_parser.css('a[data-test-selector^=\"RC-meetingItem__link_race\"]')\n        race_card_urls = [link.attributes['href'] for link in links]\n\n        # Step 2: Fetch the HTML for each race card URL concurrently\n        async def fetch_single_html(url: str):\n            response = await self.make_request(http_client, \"GET\", url, headers=self._get_headers())\n            return response.text\n\n        tasks = [fetch_single_html(url) for url in race_card_urls]\n        html_contents = await asyncio.gather(*tasks)\n\n        # Pass along the date, as it's needed for parsing the start_time\n        return {\"date\": date, \"html_contents\": html_contents}\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        date = raw_data[\"date\"]\n        html_contents = raw_data[\"html_contents\"]\n        all_races: List[Race] = []\n\n        for html in html_contents:\n            try:\n                parser = HTMLParser(html)\n                venue_raw = parser.css_first('a[data-test-selector=\"RC-course__name\"]').text(strip=True)\n                venue = normalize_venue_name(venue_raw)\n                race_time_str = parser.css_first('span[data-test-selector=\"RC-course__time\"]').text(strip=True)\n                race_datetime_str = f\"{date} {race_time_str}\"\n                start_time = datetime.strptime(race_datetime_str, \"%Y-%m-%d %H:%M\")\n                runners = self._parse_runners(parser)\n\n                if venue and runners:\n                    race_number = self._get_race_number(parser, start_time)\n                    race = Race(\n                        id=f\"rp_{venue.lower().replace(' ', '')}_{date}_{race_number}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=start_time,\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n            except (AttributeError, ValueError) as e:\n                self.logger.error(\"Failed to parse race from HTML content.\", exc_info=True)\n                # Continue parsing other races\n                continue\n        return all_races\n\n    def _get_race_number(self, parser: HTMLParser, start_time: datetime) -> int:\n        \"\"\"Derives the race number by finding the active time in the nav bar.\"\"\"\n        time_str_to_find = start_time.strftime(\"%H:%M\")\n        time_links = parser.css('a[data-test-selector=\"RC-raceTime\"]')\n        for i, link in enumerate(time_links):\n            if link.text(strip=True) == time_str_to_find:\n                return i + 1\n        return 1  # Fallback\n\n    def _parse_runners(self, parser: HTMLParser) -> list[Runner]:\n        \"\"\"Parses all runners from a single race card page.\"\"\"\n        runners = []\n        runner_nodes = parser.css('div[data-test-selector=\"RC-runnerCard\"]')\n        for node in runner_nodes:\n            try:\n                number_node = node.css_first('span[data-test-selector=\"RC-runnerNumber\"]')\n                name_node = node.css_first('a[data-test-selector=\"RC-runnerName\"]')\n                odds_node = node.css_first('span[data-test-selector=\"RC-runnerPrice\"]')\n\n                if not all([number_node, name_node, odds_node]):\n                    continue\n\n                number_str = clean_text(number_node.text())\n                number = int(number_str) if number_str and number_str.isdigit() else 0\n                name = clean_text(name_node.text())\n                odds_str = clean_text(odds_node.text())\n                scratched = \"NR\" in odds_str.upper() or not odds_str\n\n                odds = {}\n                if not scratched:\n                    win_odds = parse_odds_to_decimal(odds_str)\n                    if win_odds and win_odds < 999:\n                        odds = {\n                            self.source_name: OddsData(\n                                win=win_odds, source=self.source_name, last_updated=datetime.now()\n                            )\n                        }\n\n                runners.append(Runner(number=number, name=name, odds=odds, scratched=scratched))\n            except (ValueError, AttributeError):\n                self.logger.warning(\"Could not parse runner, skipping.\", parser=parser)\n                continue\n        return runners\n\n    def _get_headers(self) -> dict:\n        return {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/107.0.0.0 Safari/537.36\"\n            )\n        }\n",
    "python_service/adapters/racingtv_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..models import Race\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass RacingTVAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping data from racingtv.com.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"RacingTV\", base_url=\"https://www.racingtv.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"RacingTVAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {\n            \"races\": [],\n            \"source_info\": {\n                \"name\": self.source_name,\n                \"status\": \"SUCCESS\",\n                \"races_fetched\": 0,\n                \"error_message\": \"Not Implemented\",\n                \"fetch_duration\": (datetime.now() - start_time).total_seconds(),\n            },\n        }\n",
    "python_service/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\ndef _clean_text(text: Optional[str]) -> Optional[str]:\n    return \" \".join(text.strip().split()) if text else None\n\n\nclass SportingLifeAdapter(BaseAdapter):\n    \"\"\"\n    Adapter for sportinglife.com.\n    This adapter now follows the modern fetch/parse pattern.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"SportingLife\", base_url=\"https://www.sportinglife.com\", config=config)\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Optional[List[str]]:\n        \"\"\"\n        Fetches the raw HTML for all race pages. This involves first getting the\n        racecard index, then fetching each individual race page concurrently.\n        \"\"\"\n        index_response = await self.make_request(http_client, \"GET\", \"/horse-racing/racecards\")\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(http_client, \"GET\", url_path)\n            return response.text\n\n        tasks = [fetch_single_html(link) for link in links]\n        return await asyncio.gather(*tasks)\n\n    def _parse_races(self, raw_data: List[str]) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        all_races = []\n        for html in raw_data:\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n                track_name = _clean_text(soup.select_one(\"a.hr-race-header-course-name__link\").get_text())\n                race_time_str = _clean_text(soup.select_one(\"span.hr-race-header-time__time\").get_text())\n                start_time = datetime.strptime(f\"{datetime.now().date()} {race_time_str}\", \"%Y-%m-%d %H:%M\")\n                active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n                race_number = (\n                    soup.select(\"a.hr-race-header-navigation-link\").index(active_link) + 1 if active_link else 1\n                )\n                runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n\n                race = Race(\n                    id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError) as e:\n                self.logger.error(\"Error parsing race from SportingLife\", exc_info=e)\n                continue  # Continue to the next HTML page\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = _clean_text(row.select_one(\"a.hr-racing-runner-horse-name\").get_text())\n            num_str = _clean_text(row.select_one(\"span.hr-racing-runner-saddle-cloth-no\").get_text())\n            number = int(\"\".join(filter(str.isdigit, num_str)))\n            odds_str = _clean_text(row.select_one(\"span.hr-racing-runner-odds\").get_text())\n            win_odds = parse_odds_to_decimal(odds_str)\n            odds_data = (\n                {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())}\n                if win_odds and win_odds < 999\n                else {}\n            )\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError):\n            log.warning(\"Failed to parse runner from SportingLife, skipping.\")\n            return None\n",
    "python_service/adapters/tab_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..models import Race\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass TabAdapter(BaseAdapter):\n    \"\"\"Adapter for tab.com.au.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"TAB\", base_url=\"https://www.tab.com.au\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"TabAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {\n            \"races\": [],\n            \"source_info\": {\n                \"name\": self.source_name,\n                \"status\": \"SUCCESS\",\n                \"races_fetched\": 0,\n                \"error_message\": \"Not Implemented\",\n                \"fetch_duration\": (datetime.now() - start_time).total_seconds(),\n            },\n        }\n",
    "python_service/adapters/template_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# ==============================================================================\n#  Fortuna Faucet: Canonical Adapter Template\n# ==============================================================================\n# This file is the official template for creating new adapters. It is based on\n# the clean and simple design of the RacingAndSportsAdapter.\n# ==============================================================================\n\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..models import Race  # Assuming standard models\nfrom ..models import Runner  # Assuming standard models\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass TemplateAdapter(BaseAdapter):\n    \"\"\"[IMPLEMENT ME] A brief description of the data source.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"[IMPLEMENT ME] Example Source\", base_url=\"https://api.example.com\")\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"[IMPLEMENT ME] The core logic for fetching and parsing races.\"\"\"\n        start_time = datetime.now()\n        all_races: List[Race] = []\n\n        # --- Example Logic ---\n        # endpoint = f\"/v1/races/{date}\"\n        # headers = {\"X-Api-Key\": self.api_key}\n        # response_json = await self.make_request(http_client, 'GET', endpoint, headers=headers)\n        # if not response_json or 'data' not in response_json:\n        #     return self._format_response(all_races, start_time)\n        #\n        # for race_data in response_json['data']:\n        #     parsed_race = self._parse_race(race_data)\n        #     all_races.append(parsed_race)\n        # --- End Example ---\n\n        log.warning(\"TemplateAdapter.fetch_races is a stub and is not implemented.\")\n        return self._format_response(all_races, start_time)\n\n    def _parse_race(self, race_data: Dict[str, Any]) -> Race:\n        \"\"\"[IMPLEMENT ME] Logic to parse a single race from the source's data structure.\"\"\"\n        # Example:\n        # runners = self._parse_runners(race_data.get('runners', []))\n        # return Race(\n        #     id=f\"template_{race_data['id']}\",\n        #     venue=race_data['venue_name'],\n        #     race_number=race_data['race_number'],\n        #     start_time=datetime.fromisoformat(race_data['start_time']),\n        #     runners=runners,\n        #     source=self.source_name\n        # )\n        raise NotImplementedError(\"'_parse_race' is not implemented in TemplateAdapter.\")\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"[IMPLEMENT ME] Logic to parse a list of runners.\"\"\"\n        raise NotImplementedError(\"'_parse_runners' is not implemented in TemplateAdapter.\")\n",
    "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/theracingapi_adapter.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass TheRacingApiAdapter(BaseAdapter):\n    \"\"\"\n    Adapter for the high-value JSON-based The Racing API.\n    This adapter now follows the modern fetch/parse pattern.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"TheRacingAPI\",\n            base_url=\"https://api.theracingapi.com/v1/\",\n            config=config,\n        )\n        if not hasattr(config, \"THE_RACING_API_KEY\") or not config.THE_RACING_API_KEY:\n            raise AdapterConfigError(self.source_name, \"THE_RACING_API_KEY is not configured.\")\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Dict[str, Any]:\n        \"\"\"Fetches the raw racecard data from The Racing API.\"\"\"\n        endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        response = await self.make_request(http_client, \"GET\", endpoint, headers=headers)\n        return response.json()\n\n    def _parse_races(self, raw_data: Dict[str, Any]) -> List[Race]:\n        \"\"\"Parses the raw JSON response into a list of Race objects.\"\"\"\n        if not raw_data or \"racecards\" not in raw_data:\n            self.logger.warning(\"'racecards' key missing in API response.\")\n            return []\n\n        races = []\n        for race_data in raw_data[\"racecards\"]:\n            try:\n                start_time = datetime.fromisoformat(race_data[\"off_time\"].replace(\"Z\", \"+00:00\"))\n\n                race = Race(\n                    id=f\"tra_{race_data['race_id']}\",\n                    venue=race_data[\"course\"],\n                    race_number=race_data[\"race_no\"],\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get(\"runners\", [])),\n                    source=self.source_name,\n                    race_name=race_data.get(\"race_name\"),\n                    distance=race_data.get(\"distance_f\"),\n                )\n                races.append(race)\n            except Exception as e:\n                self.logger.error(\"Error parsing race\", race_id=race_data.get(\"race_id\"), error=str(e), exc_info=True)\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                odds_data = {}\n                if runner_data.get(\"odds\"):\n                    win_odds = Decimal(str(runner_data[\"odds\"][0][\"odds_decimal\"]))\n                    odds_data[self.source_name] = OddsData(\n                        win=win_odds, source=self.source_name, last_updated=datetime.now()\n                    )\n\n                runners.append(\n                    Runner(\n                        number=runner_data.get(\"number\", i + 1),\n                        name=runner_data[\"horse\"],\n                        odds=odds_data,\n                        jockey=runner_data.get(\"jockey\"),\n                        trainer=runner_data.get(\"trainer\"),\n                    )\n                )\n            except Exception as e:\n                self.logger.error(\n                    \"Error parsing runner\", runner_name=runner_data.get(\"horse\"), error=str(e), exc_info=True\n                )\n        return runners\n",
    "python_service/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nimport structlog\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\ndef _clean_text(text: Optional[str]) -> Optional[str]:\n    return \" \".join(text.strip().split()) if text else None\n\n\nclass TimeformAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"Timeform\", base_url=\"https://www.timeform.com\", config=config)\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        race_links = await self._get_race_links(http_client)\n        tasks = [self._fetch_and_parse_race(link, http_client) for link in race_links]\n        return [race for race in await asyncio.gather(*tasks) if race]\n\n    async def _get_race_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        response = await self.make_request(http_client, \"GET\", \"/horse-racing/racecards\")\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        links = {a[\"href\"] for a in soup.select(\"a.rp-racecard-off-link[href]\")}\n        return [f\"{self.base_url}{link}\" for link in links]\n\n    async def _fetch_and_parse_race(self, url: str, http_client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            response = await self.make_request(http_client, \"GET\", url)\n            soup = BeautifulSoup(response.text, \"html.parser\")\n\n            track_name = _clean_text(soup.select_one(\"h1.rp-raceTimeCourseName_name\").get_text())\n            race_time_str = _clean_text(soup.select_one(\"span.rp-raceTimeCourseName_time\").get_text())\n\n            start_time = datetime.strptime(f\"{datetime.now().date()} {race_time_str}\", \"%Y-%m-%d %H:%M\")\n            all_times = [_clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n            race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n            runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n            if not runner_rows:\n                return None\n\n            runners = [self._parse_runner(row) for row in runner_rows]\n\n            return Race(\n                id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                venue=track_name,\n                race_number=race_number,\n                start_time=start_time,\n                runners=[r for r in runners if r],\n                source=self.source_name,\n            )\n        except (AttributeError, ValueError, TypeError) as e:\n            log.error(\"Error parsing race from Timeform\", url=url, exc_info=e)\n            raise AdapterParsingError(self.source_name, f\"Failed to parse race at {url}\") from e\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = _clean_text(row.select_one(\"a.rp-horseTable_horse-name\").get_text())\n            num_str = _clean_text(row.select_one(\"span.rp-horseTable_horse-number\").get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = _clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                        self.source_name: OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            log.warning(\"Failed to parse runner from Timeform, skipping.\")\n            return None\n",
    "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\n\nimport httpx\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..core.exceptions import AdapterParsingError\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.text import clean_text\nfrom .base import BaseAdapter\n\n\nclass TVGAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching US racing data from the TVG API.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"TVG\", base_url=\"https://api.tvg.com/v2/races/\", config=config)\n        if not hasattr(config, \"TVG_API_KEY\") or not config.TVG_API_KEY:\n            raise AdapterConfigError(self.source_name, \"TVG_API_KEY is not configured.\")\n        self.tvg_api_key = config.TVG_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        \"\"\"Fetches all race details for a given date by first getting tracks.\"\"\"\n        headers = {\"X-Api-Key\": self.tvg_api_key}\n        summary_url = f\"summary?date={date}&country=USA\"\n\n        tracks_response = await self.make_request(http_client, \"GET\", summary_url, headers=headers)\n        tracks_data = tracks_response.json()\n\n        race_detail_tasks = []\n        for track in tracks_data.get('tracks', []):\n            track_id = track.get('id')\n            for race in track.get('races', []):\n                race_id = race.get('id')\n                if track_id and race_id:\n                    details_url = f\"{track_id}/{race_id}\"\n                    race_detail_tasks.append(self.make_request(http_client, \"GET\", details_url, headers=headers))\n\n        race_detail_responses = await asyncio.gather(*race_detail_tasks, return_exceptions=True)\n\n        races = []\n        for response in race_detail_responses:\n            if isinstance(response, Exception):\n                self.logger.error(\"Failed to fetch race detail\", error=response)\n                continue\n            try:\n                races.append(self._parse_race(response.json()))\n            except AdapterParsingError as e:\n                self.logger.error(\"Failed to parse TVG race detail\", error=e)\n\n        return races\n\n\n    def _parse_race(self, race_detail: dict) -> Race:\n        \"\"\"Parses a single detailed race JSON object into a Race model.\"\"\"\n        track = race_detail.get('track')\n        race_info = race_detail.get('race')\n\n        if not track or not race_info:\n            raise AdapterParsingError(self.source_name, \"Missing track or race info in race detail.\")\n\n        runners = []\n        for runner_data in race_detail.get('runners', []):\n            if runner_data.get('scratched'):\n                continue\n\n            odds = runner_data.get('odds', {})\n            current_odds = odds.get('currentPrice', {})\n            odds_str = current_odds.get('fractional') or odds.get('morningLinePrice', {}).get('fractional')\n\n            try:\n                number = int(runner_data.get('programNumber', '0').replace('A', ''))\n            except (ValueError, TypeError):\n                self.logger.warning(f\"Could not parse program number: {runner_data.get('programNumber')}\")\n                continue\n\n            runners.append(Runner(\n                number=number,\n                name=clean_text(runner_data.get('name')),\n                odds=odds_str,\n                scratched=False\n            ))\n\n        if not runners:\n            raise AdapterParsingError(self.source_name, \"No non-scratched runners found.\")\n\n        try:\n            start_time = datetime.fromisoformat(\n                race_info.get(\"postTime\").replace(\"Z\", \"+00:00\")\n            )\n        except (ValueError, TypeError, AttributeError) as e:\n            raise AdapterParsingError(\n                self.source_name, f\"Could not parse post time: {race_info.get('postTime')}\"\n            ) from e\n\n        return Race(\n            id=f\"tvg_{track.get('code', 'UNK')}_{race_info.get('date', 'NODATE')}_{race_info.get('number', 0)}\",\n            venue=track.get('name'),\n            race_number=race_info.get('number'),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name\n        )\n",
    "python_service/adapters/twinspires_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nimport httpx\nimport structlog\n\nfrom ..models import OddsData, Race, Runner\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass TwinSpiresAdapter(BaseAdapter):\n    \"\"\"\n    Adapter for twinspires.com.\n    This adapter now follows the modern fetch/parse pattern.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"TwinSpires\", base_url=\"https://www.twinspires.com\")\n\n    async def _fetch_data(self, http_client: httpx.AsyncClient, date: str) -> Any:\n        \"\"\"Fetches the raw HTML from the TwinSpires race page.\"\"\"\n        url = f\"/races/{date}\"\n        response = await self.make_request(http_client, \"GET\", url)\n        return response.text if response else None\n\n    def _parse_races(self, raw_data: str) -> List[Race]:\n        \"\"\"Parses the raw HTML into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        from bs4 import BeautifulSoup\n        from ..utils.odds import parse_odds_to_decimal\n\n        soup = BeautifulSoup(raw_data, \"html.parser\")\n        race_card = soup.select_one(\"#race-card\")\n        if not race_card:\n            return []\n\n        race_title_parts = race_card.select_one(\"h1\").text.split(\" - \")\n        race_number = int(race_title_parts[0].split(\" \")[1])\n        venue = race_title_parts[1]\n        date_str = race_title_parts[2]\n\n        start_time = self._parse_start_time(race_card, date_str)\n\n        runners = []\n        for runner_item in race_card.select(\".runners-list .runner\"):\n            if \"scratched\" in runner_item.get(\"class\", []):\n                continue\n\n            number = int(runner_item.select_one(\".runner-number\").text)\n            name = runner_item.select_one(\".runner-name\").text\n            odds_str = runner_item.select_one(\".runner-odds\").text\n\n            odds_decimal = parse_odds_to_decimal(odds_str)\n            odds = {}\n            if odds_decimal:\n                odds[self.source_name] = OddsData(win=odds_decimal, source=self.source_name, last_updated=datetime.now())\n\n            runners.append(Runner(number=number, name=name, odds=odds, scratched=False))\n\n        race = Race(\n            id=f\"ts_{venue.replace(' ', '').lower()}_{date_str}_{race_number}\",\n            venue=venue,\n            race_number=race_number,\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n        return [race]\n\n    def _parse_start_time(self, soup: \"BeautifulSoup\", race_date: str) -> datetime:\n        from dateutil.parser import parse\n        post_time_str = soup.select_one(\".post-time\").text.replace(\"Post Time: \", \"\").strip()\n        full_datetime_str = f\"{race_date} {post_time_str}\"\n        return parse(full_datetime_str)\n",
    "python_service/adapters/universal_adapter.py": "import json\nfrom typing import List\n\nimport httpx\nimport structlog\nfrom bs4 import BeautifulSoup\n\nfrom ..models import Race\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass UniversalAdapter(BaseAdapter):\n    \"\"\"An adapter that executes logic from a declarative JSON definition file.\"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, \"r\") as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition[\"adapter_name\"],\n            base_url=self.definition[\"base_url\"],\n            config=config,\n        )\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> List[Race]:\n        # NOTE: This is a simplified proof-of-concept implementation.\n        # It does not handle all cases from the JSON definition.\n        log.info(f\"Executing Universal Adapter for {self.source_name}\")\n\n        # Step 1: Get Track Links (as defined in equibase_v2.json)\n        response = await self.make_request(http_client, \"GET\", self.definition[\"start_url\"])\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        track_links = [self.base_url + a[\"href\"] for a in soup.select(self.definition[\"steps\"][0][\"selector\"])]\n\n        for link in track_links:\n            # This is a PoC; we're not actually parsing anything yet.\n            # In a full implementation, we would fetch and parse each track link.\n            pass\n\n        # This is a placeholder return for the PoC\n        return []\n",
    "python_service/adapters/utils.py": "# python_service/adapters/utils.py\n# Compatibility shim to re-export parse_odds from the centralized location.\n\nfrom ..utils.odds import parse_odds\n\n__all__ = ['parse_odds']\n",
    "python_service/adapters/xpressbet_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import Dict\n\nimport httpx\nimport structlog\n\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\n\nclass XpressbetAdapter(BaseAdapter):\n    \"\"\"Adapter for xpressbet.com.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"Xpressbet\", base_url=\"https://www.xpressbet.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"XpressbetAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time, is_success=True, error_message=\"Not Implemented\")\n",
    "python_service/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        scored_races = []\n        for race in races:\n            # The _evaluate_race method now always returns a float score.\n            race.qualification_score = self._evaluate_race(race)\n            scored_races.append(race)\n\n        scored_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\"Universal scoring complete\", total_races_scored=len(scored_races), criteria=criteria)\n\n        for race in scored_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": scored_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        score = round(final_score * 100, 2)\n        race.qualification_score = score\n        return score\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n\n    def play(self, sound_type: str):\n        if not winsound:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier() if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "python_service/api.py": "# python_service/api.py\n\nimport os\nfrom contextlib import asynccontextmanager\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import List\nfrom typing import Optional\n\nimport aiosqlite\nimport structlog\nfrom fastapi import Depends\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom slowapi import Limiter\nfrom slowapi import _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\n\nfrom .analyzer import AnalyzerEngine\nfrom .config import get_settings\nfrom .engine import FortunaEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .middleware.error_handler import validation_exception_handler\nfrom .models import AggregatedResponse\nfrom .models import QualifiedRacesResponse\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom typing import Callable\n\nclass UserFriendlyErrorMiddleware(BaseHTTPMiddleware):\n    async def dispatch(\n        self, request: Request, call_next: Callable\n    ):\n        try:\n            return await call_next(request)\n        except Exception as e:\n            # Log the exception here if you have a logger configured\n            return JSONResponse(\n                status_code=500,\n                content={\"detail\": \"An unexpected error occurred.\"},\n            )\nfrom .models import Race\nfrom .models import TipsheetRace\nfrom .security import verify_api_key\n\n# --- PyInstaller Explicit Imports ---\n# These imports are not used directly in this file but are required\n# to ensure PyInstaller bundles all necessary adapter modules.\nfrom .adapters import *\n# ------------------------------------\n\nlog = structlog.get_logger()\n\n\n# Define the lifespan context manager for robust startup/shutdown\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Manage the application's lifespan. On startup, it initializes the OddsEngine\n    with validated settings and attaches it to the app state. On shutdown, it\n    properly closes the engine's resources.\n    \"\"\"\n    configure_logging()\n    log.info(\"Server startup sequence initiated.\")\n    try:\n        settings = get_settings()\n        app.state.engine = FortunaEngine(config=settings)\n        app.state.analyzer_engine = AnalyzerEngine()\n        log.info(\"Server startup: Configuration validated and FortunaEngine initialized successfully.\")\n    except Exception as e:\n        log.critical(\"FATAL: Failed to initialize FortunaEngine during server startup.\", exc_info=True)\n        # Re-raise the exception to ensure FastAPI's startup failure handling is triggered\n        raise e\n\n    yield\n\n    # Clean up the engine resources\n    if hasattr(app.state, 'engine') and app.state.engine:\n        log.info(\"Server shutdown: Closing HTTP client resources.\")\n        await app.state.engine.close()\n    log.info(\"Server shutdown sequence complete.\")\n\n\nlimiter = Limiter(key_func=get_remote_address)\n\n# Pass the lifespan manager to the FastAPI app\napp = FastAPI(\n    title=\"Fortuna Faucet API\",\n    version=\"2.1\",\n    lifespan=lifespan,\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\",\n    openapi_url=\"/api/openapi.json\"\n)\n\n# Add the new error handling middleware FIRST, to catch exceptions from all other middleware\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\n\nsettings = get_settings()\n\n# Add middlewares (order can be important)\napp.include_router(health_router)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"GET\"],\n    allow_headers=[\"*\"],\n)\n\n\n# Dependency function to get the engine instance from the app state\ndef get_engine(request: Request) -> FortunaEngine:\n    return request.app.state.engine\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request, engine: FortunaEngine = Depends(get_engine), _=Depends(verify_api_key)\n):\n    \"\"\"Provides a list of health statuses for all adapters, required by the new frontend blueprint.\"\"\"\n    try:\n        statuses = engine.get_all_adapter_statuses()\n        return statuses\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\n    \"/api/races/qualified/{analyzer_name}\",\n    response_model=QualifiedRacesResponse,\n    description=(\n        \"Fetch and analyze races from all configured data sources, returning a list of races \"\n        \"that meet the specified analyzer's criteria.\"\n    ),\n    responses={\n        200: {\n            \"description\": \"A list of qualified races with their scores.\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\n                        \"races\": [\n                            {\n                                \"id\": \"12345_2025-10-14_1\",\n                                \"venue\": \"Santa Anita\",\n                                \"race_number\": 1,\n                                \"start_time\": \"2025-10-14T20:30:00Z\",\n                                \"runners\": [{\"number\": 1, \"name\": \"Speedy Gonzalez\", \"odds\": \"5/2\"}],\n                                \"source\": \"TVG\",\n                                \"qualification_score\": 95.5,\n                            }\n                        ],\n                        \"analyzer\": \"trifecta_analyzer\",\n                    }\n                }\n            },\n        },\n        404: {\"description\": \"The specified analyzer was not found.\"},\n    },\n)\n@limiter.limit(\"120/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    # --- Dynamic Analyzer Parameters ---\n    max_field_size: int = Query(10, ge=3, le=20),\n    min_favorite_odds: float = Query(2.5, ge=1.0, le=100.0),\n    min_second_favorite_odds: float = Query(4.0, ge=1.0, le=100.0),\n):\n    \"\"\"\n    Gets all races for a given date, filters them for qualified betting\n    opportunities, and returns the qualified races.\n    \"\"\"\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime(\"%Y-%m-%d\")\n        background_tasks = set()  # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks)\n\n        races = aggregated_data.get(\"races\", [])\n\n        analyzer_engine = request.app.state.analyzer_engine\n        custom_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds\n        }\n\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except Exception as e:\n        log.error(\"Error in /api/races/qualified\", error=str(e), exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/filter-suggestions\")\nasync def get_filter_suggestions(engine: FortunaEngine = Depends(get_engine)):\n    \"\"\"\n    Returns historical statistics to help users choose appropriate filter values.\n    \"\"\"\n    try:\n        # Fetch races for the past day\n        date_str = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n        aggregated = await engine.get_races(date_str, background_tasks=set())\n\n        if not aggregated or not aggregated.get(\"races\"):\n            return {\n                \"suggestions\": {\n                    \"max_field_size\": {\"min\": 2, \"max\": 20, \"recommended\": 10},\n                    \"min_favorite_odds\": {\"min\": 1.5, \"max\": 5, \"recommended\": 2.5},\n                    \"min_second_favorite_odds\": {\"min\": 2.0, \"max\": 8, \"recommended\": 4.0},\n                }\n            }\n\n        # Analyze field sizes\n        field_sizes = [len(r[\"runners\"]) for r in aggregated[\"races\"]]\n\n        # Analyze odds\n        favorite_odds = []\n        second_favorite_odds = []\n\n        for race_data in aggregated[\"races\"]:\n            race = Race(**race_data) # Convert dict to Race model\n            runners = race.runners\n            if len(runners) >= 2:\n                odds_list = []\n                for runner in runners:\n                    if not runner.scratched and runner.odds:\n                        # Find the best (lowest) win odd from any source for the runner\n                        best_odd = min(\n                            (\n                                o.win\n                                for o in runner.odds.values()\n                                if o.win is not None\n                            ),\n                            default=None,\n                        )\n                        if best_odd is not None:\n                            odds_list.append(float(best_odd))\n\n                if len(odds_list) >= 2:\n                    odds_list.sort()\n                    favorite_odds.append(odds_list[0])\n                    second_favorite_odds.append(odds_list[1])\n\n        return {\n            \"suggestions\": {\n                \"max_field_size\": {\n                    \"min\": 2,\n                    \"max\": 20,\n                    \"recommended\": int(sum(field_sizes) / len(field_sizes)) if field_sizes else 10,\n                    \"average\": sum(field_sizes) / len(field_sizes) if field_sizes else 0,\n                },\n                \"min_favorite_odds\": {\n                    \"min\": 1.5,\n                    \"max\": 5,\n                    \"recommended\": 2.5,\n                    \"average\": sum(favorite_odds) / len(favorite_odds) if favorite_odds else 0,\n                },\n                \"min_second_favorite_odds\": {\n                    \"min\": 2.0,\n                    \"max\": 8,\n                    \"recommended\": 4.0,\n                    \"average\": sum(second_favorite_odds) / len(second_favorite_odds) if second_favorite_odds else 0,\n                },\n            }\n        }\n    except Exception as e:\n        log.error(f\"Error generating filter suggestions: {e}\")\n        raise HTTPException(status_code=500, detail=\"Failed to generate suggestions\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = None,\n    source: Optional[str] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime(\"%Y-%m-%d\")\n        background_tasks = set()  # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks, source)\n        return aggregated_data\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\nDB_PATH = \"fortuna.db\"\n\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    \"\"\"Fetches the generated tipsheet from the database asynchronously.\"\"\"\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = \"SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC\"\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    return results\n\n\n@app.get(\"/health/legacy\", tags=[\"Health\"], summary=\"Check for Deprecated Legacy Components\")\nasync def check_legacy_files():\n    \"\"\"\n    Checks for the presence of known legacy files and returns a warning if they exist.\n    This helps operators identify and clean up obsolete parts of the codebase.\n    \"\"\"\n    legacy_files = [\"checkmate_service.py\", \"checkmate_web/main.py\"]\n    present_files = [f for f in legacy_files if os.path.exists(f)]\n\n    if present_files:\n        return {\n            \"status\": \"WARNING\",\n            \"message\": \"Legacy files detected. These are obsolete and should be removed.\",\n            \"detected_files\": present_files\n        }\n\n    return {\"status\": \"CLEAN\", \"message\": \"No known legacy files detected.\"}\n",
    "python_service/cache_manager.py": "# python_service/cache_manager.py\nimport hashlib\nimport json\nimport os\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self, redis_url: str = None):\n        self.redis_client = None\n        self.memory_cache = {}\n        if REDIS_AVAILABLE and redis_url:\n            try:\n                self.redis_client = redis.from_url(redis_url, decode_responses=True)\n                log.info(\"Redis cache connected successfully.\")\n            except Exception as e:\n                log.warning(f\"Failed to connect to Redis: {e}. Falling back to in-memory cache.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except Exception as e:\n                log.warning(f\"Redis GET failed: {e}\")\n\n        entry = self.memory_cache.get(key)\n        if entry and entry[\"expires_at\"] > datetime.now():\n            return entry[\"value\"]\n        return None\n\n    def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        serialized = json.dumps(value, default=str)\n        if self.redis_client:\n            try:\n                self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except Exception as e:\n                log.warning(f\"Redis SET failed: {e}\")\n\n        self.memory_cache[key] = {\"value\": value, \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds)}\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager(redis_url=os.getenv(\"REDIS_URL\"))\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n            cache_manager.set(cache_key, result, ttl_seconds)\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "python_service/checkmate_service.py": "# checkmate_service.py\n# The main service runner, upgraded to the final Endgame architecture.\n\nimport json\nimport logging\nimport os\nimport sqlite3\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom .engine import EnhancedTrifectaAnalyzer\nfrom .engine import Race\nfrom .engine import Settings\nfrom .engine import SuperchargedOrchestrator\n\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            # Correctly resolve paths from the service's location\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n            schema_path = os.path.join(base_dir, \"shared_database\", \"schema.sql\")\n            web_schema_path = os.path.join(base_dir, \"shared_database\", \"web_schema.sql\")\n\n            # Read both schema files\n            with open(schema_path, \"r\") as f:\n                schema = f.read()\n            with open(web_schema_path, \"r\") as f:\n                web_schema = f.read()\n\n            # Apply both schemas in a single transaction\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.executescript(schema)\n                cursor.executescript(web_schema)\n                conn.commit()\n            self.logger.info(\"CRITICAL SUCCESS: All database schemas (base + web) applied successfully.\")\n        except Exception as e:\n            self.logger.critical(f\"FATAL: Database setup failed. Other platforms will fail. Error: {e}\", exc_info=True)\n            raise\n\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO live_races (\n                        race_id, track_name, race_number, post_time, raw_data_json,\n                        checkmate_score, qualified, trifecta_factors_json, updated_at\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        race.race_id,\n                        race.track_name,\n                        race.race_number,\n                        race.post_time,\n                        race.model_dump_json(),\n                        race.checkmate_score,\n                        race.is_qualified,\n                        race.trifecta_factors_json,\n                        datetime.now(),\n                    ),\n                )\n            for status in statuses:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO adapter_status (\n                        adapter_name, status, last_run, races_found, error_message,\n                        execution_time_ms\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        status.get(\"adapter_id\"),\n                        status.get(\"status\"),\n                        status.get(\"timestamp\"),\n                        status.get(\"races_found\"),\n                        status.get(\"error_message\"),\n                        int(status.get(\"response_time\", 0) * 1000),\n                    ),\n                )\n\n            if races or statuses:\n                cursor.execute(\n                    \"INSERT INTO events (event_type, payload) VALUES (?, ?)\",\n                    (\"RACES_UPDATED\", json.dumps({\"race_count\": len(races)})),\n                )\n\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\")\n\n\nclass CheckmateBackgroundService:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        from dotenv import load_dotenv\n\n        dotenv_path = os.path.join(os.path.dirname(__file__), \"..\", \".env\")\n        load_dotenv(dotenv_path=dotenv_path)\n\n        db_path = os.getenv(\"CHECKMATE_DB_PATH\")\n        if not db_path:\n            self.logger.critical(\"FATAL: CHECKMATE_DB_PATH environment variable not set. Service cannot start.\")\n            raise ValueError(\"CHECKMATE_DB_PATH is not configured.\")\n\n        self.logger.info(f\"Database path loaded from environment: {db_path}\")\n\n        self.settings = Settings()\n        self.db_handler = DatabaseHandler(db_path)\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\n        self.python_analyzer = EnhancedTrifectaAnalyzer(self.settings)\n        self.stop_event = threading.Event()\n        self.rust_engine_path = os.path.join(\n            os.path.dirname(__file__), \"..\", \"rust_engine\", \"target\", \"release\", \"checkmate_engine.exe\"\n        )\n\n    def _analyze_with_rust(self, races: List[Race]) -> Optional[List[Race]]:\n        self.logger.info(\"Attempting analysis with external Rust engine.\")\n        try:\n            race_data_json = json.dumps([r.model_dump() for r in races])\n            result = subprocess.run(\n                [self.rust_engine_path], input=race_data_json, capture_output=True, text=True, check=True, timeout=30\n            )\n            results_data = json.loads(result.stdout)\n            results_map = {res[\"race_id\"]: res for res in results_data}\n\n            for race in races:\n                if race.race_id in results_map:\n                    res = results_map[race.race_id]\n                    race.checkmate_score = res.get(\"checkmate_score\")\n                    race.is_qualified = res.get(\"qualified\")\n                    race.trifecta_factors_json = json.dumps(res.get(\"trifecta_factors\"))\n            return races\n        except FileNotFoundError:\n            self.logger.warning(\"Rust engine not found. Falling back to Python analyzer.\")\n            return None\n        except (subprocess.CalledProcessError, json.JSONDecodeError, subprocess.TimeoutExpired) as e:\n            self.logger.error(f\"Rust engine execution failed: {e}. Falling back to Python analyzer.\")\n            return None\n\n    def _analyze_with_python(self, races: List[Race]) -> List[Race]:\n        self.logger.info(\"Performing analysis with internal Python engine.\")\n        return [self.python_analyzer.analyze_race_advanced(race) for race in races]\n\n    def run_continuously(self, interval_seconds: int = 60):\n        self.logger.info(\"Background service thread starting continuous run.\")\n\n        while not self.stop_event.is_set():\n            try:\n                self.logger.info(\"Starting data collection and analysis cycle.\")\n                races, statuses = self.orchestrator.get_races_parallel()\n\n                analyzed_races = None\n                if os.path.exists(self.rust_engine_path):\n                    analyzed_races = self._analyze_with_rust(races)\n\n                if analyzed_races is None:  # Fallback condition\n                    analyzed_races = self._analyze_with_python(races)\n\n                if analyzed_races:  # Ensure we have something to update\n                    self.db_handler.update_races_and_status(analyzed_races, statuses)\n\n            except Exception as e:\n                self.logger.critical(f\"Unhandled exception in service loop: {e}\", exc_info=True)\n\n            self.logger.info(f\"Cycle complete. Sleeping for {interval_seconds} seconds.\")\n            self.stop_event.wait(interval_seconds)\n        self.logger.info(\"Background service run loop has terminated.\")\n\n    def start(self):\n        self.stop_event.clear()\n        self.thread = threading.Thread(target=self.run_continuously)\n        self.thread.daemon = True\n        self.thread.start()\n        self.logger.info(\"CheckmateBackgroundService started.\")\n\n    def stop(self):\n        self.stop_event.set()\n        if hasattr(self, \"thread\") and self.thread.is_alive():\n            self.thread.join(timeout=10)\n        self.logger.info(\"CheckmateBackgroundService stopped.\")\n",
    "python_service/config.py": "# python_service/config.py\nimport os\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport structlog\nfrom pydantic import model_validator\nfrom pydantic_settings import BaseSettings\n\nfrom .credentials_manager import SecureCredentialsManager\n\n# --- Encryption Setup ---\ntry:\n    from cryptography.fernet import Fernet\n    ENCRYPTION_ENABLED = True\nexcept ImportError:\n    ENCRYPTION_ENABLED = False\n\nKEY_FILE = Path('.key')\nCIPHER = None\nif ENCRYPTION_ENABLED and KEY_FILE.exists():\n    with open(KEY_FILE, 'rb') as f:\n        key = f.read()\n    CIPHER = Fernet(key)\n\ndef decrypt_value(value: Optional[str]) -> Optional[str]:\n    \"\"\"If a value is encrypted, decrypts it. Otherwise, returns it as is.\"\"\"\n    if value and value.startswith('encrypted:') and CIPHER:\n        try:\n            return CIPHER.decrypt(value[10:].encode()).decode()\n        except Exception:\n            # Return the corrupted value for debugging, but it will likely fail later\n            return value\n    return value\n\n\nclass Settings(BaseSettings):\n    API_KEY: str = \"\"\n\n    # --- API Gateway Configuration ---\n    UVICORN_HOST: str = \"127.0.0.1\"\n    UVICORN_PORT: int = 8000\n    UVICORN_RELOAD: bool = True\n\n    # --- Database Configuration ---\n    DATABASE_TYPE: str = \"sqlite\"\n    DATABASE_URL: str = \"sqlite:///./fortuna.db\"\n\n    # --- Optional Betfair Credentials ---\n    BETFAIR_APP_KEY: Optional[str] = None\n    BETFAIR_USERNAME: Optional[str] = None\n    BETFAIR_PASSWORD: Optional[str] = None\n\n    # --- Caching & Performance ---\n    REDIS_URL: str = \"redis://localhost:6379\"\n    CACHE_TTL_SECONDS: int = 1800  # 30 minutes\n    MAX_CONCURRENT_REQUESTS: int = 10\n    HTTP_POOL_CONNECTIONS: int = 100\n    HTTP_POOL_MAXSIZE: int = 100\n    HTTP_MAX_KEEPALIVE: int = 50\n    DEFAULT_TIMEOUT: int = 30\n    ADAPTER_TIMEOUT: int = 20\n\n    # --- Logging ---\n    LOG_LEVEL: str = \"INFO\"\n\n    # --- Optional Adapter Keys ---\n    NEXT_PUBLIC_API_KEY: Optional[str] = None # Allow frontend key to be present in .env\n    TVG_API_KEY: Optional[str] = None\n    RACING_AND_SPORTS_TOKEN: Optional[str] = None\n    POINTSBET_API_KEY: Optional[str] = None\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    model_config = {\"env_file\": \".env\", \"case_sensitive\": True}\n\n    @model_validator(mode='after')\n    def process_settings(self) -> 'Settings':\n        \"\"\"\n        This validator runs after the initial settings are loaded from .env and\n        performs two key functions:\n        1. If API_KEY is missing, it falls back to the SecureCredentialsManager.\n        2. It decrypts any fields that were loaded from the .env file.\n        \"\"\"\n        # 1. Fallback for API_KEY\n        if not self.API_KEY:\n            self.API_KEY = SecureCredentialsManager.get_api_key() or \"MISSING\"\n\n        # 2. Decrypt sensitive fields\n        self.BETFAIR_APP_KEY = decrypt_value(self.BETFAIR_APP_KEY)\n        self.BETFAIR_USERNAME = decrypt_value(self.BETFAIR_USERNAME)\n        self.BETFAIR_PASSWORD = decrypt_value(self.BETFAIR_PASSWORD)\n\n        return self\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Loads settings and performs a proactive check for legacy paths.\"\"\"\n    log = structlog.get_logger(__name__)\n    if ENCRYPTION_ENABLED and not KEY_FILE.exists():\n        log.warning(\n            \"encryption_key_not_found\",\n            file=str(KEY_FILE),\n            recommendation=\"Run 'python manage_secrets.py' to generate a key.\",\n        )\n\n    settings = Settings()\n\n    # --- Legacy Path Detection ---\n    legacy_paths = [\"attic/\", \"checkmate_web/\", \"vba_source/\"]\n    for path in legacy_paths:\n        if os.path.exists(path):\n            log.warning(\n                \"legacy_path_detected\",\n                path=path,\n                recommendation=\"This directory is obsolete and should be removed for optimal performance and security.\"\n            )\n\n    return settings\n",
    "python_service/core/__init__.py": "",
    "python_service/core/errors.py": "# python_service/core/errors.py\nfrom enum import Enum\n\n\nclass ErrorCategory(Enum):\n    CONFIGURATION_ERROR = \"Configuration missing or invalid\"\n    NETWORK_ERROR = \"HTTP/Network request failed\"\n    PARSING_ERROR = \"Data parsing or validation unsuccessful\"\n    UNEXPECTED_ERROR = \"An unhandled exception occurred\"\n",
    "python_service/core/exceptions.py": "# python_service/core/exceptions.py\n\"\"\"\nCustom, application-specific exceptions for the Fortuna Faucet service.\n\nThis module defines a hierarchy of exception classes to provide standardized\nerror handling, particularly for the data adapter layer. Using these specific\nexceptions instead of generic ones allows for more precise error handling and\nclearer logging throughout the application.\n\"\"\"\n\nclass FortunaException(Exception):\n    \"\"\"Base class for all custom exceptions in this application.\"\"\"\n    pass\n\nclass AdapterError(FortunaException):\n    \"\"\"Base class for all adapter-related errors.\"\"\"\n    def __init__(self, adapter_name: str, message: str):\n        self.adapter_name = adapter_name\n        super().__init__(f\"[{adapter_name}] {message}\")\n\nclass AdapterRequestError(AdapterError):\n    \"\"\"Raised for general network or request-related issues.\"\"\"\n    pass\n\nclass AdapterHttpError(AdapterRequestError):\n    \"\"\"Raised for unsuccessful HTTP responses (e.g., 4xx or 5xx status codes).\"\"\"\n    def __init__(self, adapter_name: str, status_code: int, url: str):\n        self.status_code = status_code\n        self.url = url\n        message = f\"Received HTTP {status_code} from {url}\"\n        super().__init__(adapter_name, message)\n\nclass AdapterAuthError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 401/403 errors, indicating an auth failure.\"\"\"\n    pass\n\nclass AdapterRateLimitError(AdapterHttpError):\n    \"\"\"Raised specifically for HTTP 429 errors, indicating a rate limit has been hit.\"\"\"\n    pass\n\nclass AdapterTimeoutError(AdapterRequestError):\n    \"\"\"Raised when a request to an external API times out.\"\"\"\n    pass\n\nclass AdapterConnectionError(AdapterRequestError):\n    \"\"\"Raised for DNS lookup failures or refused connections.\"\"\"\n    pass\n\nclass AdapterConfigError(AdapterError):\n    \"\"\"Raised when an adapter is missing necessary configuration (e.g., an API key).\"\"\"\n    pass\n\nclass AdapterParsingError(AdapterError):\n    \"\"\"Raised when an adapter fails to parse the response from an API.\"\"\"\n    pass\n",
    "python_service/credentials_manager.py": "\nimport keyring\n\ntry:\n    import keyring.backends.windows\n    IS_WINDOWS = True\nexcept ImportError:\n    IS_WINDOWS = False\n\nclass SecureCredentialsManager:\n    \"\"\"Store secrets in Windows Credential Manager, not plaintext files\"\"\"\n\n    SERVICE_NAME = \"Fortuna Faucet\"\n\n    @staticmethod\n    def save_api_key(key: str) -> bool:\n        \"\"\"Save API key securely to Windows Credential Manager\"\"\"\n        if not IS_WINDOWS:\n            return False\n        try:\n            keyring.set_password(\n                SecureCredentialsManager.SERVICE_NAME,\n                \"api_key\",\n                key\n            )\n            return True\n        except Exception as e:\n            print(f\"\u274c Failed to save credentials: {e}\")\n            return False\n\n    @staticmethod\n    def get_api_key() -> str:\n        \"\"\"Retrieve API key from Windows Credential Manager\"\"\"\n        if not IS_WINDOWS:\n            return None\n        try:\n            key = keyring.get_password(\n                SecureCredentialsManager.SERVICE_NAME,\n                \"api_key\"\n            )\n            if not key:\n                raise ValueError(\"No stored credentials found\")\n            return key\n        except Exception as e:\n            print(f\"\u274c Failed to retrieve credentials: {e}\")\n            return None\n\n    @staticmethod\n    def delete_all_credentials():\n        \"\"\"Clear stored credentials (for uninstall)\"\"\"\n        if not IS_WINDOWS:\n            return\n        try:\n            keyring.delete_password(\n                SecureCredentialsManager.SERVICE_NAME,\n                \"api_key\"\n            )\n        except Exception:\n            pass\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport inspect\nimport os\nfrom datetime import datetime\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport httpx\nimport structlog\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.brisnet_adapter import BrisnetAdapter\nfrom .adapters.drf_adapter import DRFAdapter\nfrom .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.twinspires_adapter import TwinSpiresAdapter\n\n# from .adapters.pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .cache_manager import cache_async_result\nfrom .health import health_monitor\nfrom .models import AggregatedResponse\nfrom .models import OddsData\nfrom .models import Race\nfrom .models import Runner\nfrom .models_v3 import NormalizedRace\nfrom .notifications import send_toast\n\nlog = structlog.get_logger(__name__)\n\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n        self.logger = structlog.get_logger(__name__)\n        self.logger.info(\"Initializing FortunaEngine...\")\n\n        try:\n            self.config = config or get_settings()\n            self.logger.info(\"Configuration loaded.\")\n\n            self.logger.info(\"Initializing adapters...\")\n\n            build_type = os.getenv('FORTUNA_BUILD_TYPE', 'full')\n            self.logger.info(f\"Fortuna build type: {build_type}\")\n\n            if build_type == 'minimal':\n                self.adapters: List[BaseAdapter] = [\n                    BetfairAdapter(config=self.config),\n                    RacingPostAdapter(config=self.config),\n                    TheRacingApiAdapter(config=self.config),\n                ]\n                self.logger.info(\"Minimal adapter suite initialized.\")\n            else:\n                self.adapters: List[BaseAdapter] = [\n                    BetfairAdapter(config=self.config),\n                    BetfairGreyhoundAdapter(config=self.config),\n                    RacingAndSportsAdapter(config=self.config),\n                    RacingAndSportsGreyhoundAdapter(config=self.config),\n                    AtTheRacesAdapter(config=self.config),\n                    RacingPostAdapter(config=self.config),\n                    HarnessAdapter(config=self.config),\n                    EquibaseAdapter(config=self.config),\n                    SportingLifeAdapter(config=self.config),\n                    TimeformAdapter(config=self.config),\n                    TheRacingApiAdapter(config=self.config),\n                    GbgbApiAdapter(config=self.config),\n                    BetfairDataScientistAdapter(\n                        model_name=\"ThoroughbredModel\",\n                        url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets?id=thoroughbred-model\",\n                        config=self.config\n                    ),\n                    TVGAdapter(config=self.config),\n                    TwinSpiresAdapter(config=self.config),\n                    BrisnetAdapter(config=self.config),\n                    DRFAdapter(config=self.config),\n                ]\n                self.logger.info(\"Full adapter suite initialized.\")\n\n            self.logger.info(\"Adapters initialized.\")\n\n            self.logger.info(\"Initializing HTTP client...\")\n            self.http_limits = httpx.Limits(\n            max_connections=self.config.HTTP_POOL_CONNECTIONS,\n            max_keepalive_connections=self.config.HTTP_MAX_KEEPALIVE,\n            )\n            self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n            self.logger.info(\"HTTP client initialized.\")\n\n            self.logger.info(\"FortunaEngine initialization complete.\")\n\n        except Exception as e:\n            self.logger.critical(\"CRITICAL FAILURE during FortunaEngine initialization.\", exc_info=True)\n            raise e\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps an adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        Handles both modern async adapters and legacy sync adapters.\n        \"\"\"\n        start_time = datetime.now()\n        races: List[Race] = []\n        error_message = None\n        is_success = False\n\n        try:\n            # Check if the adapter's fetch_races method is a modern async function\n            if inspect.iscoroutinefunction(adapter.fetch_races):\n                result = await adapter.fetch_races(date, self.http_client)\n            else:\n                # This is a legacy, synchronous adapter. Run it in a separate thread.\n                self.logger.warning(\n                    \"legacy_sync_adapter_detected\",\n                    adapter=adapter.source_name,\n                    recommendation=\"This adapter should be refactored to be fully asynchronous.\"\n                )\n                result = await asyncio.to_thread(adapter.fetch_races, date, self.http_client)\n\n            # This is the new logic to handle both return types\n            if isinstance(result, list): # New exception-based adapters return a list of races\n                races = result\n                is_success = True\n            elif isinstance(result, dict) and 'races' in result: # Legacy adapters return a dict\n                races = result.get('races', [])\n                is_success = True\n            else:\n                error_message = \"Adapter returned no data or malformed response\"\n\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True\n            )\n            error_message = str(e)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        health_monitor.record_adapter_response(adapter.source_name, success=is_success, duration=duration)\n\n        # Construct a consistent source_info payload regardless of success or failure\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races, reconciles odds, and identifies the favorite.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = f\"{race.venue.upper()}-{race.start_time.strftime('%Y-%m-%d')}-{race.race_number}\"\n\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        existing_runner = runner_map[new_runner.number]\n                        updated_odds = existing_runner.odds.copy()\n                        updated_odds.update(new_runner.odds)\n                        existing_runner.odds = updated_odds\n                    else:\n                        existing_race.runners.append(new_runner)\n                existing_race.source += f\", {race.source}\"\n\n        # --- New Logic: Identify the favorite for each race ---\n        for race in race_map.values():\n            favorite_runner = None\n            lowest_odds = float('inf')\n\n            for runner in race.runners:\n                if runner.scratched:\n                    continue\n\n                # Find the best odds for the current runner across all sources\n                best_runner_odds = float('inf')\n                for odds_data in runner.odds.values():\n                    if odds_data.win is not None and odds_data.win < best_runner_odds:\n                        best_runner_odds = odds_data.win\n\n                if best_runner_odds < lowest_odds:\n                    lowest_odds = best_runner_odds\n                    favorite_runner = runner\n\n            race.favorite = favorite_runner\n\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        if source_filter:\n            self.log.info(\"Bypassing cache for source-specific request\", source=source_filter)\n            return await self._fetch_races_from_sources(date, source_filter=source_filter)\n\n        return await self._get_all_races_cached(date, background_tasks=background_tasks)\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"fortuna_engine_races\")\n    async def _get_all_races_cached(self, date: str, background_tasks: set) -> Dict[str, Any]:\n        \"\"\"This method fetches races for all sources and its result is cached.\"\"\"\n        self.log.info(\"CACHE MISS: Fetching all races from sources.\", date=date)\n        return await self._fetch_races_from_sources(date)\n\n    def _convert_v3_race_to_v2(self, v3_race: NormalizedRace) -> Race:\n        \"\"\"Converts a V3 NormalizedRace object to a V2 Race object.\"\"\"\n        import re\n\n        race_number = 0\n        match = re.search(r\"\\d+\", v3_race.race_name)\n        if match:\n            race_number = int(match.group())\n\n        runners = []\n        for v3_runner in v3_race.runners:\n            odds_data = OddsData(\n                win=Decimal(str(v3_runner.odds_decimal)),\n                source=v3_race.source_ids[0],\n                last_updated=datetime.now(timezone.utc),\n            )\n            runner = Runner(\n                id=v3_runner.runner_id,\n                name=v3_runner.name,\n                number=int(v3_runner.saddle_cloth)\n                if v3_runner.saddle_cloth and v3_runner.saddle_cloth.isdigit()\n                else 99,\n                odds={v3_race.source_ids[0]: odds_data},\n            )\n            runners.append(runner)\n\n        return Race(\n            id=v3_race.race_key,\n            venue=v3_race.track_key,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(v3_race.start_time_iso),\n            runners=runners,\n            source=v3_race.source_ids[0],\n            race_name=v3_race.race_name,\n        )\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"odds_engine_fetch\")\n    async def _fetch_races_from_sources(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"Helper method to contain the logic for fetching and aggregating races.\"\"\"\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        v3_start_time = datetime.now()  # Approximate start time for all V3 adapters\n\n        for result in results:\n            try:\n                if isinstance(result, Exception):\n                    self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                    continue\n\n                # Correctly differentiate between V2 and V3 results\n                if isinstance(result, tuple) and len(result) == 3:  # V2 Adapter Result\n                    adapter_name, adapter_result, duration = result\n                    source_info = adapter_result.get(\"source_info\", {})\n                    source_info[\"fetch_duration\"] = round(duration, 2)\n                    source_infos.append(source_info)\n                    if source_info.get(\"status\") == \"SUCCESS\":\n                        all_races.extend(adapter_result.get(\"races\", []))\n                elif isinstance(result, list) and all(\n                    isinstance(r, NormalizedRace) for r in result\n                ):  # V3 Adapter Result\n                    if result:\n                        v3_races = result\n                        adapter_name = v3_races[0].source_ids[0]\n                        translated_races = [self._translate_v3_race_to_v2(nr) for nr in result]\n                        all_races.extend(translated_races)\n\n                        v3_duration = (datetime.now() - v3_start_time).total_seconds()\n                        source_infos.append(\n                            {\n                                \"name\": adapter_name,\n                                \"status\": \"SUCCESS\",\n                                \"races_fetched\": len(translated_races),\n                                \"error_message\": None,\n                                \"fetch_duration\": round(v3_duration, 2),\n                            }\n                        )\n            except Exception:\n                self.log.error(\"Failed to process result from an adapter.\", exc_info=True)\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n            },\n        )\n\n        # --- Add Success Notification ---\n        send_toast(\n            \"Fortuna Faucet Data Refresh\",\n            f\"Successfully fetched {len(deduped_races)} races from {len(source_infos)} sources.\"\n        )\n\n        return response_obj.model_dump()\n\n    def _translate_v3_race_to_v2(self, norm_race: NormalizedRace) -> Race:\n        \"\"\"Translates a V3 NormalizedRace into a V2 Race object.\"\"\"\n        import re\n\n        race_number = 0\n        match = re.search(r\"R(\\d+)\", norm_race.race_name)\n        if match:\n            race_number = int(match.group(1))\n\n        runners = []\n        for norm_runner in norm_race.runners:\n            adapter_name = norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\"\n            odds_data = OddsData(\n                win=Decimal(str(norm_runner.odds_decimal)), source=adapter_name, last_updated=datetime.now(timezone.utc)\n            )\n\n            try:\n                runner_number = int(norm_runner.saddle_cloth)\n            except (ValueError, TypeError):\n                runner_number = None\n\n            runner = Runner(\n                id=norm_runner.runner_id, name=norm_runner.name, number=runner_number, odds={adapter_name: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=norm_race.race_key,\n            venue=norm_race.track_key,\n            start_time=datetime.fromisoformat(norm_race.start_time_iso),\n            race_number=race_number,\n            runners=runners,\n            source=norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\",\n            # Store extra V3 data in metadata for future use\n            metadata={\"v3_race_name\": norm_race.race_name},\n        )\n",
    "python_service/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return None, \"Missing core fields (id, venue, race_number, start_time, runners)\"\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\")\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "python_service/fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport structlog\n\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.config import get_settings\nfrom python_service.engine import FortunaEngine\nfrom python_service.etl import run_etl_for_yesterday\nfrom python_service.models import Race\n\nlog = structlog.get_logger(__name__)\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = FortunaEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n        try:\n            background_tasks = set() # Create a dummy set for background tasks\n            aggregated_data = await self.odds_engine.get_races(today_str, background_tasks)\n            all_races = aggregated_data.get('races', [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer('trifecta')\n            qualified_races = analyzer.qualify_races(all_races) # This now returns a sorted list with scores\n            log.info(\"Watchman: Initial target acquisition and ranking complete\", target_count=len(qualified_races))\n\n            # Log the top targets for better observability\n            for race in qualified_races[:5]:\n                log.info(\"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat()\n                )\n            return qualified_races\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        # active_targets = list(targets)\n\n        # from python_service.adapters.betfair_adapter import BetfairAdapter\n        # async with LiveOddsMonitor(betfair_adapter=BetfairAdapter(config=self.settings)) as live_monitor:\n        #     async with httpx.AsyncClient() as client:\n        #         while active_targets:\n        #             now = datetime.now(timezone.utc)\n\n        #             # Find races that are within the 5-minute monitoring window\n        #             races_to_monitor = [\n        #                 r\n        #                 for r in active_targets\n        #                 if r.start_time.replace(tzinfo=timezone.utc) > now\n        #                 and r.start_time.replace(tzinfo=timezone.utc)\n        #                 < now + timedelta(minutes=5)\n        #             ]\n\n        #             if races_to_monitor:\n        #                 for race in races_to_monitor:\n        #                     log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n        #                         race_id=race.id,\n        #                         venue=race.venue,\n        #                         score=race.qualification_score\n        #                     )\n        #                     updated_race = await live_monitor.monitor_race(race, client)\n        #                     log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n        #                     # Remove from target list to prevent re-monitoring\n        #                     active_targets = [t for t in active_targets if t.id != race.id]\n\n        #             if not active_targets:\n        #                 break # Exit loop if all targets are processed\n\n        #             await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        try:\n            initial_targets = await self.get_initial_targets()\n            if initial_targets:\n                await self.run_tactical_monitoring(initial_targets)\n            else:\n                log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n        finally:\n            await self.odds_engine.close()\n\n        # Run ETL for yesterday's data after all other operations are complete\n        try:\n            log.info(\"Starting daily ETL process for Scribe's Archives...\")\n            run_etl_for_yesterday()\n            log.info(\"Daily ETL process completed successfully.\")\n        except Exception:\n            log.error(\"Daily ETL process failed.\", exc_info=True)\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\nasync def main():\n    from python_service.logging_config import configure_logging\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "python_service/health.py": "# python_service/health.py\nfrom datetime import datetime\nfrom typing import Dict\nfrom typing import List\n\nimport psutil\nimport structlog\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\nlog = structlog.get_logger(__name__)\n\n\nclass HealthMonitor:\n    def __init__(self):\n        self.adapter_health: Dict[str, Dict] = {}\n        self.system_metrics: List[Dict] = []\n        self.max_metrics_history = 100\n\n    def record_adapter_response(self, adapter_name: str, success: bool, duration: float):\n        if adapter_name not in self.adapter_health:\n            self.adapter_health[adapter_name] = {\n                \"total_requests\": 0,\n                \"successful_requests\": 0,\n                \"failed_requests\": 0,\n                \"avg_response_time\": 0.0,\n                \"last_success\": None,\n                \"last_failure\": None,\n            }\n\n        health = self.adapter_health[adapter_name]\n        health[\"total_requests\"] += 1\n\n        if success:\n            health[\"successful_requests\"] += 1\n            health[\"last_success\"] = datetime.now().isoformat()\n        else:\n            health[\"failed_requests\"] += 1\n            health[\"last_failure\"] = datetime.now().isoformat()\n\n        health[\"avg_response_time\"] = (\n            health[\"avg_response_time\"] * (health[\"total_requests\"] - 1) + duration\n        ) / health[\"total_requests\"]\n\n    def get_system_metrics(self) -> Dict:\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage(\"/\")\n\n        metrics = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_available_gb\": round(memory.available / (1024**3), 2),\n            \"disk_percent\": disk.percent,\n            \"disk_free_gb\": round(disk.free / (1024**3), 2),\n        }\n\n        self.system_metrics.append(metrics)\n        if len(self.system_metrics) > self.max_metrics_history:\n            self.system_metrics.pop(0)\n\n        return metrics\n\n    def get_health_report(self) -> Dict:\n        system_metrics = self.get_system_metrics()\n        return {\n            \"status\": \"healthy\" if self.is_system_healthy() else \"degraded\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"system\": system_metrics,\n            \"adapters\": self.adapter_health,\n            \"metrics_history\": self.system_metrics[-10:],\n        }\n\n    def is_system_healthy(self) -> bool:\n        if not self.system_metrics:\n            return True\n        latest = self.system_metrics[-1]\n        return latest[\"cpu_percent\"] < 80 and latest[\"memory_percent\"] < 85 and latest[\"disk_percent\"] < 90\n\n\n# Global instance for the application to use\nhealth_monitor = HealthMonitor()\n\n\n@router.get(\"/health/detailed\", tags=[\"Health\"])\nasync def get_detailed_health():\n    \"\"\"Provides a comprehensive health check of the system.\"\"\"\n    return health_monitor.get_health_report()\n\n\n@router.get(\"/health\", tags=[\"Health\"])\nasync def get_basic_health():\n    \"\"\"Provides a basic health check for load balancers and uptime monitoring.\"\"\"\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n",
    "python_service/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "python_service/middleware/__init__.py": "",
    "python_service/middleware/error_handler.py": "# python_service/middleware/error_handler.py\n\nfrom fastapi import Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\n\n\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"Convert Pydantic validation errors to user-friendly messages.\"\"\"\n    return JSONResponse(\n        status_code=422,\n        content={\n            \"detail\": \"Invalid request parameters\",\n            \"errors\": [\n                {\n                    \"field\": error[\"loc\"][-1] if error[\"loc\"] else \"unknown\",\n                    \"message\": error[\"msg\"],\n                    \"type\": error[\"type\"],\n                }\n                for error in exc.errors()\n            ],\n        },\n    )\n",
    "python_service/minimal_service.py": "# python_service/minimal_service.py\n# This is the minimal, sanctioned Flask backend for Checkmate Solo.\n\nimport random\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom flask import Flask\nfrom flask import jsonify\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n# This enables CORS for all domains on all routes.\n# For a production environment, you would want to restrict this\n# to the domain of your frontend application.\nCORS(app)\n\n\ndef generate_mock_race(race_id: int):\n    \"\"\"Generates a single mock race with randomized data, mirroring the frontend's mock generator.\"\"\"\n    tracks = [\"Belmont Park\", \"Churchill Downs\", \"Santa Anita\", \"Keeneland\", \"Del Mar\"]\n    horses = [\n        \"Thunder Strike\",\n        \"Lightning Bolt\",\n        \"Swift Arrow\",\n        \"Golden Dream\",\n        \"Storm Chaser\",\n        \"Midnight Runner\",\n        \"Royal Flash\",\n        \"Desert Wind\",\n    ]\n\n    race_horses = []\n    for i in range(8):\n        betfair = round(2 + random.random() * 15, 2)\n        pointsbet = round(betfair * (0.9 + random.random() * 0.2), 2)\n        tvg = round(betfair * (0.85 + random.random() * 0.3), 2)\n\n        odds_values = {\"Betfair\": betfair, \"PointsBet\": pointsbet, \"TVG\": tvg}\n        best_source = min(odds_values, key=odds_values.get)\n        best_odds = odds_values[best_source]\n\n        avg_odds = (betfair + pointsbet + tvg) / 3\n        value_score = ((avg_odds - best_odds) / best_odds) * 100 if best_odds > 0 else 0\n\n        race_horses.append(\n            {\n                \"number\": i + 1,\n                \"name\": random.choice(horses),\n                \"odds\": {\n                    \"betfair\": f\"{betfair:.2f}\",\n                    \"pointsbet\": f\"{pointsbet:.2f}\",\n                    \"tvg\": f\"{tvg:.2f}\",\n                    \"best\": f\"{best_odds:.2f}\",\n                    \"best_source\": best_source,\n                },\n                \"value_score\": f\"{value_score:.1f}\",\n                \"trend\": random.choice([\"up\", \"down\"]),\n            }\n        )\n\n    race_horses.sort(key=lambda x: float(x[\"value_score\"]), reverse=True)\n\n    return {\n        \"id\": race_id,\n        \"track\": random.choice(tracks),\n        \"race_number\": random.randint(1, 10),\n        \"post_time\": (datetime.now() + timedelta(minutes=random.randint(5, 120))).isoformat(),\n        \"horses\": race_horses,\n    }\n\n\n@app.route(\"/api/races/live\", methods=[\"GET\"])\ndef get_live_races():\n    \"\"\"\n    This endpoint provides a list of live mock race data for the frontend.\n    It mimics the data structure the CheckmateSolo component expects.\n    \"\"\"\n    mock_races = [generate_mock_race(i) for i in range(5)]\n    return jsonify(mock_races)\n\n\nif __name__ == \"__main__\":\n    # The frontend component's TODO comment specifies port 8000.\n    print(\"Starting Checkmate Solo minimal backend service...\")\n    print(\"API available at http://localhost:8000/api/races/live\")\n    app.run(host=\"0.0.0.0\", port=8000, debug=False)\n",
    "python_service/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    class Config:\n        populate_by_name = True\n        arbitrary_types_allowed = True\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[Decimal] = None\n    place: Optional[Decimal] = None\n    show: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    field_size: Optional[int] = None\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n",
    "python_service/models_v3.py": "# python_service/models_v3.py\n# Defines the data structures for the V3 adapter architecture.\n\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import List\n\n\n@dataclass\nclass NormalizedRunner:\n    runner_id: str\n    name: str\n    saddle_cloth: str\n    odds_decimal: float\n\n\n@dataclass\nclass NormalizedRace:\n    race_key: str\n    track_key: str\n    start_time_iso: str\n    race_name: str\n    runners: List[NormalizedRunner] = field(default_factory=list)\n    source_ids: List[str] = field(default_factory=list)\n",
    "python_service/notifications.py": "# python_service/notifications.py\n\nimport sys\n\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\ndef send_toast(title: str, message: str):\n    \"\"\"\n    Sends a desktop notification. This function is platform-aware and will only\n    attempt to send a toast on Windows. On other operating systems, it will\n    log the notification content.\n    \"\"\"\n    if sys.platform == \"win32\":\n        try:\n            from windows_toasts import Toast\n            from windows_toasts import WindowsToaster\n            toaster = WindowsToaster(title)\n            new_toast = Toast()\n            new_toast.text_fields = [message]\n            toaster.show_toast(new_toast)\n            log.info(\"Sent Windows toast notification.\", title=title, message=message)\n        except ImportError:\n            log.warning(\"windows_toasts library not found, skipping notification.\",\n                        recommendation=\"Install with: pip install windows-toasts\")\n        except Exception:\n            log.error(\"Failed to send Windows toast notification.\", exc_info=True)\n    else:\n        log.info(\"Skipping toast notification on non-Windows platform.\",\n                   platform=sys.platform, title=title, message=message)\n",
    "python_service/requirements.txt": "# Fortuna Faucet - Python Dependencies (Windows Optimized v2)\n# This is the 'golden' manifest, combining the blueprint's pinned versions with all feature requirements.\n\n# --- Core Backend (FastAPI & Async) ---\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\n\n# --- HTTP & Web Scraping ---\nhttpx==0.25.1\ntenacity==8.2.3\nrequests\n\n# --- HTML & Data Parsing ---\nselectolax\n\n# --- Logging & Configuration ---\nstructlog==23.2.0\npython-dotenv==1.0.0\n\n# --- Caching ---\nredis==5.0.1\nslowapi==0.1.9\n\n# --- Database & ETL ---\nSQLAlchemy\npsycopg2-binary\n\n# --- Monitoring & GUI ---\nmatplotlib==3.8.2\n\n# --- UI & Visualization (for utility scripts) ---\nrich\nstreamlit\npikepdf\ntabula-py\n\n# --- Windows Specific (DO NOT REMOVE MARKERS) ---\npsutil==5.9.6; sys_platform == 'win32'\npywin32==306; sys_platform == 'win32'\nwindows-toasts; sys_platform == 'win32'\n\n# --- Testing ---\npytest==7.4.3\npytest-asyncio==0.21.1\n\n# --- Development ---\nblack==23.11.0\ncryptography",
    "python_service/requirements_minimal.txt": "httpx==0.25.0\nstructlog==23.2.0\npydantic==2.5.0\nuvicorn==0.24.0\nfastapi==0.104.1\ntenacity==8.2.3\n",
    "python_service/run_api.py": "# python_service/run_api.py\n\nimport uvicorn\n\n\ndef main():\n    # This entry point is for the packaged application\n    uvicorn.run(\"python_service.api:app\", host=\"127.0.0.1\", port=8000, reload=False)\n\nif __name__ == \"__main__\":\n    main()\n",
    "python_service/security.py": "# python_service/security.py\n\nimport secrets\n\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Security\nfrom fastapi import status\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings\nfrom .config import get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\n\nasync def verify_api_key(key: str = Security(api_key_header), settings: Settings = Depends(get_settings)):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n    \"\"\"\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid or missing API Key\")\n",
    "python_service/setup_wizard_gui.py": "import tkinter as tk\nfrom tkinter import messagebox\n\n\nclass FortunaSetupWizard(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"\ud83d\udc34 Fortuna Faucet - First Time Setup\")\n        self.geometry(\"700x500\")\n        self.configure(bg='#1a1a2e')\n\n        # Step tracking\n        self.current_step = 0\n        self.settings = {}\n\n        self._create_widgets()\n\n    def _create_widgets(self):\n        \"\"\"Create multi-step wizard UI\"\"\"\n        # Header\n        header = tk.Label(\n            self,\n            text=\"Welcome to Fortuna Faucet\",\n            font=(\"Segoe UI\", 18, \"bold\"),\n            bg='#1a1a2e',\n            fg='#00ff88'\n        )\n        header.pack(pady=20)\n\n        # Step indicator\n        self.step_label = tk.Label(\n            self,\n            text=\"Step 1 of 4: Generate API Key\",\n            font=(\"Segoe UI\", 11),\n            bg='#1a1a2e',\n            fg='#ffffff'\n        )\n        self.step_label.pack(pady=10)\n\n        # Content frame (will be updated for each step)\n        self.content_frame = tk.Frame(self, bg='#1a1a2e')\n        self.content_frame.pack(fill=tk.BOTH, expand=True, padx=30, pady=20)\n\n        # Buttons\n        button_frame = tk.Frame(self, bg='#1a1a2e')\n        button_frame.pack(fill=tk.X, padx=30, pady=20)\n\n        self.prev_btn = tk.Button(\n            button_frame,\n            text=\"< Back\",\n            command=self.previous_step,\n            state=tk.DISABLED,\n            bg='#404060',\n            fg='#ffffff',\n            padx=20\n        )\n        self.prev_btn.pack(side=tk.LEFT)\n\n        self.next_btn = tk.Button(\n            button_frame,\n            text=\"Next >\",\n            command=self.next_step,\n            bg='#00ff88',\n            fg='#000000',\n            font=(\"Segoe UI\", 11, \"bold\"),\n            padx=20\n        )\n        self.next_btn.pack(side=tk.RIGHT)\n\n        # Show step 1\n        self.show_step_1()\n\n    def show_step_1(self):\n        \"\"\"Generate API Key\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83d\udd10 Secure API Key\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg='#1a1a2e',\n            fg='#ffffff'\n        ).pack(anchor=\"w\", pady=(0, 10))\n\n        tk.Label(\n            self.content_frame,\n            text=(\n                \"A secure API key will be generated and stored in Windows Credential Manager.\\n\"\n                \"No file will contain your secrets.\"\n            ),\n            wraplength=600,\n            justify=tk.LEFT,\n            bg='#1a1a2e',\n            fg='#cccccc'\n        ).pack(anchor=\"w\", pady=10)\n\n        self.api_key_display = tk.Entry(\n            self.content_frame,\n            font=(\"Courier\", 10),\n            width=60,\n            state=tk.DISABLED\n        )\n        self.api_key_display.pack(pady=10, fill=tk.X)\n\n        gen_btn = tk.Button(\n            self.content_frame,\n            text=\"\ud83d\udd04 Generate New Key\",\n            command=self.generate_api_key,\n            bg='#0f6cbd',\n            fg='#ffffff'\n        )\n        gen_btn.pack(pady=10)\n\n        self.current_step = 0\n        self.update_buttons()\n\n    def show_step_2(self):\n        \"\"\"Betfair Configuration\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83c\udfc7 Betfair Exchange (Optional)\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg='#1a1a2e',\n            fg='#ffffff'\n        ).pack(anchor=\"w\", pady=(0, 10))\n\n        tk.Label(\n            self.content_frame,\n            text=\"Optional: Add Betfair credentials for live odds monitoring.\\nLeave blank to skip.\",\n            bg='#1a1a2e',\n            fg='#cccccc'\n        ).pack(anchor=\"w\", pady=10)\n\n        # Betfair form\n        tk.Label(self.content_frame, text=\"App Key:\", bg='#1a1a2e', fg='#ffffff').pack(anchor=\"w\")\n        self.betfair_appkey = tk.Entry(self.content_frame, width=60, show=\"*\")\n        self.betfair_appkey.pack(fill=tk.X, pady=(0, 10))\n\n        tk.Label(self.content_frame, text=\"Username:\", bg='#1a1a2e', fg='#ffffff').pack(anchor=\"w\")\n        self.betfair_user = tk.Entry(self.content_frame, width=60)\n        self.betfair_user.pack(fill=tk.X, pady=(0, 10))\n\n        tk.Label(self.content_frame, text=\"Password:\", bg='#1a1a2e', fg='#ffffff').pack(anchor=\"w\")\n        self.betfair_pass = tk.Entry(self.content_frame, width=60, show=\"*\")\n        self.betfair_pass.pack(fill=tk.X, pady=(0, 10))\n\n        # Test connection button\n        test_btn = tk.Button(\n            self.content_frame,\n            text=\"\ud83e\uddea Test Connection\",\n            command=self.test_betfair_connection,\n            bg='#0f6cbd',\n            fg='#ffffff'\n        )\n        test_btn.pack(pady=10)\n\n        self.current_step = 1\n        self.update_buttons()\n\n    def show_step_3(self):\n        \"\"\"Verify Installation\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\u2713 Verifying Setup\",\n            font=(\"Segoe UI\", 12, \"bold\"),\n            bg='#1a1a2e',\n            fg='#00ff88'\n        ).pack(anchor=\"w\", pady=(0, 20))\n\n        checks = [\n            (\"Python 3.11+\", self.verify_python),\n            (\"Node.js installed\", self.verify_nodejs),\n            (\"pip packages ready\", self.verify_pip),\n            (\"npm packages ready\", self.verify_npm),\n        ]\n\n        for check_name, check_func in checks:\n            result = check_func()\n            status = \"\u2705\" if result else \"\u274c\"\n            color = \"#00ff88\" if result else \"#ff4444\"\n\n            label = tk.Label(\n                self.content_frame,\n                text=f\"{status} {check_name}\",\n                bg='#1a1a2e',\n                fg=color\n            )\n            label.pack(anchor=\"w\", pady=5)\n\n        self.current_step = 2\n        self.update_buttons()\n\n    def show_step_4(self):\n        \"\"\"Complete\"\"\"\n        self._clear_content()\n\n        tk.Label(\n            self.content_frame,\n            text=\"\ud83c\udf89 Setup Complete!\",\n            font=(\"Segoe UI\", 14, \"bold\"),\n            bg='#1a1a2e',\n            fg='#00ff88'\n        ).pack(pady=20)\n\n        tk.Label(\n            self.content_frame,\n            text=\"Fortuna Faucet is ready to launch.\\n\\nClick 'Finish' to start the application.\",\n            wraplength=600,\n            bg='#1a1a2e',\n            fg='#ffffff'\n        ).pack(pady=20)\n\n        self.current_step = 3\n        self.next_btn.config(text=\"\u2713 Finish\", command=self.finish_setup)\n        self.update_buttons()\n\n    def _clear_content(self):\n        \"\"\"Clear content frame\"\"\"\n        for widget in self.content_frame.winfo_children():\n            widget.destroy()\n\n    def update_buttons(self):\n        \"\"\"Enable/disable navigation buttons\"\"\"\n        self.prev_btn.config(state=tk.NORMAL if self.current_step > 0 else tk.DISABLED)\n        step_titles = [\"API Key\", \"Betfair (Optional)\", \"Verification\", \"Complete\"]\n        self.step_label.config(\n            text=f\"Step {self.current_step + 1} of 4: {step_titles[self.current_step]}\"\n        )\n\n    def next_step(self):\n        if self.current_step == 0:\n            self.show_step_2()\n        elif self.current_step == 1:\n            self.show_step_3()\n        elif self.current_step == 2:\n            self.show_step_4()\n\n    def previous_step(self):\n        if self.current_step == 3:\n            self.show_step_3()\n        elif self.current_step == 2:\n            self.show_step_2()\n        elif self.current_step == 1:\n            self.show_step_1()\n\n    def generate_api_key(self):\n        import secrets\n        key = secrets.token_urlsafe(32)\n        self.settings['api_key'] = key\n        self.api_key_display.config(state=tk.NORMAL)\n        self.api_key_display.delete(0, tk.END)\n        self.api_key_display.insert(0, key)\n        self.api_key_display.config(state=tk.DISABLED)\n        messagebox.showinfo(\"Success\", \"API Key generated and stored securely!\")\n\n    def test_betfair_connection(self):\n        # Test credentials\n        messagebox.showinfo(\"Testing...\", \"Attempting to connect to Betfair...\")\n        # Actual implementation would test the API\n\n    def verify_python(self) -> bool:\n        # Check Python version\n        return True\n\n    def verify_nodejs(self) -> bool:\n        # Check Node.js installation\n        return True\n\n    def verify_pip(self) -> bool:\n        # Verify pip packages\n        return True\n\n    def verify_npm(self) -> bool:\n        # Verify npm packages\n        return True\n\n    def finish_setup(self):\n        # Save credentials to Windows Credential Manager\n        # Launch Fortuna\n        self.destroy()\n\nif __name__ == \"__main__\":\n    app = FortunaSetupWizard()\n    app.mainloop()\n",
    "python_service/utils/__init__.py": "",
    "python_service/utils/odds.py": "# Centralized odds parsing utility, created by Operation: The A+ Trifecta\nfrom decimal import Decimal\nfrom decimal import InvalidOperation\nfrom typing import Optional\nfrom typing import Union\n\n\ndef parse_odds_to_decimal(odds: Union[str, int, float, None]) -> Optional[Decimal]:\n    \"\"\"\n    Parse various odds formats to Decimal for precise financial calculations.\n    Handles fractional, decimal, and special cases ('EVS', 'SP', etc.).\n    Returns None for unparseable or invalid values.\n    \"\"\"\n    if odds is None:\n        return None\n\n    if isinstance(odds, (int, float)):\n        return Decimal(str(odds))\n\n    odds_str = str(odds).strip().upper()\n\n    SPECIAL_CASES = {\n        \"EVS\": Decimal(\"2.0\"),\n        \"EVENS\": Decimal(\"2.0\"),\n        \"SP\": None,\n        \"SCRATCHED\": None,\n        \"SCR\": None,\n        \"\": None,\n    }\n\n    if odds_str in SPECIAL_CASES:\n        return SPECIAL_CASES[odds_str]\n\n    if \"/\" in odds_str:\n        try:\n            parts = odds_str.split(\"/\")\n            if len(parts) != 2:\n                return None\n            num, den = map(Decimal, parts)\n            if den <= 0:\n                return None\n            return Decimal(\"1.0\") + (num / den)\n        except (ValueError, InvalidOperation):\n            return None\n\n    try:\n        return Decimal(odds_str)\n    except (ValueError, InvalidOperation):\n        return None\n",
    "python_service/utils/text.py": "# python_service/utils/text.py\n# Centralized text and name normalization utilities\nimport re\nfrom typing import Optional\n\n\ndef clean_text(text: Optional[str]) -> Optional[str]:\n    \"\"\"Strips leading/trailing whitespace and collapses internal whitespace.\"\"\"\n    if not text:\n        return None\n    return \" \".join(text.strip().split())\n\n\ndef normalize_venue_name(name: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Normalizes a UK or Irish racecourse name to a standard format.\n    Handles common abbreviations and variations.\n    \"\"\"\n    if not name:\n        return None\n\n    # Use a temporary variable for matching, but return the properly cased name\n    cleaned_name_upper = clean_text(name).upper()\n\n    VENUE_MAP = {\n        \"ASCOT\": \"Ascot\",\n        \"AYR\": \"Ayr\",\n        \"BANGOR-ON-DEE\": \"Bangor-on-Dee\",\n        \"CATTERICK BRIDGE\": \"Catterick\",\n        \"CHELMSFORD CITY\": \"Chelmsford\",\n        \"EPSOM DOWNS\": \"Epsom\",\n        \"FONTWELL\": \"Fontwell Park\",\n        \"HAYDOCK\": \"Haydock Park\",\n        \"KEMPTON\": \"Kempton Park\",\n        \"LINGFIELD\": \"Lingfield Park\",\n        \"NEWMARKET (ROWLEY)\": \"Newmarket\",\n        \"NEWMARKET (JULY)\": \"Newmarket\",\n        \"SANDOWN\": \"Sandown Park\",\n        \"STRATFORD\": \"Stratford-on-Avon\",\n        \"YARMOUTH\": \"Great Yarmouth\",\n        \"CURRAGH\": \"Curragh\",\n        \"DOWN ROYAL\": \"Down Royal\",\n    }\n\n    # Check primary map first\n    if cleaned_name_upper in VENUE_MAP:\n        return VENUE_MAP[cleaned_name_upper]\n\n    # Handle cases where the key is the desired output but needs to be mapped from a variation\n    # e.g. CHELMSFORD maps to Chelmsford\n    # Title case the cleaned name for a sensible default\n    title_cased_name = clean_text(name).title()\n    if title_cased_name in VENUE_MAP.values():\n        return title_cased_name\n\n    # Return the title-cased cleaned name as a fallback\n    return title_cased_name\n\n\ndef normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n",
    "python_service/windows_service_wrapper.py": "# windows_service_wrapper.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Add the service's directory to the Python path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom checkmate_service import CheckmateBackgroundService\n\n\nclass CheckmateWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"CheckmateV8Service\"\n    _svc_display_name_ = \"Checkmate V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.checkmate_service = CheckmateBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO, format=\"%(name)s - %(levelname)s - %(message)s\", handlers=[servicemanager.LogHandler()]\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.checkmate_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE, servicemanager.PYS_SERVICE_STARTED, (self._svc_name_, \"\")\n        )\n        self.main()\n\n    def main(self):\n        self.checkmate_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(CheckmateWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(CheckmateWindowsService)\n"
}