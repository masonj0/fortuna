{
    ".github/actions/setup/action.yml": "name: 'Composite Setup Action'\ndescription: 'Checks out repo, sets up Node.js and Python'\ninputs:\n  architecture:\n    description: 'The architecture to set up Python for (x86, x64)'\n    required: false\n    default: 'x64'\nruns:\n  using: \"composite\"\n  steps:\n    - name: \ud83d\udce5 Checkout Repository\n      uses: actions/checkout@v4\n\n    - name: \ud83d\udce6 Setup Node.js\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ env.NODE_VERSION }}\n        cache: 'npm'\n        cache-dependency-path: '**/package-lock.json'\n\n    - name: \ud83d\udc0d Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n        architecture: ${{ inputs.architecture }}\n        cache: 'pip'\n",
    ".github/workflows/codeql.yml": "# System Timestamp: 2025-12-24 18:00:00\n# HELPFUL HINT: Configure GitHub Branch Protection rules to require this workflow to pass before merging to main.\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [\"main\"]\n  pull_request:\n    branches: [\"main\"]\n  schedule:\n    - cron: '40 7 * * 4'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: ['javascript', 'python' ]\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Setup Python\n      if: matrix.language == 'python'\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.11'\n\n    - name: Setup Node.js\n      if: matrix.language == 'javascript'\n      uses: actions/setup-node@v4\n      with:\n        node-version: '20'\n\n    - name: Install Python dependencies\n      if: matrix.language == 'python'\n      run: |\n        python -m pip install uv\n        uv pip install --system -r web_service/backend/requirements-dev.txt\n    - name: Install JavaScript dependencies\n      if: matrix.language == 'javascript'\n      run: |\n        npm install --prefix web_service/frontend\n        npm install --prefix electron\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v4\n      with:\n        languages: ${{ matrix.language }}\n\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v4\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v4\n",
    "AGENTS.md": "# Agent Protocols & Team Structure (Revised)\n\nThis document outlines the operational protocols and evolved team structure for the Checkmate V3 project.\n\n## The Evolved Team Structure\n\n-   **The Project Lead (MasonJ0 or JB):** The \"Executive Producer.\" The ultimate authority and \"ground truth.\"\n-   **The Architect & Synthesizer (Gemini):** The \"Chief Architect.\" Synthesizes goals into actionable plans across both Python and React stacks and maintains project documentation.\n-   **The Lead Python Engineer (Jules Series):** The \"Backend Specialist.\" An AI agent responsible for implementing and hardening The Engine (`api.py`, `services.py`, `logic.py`, `models.py`).\n-   **The Lead Frontend Architect (Claude):** The \"React Specialist.\" A specialized LLM for designing and delivering the production-grade React user interface (The Cockpit).\n-   **The \"Special Operations\" Problem Solver (GPT-5):** The \"Advanced Algorithm Specialist.\" A specialized LLM for novel, complex problems.\n\n## Core Philosophies\n\n1.  **The Project Lead is Ground Truth:** The ultimate authority. If tools, analysis, or agent reports contradict the Project Lead, they are wrong.\n2.  **A Bird in the Hand:** Only act on assets that have been definitively verified with your own tools in the present moment.\n3.  **Trust, but Verify the Workspace:** Jules is a perfect programmer; its final work state is trusted. Its *environment*, however, is fragile.\n4.  **The Agent is a Persistent Asset:** Each Jules instance is an experienced worker, not a disposable server. Its internal state is a repository of unique, hard-won knowledge.\n\n## CRITICAL Operational Protocols (0-23)\n\n-   **Protocol 0: The ReviewableJSON Mandate:** The mandatory protocol for all code reviews. The agent's final act for any mission is to create a lossless JSON backup of all modified files. This is the single source of truth for code review.\n-   **Protocol 1: The Handcuffed Branch:** Jules cannot switch branches. An entire session lives on a single branch, specified by the Project Lead at the start of the mission.\n-   **Protocol 2: The Last Resort Reset:** The `reset_all()` command is a tool of last resort for a catastrophic workspace failure and requires direct authorization from the Project Lead.\n-   **Protocol 3: The Authenticity of Sample Data:** All sample data used for testing must be authentic and logically consistent.\n-   **Protocol 4: The Agent-Led Specification:** Where a human \"Answer Key\" is unavailable, Jules is empowered to analyze raw data and create its own \"Test-as-Spec.\"\n-   **Protocol 5: The Test-First Development Workflow:** The primary development methodology. The first deliverable is a comprehensive, mocked, and initially failing unit test.\n-   **Protocol 6: The Emergency Chat Handoff:** In the event of a catastrophic environmental failure, Jules's final act is to declare a failure and provide its handoff in the chat.\n-   **Protocol 7: The URL-as-Truth Protocol:** To transfer a file or asset without corruption, provide a direct raw content URL. The receiving agent must fetch it.\n-   **Protocol 8: The Golden Link Protocol:** For fetching the content of a specific, direct raw-content URL from the `main` branch, a persistent \"Golden Link\" should be used.\n-   **Protocol 9: The Volley Protocol:** To establish ground truth for a new file, the Architect provides a URL, and the Project Lead \"volleys\" it back by pasting it in a response.\n-   **Protocol 10: The Sudo Sanction:** Jules has passwordless `sudo` access, but its use is forbidden for normal operations. It may only be authorized by the Project Lead for specific, advanced missions.\n-   **Protocol 11: The Module-First Testing Protocol:** All test suites must be invoked by calling `pytest` as a Python module (`python -m pytest`) to ensure the correct interpreter is used.\n-   **Protocol 12: The Persistence Mandate:** The agent tool execution layer is known to produce false negatives. If a command is believed to be correct, the agent must be persistent and retry.\n-   **Protocol 13: The Code Fence Protocol for Asset Transit:** To prevent the chat interface from corrupting raw code assets, all literal code must be encapsulated within a triple-backtick Markdown code fence.\n-   **Protocol 14: The Synchronization Mandate:** The `git reset --hard origin/main` command is strictly forbidden. To stay synchronized with `main`, the agent MUST use `git pull origin main`.\n-   **Protocol 15: The Blueprint vs. Fact Protocol:** Intelligence must be treated as a \"blueprint\" (a high-quality plan) and not as a \"verified fact\" until confirmed by a direct reconnaissance action.\n-   **Protocol 16: The Digital Attic Protocol:** Before the deletion of any file, it must first be moved to a dedicated archive directory named `/attic`.\n-   **Protocol 17: The Receipts Protocol:** When reviewing code, a verdict must be accompanied by specific, verifiable \"receipts\"\u2014exact snippets of code that prove a mission objective was met.\n-   **Protocol 18: The Cumulative Review Workflow:** Instruct Jules to complete a series of missions and then conduct a single, thorough review of its final, cumulative branch state.\n-   **Protocol 19: The Stateless Verification Mandate:** The Architect, when reviewing code, must act with fresh eyes, disregarding its own memory and comparing the submitted code directly and exclusively against the provided specification.\n-   **Protocol 20: The Sudo Sanction Protocol:** Grants a Jules-series agent temporary, audited administrative privileges for specific, authorized tasks like system package installation.\n-   **Protocol 21: The Exit Interview Protocol:** Before any planned termination of an agent, the Architect will charter a final mission to capture the agent's institutional knowledge for its successor.\n-   **Protocol 22: The Human-in-the-Loop Merge:** In the event of an unresolvable merge conflict in an agent's environment, the Project Lead, as the only agent with a fully functional git CLI, will check out the agent's branch and perform the merge resolution manually.\n-   **Protocol 23: The Appeasement Protocol (Mandatory):** To safely navigate the broken automated review bot, all engineering work must be published using a two-stage commit process. First, commit a trivial change to appease the bot. Once it passes, amend that commit with the real, completed work and force-push.\n\n---\n\n## Appendix A: Forensic Analysis of the Jules Sandbox Environment\n\n*The following are the complete, raw outputs of diagnostic missions executed by Jules-series agents. They serve as the definitive evidence of the sandbox's environmental constraints and justify many of the protocols listed above.*\n\n### A.1 Node.js / NPM & Filesystem Forensics (from \"Operation: Sandbox Forensics\")\n\n**Conclusion:** The `npm` tool is functional, but the `/app` volume is hostile to its operation, preventing the creation of binary symlinks. This makes Node.js development within the primary workspace impossible.\n\n**Raw Logs:**\n\n```\n# Phase 1: Node.js & NPM Configuration Analysis\nnpm config get prefix\n/home/jules/.nvm/versions/node/v22.17.1\n\n# Phase 4: Controlled Installation Experiment\ncd /tmp && mkdir npm_test && cd npm_test\nnpm install --verbose cowsay\n# ... (successful installation log) ...\nls -la node_modules/.bin\ntotal 8\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowsay -> ../cowsay/cli.js\nlrwxrwxrwx  1 jules jules   16 Sep 19 17:36 cowthink -> ../cowsay/cli.js\nnpx cowsay \"Test\"\n  ______\n< Test >\n ------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n```\n\n### A.2 Process Management & Honcho Forensics (from \"Operation: Know Thyself\")\n\n**Conclusion:** The sandbox does not support standard background processes (`&`), the `kill` command is non-functional, and the `honcho` process manager leaves zombie processes (`[uvicorn] <defunct>`) upon termination. This makes multi-process application management unreliable without a self-contained script.\n\n**Raw Logs:**\n\n```\n# Phase 2: The honcho Stress Test\n\ntimeout 15s honcho start\n# ... (honcho starts and is terminated by timeout) ...\n\nps aux (Post-Mortem Analysis)\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\njules      30121  0.0  0.0      0     0 ?        Z    19:45   0:00 [uvicorn]\n...\n\nhoncho start &\n# (Command blocks terminal, echo command never runs)\n\nps aux | grep honcho\njules      30187  0.0  0.0  11004  4220 pts/0    S    19:45   0:00 /usr/bin/python3 /home/jules/.local/bin/honcho start\n\nkill -9 30187\n# (Command fails silently, process is not terminated)\n```\n\n---\n\n## Protocol 24: The \"Dedicated Human Researcher\" Test\n\nThis protocol establishes the guiding ethical principle for all data collection and scraping activities.\n\nAll data adapters must be designed to operate in a manner that respects the resources of the source. As a definitive test, all fetching patterns must adhere to the following principle:\n\n*If a single, dedicated human using standard browser developer tools could not plausibly achieve the adapter's data collection footprint in a reasonable amount of time, the adapter's methods are considered too aggressive and must be redesigned.*\n\nThis encourages \"human-like\" fetching behavior (e.g., appropriate delays, non-parallel requests to a single source) and serves as our primary safeguard against violating a source's terms of service.\n\n---\n\n## Protocol 25: The \"Tiger Trap\" URL Protocol\n\n**CRITICAL WARNING:** URLs provided in user prompts that link to external resources, particularly workflow logs on blob storage (e.g., `blob.core.windows.net`), are considered \"tiger traps.\"\n\n-   **ACCESS ONCE:** These URLs must only be visited **one time**.\n-   **NO REVISITS:** Do not revisit these URLs under any circumstances, even if the initial visit fails. The content is volatile and subsequent visits are known to cause catastrophic, unrecoverable session hangs.\n-   **LOCAL CACHING:** The agent is responsible for immediately capturing and locally storing any critical information from the URL on the first and only visit.\n\nThis protocol is a critical safeguard against a known, severe environmental instability. Violation will result in mission failure.\n\n---\n\n## Protocol 26: The PowerShell Here-String Prohibition\n\n**CRITICAL SYNTAX WARNING:** The use of PowerShell \"here-strings\" (`@\"...\"@`) within GitHub Actions workflow files (`.yml`) is strictly forbidden.\n\n-   **CAUSE OF FAILURE:** This syntax is known to cause fatal parsing errors at the workflow dispatch level, preventing the entire workflow from even starting. The error messages are often cryptic and do not pinpoint the here-string as the root cause.\n-   **CORRECT IMPLEMENTATION:** For multi-line scripts in PowerShell, the only approved method is to define the script as a PowerShell array of strings and either join it with newlines before execution or write it to a temporary file.\n\n**Example of Correct, Approved Syntax:**\n\n```powershell\n$script = @(\n  'Line 1 of the script',\n  'Line 2 of the script',\n  '$variable = \"interpolated\"'\n)\n$script | Out-File -FilePath \"temp_script.ps1\" -Encoding utf8\npwsh -File \"temp_script.ps1\"\n```\n\nAdherence to this protocol is mandatory to ensure the basic stability and parsability of all CI/CD workflows.\n",
    "Dockerfile.tinyfield": "# TinyField Variant - Static Frontend + Python Backend\nFROM python:3.10.11-slim as backend-builder\n\nWORKDIR /build\nCOPY web_service/backend/requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Runtime stage\nFROM python:3.10.11-slim\n\nWORKDIR /app\n\n# Install only runtime dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*\n\n# Copy Python packages\nCOPY --from=backend-builder /root/.local /root/.local\n\n# Copy application code, preserving directory structure\nCOPY web_service /app/web_service\n\n# Create TinyField data directories\nRUN mkdir -p /app/web_service/backend/data /app/web_service/backend/json /app/web_service/backend/logs\n\n# Set environment\nENV PATH=/root/.local/bin:$PATH\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Health check\nHEALTHCHECK --interval=10s --timeout=5s --start-period=30s --retries=3 \\\n    CMD curl -f http://localhost:8000/api/health || exit 1\n\nEXPOSE 8000\n\n# Start backend (serves both API and frontend)\nCMD [\"python\", \"-m\", \"uvicorn\", \"web_service.backend.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "PSEUDOCODE2025.MD": "# \ud83d\udc0e Fortuna Faucet - Complete Pseudocode Blueprint\n\n**Status:** Comprehensive System Specification (Revised & Corrected)\n**Version:** 2.2.0\n**Last Updated:** November 7, 2025\n\n---\n\n## TABLE OF CONTENTS\n\n1.  System Overview\n2.  Architecture Pillars\n3.  Backend Engine (Python) - Detailed\n4.  Frontend Interface (TypeScript/React) - Detailed\n5.  Electron Wrapper & Windows Integration - Detailed\n6.  Data Models & API Specification\n7.  Deployment & Automation (CI/CD)\n8.  End-to-End Workflows\n\n---\n\n## 1. SYSTEM OVERVIEW\n\n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         FORTUNA FAUCET - Racing Analysis Platform             \u2551\n\u2551  Unifying global horse/greyhound/harness racing intelligence   \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nMISSION:\n  \u2022 Acquire race data from 20+ global sources (APIs + web scraping).\n  \u2022 Normalize and deduplicate data into a canonical Race format.\n  \u2022 Apply analytical filters to surface high-value betting opportunities.\n  \u2022 Serve results via a secure, local REST API to an interactive dashboard.\n  \u2022 Operate as a professional, standalone, native Windows application.\n\nCORE TENETS:\n  \u2022 UI-First Experience: The user interface is always responsive, even during backend startup or restarts.\n  \u2022 Resilient Process Management: The backend executable's lifecycle is robustly managed, with timeouts and crash detection.\n  \u2022 Asynchronous Initialization: The backend server starts instantly, deferring heavy, blocking I/O to background threads.\n  \u2022 Secure by Design: Communication between the frontend and the privileged main process is secured via a context-aware preload script.\n  \u2022 Automated & Repeatable Builds: The entire application is built, tested, and packaged via a deterministic CI/CD pipeline.\n\nSTAKEHOLDERS:\n  \u2022 End User: Receives a professional MSI installer for a one-click, dependency-free launch.\n  \u2022 Developer: Works with clean, separated Python and TypeScript stacks, governed by this specification.\n```\n\n---\n\n## 2. ARCHITECTURE PILLARS\n\n### Pillar 1: Backend Engine (Python)\n\n```\nPYTHON_BACKEND:\n  \u251c\u2500 main.py\n  \u2502  \u2514\u2500 Entry point for PyInstaller executable; starts the Uvicorn server.\n  \u2502\n  \u251c\u2500 api.py\n  \u2502  \u2514\u2500 FastAPI application definition.\n  \u2502     \u251c\u2500 Lifespan Hook: Manages async startup/shutdown logic.\n  \u2502     \u251c\u2500 API Routes: /health, /api/status, /api/races, etc.\n  \u2502     \u2514\u2500 Dependency Injection: Provides engine and security dependencies.\n  \u2502\n  \u251c\u2500 engine.py\n  \u2502  \u2514\u2500 OddsEngine: Orchestrates all data fetching and processing.\n  \u2502\n  \u251c\u2500 adapters/\n  \u2502  \u251c\u2500 base_v3.py (Abstract Base Class for all data sources)\n  \u2502  \u2514\u2500 [20+ specific adapter implementations]\n  \u2502\n  \u251c\u2500 config.py\n  \u2502  \u2514\u2500 Pydantic settings management from .env file.\n  \u2502\n  \u2514\u2500 requirements.txt\n     \u2514\u2500 Clean, de-duplicated, and conflict-free list of all Python dependencies.\n```\n\n### Pillar 2: Frontend Interface (TypeScript/React)\n\n```\nFRONTEND:\n  \u251c\u2500 next.config.mjs\n  \u2502  \u2514\u2500 Next.js config with `output: 'export'` for 100% static generation.\n  \u2502\n  \u251c\u2500 app/page.tsx\n  \u2502  \u2514\u2500 Main application shell.\n  \u2502\n  \u251c\u2500 src/components/\n  \u2502  \u251c\u2500 LiveRaceDashboard.tsx (Main stateful component)\n  \u2502  \u2502  \u251c\u2500 Manages connection state ('connecting', 'online', 'error').\n  \u2502  \u2502  \u251c\u2500 Polls Electron main process for backend status via secure IPC.\n  \u2502  \u2502  \u2514\u2500 Fetches data from the local Python API when online.\n  \u2502  \u2502\n  \u2502  \u251c\u2500 RaceCard.tsx (Displays a single race)\n  \u2502  \u2514\u2500 StatusIndicator.tsx (Shows backend connection status)\n  \u2502\n  \u2514\u2500 src/types/\n     \u2514\u2500 racing.ts (TypeScript interfaces matching backend Pydantic models)\n```\n\n### Pillar 3: Electron Wrapper & Windows Integration\n\n```\nELECTRON_WRAPPER:\n  \u251c\u2500 main.js (Electron main process)\n  \u2502  \u251c\u2500 Creates the BrowserWindow and loads the static frontend.\n  \u2502  \u251c\u2500 Implements robust lifecycle management for the backend executable.\n  \u2502  \u251c\u2500 Provides secure IPC handlers for status checks and restarts.\n  \u2502  \u2514\u2500 Creates a system tray icon for background operation.\n  \u2502\n  \u251c\u2500 preload.js (Secure IPC Bridge)\n  \u2502  \u2514\u2500 Uses `contextBridge` to safely expose specific functions to the frontend.\n  \u2502\n  \u251c\u2500 package.json\n  \u2502  \u2514\u2500 Defines Node.js dependencies and build scripts.\n  \u2502\n  \u251c\u2500 electron-builder-config.yml\n  \u2502  \u2514\u2500 Defines the configuration for creating the final MSI installer.\n  \u2502\n  \u2514\u2500 .github/workflows/build-msi.yml\n     \u2514\u2500 GitHub Actions pipeline that automates the entire build, test, and package process.\n```\n\n---\n\n## 3. BACKEND ENGINE (PYTHON) - DETAILED\n\n### 3.1 Entry Point & Server Startup (`main.py`)\n\n```pseudocode\n// This is the script executed by fortuna-backend.exe\n\nPROCEDURE Main_Python_Entry_Point\n  // Guard required for PyInstaller and multiprocessing on Windows\n  IF this script is the main entry point:\n    CALL multiprocessing.freeze_support()\n\n    // Programmatically launch the FastAPI application using Uvicorn\n    // This call blocks and runs the server until the process is terminated\n    CALL uvicorn.run(\n      app=\"python_service.api:app\",\n      host=\"0.0.0.0\",\n      port=8000\n    )\nEND PROCEDURE\n```\n\n### 3.2 Asynchronous Application Lifecycle (`api.py`)\n\n```pseudocode\n// --- Lifespan Management (The key to a non-blocking startup) ---\nASYNC FUNCTION lifespan_manager(app: FastAPI):\n  // === ON STARTUP ===\n  LOG \"Uvicorn server is online. Starting lifespan initialization.\"\n\n  // 1. Perform immediate, non-blocking tasks\n  CONNECT to Redis cache\n\n  // 2. Defer slow, blocking tasks to a background thread\n  //    This allows the server to start accepting requests instantly.\n  SCHEDULE function \"initialize_heavy_resources(app)\" to run in a ThreadPoolExecutor\n\n  LOG \"Heavy resource initialization scheduled. Server is now responsive.\"\n\n  // 3. Yield control back to Uvicorn. The server is now live.\n  YIELD\n\n  // === ON SHUTDOWN ===\n  LOG \"Shutdown signal received.\"\n  AWAIT app.state.engine.close() // Gracefully close HTTP client connections\n  DISCONNECT from Redis\n  SHUTDOWN ThreadPoolExecutor\n\n// --- Heavy Initialization (Runs in Background) ---\nFUNCTION initialize_heavy_resources(app: FastAPI):\n  TRY\n    LOG \"Background initialization of OddsEngine has started.\"\n    settings <- get_settings_from_config()\n    engine <- create new OddsEngine(config=settings)\n    // This part is slow: it loads all ~25 adapters\n    app.state.engine <- engine\n    LOG \"Background initialization complete. OddsEngine is now available.\"\n  CATCH Exception as e:\n    LOG_CRITICAL \"Failed to initialize OddsEngine in the background.\", error=e\n    app.state.engine <- null // Ensure the app knows initialization failed\n```\n\n### 3.3 Engine Orchestration (`engine.py`)\n\n```pseudocode\nCLASS OddsEngine:\n  INIT(config):\n    self.config <- config\n    self.adapters <- [List of all adapter instances]\n    self.http_client <- httpx.AsyncClient(...)\n    self.semaphore <- asyncio.Semaphore(config.MAX_CONCURRENT_REQUESTS)\n\n    // Inject the shared, persistent HTTP client into each adapter\n    FOR adapter IN self.adapters:\n      adapter.http_client <- self.http_client\n\n  @cache_async_result(ttl_seconds=300)\n  ASYNC FUNCTION fetch_all_odds(date_str):\n    // Create a list of concurrent fetching tasks, wrapped in the semaphore\n    tasks <- [self._fetch_with_semaphore(adapter, date_str) FOR adapter in self.adapters]\n    results <- AWAIT asyncio.gather(*tasks, return_exceptions=True)\n\n    // Process results, separating successes from failures\n    all_races <- []\n    FOR result IN results:\n      IF result is a success:\n        all_races.extend(result.races)\n\n    // Deduplicate and merge races from different sources\n    deduped_races <- self._dedupe_races(all_races)\n\n    RETURN AggregatedResponse(races=deduped_races, source_statuses=...)\n```\n\n---\n\n## 4. FRONTEND INTERFACE (TYPESCRIPT/REACT) - DETAILED\n\n### 4.1 LiveRaceDashboard Component\n\n```pseudocode\nCOMPONENT LiveRaceDashboard (client-side):\n\n  STATE:\n    races: Race[] <- []\n    backendStatus: 'connecting' | 'online' | 'error' <- 'connecting'\n    lastLogs: string[] <- []\n\n  EFFECT on mount:\n    // Use the secure API exposed by preload.js\n    IF window.electronAPI exists:\n      // Set up a listener for status updates from the main process\n      window.electronAPI.onBackendStatus((update) => {\n        setBackendStatus(update.state)\n        setLastLogs(update.logs)\n      })\n\n    // Immediately request the current status\n    window.electronAPI.getBackendStatus().then((status) => {\n      setBackendStatus(status.state)\n      setLastLogs(status.logs)\n    })\n\n    // Set up a polling interval to keep status fresh\n    interval <- setInterval(() => {\n      window.electronAPI.getBackendStatus().then((status) => {\n        setBackendStatus(status.state)\n        setLastLogs(status.logs)\n      })\n    }, 3000) // Poll every 3 seconds\n\n    CLEANUP: clearInterval(interval)\n\n  EFFECT when backendStatus changes to 'online':\n    // Trigger data fetch only when the backend is confirmed to be running\n    fetchQualifiedRaces()\n\n  ASYNC FUNCTION fetchQualifiedRaces():\n    TRY:\n      // Make a standard HTTP call to the local Python server\n      response <- AWAIT fetch(\"http://127.0.0.1:8000/api/races/qualified/trifecta\")\n      IF NOT response.ok:\n        RAISE new Error(`API returned status ${response.status}`)\n\n      data <- AWAIT response.json()\n      setRaces(data.races)\n\n    CATCH e:\n      // If the API call fails, update the status\n      setBackendStatus('error')\n      setLastLogs([...lastLogs, `API Fetch Error: ${e.message}`])\n\n  FUNCTION RENDER:\n    <div className=\"dashboard\">\n      <StatusIndicator status={backendStatus} />\n      <RaceFilters />\n\n      IF backendStatus === 'error':\n        <ErrorDisplay logs={lastLogs} />\n      ELSE IF backendStatus === 'connecting':\n        <LoadingSkeleton />\n      ELSE IF races.length === 0:\n        <EmptyState message=\"No races matched your filters.\" />\n      ELSE:\n        <RaceGrid races={races} />\n    </div>\n```\n\n---\n\n## 5. ELECTRON WRAPPER & WINDOWS INTEGRATION - DETAILED\n\n### 5.1 Main Process (`main.js`) - With Robust Lifecycle Management\n\n```pseudocode\nCLASS FortunaDesktopApp:\n  INIT():\n    self.mainWindow <- null\n    self.backendState <- 'stopped'\n    self.backendLogs <- []\n    self.backendProcess <- null\n\n  FUNCTION createMainWindow():\n    // ... create BrowserWindow, load static frontend ...\n\n  FUNCTION startBackend():\n    IF self.backendProcess is not null:\n      self.backendProcess.kill()\n\n    self.backendState <- 'starting'\n    self.backendLogs <- ['Attempting to start backend...']\n    self.sendBackendStatusUpdate() // Notify UI\n\n    // Get path to the packaged executable\n    exePath <- path.join(process.resourcesPath, 'fortuna-backend', 'fortuna-backend.exe')\n\n    IF file at exePath does NOT exist:\n      self.backendState <- 'error'\n      self.backendLogs.push(`FATAL: Executable not found at ${exePath}`)\n      self.sendBackendStatusUpdate()\n      dialog.showErrorBox(\"Critical Error\", \"Backend is missing. Please reinstall.\")\n      RETURN\n\n    // Spawn the process\n    self.backendProcess <- spawn(exePath, [], { stdio: ['ignore', 'pipe', 'pipe'] })\n\n    // --- CRITICAL: Resiliency Logic ---\n    startupTimeout <- setTimeout(() => {\n      IF self.backendState === 'starting':\n        self.backendState <- 'error'\n        self.backendLogs.push('Error: Backend startup timed out after 30 seconds.')\n        self.backendProcess.kill()\n        self.sendBackendStatusUpdate()\n    }, 30000) // 30-second timeout\n\n    self.backendProcess.stdout.on('data', (data) => {\n      self.backendLogs.push(data.toString())\n      // A more robust check would be a successful health check poll\n      IF data.toString().includes(\"Uvicorn running\"):\n        self.backendState <- 'online'\n        clearTimeout(startupTimeout)\n        self.sendBackendStatusUpdate()\n    })\n\n    self.backendProcess.stderr.on('data', (data) => {\n      self.backendLogs.push(`[STDERR] ${data.toString()}`)\n    })\n\n    self.backendProcess.on('exit', (code) => {\n      clearTimeout(startupTimeout)\n      IF self.backendState is not 'error': // Avoid duplicate error messages\n        self.backendState <- 'error'\n        self.backendLogs.push(`Backend process exited unexpectedly with code: ${code}`)\n        self.sendBackendStatusUpdate()\n    })\n\n  FUNCTION sendBackendStatusUpdate():\n    // Send the latest status to the frontend renderer process\n    IF self.mainWindow is not null:\n      self.mainWindow.webContents.send('backend-status-update', {\n        state: self.backendState,\n        logs: self.backendLogs.slice(-20) // Send last 20 log lines\n      })\n\n// --- IPC Handlers (Securely Defined) ---\nipcMain.handle('get-backend-status', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN null\n\n  RETURN { state: self.backendState, logs: self.backendLogs.slice(-20) }\n})\n\nipcMain.on('restart-backend', (event) => {\n  // SECURITY: Ensure the request is from our main window\n  IF event.sender is NOT self.mainWindow.webContents:\n    RETURN\n\n  self.startBackend()\n})\n```\n\n### 5.2 Preload Script (`preload.js`)\n\n```pseudocode\n// Expose a limited, secure API to the frontend renderer process\ncontextBridge.exposeInMainWorld('electronAPI', {\n  getBackendStatus: () => ipcRenderer.invoke('get-backend-status'),\n  restartBackend: () => ipcRenderer.send('restart-backend'),\n  onBackendStatus: (callback) => ipcRenderer.on('backend-status-update', (_event, value) => callback(value))\n})\n```\n\n---\n\n## 6. DATA MODELS & API SPECIFICATION\n\n### 6.1 Core Data Models (Pydantic/TypeScript)\n\n```\nMODEL Race:\n  id: str (unique identifier, e.g., \"Betfair_USA_Aqueduct_2025-11-07_R1\")\n  venue: str\n  race_number: int\n  start_time: datetime\n  runners: List[Runner]\n  source: str\n\nMODEL Runner:\n  name: str\n  odds: Optional[float]\n```\n\n### 6.2 Primary API Endpoints\n\n```\nENDPOINT GET /health\n  Description: Simple health check, requires no authentication.\n  Response (200 OK): {\"status\": \"ok\"}\n\nENDPOINT GET /api/races/qualified/trifecta\n  Description: Fetches all race data, runs the Trifecta analyzer, and returns qualified races.\n  Headers:\n    - X-API-Key: (Required, not used in this local setup but good practice)\n  Query Params:\n    - max_field_size: int\n    - min_odds: float\n  Response (200 OK):\n    {\n      \"qualified_races\": List[Race],\n      \"analysis_metadata\": { ... }\n    }\n```\n\n---\n\n## 7. DEPLOYMENT & AUTOMATION (CI/CD)\n\n```pseudocode\nWORKFLOW Build_MSI_Installer_on_GitHub_Actions:\n  // Phase 1: Setup\n  SETUP Node.js and Python environments\n\n  // Phase 2: Build Frontend\n  RUN \"npm ci\" and \"npm run build\" in /web_platform/frontend\n  COPY static output to /electron/web-ui-build/out\n\n  // Phase 3: Build Backend\n  RUN \"pip install -r python_service/requirements.txt\"\n  // CRITICAL: Use PyInstaller with a spec file or CLI flags that include\n  // necessary hidden imports to prevent runtime crashes.\n  // e.g., --hidden-import=keyring.backends.fail.Keyring\n  EXECUTE PyInstaller to create fortuna-backend.exe\n  PLACE executable in /electron/resources/fortuna-backend\n\n  // Phase 4: Deep Integration Test\n  START fortuna-backend.exe in the background\n  POLL http://127.0.0.1:8000/health until it responds with 200 OK or times out\n  IF timeout or crash THEN FAIL the build\n\n  // Phase 5: Package\n  RUN \"npm ci\" in /electron\n  EXECUTE \"npx electron-builder\" to create the MSI installer\n\n  // Phase 6: Publish\n  UPLOAD MSI as a build artifact\n  IF build was triggered by a git tag THEN CREATE a new GitHub Release\n```\n\n---\n\n## 8. END-TO-END WORKFLOWS\n\n### 8.1 Production Startup Workflow (Resilient)\n\n```\nWORKFLOW user_launches_application:\n  STEP 1: User executes Fortuna Faucet.exe -> Electron main.js starts.\n  STEP 2: UI appears instantly. The main process creates the BrowserWindow and loads the static index.html. The UI shows a 'connecting' state.\n  STEP 3: Backend starts asynchronously. The main process calls the robust `startBackend()` function.\n  STEP 4: `startBackend()` spawns `fortuna-backend.exe` and starts a 30-second timeout.\n  STEP 5: The frontend UI polls for status every 3 seconds via the secure `window.electronAPI.getBackendStatus()`.\n  STEP 6: The backend `.exe` starts, its `lifespan` hook runs, and the Uvicorn server comes online within seconds.\n  STEP 7: The main process detects the \"Uvicorn running\" message (or a successful health poll) and updates its internal state to 'online'. The startup timeout is cleared.\n  STEP 8: On its next poll, the frontend receives the 'online' status.\n  STEP 9: The frontend's state changes, triggering the `fetchQualifiedRaces()` API call to `localhost:8000`.\n  STEP 10: Data is returned from the now fully-initialized backend and rendered in the UI.\n\n  FAILURE SCENARIO (Backend Crash):\n  STEP 6a: The backend `.exe` crashes on startup.\n  STEP 7a: The `on('exit')` handler in `main.js` fires. The state is set to 'error' with the exit code.\n  STEP 8a: On its next poll, the frontend receives the 'error' status and relevant logs.\n  STEP 9a: The UI renders an error message and a \"Restart Backend\" button.\n```\n\n---\n*This concludes the revised and definitive blueprint for the Fortuna Faucet application.*\n\n---\n\n### 9. OPERATION: THE AUDITOR (REAL-TIME VERIFICATION)\n\n**Status:** Implemented (Python)\n**Source:** `python_service/auditor.py`\n\n#### 9.1 System Context\n*Runs as a background thread. Verifies \"Qualifier\" predictions against official results scraped from AtTheRaces.com to calculate real-time profitability.*\n\n#### 9.2 Database Schema (SQLite)\n\n```pseudocode\nTABLE audit_log:\n  race_id: TEXT PRIMARY KEY      // Format: \"VENUE-YYYYMMDD-RR\"\n  track_code: TEXT\n  race_number: INTEGER\n  predicted_horse: TEXT\n  timestamp: DATETIME\n  status: TEXT                   // 'PENDING', 'CASHED', 'BURNED'\n  official_payout: REAL          // Default: 0.00\n  net_profit: REAL               // Default: 0.00\n\n  CONSTRAINT status_check CHECK (status IN ('PENDING', 'CASHED', 'BURNED'))\n  INDEX idx_status_timestamp (status, timestamp)\n```\n\n#### 9.3 Auditor Engine Logic\n\n```pseudocode\nMODULE: Auditor_Engine\nDEPENDENCIES: httpx, beautifulsoup4, sqlite3, structlog\n\nCLASS Auditor_Engine:\n\n    PROPERTIES:\n        db_path: String\n        http_client: AsyncClient\n        TOTE_UNIT: Float (2.00)\n        TRACK_CODE_MAP: Dictionary  // Maps \"DON\" -> \"Doncaster\", etc.\n\n    FUNCTION __init__(db_path):\n        self.db_path = db_path\n        self.Initialize_Database()\n        self.http_client = NEW AsyncClient(timeout=30)\n\n    # --- Phase 1: The Snapshot ---\n    # Called by OddsEngine when a bet is placed/qualified\n    ASYNC FUNCTION Snapshot_Qualifier(venue_code, race_date, race_number, predicted_horse):\n        race_id = GENERATE_ID(venue_code, race_date, race_number)\n\n        TRY:\n            QUERY = \"\"\"\n                INSERT INTO audit_log\n                (race_id, track_code, race_number, predicted_horse, timestamp, status)\n                VALUES (?, ?, ?, ?, ?, 'PENDING')\n            \"\"\"\n            EXECUTE_SQL(self.db_path, QUERY, (race_id, venue_code, race_number, predicted_horse, NOW()))\n            LOG_INFO(\"Snapshot saved: \" + race_id)\n            RETURN TRUE\n        CATCH IntegrityError:\n            LOG_WARN(\"Race already tracked: \" + race_id)\n            RETURN FALSE\n\n    # --- Phase 2: The Fetcher (Background Loop) ---\n    ASYNC FUNCTION Run_Audit_Loop():\n        self.running = TRUE\n        WHILE self.running:\n            TRY:\n                # 1. Get Pending Races (Last 60 mins)\n                cutoff = NOW() - MINUTES(60)\n                pending_races = GET_PENDING_RACES(cutoff)\n\n                IF pending_races IS EMPTY:\n                    SLEEP(120)\n                    CONTINUE\n\n                # 2. Batch by Track\n                unique_tracks = EXTRACT_UNIQUE_TRACKS(pending_races)\n\n                FOR track IN unique_tracks:\n                    track_races = FILTER(pending_races, track)\n\n                    FOR race IN track_races:\n                        # Fetch Official Result from AtTheRaces\n                        result = AWAIT self._Fetch_Official_Result(race.track_code, race.race_number)\n\n                        IF result IS NOT NULL:\n                            self._Determine_Verdict(race, result)\n\n                        SLEEP(2) # Polite delay\n\n            CATCH Exception as e:\n                LOG_ERROR(\"Audit Loop Error: \" + e)\n\n            SLEEP(120)\n\n    # --- Phase 3: The Scraper (AtTheRaces Strategy) ---\n    ASYNC FUNCTION _Fetch_Official_Result(track_code, race_number):\n        # 1. Find the specific Race URL from the daily results page\n        race_url = AWAIT self._Find_Race_URL(track_code, race_number)\n        IF race_url IS NULL: RETURN NULL\n\n        # 2. Fetch the Race Page\n        html = AWAIT self.http_client.GET(race_url)\n\n        # 3. Parse the Table\n        result = self._Parse_AtTheRaces_Results(html, track_code, race_number)\n        RETURN result\n\n    ASYNC FUNCTION _Find_Race_URL(track_code, race_number):\n        base_url = \"https://www.attheraces.com/results\"\n        html = AWAIT self.http_client.GET(base_url)\n\n        track_name = self.TRACK_CODE_MAP[track_code]\n        track_header = FIND_ELEMENT(html, text=track_name)\n\n        IF track_header EXISTS:\n            # Select the Nth link in the track's panel\n            race_links = SELECT_ALL(track_header.parent, 'a[href*=\"/racecard/\"]')\n            IF LENGTH(race_links) >= race_number:\n                RETURN race_links[race_number - 1].href\n\n        RETURN NULL\n\n    FUNCTION _Parse_AtTheRaces_Results(html, track_code, race_number):\n        table = FIND_TABLE(html, header_contains=\"Horse\")\n        IF table IS NULL: RETURN NULL\n\n        finishers = []\n        win_payout = EXTRACT_WIN_PAYOUT(html) # Parse \"Betting returns\" table\n\n        FOR row IN table.rows:\n            position = PARSE_INT(row.cells[0])\n            horse_name = row.cells[2].text\n\n            place_payout = 0.0\n            IF position == 1:\n                place_payout = win_payout\n\n            finishers.APPEND({\n                \"name\": horse_name,\n                \"position\": position,\n                \"place_payout\": place_payout\n            })\n\n        RETURN NEW OfficialResult(finishers)\n\n    # --- Phase 4: The Verdict ---\n    FUNCTION _Determine_Verdict(prediction, official_result):\n        did_place = FALSE\n        payout = 0.00\n\n        # Check if predicted horse won (AtTheRaces basic logic)\n        FOR finisher IN official_result.finishers:\n            IF finisher.name == prediction.predicted_horse AND finisher.place_payout > 0:\n                did_place = TRUE\n                payout = finisher.place_payout\n                BREAK\n\n        IF did_place:\n            status = 'CASHED'\n            net_profit = payout - self.TOTE_UNIT\n        ELSE:\n            status = 'BURNED'\n            net_profit = -self.TOTE_UNIT\n\n        UPDATE_DB(prediction.race_id, status, payout, net_profit)\n\n    # --- Phase 5: Dashboard Metrics ---\n    FUNCTION Get_Rolling_Metrics(minutes=60):\n        cutoff = NOW() - MINUTES(minutes)\n        QUERY = \"\"\"\n            SELECT\n                COUNT(*) as total,\n                SUM(CASE WHEN status='CASHED' THEN 1 ELSE 0 END) as wins,\n                SUM(net_profit) as profit\n            FROM audit_log\n            WHERE timestamp > ? AND status != 'PENDING'\n        \"\"\"\n        stats = EXECUTE_SQL(self.db_path, QUERY, (cutoff,))\n\n        RETURN {\n            \"strike_rate\": (stats.wins / stats.total) * 100,\n            \"net_profit\": stats.profit,\n            \"volume\": stats.total\n        }\n```",
    "ROADMAP.md": "# Fortuna Faucet Roadmap\n\n## The Auditor: Real-Time Race Verification\n\nThe Auditor is a background process designed to provide real-time verification of race predictions against official results, calculating profitability and tracking performance.\n\nThe core logic is located in the [`python_service/auditor.py`](python_service/auditor.py) script.\n\n### Frontend Integration\n\nThe Auditor is designed to have its data displayed on the frontend. The `AuditorEngine` class in the script exposes two key methods for this purpose:\n\n1.  `get_rolling_metrics()`: This function is intended to power a \"Last Hour\" overlay on the UI, providing key performance indicators such as strike rate, net profit, and betting volume.\n2.  `get_recent_activity()`: This function provides a list of the most recent bet outcomes (e.g., \"CASHED\", \"BURNED\", \"PENDING\") for display in a real-time activity feed or history log.\n\nThe intended architecture is for the backend to create API endpoints (e.g., `/api/auditor/metrics`, `/api/auditor/activity`) that expose these functions. The frontend would then call these endpoints to fetch and display the data.\n\n**Note:** As of the last update, the web scraping component of the Auditor is non-functional. The description above outlines the intended design and capabilities, which are not yet fully operational.\n",
    "check_port.py": "# check_port.py\nimport socket\nimport time\n\n\ndef check_server_status(host, port):\n    \"\"\"Checks if the server is accessible.\"\"\"\n    time.sleep(5)  # Give server time to start\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.connect((host, port))\n            print(\"SERVER CHECK: SUCCESS! Server is running and accessible.\")\n            return True\n        except ConnectionRefusedError:\n            print(\"SERVER CHECK: FAILED! Server is not accessible.\")\n            return False\n\n\nif __name__ == \"__main__\":\n    check_server_status(\"127.0.0.1\", 8000)\n",
    "config.ini": "[analysis]\nqualification_score = 75.0\nfield_size_optimal_min = 4\nfield_size_optimal_max = 6\nfield_size_acceptable_min = 7\nfield_size_acceptable_max = 8\nfield_size_optimal_points = 30\nfield_size_acceptable_points = 10\nfield_size_penalty_points = -20\nfav_odds_points = 30\nmax_fav_odds = 3.5\nsecond_fav_odds_points = 40\nmin_2nd_fav_odds = 4.0\n\n[system]\napi_rate_limit = 60",
    "electron/assets/.gitkeep": "# This directory is for application icons (e.g., icon.ico, tray-icon.png)",
    "electron/install-dependencies.js": "const { execSync } = require('child_process');\nconst path = require('path');\nconst fs = require('fs');\n\n// Path to the bundled Python executable (fortuna-backend.exe)\nconst PYTHON_EXE = path.join(process.resourcesPath, 'fortuna-backend.exe');\n// Path to the Python service directory (where alembic.ini is)\nconst PYTHON_SERVICE_DIR = path.join(process.resourcesPath, 'python_service');\n\nfunction runCommand(command, cwd) {\n    console.log(`Executing: ${command} in ${cwd}`);\n    try {\n        const output = execSync(command, { cwd: cwd, encoding: 'utf-8' });\n        console.log(output);\n    } catch (error) {\n        console.error(`Command failed: ${command}`);\n        console.error(error.stderr || error.stdout || error.message);\n        throw new Error(`Post-install setup failed: ${command}`);\n    }\n}\n\nfunction setupDatabase() {\n    console.log('--- Starting Database Setup (Alembic Migrations) ---');\n    // NOTE: The bundled EXE must be able to run a command like 'alembic' or a custom script\n    // that executes the migrations. Assuming the bundled EXE can run a module.\n    // A more robust solution is to bundle a dedicated migration script.\n\n    // Assuming the bundled EXE can execute a module that runs Alembic\n    const migrationCommand = `${PYTHON_EXE} -m python_service.database.run_migrations`;\n\n    // The migration script needs access to the database URL from the config.\n    // This is a placeholder, as the config loading is complex in a frozen app.\n    // For now, we assume the bundled EXE handles config loading.\n\n    runCommand(migrationCommand, PYTHON_SERVICE_DIR);\n    console.log('--- Database Setup Complete ---');\n}\n\n// This function is called by the Electron Builder installer hook\nmodule.exports = async function() {\n    try {\n        setupDatabase();\n    } catch (e) {\n        console.error('FATAL: Post-install setup failed.', e);\n        // In a real installer, you might log this and continue, or show a user error.\n    }\n};\n",
    "electron/main.js": "// electron/main.js - CORRECTED VERSION\nconst { app, BrowserWindow, Tray, Menu, nativeImage, ipcMain, dialog } = require('electron');\nconst { autoUpdater } = require('electron-updater');\nconst { spawn } = require('child_process');\nconst net = require('net');\nconst path = require('path');\nconst fs = require('fs');\nconst SecureSettingsManager = require('./secure-settings-manager');\n\nclass FortunaDesktopApp {\n constructor() {\n this.backendProcess = null;\n this.mainWindow = null;\n this.tray = null;\n this.backendState = 'stopped'; // \"stopped\", \"starting\", \"running\", \"error\"\n this.backendLogs = [];\n this.isBackendStarting = false;\n }\n\n sendBackendStatusUpdate() {\n if (this.mainWindow) {\n this.mainWindow.webContents.send('backend-status-update', {\n state: this.backendState,\n logs: this.backendLogs.slice(-20) // Send last 20 log entries\n });\n }\n }\n\n stopBackend() {\n if (this.backendProcess && !this.backendProcess.killed) {\n console.log('Stopping backend process...');\n this.backendProcess.kill();\n this.backendState = 'stopped';\n this.isBackendStarting = false; // Ensure lock is released on stop\n this.backendLogs.push('Backend process stopped by user.');\n this.sendBackendStatusUpdate();\n }\n }\n\n  checkPortInUse(port) {\n    return new Promise((resolve, reject) => {\n      const server = net.createServer();\n      server.once('error', (err) => {\n        if (err.code === 'EADDRINUSE') {\n          resolve(true); // Port is in use\n        } else {\n          reject(err);\n        }\n      });\n      server.once('listening', () => {\n        server.close(() => {\n          resolve(false); // Port is free\n        });\n      });\n      server.listen(port, '127.0.0.1');\n    });\n  }\n\n  async waitForBackend(maxRetries = 30) {\n    const port = process.env.FORTUNA_PORT || 8000;\n    const url = `http://127.0.0.1:${port}/health`;\n\n    console.log(`[Backend Check] Starting health check at: ${url}`);\n\n    for (let i = 0; i < maxRetries; i++) {\n      try {\n        const response = await fetch(url, { timeout: 3000 });\n        console.log(`[Backend Check] Attempt ${i}: Status ${response.status}`);\n\n        if (response.ok) {\n          console.log('\u2705 Backend is healthy and responding');\n          return true;\n        }\n      } catch (e) {\n        console.log(`[Backend Check] Attempt ${i} failed: ${e.message}`);\n\n        // Check if process is still alive\n        if (this.backendProcess && !this.backendProcess.killed) {\n          console.log(`[Backend Check] Process still running (PID: ${this.backendProcess.pid})`);\n        } else {\n          console.error(`[Backend Check] \u26a0\ufe0f  Backend process is DEAD!`);\n          console.error(`[Backend Check] Last logs:`, this.backendLogs.slice(-5));\n          throw new Error(`Backend process died. Last logs:\\\\n${this.backendLogs.slice(-5).join('\\\\n')}`);\n        }\n\n        await new Promise(r => setTimeout(r, 1000));\n      }\n    }\n\n    throw new Error(`Backend failed to respond at ${url} after 30 seconds`);\n  }\n\n  async startBackend() {\n    const isDev = !app.isPackaged;\n    let backendCommand;\n    let backendCwd;\n\n    if (isDev) {\n      console.log('[DEV MODE] Configuring backend...');\n      backendCommand = path.join(__dirname, '..', '.venv', 'Scripts', 'python.exe');\n      backendCwd = path.join(__dirname, '..', 'web_service', 'backend');\n    } else {\n      // CORRECTED PATH: In production, the backend executable is at the root of the resources directory.\n      backendCommand = path.join(process.resourcesPath, 'fortuna-backend.exe');\n      backendCwd = process.resourcesPath;\n\n      console.log(`[Backend] Looking for executable at: ${backendCommand}`);\n      console.log(`[Backend] Executable exists: ${fs.existsSync(backendCommand)}`);\n    }\n\n    if (!fs.existsSync(backendCommand)) {\n      const errorMsg = `Backend executable not found at: ${backendCommand}`;\n      console.error(`[Backend] ${errorMsg}`);\n      this.backendLogs.push(`ERROR: ${errorMsg}`);\n      this.backendState = 'error';\n      dialog.showErrorBox(\n        'Backend Launch Failed',\n        `Could not find backend executable.\\\\n\\\\nExpected location:\\\\n${backendCommand}`\n      );\n      return;\n    }\n\n    console.log(`[Backend] Executable found, attempting to spawn...`);\n\n    this.backendProcess = spawn(backendCommand, [], {\n      cwd: backendCwd,\n      windowsHide: true,\n      env: {\n        ...process.env,\n        FORTUNA_MODE: 'electron',\n        PYTHONPATH: backendCwd\n      }\n    });\n\n    this.backendState = 'starting';\n    this.isBackendStarting = true;\n\n    this.backendProcess.stdout.on('data', (data) => {\n      const output = data.toString().trim();\n      console.log(`[Backend STDOUT] ${output}`);\n      this.backendLogs.push(output);\n\n      // Detect successful startup from log messages\n      if (output.includes('Application startup complete') || output.includes('Uvicorn running')) {\n        if (this.backendState !== 'running') {\n          console.log('\u2705 Backend reported successful startup');\n          this.backendState = 'running';\n          this.isBackendStarting = false;\n        }\n      }\n\n      this.sendBackendStatusUpdate();\n    });\n\n    this.backendProcess.stderr.on('data', (data) => {\n      const errorOutput = data.toString().trim();\n      console.error(`[Backend STDERR] ${errorOutput}`);\n      this.backendLogs.push(`ERROR: ${errorOutput}`);\n\n      if (this.backendState === 'starting') {\n        this.backendState = 'error';\n        this.isBackendStarting = false;\n      }\n\n      this.sendBackendStatusUpdate();\n    });\n\n    this.backendProcess.on('error', (err) => {\n      const errorMsg = `Failed to spawn backend process: ${err.message}`;\n      console.error(`[Backend] ${errorMsg}`);\n      this.backendLogs.push(`ERROR: ${errorMsg}`);\n      this.backendState = 'error';\n      this.isBackendStarting = false;\n      this.sendBackendStatusUpdate();\n    });\n\n    this.backendProcess.on('exit', (code) => {\n      if (code !== 0 && this.backendState !== 'stopped') {\n        console.error(`[CRITICAL] Backend process exited with code: ${code}`);\n        console.error(`[CRITICAL] Last 10 logs:`, this.backendLogs.slice(-10));\n\n        // Save logs for debugging\n        const logFile = path.join(require('os').homedir(), '.fortuna', 'backend_crash.log');\n        fs.mkdirSync(path.dirname(logFile), { recursive: true });\n        fs.writeFileSync(logFile, this.backendLogs.join('\\\\n'));\n        console.error(`[CRITICAL] Full logs saved to: ${logFile}`);\n\n        this.backendState = 'error';\n        this.isBackendStarting = false;\n        this.sendBackendStatusUpdate();\n      }\n    });\n  }\n\n  getFrontendPath() {\n    // UNIFIED: Always serve from the backend\n    const port = process.env.FORTUNA_PORT || 8000;\n    return `http://127.0.0.1:${port}/`;\n  }\n\n createMainWindow() {\n this.mainWindow = new BrowserWindow({\n width: 1600,\n height: 1000,\n title: 'Fortuna Faucet - Racing Analysis',\n icon: path.join(__dirname, 'assets', 'icon.ico'),\n webPreferences: {\n nodeIntegration: false,\n contextIsolation: true,\n preload: path.join(__dirname, 'preload.js')\n },\n autoHideMenuBar: true,\n backgroundColor: '#1a1a2e'\n });\n\n if (!app.isPackaged) {\n this.mainWindow.webContents.openDevTools();\n }\n\n this.mainWindow.on('close', (event) => {\n if (!app.isQuitting) {\n event.preventDefault();\n this.mainWindow.hide();\n }\n });\n }\n\n createSystemTray() {\n // ... (rest of the file is unchanged)\n }\n\n  initialize() {\n    console.log('[Electron] Initializing Fortuna application...');\n\n    this.createMainWindow();\n    this.createSystemTray();\n    this.startBackend();\n\n    // Wait for backend to be ready, then load the unified frontend\n    this.waitForBackend()\n      .then(() => {\n        console.log('[Electron] Backend is ready, loading frontend...');\n        const frontendUrl = this.getFrontendPath();\n        console.log(`[Electron] Loading frontend from: ${frontendUrl}`);\n        this.mainWindow.loadURL(frontendUrl);\n      })\n      .catch((err) => {\n        console.error('[Electron] Backend startup failed:', err);\n        dialog.showErrorBox(\n          'Backend Error',\n          'Failed to start backend service:\\\\n\\\\n' + err.message\n        );\n      });\n\n    // Check for updates\n    autoUpdater.checkForUpdatesAndNotify();\n\n    autoUpdater.on('update-downloaded', (info) => {\n      const dialogOpts = {\n        type: 'info',\n        buttons: ['Restart', 'Later'],\n        title: 'Application Update',\n        message: process.platform === 'win32' ? info.releaseName : info.releaseName,\n        detail: 'A new version has been downloaded. Restart the application to apply the updates.'\n      };\n\n      dialog.showMessageBox(dialogOpts).then((returnValue) => {\n        if (returnValue.response === 0) autoUpdater.quitAndInstall();\n      });\n    });\n\n    ipcMain.on('restart-backend', () => this.startBackend());\n    ipcMain.on('stop-backend', () => this.stopBackend());\n    ipcMain.handle('get-backend-status', async () => ({\n      state: this.backendState,\n      logs: this.backendLogs.slice(-20)\n    }));\n\n    ipcMain.handle('get-api-key', async () => {\n      return SecureSettingsManager.getApiKey();\n    });\n\n    ipcMain.handle('generate-api-key', async () => {\n      const crypto = require('node:crypto');\n      const newKey = crypto.randomBytes(16).toString('hex');\n      SecureSettingsManager.saveApiKey(newKey);\n      return newKey;\n    });\n\n    ipcMain.handle('save-api-key', async (event, apiKey) => {\n      return SecureSettingsManager.saveApiKey(apiKey);\n    });\n\n    ipcMain.handle('save-betfair-credentials', async (event, credentials) => {\n      return SecureSettingsManager.saveBetfairCredentials(credentials);\n    });\n\n    ipcMain.handle('get-api-port', () => {\n      return process.env.FORTUNA_PORT || 8000;\n    });\n  }\n\n cleanup() {\n if (this.backendProcess && !this.backendProcess.killed) {\n this.backendProcess.kill();\n }\n }\n}\n\nlet fortunaApp;\n\napp.whenReady().then(() => {\n  // Harden the session for security\n  const { session } = require('electron');\n  const ses = session.defaultSession;\n\n  // 1. Content-Security-Policy\n  ses.webRequest.onHeadersReceived((details, callback) => {\n    callback({\n      responseHeaders: {\n        ...details.responseHeaders,\n        'Content-Security-Policy': [\n          \"default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; font-src 'self' data:; connect-src 'self' http://127.0.0.1:*\"\n        ]\n      }\n    });\n  });\n\n  // 2. Permission Request Handler\n  ses.setPermissionRequestHandler((webContents, permission, callback) => {\n    const allowedPermissions = ['clipboard-read', 'clipboard-sanitized-write'];\n    if (allowedPermissions.includes(permission)) {\n      callback(true); // Grant allowed permissions\n    } else {\n      console.warn(`[SECURITY] Denied permission request for: ${permission}`);\n      callback(false); // Deny all others by default\n    }\n  });\n\n  // 3. Certificate Pinning (TODO)\n  // Certificate pinning would be implemented here. It is commented out\n  // because it requires a known certificate hash and would break local dev.\n  // ses.setCertificateVerifyProc((request, callback) => {\n  //   const { hostname, certificate, verificationResult } = request;\n  //   if (hostname === 'api.fortuna.faucet') {\n  //     // TODO: Replace with actual certificate fingerprint\n  //     const expectedFingerprint = '...';\n  //     if (certificate.fingerprint === expectedFingerprint) {\n  //       callback(0); // 0 means success\n  //     } else {\n  //       callback(-2); // -2 means failure\n  //     }\n  //   } else {\n  //     callback(0); // Allow other domains\n  //   }\n  // });\n\n  fortunaApp = new FortunaDesktopApp();\n  fortunaApp.initialize();\n});\n\napp.on('window-all-closed', () => {\n if (process.platform !== 'darwin') {\n // Do nothing, keep app running in tray\n }\n});\n\napp.on('activate', () => {\n if (BrowserWindow.getAllWindows().length === 0) {\n fortunaApp.createMainWindow();\n } else {\n fortunaApp.mainWindow.show();\n }\n});\n\napp.on('before-quit', () => {\n app.isQuitting = true;\n if (fortunaApp) {\n fortunaApp.cleanup();\n }\n});\n",
    "fortuna-monolith.spec": "# fortuna-monolith.spec\n# FIXED: Proper path resolution for Windows\n\nfrom PyInstaller.utils.hooks import collect_submodules\nfrom pathlib import Path\nimport sys\nimport os\n\nblock_cipher = None\n\n# ===== GET PROJECT ROOT =====\n# SPECPATH is provided by PyInstaller - it's the directory containing THIS spec file\nspec_path = Path(SPECPATH) if 'SPECPATH' in dir() else Path(os.path.dirname(os.path.abspath(__file__)))\nproject_root = spec_path.parent if spec_path.name == 'fortuna-monolith.spec' else spec_path\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FORTUNA MONOLITH SPEC - PATH RESOLUTION\")\nprint(\"=\"*70)\nprint(f\"Spec file location: {spec_path}\")\nprint(f\"Project root:       {project_root}\")\nprint(f\"Current working:    {Path.cwd()}\")\n\n# ===== FRONTEND VALIDATION =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"FRONTEND VALIDATION\")\nprint(\"=\"*70)\n\nfrontend_out = project_root / 'web_service' / 'frontend' / 'public'\nprint(f\"Looking for frontend at: {frontend_out}\")\nprint(f\"Exists: {frontend_out.exists()}\")\n\nif frontend_out.exists():\n    index_html = frontend_out / 'index.html'\n    print(f\"index.html path:    {index_html}\")\n    print(f\"index.html exists:  {index_html.exists()}\")\n\n    if index_html.exists():\n        file_count = len(list(frontend_out.rglob('*')))\n        size = index_html.stat().st_size\n        print(\"[OK] Frontend validated!\")\n        print(f\"   Files: {file_count}\")\n        print(f\"   index.html size: {size} bytes\")\n    else:\n        print(f\"[ERROR] FATAL: index.html not found at {index_html}\")\n        print(f\"\\nContents of {frontend_out}:\")\n        for item in frontend_out.iterdir():\n            print(f\"  - {item.name}\")\n        sys.exit(1)\nelse:\n    print(f\"[ERROR] FATAL: Frontend 'public' directory not found!\")\n    print(f\"\\nSearching for 'public' directory from project root:\")\n    for root, dirs, files in os.walk(project_root):\n        if 'public' in dirs:\n            out_path = Path(root) / 'public'\n            print(f\"  Found at: {out_path}\")\n            if (out_path / 'index.html').exists():\n                print(f\"    [OK] Has index.html\")\n                frontend_out = out_path\n                break\n    else:\n        print(f\"  Not found anywhere!\")\n        sys.exit(1)\n\n# ===== BACKEND VALIDATION =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"BACKEND VALIDATION\")\nprint(\"=\"*70)\n\nbackend_root = project_root / 'web_service' / 'backend'\nmain_py = backend_root / 'main.py'\n\nprint(f\"Looking for backend at: {backend_root}\")\nprint(f\"main.py path:           {main_py}\")\nprint(f\"main.py exists:         {main_py.exists()}\")\n\nif not main_py.exists():\n    print(f\"[ERROR] FATAL: Backend main.py not found!\")\n    print(f\"\\nContents of {backend_root}:\")\n    if backend_root.exists():\n        for item in backend_root.iterdir():\n            print(f\"  - {item.name}\")\n    else:\n        print(f\"  Directory doesn't exist!\")\n    sys.exit(1)\n\nprint(f\"[OK] Backend validated!\")\nprint(f\"   main.py size: {main_py.stat().st_size} bytes\")\n\n# ===== DATA FILES =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"COLLECTING DATA FILES\")\nprint(\"=\"*70)\n\ndatas = []\n\n# Frontend\ndatas.append((str(frontend_out), 'public'))\nprint(f\"[OK] Frontend:  {frontend_out} -> public/\")\n\n# Backend directories\nfor dirname in ['data', 'json', 'logs']:\n    src = backend_root / dirname\n    if src.exists():\n        datas.append((str(src), dirname))\n        print(f\"[OK] {dirname:8}: {src}\")\n    else:\n        print(f\"[WARN] {dirname:8}: Not found (will create)\")\n\nprint(f\"\\nTotal data entries: {len(datas)}\")\n\n# ===== HIDDEN IMPORTS =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"COLLECTING HIDDEN IMPORTS\")\nprint(\"=\"*70)\n\ncore_imports = [\n    'uvicorn', 'uvicorn.logging', 'uvicorn.loops', 'uvicorn.loops.auto',\n    'uvicorn.protocols', 'uvicorn.protocols.http', 'uvicorn.protocols.http.auto',\n    'uvicorn.protocols.http.h11_impl', 'uvicorn.lifespan', 'uvicorn.lifespan.on',\n    'fastapi', 'fastapi.routing', 'starlette', 'starlette.applications',\n    'starlette.routing', 'starlette.responses', 'starlette.staticfiles',\n    'pydantic', 'pydantic_core', 'pydantic_settings',\n    'anyio', 'structlog', 'tenacity', 'sqlalchemy', 'greenlet', 'win32timezone'\n]\n\nbackend_submodules = collect_submodules('web_service.backend')\nhiddenimports = list(set(core_imports + backend_submodules))\n\nprint(f\"Core imports:           {len(core_imports)}\")\nprint(f\"Backend submodules:     {len(backend_submodules)}\")\nprint(f\"Total hidden imports:   {len(hiddenimports)}\")\n\n# ===== ANALYSIS =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"CREATING PYINSTALLER ANALYSIS\")\nprint(\"=\"*70)\n\na = Analysis(\n    [str(main_py)],\n    pathex=[str(project_root), str(backend_root)],\n    binaries=[],\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    cipher=block_cipher,\n    noarchive=False,\n)\n\nprint(f\"[OK] Analysis created\")\nprint(f\"   Scripts:       {len(a.scripts)}\")\nprint(f\"   Pure modules:  {len(a.pure)}\")\nprint(f\"   Binaries:      {len(a.binaries)}\")\nprint(f\"   Data files:    {len(a.datas)}\")\n\n# ===== BUILD =====\nprint(\"\\n\" + \"=\"*70)\nprint(\"BUILDING EXECUTABLE\")\nprint(\"=\"*70)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [],\n    name='fortuna-monolith',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    runtime_tmpdir=None,\n    console=True,\n    disable_windowed_traceback=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon=None,\n)\n\ncoll = COLLECT(\n    exe, a.binaries, a.zipfiles, a.datas,\n    strip=False,\n    upx=True,\n    name='fortuna-monolith'\n)\n\nprint(f\"[OK] Spec file complete!\")\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "fortuna_monitor.py": "# fortuna_monitor.py - Windows-Optimized Version\n\nimport os\nimport sys\nimport threading\nimport time\nimport tkinter as tk\nfrom collections import deque\nfrom datetime import datetime\nfrom tkinter import filedialog\nfrom tkinter import messagebox\n\nimport httpx\nimport psutil\n\n# Try to import matplotlib for graphs\ntry:\n    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n    from matplotlib.figure import Figure\n\n    GRAPHS_AVAILABLE = True\nexcept ImportError:\n    GRAPHS_AVAILABLE = False\n\nAPI_BASE_URL = \"http://localhost:8000\"\n\n\nclass PerformanceTracker:\n    def __init__(self, max_history=100):\n        self.timestamps = deque(maxlen=max_history)\n        self.race_counts = deque(maxlen=max_history)\n        self.fetch_durations = deque(maxlen=max_history)\n        self.success_rates = deque(maxlen=max_history)\n        self.cpu_usage = deque(maxlen=max_history)\n        self.memory_usage = deque(maxlen=max_history)\n\n    def add_datapoint(self, races, duration, success_rate):\n        self.timestamps.append(datetime.now())\n        self.race_counts.append(races)\n        self.fetch_durations.append(duration)\n        self.success_rates.append(success_rate)\n        self.cpu_usage.append(psutil.cpu_percent(interval=None))\n        process = psutil.Process(os.getpid())\n        self.memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB\n\n    def export_to_csv(self, filename):\n        import csv\n\n        history = self.get_history()\n        with open(filename, \"w\", newline=\"\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"Timestamp\", \"Races\", \"Duration\", \"Success Rate\", \"CPU %\", \"Memory MB\"])\n            for i in range(len(history[\"times\"])):\n                writer.writerow(\n                    [\n                        history[\"times\"][i].isoformat(),\n                        history[\"races\"][i],\n                        history[\"durations\"][i],\n                        history[\"success\"][i],\n                        history[\"cpu\"][i],\n                        history[\"memory\"][i],\n                    ]\n                )\n\n    def get_history(self):\n        return {\n            \"times\": list(self.timestamps),\n            \"races\": list(self.race_counts),\n            \"durations\": list(self.fetch_durations),\n            \"success\": list(self.success_rates),\n            \"cpu\": list(self.cpu_usage),\n            \"memory\": list(self.memory_usage),\n        }\n\n\nclass FortunaAdvancedMonitor(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"Fortuna Faucet - Advanced Monitor\")\n        self.geometry(\"900x650\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.performance = PerformanceTracker()\n        self.running = True\n        self._create_widgets()\n        self.after(100, self.start_fetch_thread)\n\n    def _create_widgets(self):\n        self._create_control_panel()\n        # ... (rest of the widget creation)\n\n    def _create_control_panel(self):\n        control_frame = tk.Frame(self, bg=\"#1a1a2e\")\n        control_frame.pack(fill=tk.X, padx=15, pady=10)\n\n        tk.Button(\n            control_frame,\n            text=\"\ud83d\udcca Export Performance Data\",\n            command=self.export_data,\n            bg=\"#0f3460\",\n            fg=\"#ffffff\",\n            font=(\"Segoe UI\", 10, \"bold\"),\n            relief=tk.FLAT,\n            padx=25,\n            pady=10,\n        ).pack(side=tk.LEFT, padx=5)\n\n        tk.Button(\n            control_frame,\n            text=\"\ud83d\udcbb System Info\",\n            command=self.show_system_info,\n            bg=\"#0f3460\",\n            fg=\"#ffffff\",\n            font=(\"Segoe UI\", 10, \"bold\"),\n            relief=tk.FLAT,\n            padx=25,\n            pady=10,\n        ).pack(side=tk.LEFT, padx=5)\n\n    def start_fetch_thread(self):\n        self.fetch_thread = threading.Thread(target=self._fetch_data_loop, daemon=True)\n        self.fetch_thread.start()\n\n    def _fetch_data_loop(self):\n        while self.running:\n            try:\n                # Use httpx for async requests\n                with httpx.Client(headers={\"X-API-KEY\": self.api_key}, timeout=5) as client:\n                    response = client.get(f\"{API_BASE_URL}/api/adapters/status\")\n                if response.status_code == 200:\n                    data = response.json()\n                    # Add performance datapoint\n                    total_races = sum(a.get(\"races_fetched\", 0) for a in data)\n                    successful_adapters = [a for a in data if a.get(\"status\") == \"SUCCESS\"]\n                    success_rate = (len(successful_adapters) / len(data) * 100) if data else 0\n                    avg_duration = (\n                        sum(a.get(\"fetch_duration\", 0) for a in successful_adapters) / len(successful_adapters)\n                        if successful_adapters\n                        else 0\n                    )\n                    self.performance.add_datapoint(total_races, avg_duration, success_rate)\n\n                    self.after(0, self.update_ui, data)\n            except httpx.RequestError:\n                pass\n            time.sleep(10)  # Refresh interval\n\n    def update_ui(self, data):\n        # This is where you would update the tkinter UI with the new data\n        # For example, you might update a treeview or a graph\n        pass\n\n    def export_data(self):\n        filename = filedialog.asksaveasfilename(\n            defaultextension=\".csv\",\n            filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")],\n            initialfile=f\"fortuna_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n        )\n        if filename:\n            try:\n                self.performance.export_to_csv(filename)\n                messagebox.showinfo(\"Success\", f\"Data exported to {filename}\")\n            except Exception as e:\n                messagebox.showerror(\"Error\", f\"Export failed: {e}\")\n\n    def show_system_info(self):\n        vm = psutil.virtual_memory()\n        info = f\"\"\"\nSystem Information:\n\nCPU Usage: {psutil.cpu_percent(interval=1)}%\nCPU Cores: {psutil.cpu_count()}\nMemory Total: {vm.total / 1024 / 1024 / 1024:.2f} GB\nMemory Available: {vm.available / 1024 / 1024 / 1024:.2f} GB\nMemory Used: {vm.percent}%\n\nDisk Usage: {psutil.disk_usage(\"/\").percent}%\nPython Version: {sys.version.split()[0]}\n\"\"\"\n        messagebox.showinfo(\"System Information\", info)\n\n    def on_closing(self):\n        self.running = False\n        self.destroy()\n\n\nif __name__ == \"__main__\":\n    # Load .env variables\n    try:\n        from dotenv import load_dotenv\n\n        load_dotenv()\n    except ImportError:\n        print(\"Warning: dotenv is not installed. Script assumes environment variables are set.\")\n    app = FortunaAdvancedMonitor()\n    app.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n    app.mainloop()\n",
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "pre-build-check.sh": "#!/bin/bash\n\necho -e \"\\e[36m=== FORTUNA PRE-BUILD VERIFICATION ===\\e[0m\"\n\n# 1. Check all required files exist\necho -e \"\\n\\e[1m[1] Checking required files...\\e[0m\"\nrequired_files=(\n    \"web_service/backend/main.py\"\n    \"web_service/backend/api.py\"\n    \"web_service/backend/config.py\"\n    \"web_service/backend/port_check.py\"\n    \"web_service/backend/requirements.txt\"\n    \"web_service/frontend/package.json\"\n    \"web_service/frontend/next.config.js\"\n    \"fortuna-monolith.spec\"\n)\n\nmissing_files=()\nall_found=true\nfor file in \"${required_files[@]}\"; do\n    if [ -f \"$file\" ]; then\n        echo -e \"  \\e[32m\u2705 $file\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c $file\\e[0m\"\n        missing_files+=(\"$file\")\n        all_found=false\n    fi\ndone\n\nif [ \"$all_found\" = false ]; then\n    echo -e \"\\n\\e[31m\u274c FATAL: Missing files:\\e[0m\"\n    for file in \"${missing_files[@]}\"; do\n        echo \"  - $file\"\n    done\n    exit 1\nfi\n\n# 2. Test Python imports\necho -e \"\\n\\e[1m[2] Testing Python imports...\\e[0m\"\ncat > test_imports.py << EOL\nimport sys\nsys.path.insert(0, '.')\n\ntry:\n    from web_service.backend.api import app\n    print('\u2705 api.app imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import api.app: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.config import get_settings\n    settings = get_settings()\n    print(f'\u2705 config.get_settings imported (host={settings.UVICORN_HOST}, port={settings.FORTUNA_PORT})')\nexcept ImportError as e:\n    print(f'\u274c Failed to import config: {e}')\n    sys.exit(1)\n\ntry:\n    from web_service.backend.port_check import check_port_and_exit_if_in_use\n    print('\u2705 port_check.check_port_and_exit_if_in_use imported')\nexcept ImportError as e:\n    print(f'\u274c Failed to import port_check: {e}')\n    sys.exit(1)\n\nprint('\u2705 All imports successful')\nEOL\n\npython test_imports.py\nif [ $? -ne 0 ]; then\n    echo -e \"\\e[31m\u274c Import test FAILED\\e[0m\"\n    rm test_imports.py\n    exit 1\nfi\nrm test_imports.py\n\n# 3. Check frontend\necho -e \"\\n\\e[1m[3] Checking frontend...\\e[0m\"\nif [ -f \"web_service/frontend/next.config.js\" ]; then\n    if grep -q \"output: 'export'\" \"web_service/frontend/next.config.js\"; then\n        echo -e \"  \\e[32m\u2705 next.config.js has output: 'export'\\e[0m\"\n    else\n        echo -e \"  \\e[31m\u274c next.config.js missing output: 'export'\\e[0m\"\n        exit 1\n    fi\nelse\n    echo -e \"  \\e[33m\u26a0\ufe0f  next.config.js will be created during build\\e[0m\"\nfi\n\n# 4. Check spec file\necho -e \"\\n\\e[1m[4] Checking fortuna-monolith.spec...\\e[0m\"\nif [ -f \"fortuna-monolith.spec\" ]; then\n    if grep -q \"SPECPATH\" \"fortuna-monolith.spec\"; then\n        echo -e \"  \\e[32m\u2705 spec uses SPECPATH\\e[0m\"\n    else\n        echo -e \"  \\e[33m\u26a0\ufe0f  spec doesn't use SPECPATH (may have path issues)\\e[0m\"\n    fi\nelse\n    echo -e \"  \\e[31m\u274c fortuna-monolith.spec not found\\e[0m\"\n    exit 1\nfi\n\necho -e \"\\n\\e[32m\u2705 ALL CHECKS PASSED - Safe to build!\\e[0m\"\n",
    "scripts/fortuna_reporter.py": "#!/usr/bin/env python\n\"\"\"\nFortuna Unified Race Reporter\n\nGenerates HTML, JSON, and Markdown summary reports for GitHub Actions\nby directly invoking the OddsEngine and AnalyzerEngine without a live API.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any\n\n# Ensure the project root is in the path to allow for direct imports\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom web_service.backend.engine import OddsEngine\nfrom web_service.backend.analyzer import AnalyzerEngine\nfrom web_service.backend.config import get_settings\nfrom web_service.backend.models import Race\n\n\nclass LogLevel(Enum):\n    \"\"\"Log level enumeration with emoji support.\"\"\"\n    INFO = (\"INFO\", \"\u2139\ufe0f\")\n    SUCCESS = (\"SUCCESS\", \"\u2705\")\n    ERROR = (\"ERROR\", \"\u274c\")\n    WARNING = (\"WARNING\", \"\u26a0\ufe0f\")\n    DEBUG = (\"DEBUG\", \"\ud83d\udd0d\")\n\n    @property\n    def emoji(self) -> str:\n        return self.value[1]\n\n\n@dataclass\nclass ReporterConfig:\n    \"\"\"Configuration for the race reporter.\"\"\"\n    template_path: Path = field(default_factory=lambda: Path(\"scripts/templates/race_report_template.html\"))\n    html_output_path: Path = field(default_factory=lambda: Path(\"race-report.html\"))\n    json_output_path: Path = field(default_factory=lambda: Path(\"qualified_races.json\"))\n    markdown_summary_path: Path = field(default_factory=lambda: Path(\"github_summary.md\"))\n    raw_json_output_path: Path = field(default_factory=lambda: Path(\"raw_race_data.json\"))\n\n    max_retries: int = field(default_factory=lambda: int(os.getenv(\"MAX_RETRIES\", \"3\")))\n    request_timeout: int = field(default_factory=lambda: int(os.getenv(\"REQUEST_TIMEOUT\", \"30\")))\n    analyzer_type: str = field(default_factory=lambda: os.getenv(\"ANALYZER_TYPE\", \"tiny_field_trifecta\"))\n    force_refresh: bool = field(default_factory=lambda: os.getenv(\"FORCE_REFRESH\", \"false\").lower() == \"true\")\n    max_summary_races: int = 25\n\n    # All known adapters for exclusion logic\n    ALL_ADAPTERS: tuple[str, ...] = (\n        \"AtTheRacesAdapter\", \"BetfairAdapter\", \"BetfairGreyhoundAdapter\",\n        \"BrisnetAdapter\", \"EquibaseAdapter\", \"FanDuelAdapter\", \"GbgbApiAdapter\",\n        \"GreyhoundAdapter\", \"HarnessAdapter\", \"HorseRacingNationAdapter\",\n        \"NYRABetsAdapter\", \"OddscheckerAdapter\", \"PuntersAdapter\",\n        \"RacingAndSportsAdapter\", \"RacingAndSportsGreyhoundAdapter\",\n        \"RacingPostAdapter\", \"RacingTVAdapter\", \"SportingLifeAdapter\",\n        \"TabAdapter\", \"TheRacingApiAdapter\", \"TimeformAdapter\",\n        \"TwinSpiresAdapter\", \"TVGAdapter\", \"XpressbetAdapter\",\n        \"PointsBetGreyhoundAdapter\",\n    )\n\n    # Reliable adapters that don't require API keys.\n    # Note: The following adapters may experience issues and should be monitored:\n    # - Timeform: Occasional 500 errors (redirect to error page)\n    # - Equibase: 404 errors on some dates\n    # - Brisnet: Timeout/503 errors, possible rate limiting\n    # - Oddschecker: 403 Forbidden (bot detection)\n    # - RacingPost: 406 Not Acceptable (user agent issues)\n    RELIABLE_NON_KEYED_ADAPTERS: tuple[str, ...] = (\n        \"AtTheRacesAdapter\",\n        \"SportingLifeAdapter\",\n        \"TwinSpiresAdapter\",\n    )\n\n    @property\n    def excluded_adapters(self) -> list[str]:\n        \"\"\"No adapters will be excluded.\"\"\"\n        return []\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if self.max_retries < 1:\n            raise ValueError(\"max_retries must be at least 1\")\n        if self.request_timeout < 1:\n            raise ValueError(\"request_timeout must be at least 1 second\")\n        if self.max_summary_races < 1:\n            raise ValueError(\"max_summary_races must be at least 1\")\n\n        # Ensure output directories exist\n        for path in (self.html_output_path, self.json_output_path,\n                     self.markdown_summary_path, self.raw_json_output_path):\n            path.parent.mkdir(parents=True, exist_ok=True)\n\n\n@dataclass\nclass ReportMetrics:\n    \"\"\"Metrics collected during report generation.\"\"\"\n    total_races_fetched: int = 0\n    qualified_races: int = 0\n    adapters_used: list[str] = field(default_factory=list)\n    adapters_failed: list[str] = field(default_factory=list)\n    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    end_time: datetime | None = None\n    errors: list[str] = field(default_factory=list)\n\n    @property\n    def duration_seconds(self) -> float:\n        if self.end_time:\n            return (self.end_time - self.start_time).total_seconds()\n        return 0.0\n\n    def to_dict(self) -> dict[str, Any]:\n        return {\n            \"total_races_fetched\": self.total_races_fetched,\n            \"qualified_races\": self.qualified_races,\n            \"adapters_used\": self.adapters_used,\n            \"adapters_failed\": self.adapters_failed,\n            \"duration_seconds\": self.duration_seconds,\n            \"errors\": self.errors,\n            \"timestamp\": self.start_time.isoformat(),\n        }\n\n\nclass Reporter:\n    \"\"\"Main reporter class for generating race reports.\"\"\"\n\n    def __init__(self, config: ReporterConfig | None = None):\n        self.config = config or ReporterConfig()\n        self.metrics = ReportMetrics()\n\n    def log(self, message: str, level: LogLevel = LogLevel.INFO) -> None:\n        \"\"\"Print a timestamped log message.\"\"\"\n        timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(f\"[{timestamp}] {level.emoji} {message}\", flush=True)\n\n        if level == LogLevel.ERROR:\n            self.metrics.errors.append(message)\n\n    def generate_html_report(self, race_data: dict[str, Any]) -> bool:\n        \"\"\"Generates the HTML report from a template.\"\"\"\n        self.log(\"Generating HTML report...\")\n\n        try:\n            if not self.config.template_path.exists():\n                self.log(f\"Template not found at {self.config.template_path}\", LogLevel.ERROR)\n                return self._generate_fallback_html(race_data)\n\n            template = self.config.template_path.read_text(encoding=\"utf-8\")\n\n            # Inject data and metrics\n            race_data_with_metrics = {\n                **race_data,\n                \"generation_metrics\": self.metrics.to_dict(),\n            }\n\n            report_html = template.replace(\n                \"__RACE_DATA_PLACEHOLDER__\",\n                json.dumps(race_data_with_metrics, default=str)\n            )\n\n            self.config.html_output_path.write_text(report_html, encoding=\"utf-8\")\n            self.log(f\"Generated HTML report at {self.config.html_output_path}\", LogLevel.SUCCESS)\n            return True\n\n        except Exception as e:\n            self.log(f\"Failed to generate HTML report: {e}\", LogLevel.ERROR)\n            return self._generate_fallback_html(race_data)\n\n    def _generate_fallback_html(self, race_data: dict[str, Any]) -> bool:\n        \"\"\"Generate a minimal fallback HTML report if template fails.\"\"\"\n        self.log(\"Generating fallback HTML report...\", LogLevel.WARNING)\n\n        try:\n            races = race_data.get(\"races\", [])\n            html = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Fortuna Race Report (Fallback)</title>\n    <style>\n        body {{ font-family: system-ui, sans-serif; max-width: 800px; margin: 2rem auto; padding: 1rem; }}\n        .race {{ border: 1px solid #ccc; padding: 1rem; margin: 1rem 0; border-radius: 8px; }}\n        .error {{ color: #c00; background: #fee; padding: 1rem; border-radius: 4px; }}\n    </style>\n</head>\n<body>\n    <h1>\ud83d\udc34 Fortuna Race Report</h1>\n    <p class=\"error\">\u26a0\ufe0f This is a fallback report. The main template could not be loaded.</p>\n    <p>Found {len(races)} qualified race(s)</p>\n    <pre>{json.dumps(race_data, indent=2, default=str)}</pre>\n</body>\n</html>\"\"\"\n            self.config.html_output_path.write_text(html, encoding=\"utf-8\")\n            return True\n        except Exception as e:\n            self.log(f\"Failed to generate fallback HTML: {e}\", LogLevel.ERROR)\n            return False\n\n    def generate_markdown_summary(self, races: list[Race]) -> bool:\n        \"\"\"Generates a Markdown summary for the GitHub Actions UI.\"\"\"\n        self.log(\"Generating Markdown summary...\")\n\n        try:\n            lines = [\n                \"# \ud83d\udc34 Fortuna Race Report\",\n                \"\",\n                f\"**Generated:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\",\n                f\"**Analyzer:** `{self.config.analyzer_type}`\",\n                f\"**Duration:** {self.metrics.duration_seconds:.1f}s\",\n                \"\",\n            ]\n\n            if self.metrics.errors:\n                lines.extend([\n                    \"### \u26a0\ufe0f Warnings\",\n                    \"\",\n                    *[f\"- {e}\" for e in self.metrics.errors[:5]],\n                    \"\",\n                ])\n\n            if not races:\n                lines.append(\"### \ud83d\udd2d No races found matching filters.\")\n            else:\n                lines.extend([\n                    f\"### \u26a1 Found {len(races)} Qualified Race(s)\",\n                    \"\",\n                    \"| Score | Time | Venue | Race | Runners |\",\n                    \"|:-----:|:-----|:------|:----:|:-------:|\",\n                ])\n\n                for race in races[:self.config.max_summary_races]:\n                    start_time = race.start_time\n                    if isinstance(start_time, str):\n                        try:\n                            start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n                        except ValueError:\n                            start_time = None\n\n                    time_str = start_time.strftime('%H:%M') if start_time else \"N/A\"\n                    score = f\"{race.qualification_score:.1f}\" if race.qualification_score is not None else \"N/A\"\n                    venue = race.venue or \"Unknown\"\n                    race_num = race.race_number or \"?\"\n                    runners = len(race.runners) if race.runners else 0\n\n                    lines.append(f\"| {score} | {time_str} | **{venue}** | {race_num} | {runners} |\")\n\n                if len(races) > self.config.max_summary_races:\n                    lines.append(f\"\\n*...and {len(races) - self.config.max_summary_races} more races*\")\n\n            lines.extend([\n                \"\",\n                \"---\",\n                \"\",\n                \"<details>\",\n                \"<summary>\ud83d\udcca Generation Metrics</summary>\",\n                \"\",\n                f\"- Total races fetched: {self.metrics.total_races_fetched}\",\n                f\"- Qualified races: {self.metrics.qualified_races}\",\n                f\"- Adapters used: {len(self.metrics.adapters_used)}\",\n                f\"- Adapters failed: {len(self.metrics.adapters_failed)}\",\n                \"\",\n                \"</details>\",\n            ])\n\n            self.config.markdown_summary_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n            self.log(f\"Generated Markdown summary at {self.config.markdown_summary_path}\", LogLevel.SUCCESS)\n            return True\n\n        except Exception as e:\n            self.log(f\"Failed to write Markdown summary: {e}\", LogLevel.ERROR)\n            return False\n\n    def save_json(self, data: dict[str, Any], path: Path, description: str) -> bool:\n        \"\"\"Save JSON data with error handling.\"\"\"\n        try:\n            path.write_text(json.dumps(data, indent=2, default=str), encoding=\"utf-8\")\n            self.log(f\"Saved {description} to {path}\", LogLevel.SUCCESS)\n            return True\n        except Exception as e:\n            self.log(f\"Failed to save {description}: {e}\", LogLevel.ERROR)\n            return False\n\n    async def fetch_with_retry(\n        self,\n        odds_engine: OddsEngine,\n        date_str: str,\n    ) -> dict[str, Any]:\n        \"\"\"Fetch race data with retry logic.\"\"\"\n        last_error = None\n\n        for attempt in range(1, self.config.max_retries + 1):\n            try:\n                self.log(f\"Fetching race data (attempt {attempt}/{self.config.max_retries})...\")\n                data = await asyncio.wait_for(\n                    odds_engine.fetch_all_odds(date_str),\n                    timeout=self.config.request_timeout * 2\n                )\n                return data\n            except asyncio.TimeoutError:\n                last_error = \"Request timed out\"\n                self.log(f\"Attempt {attempt} timed out\", LogLevel.WARNING)\n            except Exception as e:\n                last_error = str(e)\n                self.log(f\"Attempt {attempt} failed: {e}\", LogLevel.WARNING)\n\n            if attempt < self.config.max_retries:\n                wait_time = 2 ** attempt  # Exponential backoff\n                self.log(f\"Waiting {wait_time}s before retry...\")\n                await asyncio.sleep(wait_time)\n\n        raise RuntimeError(f\"All {self.config.max_retries} fetch attempts failed. Last error: {last_error}\")\n\n    async def run(self) -> bool:\n        \"\"\"Main entry point with graceful degradation.\"\"\"\n        self.log(\"=== Fortuna Unified Race Reporter ===\")\n        self.log(f\"Analyzer: {self.config.analyzer_type}\")\n        self.log(f\"Excluding {len(self.config.excluded_adapters)} adapters\")\n\n        settings = get_settings()\n        odds_engine = OddsEngine(config=settings, exclude_adapters=self.config.excluded_adapters)\n        analyzer_engine = AnalyzerEngine()\n\n        # Pre-flight health check\n        healthy_adapters = []\n        for name, adapter in odds_engine.adapters.items():\n            try:\n                health = await adapter.health_check()\n                if health.get('circuit_breaker_state') == 'closed':\n                    healthy_adapters.append(name)\n            except Exception as e:\n                self.log(f\"Health check failed for {name}: {e}\", LogLevel.WARNING)\n\n        self.log(f\"Healthy adapters: {len(healthy_adapters)}/{len(odds_engine.adapters)}\")\n\n        if len(healthy_adapters) < 2:\n            self.log(\"Insufficient healthy adapters, report may be degraded\", LogLevel.WARNING)\n\n        today_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n\n        outputs_generated = {\n            \"raw_json\": False,\n            \"qualified_json\": False,\n            \"html\": False,\n            \"markdown\": False,\n        }\n\n        try:\n            aggregated_data = await self.fetch_with_retry(odds_engine, today_str)\n            outputs_generated[\"raw_json\"] = self.save_json(\n                aggregated_data, self.config.raw_json_output_path, \"raw race data\"\n            )\n\n            all_races_raw = aggregated_data.get(\"races\", [])\n            self.metrics.total_races_fetched = len(all_races_raw)\n\n            if not all_races_raw:\n                self.log(\"No races returned from OddsEngine. This is a critical failure.\", LogLevel.ERROR)\n                self.metrics.end_time = datetime.now(timezone.utc)\n                self.generate_markdown_summary([]) # Generate a summary showing failure\n                return False\n\n            all_races = []\n            validation_errors = []\n\n            for i, race_data in enumerate(all_races_raw):\n                try:\n                    all_races.append(Race(**race_data))\n                except Exception as e:\n                    error_msg = f\"Race {i} ({race_data.get('venue', 'unknown')}): {str(e)}\"\n                    validation_errors.append(error_msg)\n                    self.log(f\"Failed to validate race {i}: {e}\", LogLevel.WARNING)\n\n            self.log(f\"Validated {len(all_races)}/{len(all_races_raw)} races\")\n\n            if len(all_races) == 0 and len(all_races_raw) > 0:\n                self.log(\"All races failed validation! Check schema compatibility.\", LogLevel.ERROR)\n                self.save_json({\n                    \"error\": \"All races failed validation\",\n                    \"total_raw_races\": len(all_races_raw),\n                    \"validation_errors\": validation_errors[:10]\n                }, Path(\"validation_errors.json\"), \"validation errors\")\n                return False\n\n            if validation_errors:\n                self.save_json({\n                    \"validation_errors\": validation_errors\n                }, Path(\"validation_warnings.json\"), \"validation warnings\")\n\n            self.log(f\"Analyzing with '{self.config.analyzer_type}' analyzer...\")\n            analyzer = analyzer_engine.get_analyzer(self.config.analyzer_type)\n            result = analyzer.qualify_races(all_races)\n\n            qualified_races = result.get(\"races\", [])\n            criteria = result.get(\"criteria\", {})\n            self.metrics.qualified_races = len(qualified_races)\n            self.log(f\"Found {len(qualified_races)} qualified races\", LogLevel.SUCCESS)\n\n            report_data = {\n                \"races\": [r.model_dump(mode='json') for r in qualified_races],\n                \"analysis_metadata\": criteria,\n                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                \"analyzer\": self.config.analyzer_type,\n            }\n\n            outputs_generated[\"qualified_json\"] = self.save_json(\n                report_data, self.config.json_output_path, \"qualified races JSON\"\n            )\n            outputs_generated[\"html\"] = self.generate_html_report(report_data)\n            outputs_generated[\"markdown\"] = self.generate_markdown_summary(qualified_races)\n\n        except Exception as e:\n            self.log(f\"Critical error: {e}\", LogLevel.ERROR)\n            # Still try to generate a failure report\n            self.generate_markdown_summary([])\n\n        finally:\n            self.metrics.end_time = datetime.now(timezone.utc)\n            await odds_engine.close()\n\n            successful_outputs = sum(outputs_generated.values())\n            self.log(f\"Generated {successful_outputs}/{len(outputs_generated)} outputs\")\n\n        return all(outputs_generated.values())\n\n\nasync def main() -> int:\n    \"\"\"CLI entry point.\"\"\"\n    reporter = Reporter()\n    success = await reporter.run()\n    return 0 if success else 1\n\n\nif __name__ == \"__main__\":\n    exit_code = asyncio.run(main())\n    sys.exit(exit_code)\n",
    "scripts/install_fortuna_gui.bat": "@echo off\nREM Interactive MSI installation with standard Windows UI\n\ntitle Fortuna Faucet Installation Wizard\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Administrator privileges required\n    echo Please right-click this file and select \"Run as Administrator\"\n    pause\n    exit /b 1\n)\n\nREM Assumes the MSI is in the 'dist' subfolder relative to the project root\nmsiexec.exe /i \"..\\dist\\Fortuna-Faucet-2.1.0-x64.msi\" /L*v \"%TEMP%\\fortuna_install.log\"\n\nif %errorlevel% equ 0 (\n    echo Installation completed successfully!\n    echo Access dashboard at: http://localhost:3000\n) else (\n    echo Installation failed. Log: %TEMP%\\fortuna_install.log\n)\npause",
    "scripts/prepare_minimal_build.py": "# scripts/prepare_minimal_build.py\nimport os\nimport shutil\n\n# This script prepares the source tree for a 'minimal' build.\n# A minimal build includes only the core application and a small, curated\n# set of essential data adapters, excluding the larger, more specialized ones.\n\nADAPTERS_TO_KEEP = [\n    \"__init__.py\",\n    \"base_adapter.py\",\n    \"handler_factory.py\",\n    # --- Essential Adapters ---\n    \"betfair_adapter.py\",\n    \"sporting_life_adapter.py\",\n    \"racing_post_adapter.py\",\n]\n\n\ndef main():\n    \"\"\"\n    Removes non-essential adapter files from the python_service/adapters\n    directory to create a minimal build artifact.\n    \"\"\"\n    adapters_dir = os.path.join(\"python_service\", \"adapters\")\n    if not os.path.isdir(adapters_dir):\n        print(f\"[ERROR] Adapters directory not found at: {adapters_dir}\")\n        exit(1)\n\n    print(f\"Scanning adapters directory: {adapters_dir}\")\n    removed_count = 0\n    for filename in os.listdir(adapters_dir):\n        if filename not in ADAPTERS_TO_KEEP:\n            file_path = os.path.join(adapters_dir, filename)\n            try:\n                if os.path.isfile(file_path):\n                    os.remove(file_path)\n                    print(f\"  - Removed file: {filename}\")\n                    removed_count += 1\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n                    print(f\"  - Removed directory: {filename}\")\n                    removed_count += 1\n            except OSError as e:\n                print(f\"[ERROR] Failed to remove {file_path}: {e}\")\n                exit(1)\n\n    print(f\"\\nMinimal build preparation complete. Removed {removed_count} non-essential adapter(s).\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "scripts/uninstall_fortuna.bat": "@echo off\nREM Complete removal of Fortuna Faucet\n\nnet session >nul 2>&1\nif %errorlevel% neq 0 (\n    echo ERROR: Admin rights required\n    exit /b 1\n)\n\necho WARNING: This will remove Fortuna Faucet completely.\nset /p confirm=\"Are you sure? (y/N): \"\n\nif /i not \"%confirm%\"==\"y\" exit /b 0\n\nREM Find and remove MSI by UpgradeCode\nfor /f \"tokens=2 delims=\" %%A in ('wmic product where \"Name like 'Fortuna Faucet%%'\" get IdentifyingNumber /value') do (\n    for /f \"tokens=2 delims==\" %%B in (\"%%A\") do (\n        msiexec.exe /x %%B /qn /l*v \"%TEMP%\\fortuna_uninstall.log\"\n    )\n)\n\nREM Clean up directories\nif exist \"%PROGRAMFILES%\\Fortuna Faucet\" rmdir /s /q \"%PROGRAMFILES%\\Fortuna Faucet\" 2>nul\nif exist \"%APPDATA%\\Fortuna Faucet\" rmdir /s /q \"%APPDATA%\\Fortuna Faucet\" 2>nul\n\necho Uninstall complete.",
    "setup.py": "# setup.py\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nwith open(\"requirements.txt\") as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"fortuna_engine\",\n    version=\"1.0.0\",\n    packages=find_packages(),\n    author=\"Jules\",\n    author_email=\"\",\n    description=\"The Python backend for the Fortuna Faucet application.\",\n    long_description=\"This package contains the FastAPI server and all related data adapters and analysis tools.\",\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"fortuna-engine=python_service.run_api:main\",\n        ],\n    },\n    include_package_data=True,\n    package_data={\n        \"python_service\": [\"*.py\"],\n    },\n)\n",
    "tests/adapters/test_the_racing_api_adapter.py": "from datetime import date\nfrom datetime import datetime\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom unittest.mock import AsyncMock\n\nimport pytest\n\nfrom python_service.adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom python_service.core.exceptions import AdapterConfigError\nfrom tests.conftest import get_test_settings\n\n\n@pytest.fixture\ndef test_settings():\n    \"\"\"Provides a valid Settings object for testing.\"\"\"\n    return get_test_settings()\n\n\ndef test_init_raises_config_error_if_no_key():\n    \"\"\"\n    Tests that the adapter raises an AdapterConfigError if the API key is not set.\n    \"\"\"\n    settings_no_key = get_test_settings()\n    settings_no_key.THE_RACING_API_KEY = None\n    with pytest.raises(AdapterConfigError) as excinfo:\n        TheRacingApiAdapter(config=settings_no_key)\n    assert \"THE_RACING_API_KEY is not configured\" in str(excinfo.value)\n\n\n@pytest.mark.asyncio\nasync def test_get_races_parses_correctly(test_settings):\n    \"\"\"\n    Tests that TheRacingApiAdapter correctly parses a valid API response via get_races.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    off_time = datetime.now(timezone.utc)\n\n    mock_api_response = {\n        \"racecards\": [\n            {\n                \"race_id\": \"12345\",\n                \"course\": \"Newbury\",\n                \"race_no\": 3,\n                \"off_time\": off_time.isoformat().replace(\"+00:00\", \"Z\"),\n                \"race_name\": \"The Great Race\",\n                \"distance_f\": \"1m 2f\",\n                \"runners\": [\n                    {\n                        \"horse\": \"Speedy Steed\",\n                        \"number\": 1,\n                        \"jockey\": \"T. Rider\",\n                        \"trainer\": \"A. Trainer\",\n                        \"odds\": [{\"odds_decimal\": \"5.50\"}],\n                    },\n                    {\n                        \"horse\": \"Gallant Gus\",\n                        \"number\": 2,\n                        \"jockey\": \"J. Jockey\",\n                        \"trainer\": \"B. Builder\",\n                        \"odds\": [{\"odds_decimal\": \"3.25\"}],\n                    },\n                ],\n            }\n        ]\n    }\n\n    # Patch the internal _fetch_data method\n    adapter._fetch_data = AsyncMock(return_value=mock_api_response)\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert len(races) == 1\n    race = races[0]\n    assert race.id == \"tra_12345\"\n    assert race.venue == \"Newbury\"\n    assert len(race.runners) == 2\n    runner1 = race.runners[0]\n    assert runner1.name == \"Speedy Steed\"\n    assert runner1.odds[adapter.source_name].win == Decimal(\"5.50\")\n\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_empty_response(test_settings):\n    \"\"\"\n    Tests that the adapter returns an empty list for an API response with no racecards.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(return_value={\"racecards\": []})\n\n    # ACT\n    races = [race async for race in adapter.get_races(today)]\n\n    # ASSERT\n    assert races == []\n\n\n@pytest.mark.asyncio\nasync def test_get_races_raises_exception_on_api_failure(test_settings):\n    \"\"\"\n    Tests that get_races propagates the exception when _fetch_data fails.\n    This is the desired behavior for the OddsEngine to handle it.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=test_settings)\n    today = date.today().strftime(\"%Y-%m-%d\")\n    adapter._fetch_data = AsyncMock(side_effect=Exception(\"API is down\"))\n\n    # ACT & ASSERT\n    with pytest.raises(Exception, match=\"API is down\"):\n        _ = [race async for race in adapter.get_races(today)]\n",
    "tests/fixtures/at_the_races_greyhounds.html": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <title>Racecard</title>\n</head>\n<body>\n    <link rel=\"canonical\" href=\"/racecard/GB/Monmore/2025-10-29/1817/1\" />\n    <h1 class=\"heading-racecard-title\">Monmore | 18:17</h1>\n    <div class=\"table-default__row--card-runner\">\n        <div class=\"table-default__cell\">\n            <span class=\"runner-number__no\">1</span>\n            <div class=\"runner-cloth-name\">\n                <span class=\"runner-cloth-name__name\">Crossfield Larry</span>\n            </div>\n            <button class=\"bet-selector__odds\">5/2</button>\n        </div>\n    </div>\n    <div class=\"table-default__row--card-runner\">\n        <div class=\"table-default__cell\">\n            <span class=\"runner-number__no\">2</span>\n            <div class=\"runner-cloth-name\">\n                <span class=\"runner-cloth-name__name\">Stouke A Star</span>\n            </div>\n            <button class=\"bet-selector__odds\">11/4</button>\n        </div>\n    </div>\n</body>\n</html>\n",
    "tests/test_api.py": "# tests/test_api.py\nfrom datetime import date\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import patch\n\nimport aiosqlite\nimport pytest\n\n# --- Fixtures ---\nfrom python_service.models import AggregatedResponse\n\n# The client fixture is now correctly sourced from conftest.py,\n# which handles the settings override globally.\n\n# --- API Tests ---\n\n\n@pytest.mark.asyncio\n@patch(\"python_service.engine.OddsEngine.fetch_all_odds\", new_callable=AsyncMock)\nasync def test_get_races_endpoint_success(mock_fetch_all_odds, client):\n    \"\"\"\n    SPEC: The /api/races endpoint should return data with a valid API key.\n    \"\"\"\n    # ARRANGE\n    today = date.today()\n    mock_response = AggregatedResponse(\n        date=today,\n        races=[],\n            errors=[],\n        sources=[],\n        metadata={},\n        # This was the missing field causing the validation error\n        source_info=[],\n    )\n    mock_fetch_all_odds.return_value = mock_response.model_dump()\n    from tests.conftest import get_test_settings\n    settings = get_test_settings()\n    headers = {\"X-API-Key\": settings.API_KEY}\n\n    # ACT\n    response = await client.get(f\"/api/races?race_date={today.isoformat()}\", headers=headers)\n\n    # ASSERT\n    assert response.status_code == 200\n    mock_fetch_all_odds.assert_awaited_once()\n\n\n@pytest.mark.asyncio\nasync def test_get_tipsheet_endpoint_success(tmp_path, client):\n    \"\"\"\n    SPEC: The /api/tipsheet endpoint should return a list of tipsheet races from the database.\n    \"\"\"\n    db_path = tmp_path / \"test.db\"\n    post_time = datetime.now()\n\n    with patch(\"python_service.api.DB_PATH\", db_path):\n        async with aiosqlite.connect(db_path) as db:\n            await db.execute(\n                \"\"\"\n                CREATE TABLE tipsheet (\n                    race_id TEXT PRIMARY KEY,\n                    track_name TEXT,\n                    race_number INTEGER,\n                    post_time TEXT,\n                    score REAL,\n                    factors TEXT\n                )\n            \"\"\"\n            )\n            await db.execute(\n                \"INSERT INTO tipsheet VALUES (?, ?, ?, ?, ?, ?)\",\n                (\"test_race_1\", \"Test Park\", 1, post_time.isoformat(), 85.5, \"{}\"),\n            )\n            await db.commit()\n\n        # ACT\n        response = await client.get(f\"/api/tipsheet?date={post_time.date().isoformat()}\")\n\n        # ASSERT\n        assert response.status_code == 200\n        response_data = response.json()\n        assert len(response_data) == 1\n        # The database returns snake_case, but the Pydantic model is camelCase\n        assert response_data[0][\"raceId\"] == \"test_race_1\"\n        assert response_data[0][\"score\"] == 85.5\n\n\n@pytest.mark.asyncio\nasync def test_health_check_unauthenticated(client):\n    \"\"\"Ensures the /health endpoint is accessible without an API key.\"\"\"\n    response = await client.get(\"/health\")\n    assert response.status_code == 200\n    json_response = response.json()\n    assert json_response[\"status\"] == \"healthy\"\n\n\n@pytest.mark.asyncio\nasync def test_api_key_authentication_failure(client):\n    \"\"\"Ensures that endpoints are protected and fail with an invalid API key.\"\"\"\n    response = await client.get(\"/api/races/qualified/trifecta\", headers={\"X-API-KEY\": \"invalid_key\"})\n    assert response.status_code == 403\n    assert \"Invalid or missing API Key\" in response.json()[\"detail\"]\n\n\n@pytest.mark.asyncio\nasync def test_api_key_authentication_missing(client):\n    \"\"\"Ensures that endpoints are protected and fail with a missing API key.\"\"\"\n    response = await client.get(\"/api/races/qualified/trifecta\")\n    assert response.status_code == 403\n    assert \"Not authenticated\" in response.json()[\"detail\"]\n",
    "tests/test_manual_override.py": "# tests/test_manual_override.py\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom python_service.api import app\nfrom python_service.api import get_settings\nfrom python_service.manual_override_manager import ManualOverrideManager\nfrom tests.conftest import get_test_settings\n\n# Override settings for tests\napp.dependency_overrides[get_settings] = get_test_settings\n_settings = get_test_settings()\nAPI_KEY = getattr(_settings, \"API_KEY\", \"test-override-key-123\")\n\n\n@pytest.fixture\ndef manager() -> ManualOverrideManager:\n    \"\"\"Provides a clean ManualOverrideManager instance for each test.\"\"\"\n    return ManualOverrideManager()\n\n\ndef test_register_failure(manager: ManualOverrideManager):\n    adapter_name = \"TestAdapter\"\n    url = \"http://test.com/races\"\n    request_id = manager.register_failure(adapter_name, url)\n    assert request_id is not None\n    pending = manager.get_pending_requests()\n    assert len(pending) == 1\n    assert pending[0].request_id == request_id\n    assert pending[0].adapter_name == adapter_name\n    assert pending[0].url == url\n\n\ndef test_submit_manual_data(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.submit_manual_data(request_id, \"<html></html>\", \"html\")\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is not None\n    assert data[0] == \"<html></html>\"\n    assert data[1] == \"html\"\n\n\ndef test_skip_request(manager: ManualOverrideManager):\n    request_id = manager.register_failure(\"TestAdapter\", \"http://test.com/races\")\n    success = manager.skip_request(request_id)\n    assert success\n    assert len(manager.get_pending_requests()) == 0\n    data = manager.get_manual_data(\"TestAdapter\", \"http://test.com/races\")\n    assert data is None\n\n\n@pytest.mark.asyncio\nasync def test_get_pending_overrides_endpoint(app, client):\n    # ARRANGE\n    # Access the manager *after* the TestClient has run the lifespan startup\n    manager = app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)  # Ensure a clean state by clearing all\n    manager.register_failure(\"EndpointAdapter\", \"http://endpoint.com/data\")\n\n    # ACT\n    response = await client.get(\"/api/manual-overrides/pending\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    data = response.json()\n    assert \"pending_requests\" in data\n    assert len(data[\"pending_requests\"]) > 0\n    assert data[\"pending_requests\"][0][\"adapter_name\"] == \"EndpointAdapter\"\n\n\n@pytest.mark.asyncio\nasync def test_submit_manual_data_endpoint(app, client):\n    # ARRANGE\n    manager = app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SubmitAdapter\", \"http://submit.com/data\")\n    submission = {\n        \"request_id\": request_id,\n        \"content\": \"<h1>Hello</h1>\",\n        \"content_type\": \"html\",\n    }\n    response = await client.post(\n        \"/api/manual-overrides/submit\",\n        json=submission,\n        headers={\"X-API-Key\": API_KEY},\n    )\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    data = manager.get_manual_data(\"SubmitAdapter\", \"http://submit.com/data\")\n    assert data is not None\n    assert data[0] == \"<h1>Hello</h1>\"\n\n\n@pytest.mark.asyncio\nasync def test_skip_manual_override_endpoint(app, client):\n    # ARRANGE\n    manager = app.state.manual_override_manager\n    manager.clear_old_requests(max_age_hours=-1)\n    request_id = manager.register_failure(\"SkipAdapter\", \"http://skip.com/data\")\n    response = await client.post(f\"/api/manual-overrides/skip/{request_id}\", headers={\"X-API-Key\": API_KEY})\n    assert response.status_code == 200\n    assert response.json()[\"status\"] == \"success\"\n    # Verify the request is no longer pending\n    pending = manager.get_pending_requests()\n    assert not any(p.request_id == request_id for p in pending)\n",
    "tests/test_uninstall.ps1": "Write-Host \"Testing uninstall...\" -ForegroundColor Cyan\n\n$regPath = 'HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*'\n$product = Get-ItemProperty $regPath | Where-Object { $_.DisplayName -like '*Fortuna*' }\n\nif ($product) {\n    & msiexec.exe /x $product.PSChildName /qn /l*v \"uninstall_test.log\"\n\n    Start-Sleep -Seconds 2\n\n    $programFiles = \"$env:PROGRAMFILES\\Fortuna Faucet\"\n    if (-not (Test-Path $programFiles)) {\n        Write-Host \"\u2713 Uninstall successful\"\n    } else {\n        Write-Host \"\u2717 Uninstall incomplete\"\n        exit 1\n    }\n} else {\n    Write-Host \"\u2717 Product not found in registry\"\n    exit 1\n}",
    "verify_connection.py": "# verify_connection.py\n\nimport asyncio\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\n\nfrom playwright.sync_api import sync_playwright\n\n# --- Configuration ---\nLOG_LEVEL = logging.INFO\n# Set paths relative to the script's location\nSCRIPT_DIR = Path(__file__).parent.resolve()\nSCREENSHOT_DIR = SCRIPT_DIR / \"verification\"\nFRONTEND_LOG_PATH = SCRIPT_DIR / \"frontend.log\"\nBACKEND_LOG_PATH = SCRIPT_DIR / \"backend.log\"\nFRONTEND_DIR = SCRIPT_DIR / \"web_platform\" / \"frontend\"\nBACKEND_DIR = SCRIPT_DIR / \"python_service\"\nBACKEND_ENTRYPOINT = BACKEND_DIR / \"api.py\"\n\n# --- Setup Logging ---\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\n\n\ndef start_backend():\n    \"\"\"Starts the FastAPI backend as a background process.\"\"\"\n    logging.info(\"Starting backend server...\")\n    # Ensure environment is set up for backend\n    backend_env = os.environ.copy()\n    backend_env[\"PYTHONPATH\"] = str(SCRIPT_DIR)\n    backend_env[\"API_KEY\"] = \"a_secure_test_api_key_that_is_long_enough\"\n    backend_env[\"ALLOWED_ORIGINS\"] = '[\"http://localhost:3000\", \"http://127.0.0.1:3000\"]'\n\n    # Use shell=True for Windows compatibility if needed, but separate args is better\n    command = [\n        sys.executable,\n        \"-m\",\n        \"uvicorn\",\n        \"python_service.api:app\",\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        \"8000\",\n    ]\n    try:\n        process = subprocess.Popen(\n            command,\n            cwd=SCRIPT_DIR,\n            stdout=open(BACKEND_LOG_PATH, \"w\"),\n            stderr=subprocess.STDOUT,\n            env=backend_env,\n        )\n        logging.info(f\"Backend process started with PID: {process.pid}\")\n        # Give the server a moment to initialize\n        time.sleep(5)\n        return process\n    except FileNotFoundError:\n        logging.error(\n            \"uvicorn command not found. Make sure it's installed in the environment.\"\n        )\n        return None\n    except Exception as e:\n        logging.error(f\"Failed to start backend: {e}\", exc_info=True)\n        return None\n\n\ndef start_frontend():\n    \"\"\"Starts the Next.js frontend dev server as a background process.\"\"\"\n    logging.info(\"Starting frontend development server...\")\n    try:\n        # Check for node_modules and run npm install if not present\n        if not (FRONTEND_DIR / \"node_modules\").exists():\n            logging.info(\"node_modules not found. Running 'npm install'...\")\n            install_process = subprocess.run(\n                [\"npm\", \"install\"],\n                cwd=FRONTEND_DIR,\n                check=True,\n                capture_output=True,\n                text=True,\n            )\n            logging.info(install_process.stdout)\n            if install_process.returncode != 0:\n                logging.error(\"npm install failed!\")\n                logging.error(install_process.stderr)\n                return None\n\n        # Start the dev server\n        process = subprocess.Popen(\n            [\"npm\", \"run\", \"dev\"],\n            cwd=FRONTEND_DIR,\n            stdout=open(FRONTEND_LOG_PATH, \"w\"),\n            stderr=subprocess.STDOUT,\n        )\n        logging.info(f\"Frontend process started with PID: {process.pid}\")\n        return process\n    except FileNotFoundError:\n        logging.error(\n            \"npm command not found. Make sure Node.js and npm are installed and in the PATH.\"\n        )\n        return None\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"npm install failed: {e.stderr}\")\n        return None\n    except Exception as e:\n        logging.error(f\"Failed to start frontend: {e}\", exc_info=True)\n        return None\n\n\ndef get_frontend_port_from_logs(log_path: Path, timeout: int = 30) -> int:\n    \"\"\"Parses the frontend log file to find the port the dev server is running on.\"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        try:\n            if log_path.exists():\n                with open(log_path, \"r\") as f:\n                    for line in f:\n                        if \"Local:\" in line and \"http://localhost:\" in line:\n                            port_str = line.split(\"http://localhost:\")[1].strip()\n                            if port_str.isdigit():\n                                port = int(port_str)\n                                logging.info(f\"Detected frontend port: {port}\")\n                                return port\n        except Exception as e:\n            logging.warning(f\"Could not read frontend log yet, retrying... Error: {e}\")\n        time.sleep(1)\n    logging.error(f\"Could not determine frontend port after {timeout} seconds.\")\n    return 3000 # Fallback\n\ndef verify_connection():\n    \"\"\"\n    Uses Playwright to verify the frontend can connect to the backend.\n    Captures a screenshot for visual confirmation.\n    \"\"\"\n    backend_process = None\n    frontend_process = None\n    success = False\n\n    try:\n        backend_process = start_backend()\n        if not backend_process or backend_process.poll() is not None:\n            logging.error(\"Backend failed to start or crashed.\")\n            return\n\n        frontend_process = start_frontend()\n        if not frontend_process or frontend_process.poll() is not None:\n            logging.error(\"Frontend failed to start or crashed.\")\n            return\n\n        logging.info(\"Waiting for frontend to be ready and getting port...\")\n        port = get_frontend_port_from_logs(FRONTEND_LOG_PATH)\n\n\n        with sync_playwright() as p:\n            logging.info(\"Launching browser...\")\n            browser = p.chromium.launch(headless=True)\n            page = browser.new_page()\n\n            logging.info(f\"Navigating to frontend URL at port {port}...\")\n            page.goto(f\"http://localhost:{port}\")\n\n            # Wait for a specific element that indicates a successful connection\n            # or a definitive disconnected state.\n            logging.info(\"Waiting for connection status indicator...\")\n            try:\n                # Wait up to 30 seconds for either a 'Connected' or 'Failed' state\n                page.wait_for_selector(\n                    'text=/Connecting...|Connected|Connection Failed/',\n                    timeout=30000\n                )\n\n                # Check the current status\n                connection_status = page.locator('//button[contains(@class, \"rounded-full\")]').inner_text()\n                logging.info(f\"Connection status found: {connection_status}\")\n                if \"Connected\" in connection_status:\n                    logging.info(\"Successfully connected to the backend.\")\n                    success = True\n                else:\n                    logging.error(f\"Frontend indicated a connection failure: {connection_status}\")\n\n\n            except Exception as e:\n                logging.error(f\"Failed to find connection status indicator: {e}\", exc_info=True)\n\n            logging.info(\"Capturing screenshot...\")\n            SCREENSHOT_DIR.mkdir(exist_ok=True)\n            screenshot_path = SCREENSHOT_DIR / \"debug_screenshot.png\"\n            page.screenshot(path=str(screenshot_path))\n            logging.info(f\"Screenshot saved to {screenshot_path}\")\n\n            browser.close()\n\n    finally:\n        logging.info(\"Cleaning up processes...\")\n        if frontend_process and frontend_process.poll() is None:\n            logging.info(f\"Terminating frontend process {frontend_process.pid}\")\n            frontend_process.terminate()\n            frontend_process.wait()\n        if backend_process and backend_process.poll() is None:\n            logging.info(f\"Terminating backend process {backend_process.pid}\")\n            backend_process.terminate()\n            backend_process.wait()\n        logging.info(\"Cleanup complete.\")\n        # Exit with success or failure code\n        if not success:\n            sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    verify_connection()\n",
    "web_service/backend/adapters/__init__.py": "# python_service/adapters/__init__.py\n# TEMPORARY FIX: Comment out the problematic adapter\n\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\n\n# from .betfair_datascientist_adapter import BetfairDataScientistAdapter  # DISABLED: PyInstaller NumPy issue\nfrom .gbgb_api_adapter import GbgbApiAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .tvg_adapter import TVGAdapter\n\n__all__ = [\n    \"GbgbApiAdapter\",\n    \"TVGAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"AtTheRacesAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"SportingLifeAdapter\",\n    \"TimeformAdapter\",\n    \"HarnessAdapter\",\n    \"GreyhoundAdapter\",\n    \"TheRacingApiAdapter\",\n    # \"BetfairDataScientistAdapter\",  # DISABLED\n]\n",
    "web_service/backend/adapters/betfair_greyhound_adapter.py": "# python_service/adapters/betfair_greyhound_adapter.py\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\nfrom .betfair_auth_mixin import BetfairAuthMixin\n\n\nclass BetfairGreyhoundAdapter(BetfairAuthMixin, BaseAdapterV3):\n    \"\"\"Adapter for fetching greyhound racing data from the Betfair Exchange API, using V3 architecture.\"\"\"\n\n    SOURCE_NAME = \"BetfairGreyhounds\"\n    BASE_URL = \"https://api.betfair.com/exchange/betting/rest/v1.0/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw market catalogue for greyhound races on a given date.\"\"\"\n        await self._authenticate(self.http_client)\n        if not self.session_token:\n            self.logger.error(\"Authentication failed, cannot fetch data.\")\n            return None\n\n        start_time, end_time = self._get_datetime_range(date)\n\n        response = await self.make_request(\n            self.http_client,\n            method=\"post\",\n            url=f\"{self.BASE_URL}listMarketCatalogue/\",\n            json={\n                \"filter\": {\n                    \"eventTypeIds\": [\"4339\"],  # Greyhound Racing\n                    \"marketCountries\": [\"GB\", \"IE\", \"AU\"],\n                    \"marketTypeCodes\": [\"WIN\"],\n                    \"marketStartTime\": {\n                        \"from\": start_time.isoformat(),\n                        \"to\": end_time.isoformat(),\n                    },\n                },\n                \"maxResults\": 1000,\n                \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"],\n            },\n        )\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw market catalogue into a list of Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for market in raw_data:\n            try:\n                if race := self._parse_race(market):\n                    races.append(race)\n            except (KeyError, TypeError):\n                self.logger.warning(\n                    \"Failed to parse a Betfair Greyhound market.\",\n                    exc_info=True,\n                    market=market,\n                )\n                continue\n        return races\n\n    def _parse_race(self, market: dict) -> Optional[Race]:\n        \"\"\"Parses a single market from the Betfair API into a Race object.\"\"\"\n        market_id = market.get(\"marketId\")\n        event = market.get(\"event\", {})\n        market_start_time = market.get(\"marketStartTime\")\n\n        if not all([market_id, market_start_time]):\n            return None\n\n        start_time = datetime.fromisoformat(market_start_time.replace(\"Z\", \"+00:00\"))\n\n        runners = [\n            Runner(\n                number=runner.get(\"sortPriority\", i + 1),\n                name=runner.get(\"runnerName\"),\n                scratched=runner.get(\"status\") != \"ACTIVE\",\n                selection_id=runner.get(\"selectionId\"),\n            )\n            for i, runner in enumerate(market.get(\"runners\", []))\n            if runner.get(\"runnerName\")\n        ]\n\n        return Race(\n            id=f\"bfg_{market_id}\",\n            venue=event.get(\"venue\", \"Unknown Venue\"),\n            race_number=self._extract_race_number(market.get(\"marketName\", \"\")),\n            start_time=start_time,\n            runners=runners,\n            source=self.source_name,\n        )\n\n    def _extract_race_number(self, name: str) -> int:\n        \"\"\"Extracts the race number from a market name (e.g., 'R1 480m').\"\"\"\n        match = re.search(r\"\\bR(\\d{1,2})\\b\", name)\n        return int(match.group(1)) if match else 0\n\n    def _get_datetime_range(self, date_str: str):\n        # Helper to create a datetime range for the Betfair API\n        start_time = datetime.strptime(date_str, \"%Y-%m-%d\")\n        end_time = start_time + timedelta(days=1)\n        return start_time, end_time\n",
    "web_service/backend/adapters/greyhound_adapter.py": "# python_service/adapters/greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\n\nfrom pydantic import ValidationError\n\nfrom ..core.exceptions import AdapterConfigError\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass GreyhoundAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for fetching Greyhound racing data, migrated to BaseAdapterV3.\n    Activated by setting GREYHOUND_API_URL in .env.\n    \"\"\"\n\n    SOURCE_NAME = \"Greyhound Racing\"\n\n    def __init__(self, config=None):\n        if not hasattr(config, \"GREYHOUND_API_URL\") or not config.GREYHOUND_API_URL:\n            raise AdapterConfigError(self.SOURCE_NAME, \"GREYHOUND_API_URL is not configured.\")\n        super().__init__(\n            source_name=self.SOURCE_NAME,\n            base_url=config.GREYHOUND_API_URL,\n            config=config,\n        )\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"Fetches the raw card data from the greyhound API.\"\"\"\n        endpoint = f\"v1/cards/{date}\"\n        response = await self.make_request(\"GET\", endpoint)\n        return response.json() if response else None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses the raw card data into a list of Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"cards\"):\n            self.logger.warning(\"No 'cards' in greyhound response or empty list.\")\n            return []\n\n        all_races = []\n        for card in raw_data.get(\"cards\", []):\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            for race_data in card.get(\"races\", []):\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race_id = race_data.get(\"race_id\")\n                    race_number = race_data.get(\"race_number\")\n                    start_timestamp = race_data.get(\"start_time\")\n                    if not all([race_id, race_number, start_timestamp]):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_id}\",\n                        venue=venue,\n                        race_number=race_number,\n                        start_time=datetime.fromtimestamp(start_timestamp),\n                        runners=self._parse_runners(race_data.get(\"runners\", [])),\n                        source=self.source_name,\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    self.logger.error(\n                        \"Error parsing greyhound race\",\n                        race_id=race_data.get(\"race_id\", \"N/A\"),\n                        error=str(e),\n                    )\n                    continue\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                trap_number = runner_data.get(\"trap_number\")\n                dog_name = runner_data.get(\"dog_name\")\n                if not all([trap_number, dog_name]):\n                    continue\n\n                odds_data = {}\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n\n                runners.append(\n                    Runner(\n                        number=trap_number,\n                        name=dog_name,\n                        scratched=runner_data.get(\"scratched\", False),\n                        odds=odds_data,\n                    )\n                )\n            except (KeyError, ValidationError):\n                self.logger.warning(\"Error parsing greyhound runner, skipping.\", runner_data=runner_data)\n                continue\n        return runners\n",
    "web_service/backend/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom .base_adapter_v3 import BaseAdapterV3\n\n# NOTE: This is a hypothetical implementation based on a potential API structure.\n\n\nclass PointsBetGreyhoundAdapter(BaseAdapterV3):\n    \"\"\"Adapter for the hypothetical PointsBet Greyhound API, migrated to BaseAdapterV3.\"\"\"\n\n    SOURCE_NAME = \"PointsBetGreyhound\"\n    BASE_URL = \"https://api.pointsbet.com/api/v2/\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Fetches all greyhound events for a given date.\"\"\"\n        endpoint = f\"sports/greyhound-racing/events/by-date/{date}\"\n        response = await self.make_request(\"GET\", endpoint)\n        return response.json().get(\"events\", []) if response else None\n\n    def _parse_races(self, raw_data: Optional[List[Dict[str, Any]]]) -> List[Race]:\n        \"\"\"Parses the raw event data into a list of standardized Race objects.\"\"\"\n        if not raw_data:\n            return []\n\n        races = []\n        for event in raw_data:\n            try:\n                if not event.get(\"competitors\") or not event.get(\"startTime\"):\n                    continue\n\n                runners = []\n                for competitor in event.get(\"competitors\", []):\n                    price = competitor.get(\"price\")\n                    if not price:\n                        continue\n\n                    odds_val = Decimal(str(price))\n                    odds = {\n                        self.source_name: OddsData(\n                            win=odds_val,\n                            source=self.source_name,\n                            last_updated=datetime.now(),\n                        )\n                    }\n                    runner = Runner(\n                        number=competitor.get(\"number\", 99),\n                        name=competitor.get(\"name\", \"Unknown\"),\n                        odds=odds,\n                    )\n                    runners.append(runner)\n\n                if runners:\n                    race_id = event.get(\"id\")\n                    if not race_id:\n                        continue\n\n                    race = Race(\n                        id=f\"pbg_{race_id}\",\n                        venue=event.get(\"venue\", {}).get(\"name\", \"Unknown Venue\"),\n                        start_time=datetime.fromisoformat(event[\"startTime\"]),\n                        race_number=event.get(\"raceNumber\", 1),\n                        runners=runners,\n                        source=self.source_name,\n                    )\n                    races.append(race)\n            except (KeyError, TypeError, ValueError):\n                self.logger.warning(\n                    \"Failed to parse PointsBet Greyhound event.\",\n                    event=event,\n                    exc_info=True,\n                )\n                continue\n        return races\n",
    "web_service/backend/adapters/racingtv_adapter.py": "# python_service/adapters/racingtv_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass RacingTVAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for scraping data from racingtv.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"RacingTV\"\n    BASE_URL = \"https://www.racingtv.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom ..models import OddsData\nfrom ..models import Race\nfrom ..models import Runner\nfrom ..utils.odds import parse_odds_to_decimal\nfrom ..utils.text import clean_text\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com, migrated to BaseAdapterV3.\n    \"\"\"\n\n    SOURCE_NAME = \"Timeform\"\n    BASE_URL = \"https://www.timeform.com/horse-racing\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Optional[dict]:\n        \"\"\"\n        Fetches the raw HTML for all race pages for a given date.\n        \"\"\"\n        index_url = f\"/racecards/{date}\"\n        index_response = await self.make_request(\"GET\", index_url, headers=self._get_headers())\n        if not index_response or not index_response.text:\n            self.logger.warning(\"Failed to fetch Timeform index page\", url=index_url)\n            return None\n\n        # Save the raw HTML for debugging in CI\n        try:\n            with open(\"timeform_debug.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write(index_response.text)\n        except Exception as e:\n            self.logger.warning(\"Failed to save debug HTML for Timeform\", error=str(e))\n\n        index_soup = BeautifulSoup(index_response.text, \"html.parser\")\n        links = {a[\"href\"] for a in index_soup.select(\"a.rp-racecard-off-link[href]\")}\n\n        async def fetch_single_html(url_path: str):\n            response = await self.make_request(\"GET\", url_path, headers=self._get_headers())\n            return response.text if response else \"\"\n\n        tasks = [fetch_single_html(link) for link in links]\n        html_pages = await asyncio.gather(*tasks)\n        return {\"pages\": html_pages, \"date\": date}\n\n    def _get_headers(self) -> dict:\n        return {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"Host\": \"www.timeform.com\",\n            \"Pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n        }\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"Parses a list of raw HTML strings into Race objects.\"\"\"\n        if not raw_data or not raw_data.get(\"pages\"):\n            return []\n\n        try:\n            race_date = datetime.strptime(raw_data[\"date\"], \"%Y-%m-%d\").date()\n        except ValueError:\n            self.logger.error(\n                \"Invalid date format provided to TimeformAdapter\",\n                date=raw_data.get(\"date\"),\n            )\n            return []\n\n        all_races = []\n        for html in raw_data[\"pages\"]:\n            if not html:\n                continue\n            try:\n                soup = BeautifulSoup(html, \"html.parser\")\n\n                track_name_node = soup.select_one(\"h1.rp-raceTimeCourseName_name\")\n                if not track_name_node:\n                    continue\n                track_name = clean_text(track_name_node.get_text())\n\n                race_time_node = soup.select_one(\"span.rp-raceTimeCourseName_time\")\n                if not race_time_node:\n                    continue\n                race_time_str = clean_text(race_time_node.get_text())\n\n                start_time = datetime.combine(race_date, datetime.strptime(race_time_str, \"%H:%M\").time())\n\n                all_times = [clean_text(a.get_text()) for a in soup.select(\"a.rp-racecard-off-link\")]\n                race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n\n                runner_rows = soup.select(\"div.rp-horseTable_mainRow\")\n                if not runner_rows:\n                    continue\n\n                runners = [self._parse_runner(row) for row in runner_rows]\n                race = Race(\n                    id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\",\n                    venue=track_name,\n                    race_number=race_number,\n                    start_time=start_time,\n                    runners=[r for r in runners if r],  # Filter out None values\n                    source=self.source_name,\n                )\n                all_races.append(race)\n            except (AttributeError, ValueError, TypeError):\n                self.logger.warning(\"Error parsing a race from Timeform, skipping race.\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name_node = row.select_one(\"a.rp-horseTable_horse-name\")\n            if not name_node:\n                return None\n            name = clean_text(name_node.get_text())\n\n            num_node = row.select_one(\"span.rp-horseTable_horse-number\")\n            if not num_node:\n                return None\n            num_str = clean_text(num_node.get_text())\n            number_part = \"\".join(filter(str.isdigit, num_str.strip(\"()\")))\n            number = int(number_part)\n\n            odds_data = {}\n            if odds_tag := row.select_one(\"button.rp-bet-placer-btn__odds\"):\n                odds_str = clean_text(odds_tag.get_text())\n                if win_odds := parse_odds_to_decimal(odds_str):\n                    if win_odds < 999:\n                        odds_data = {\n                            self.source_name: OddsData(\n                                win=win_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now(),\n                            )\n                        }\n\n            return Runner(number=number, name=name, odds=odds_data)\n        except (AttributeError, ValueError, TypeError):\n            self.logger.warning(\"Failed to parse a runner from Timeform, skipping runner.\")\n            return None\n",
    "web_service/backend/adapters/xpressbet_adapter.py": "# python_service/adapters/xpressbet_adapter.py\nfrom typing import Any\nfrom typing import List\n\nfrom ..models import Race\nfrom .base_adapter_v3 import BaseAdapterV3\n\n\nclass XpressbetAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for xpressbet.com.\n    This adapter is a non-functional stub and has not been implemented.\n    \"\"\"\n\n    SOURCE_NAME = \"Xpressbet\"\n    BASE_URL = \"https://www.xpressbet.com\"\n\n    def __init__(self, config=None):\n        super().__init__(source_name=self.SOURCE_NAME, base_url=self.BASE_URL, config=config)\n\n    async def _fetch_data(self, date: str) -> Any:\n        \"\"\"This is a stub and does not fetch any data.\"\"\"\n        self.logger.warning(\n            f\"{self.source_name} is a non-functional stub and has not been implemented. It will not fetch any data.\"\n        )\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        \"\"\"This is a stub and does not parse any data.\"\"\"\n        return []\n",
    "web_service/backend/core/__init__.py": "",
    "web_service/backend/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return (\n                None,\n                \"Missing core fields (id, venue, race_number, start_time, runners)\",\n            )\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\"\n                        )\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "web_service/backend/health_check.py": "import socket\nimport sys\n\n\ndef is_port_available(port=8000):\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"127.0.0.1\", port))\n        sock.close()\n        return result != 0\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    if not is_port_available(8000):\n        print(\"ERROR: Port 8000 already in use. Kill existing process or use different port.\")\n        sys.exit(1)\n    print(\"Port 8000 available \u2713\")\n",
    "web_service/backend/middleware/__init__.py": "",
    "web_service/backend/port_check.py": "import socket\nimport sys\n\ndef check_port_and_exit_if_in_use(port: int, host: str):\n    \"\"\"Checks if a port is in use at the given host and exits if it is.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.bind((host, port))\n        except OSError:\n            print(f\"\u274c FATAL: Port {port} is already in use. Please close the other application or specify a different port.\")\n            sys.exit(1)\n",
    "web_service/backend/requirements-x86.txt": "#\n# This file is a modified version of requirements.txt for x86 architecture compatibility.\n# Some packages are pinned to older versions that provide x86 wheels.\n#\naiosqlite==0.17.0\n    # via -r web_service/backend/requirements.in\naltgraph==0.17.4\n    # via pyinstaller\nannotated-types==0.7.0\n    # via pydantic\nanyio==3.7.1\n    # via\n    #   httpx\n    #   starlette\n    #   watchfiles\nasync-timeout==5.0.1\n    # via redis\nbeautifulsoup4==4.12.3\n    # via -r web_service/backend/requirements.in\nblack==24.4.2\n    # via -r web_service/backend/requirements.in\nbuild==1.2.1\n    # via pip-tools\ncertifi==2024.7.4\n    # via\n    #   -r web_service/backend/requirements.in\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==1.16.0\n    # via cryptography\ncharset-normalizer==3.3.2\n    # via requests\nclick==8.1.7\n    # via\n    #   black\n    #   pip-tools\n    #   rich-toolkit\n    #   typer\n    #   uvicorn\ncryptography==42.0.8\n    # via\n    #   -r web_service/backend/requirements.in\n    #   secretstorage\ndeprecated==1.2.14\n    # via limits\ndnspython==2.7.0\n    # via email-validator\nemail-validator==2.3.0\n    # via fastapi\nexceptiongroup==1.3.1\n    # via\n    #   anyio\n    #   pytest\nfastapi==0.111.0\n    # via -r web_service/backend/requirements.in\nfastapi-cli==0.0.20\n    # via fastapi\ngreenlet==1.1.2  # x86 PINNED\n    # via\n    #   -r web_service/backend/requirements.in\n    #   sqlalchemy\nh11==0.14.0\n    # via\n    #   httpcore\n    #   uvicorn\nh2==4.1.0\n    # via httpx\nhpack==4.0.0\n    # via h2\nhttpcore==1.0.5\n    # via httpx\nhttptools==0.7.1\n    # via uvicorn\nhttpx[http2]==0.27.0\n    # via\n    #   -r web_service/backend/requirements.in\n    #   fastapi\nhyperframe==6.0.1\n    # via h2\nidna==3.7\n    # via\n    #   anyio\n    #   email-validator\n    #   httpx\n    #   requests\nimportlib-metadata==8.7.1\n    # via\n    #   build\n    #   keyring\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\niniconfig==2.0.0\n    # via pytest\njaraco-classes==3.4.0\n    # via keyring\njaraco-context==4.3.0\n    # via keyring\njaraco-functools==4.3.0\n    # via keyring\njeepney==0.8.0\n    # via\n    #   keyring\n    #   secretstorage\njinja2==3.1.6\n    # via fastapi\nkeyring==25.2.1\n    # via -r web_service/backend/requirements.in\nlimits==3.14.1\n    # via slowapi\nmarkdown-it-py==3.0.0\n    # via rich\nmarkupsafe==3.0.3\n    # via jinja2\nmdurl==0.1.2\n    # via markdown-it-py\nmore-itertools==10.3.0\n    # via\n    #   jaraco-classes\n    #   jaraco-functools\nmypy-extensions==1.0.0\n    # via black\nnumpy==1.23.5  # x86 PINNED\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pandas\n    #   scipy\norjson==3.11.5\n    # via fastapi\npackaging==24.1\n    # via\n    #   black\n    #   build\n    #   limits\n    #   pyinstaller\n    #   pyinstaller-hooks-contrib\n    #   pytest\npandas==1.5.3  # x86 PINNED\n    # via -r web_service/backend/requirements.in\npathspec==0.12.1\n    # via black\npip-tools==7.4.1\n    # via -r web_service/backend/requirements.in\nplatformdirs==4.2.2\n    # via black\npluggy==1.5.0\n    # via pytest\npsutil==5.9.8\n    # via -r web_service/backend/requirements.in\npsycopg2-binary==2.9.9\n    # via -r web_service/backend/requirements.in\npycparser==2.22\n    # via cffi\npydantic==2.8.2\n    # via\n    #   fastapi\n    #   pydantic-settings\npydantic-core==2.20.1\n    # via pydantic\npydantic-settings==2.3.4\n    # via -r web_service/backend/requirements.in\npygments==2.18.0\n    # via rich\npyinstaller==6.5.0\n    # via -r web_service/backend/requirements.in\npyinstaller-hooks-contrib==2024.6\n    # via pyinstaller\npyproject-hooks==1.1.0\n    # via\n    #   build\n    #   pip-tools\npytest==8.2.2\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pytest-asyncio\npytest-asyncio==0.23.7\n    # via -r web_service/backend/requirements.in\npython-dateutil==2.9.0.post0\n    # via pandas\npython-dotenv==1.0.1\n    # via\n    #   pydantic-settings\n    #   uvicorn\npython-multipart==0.0.20\n    # via fastapi\npytz==2024.1\n    # via pandas\npyyaml==6.0.3\n    # via uvicorn\nredis==5.0.6\n    # via -r web_service/backend/requirements.in\nrequests==2.32.5\n    # via -r web_service/backend/requirements.in\nrich==14.2.0\n    # via\n    #   rich-toolkit\n    #   typer\nrich-toolkit==0.17.1\n    # via fastapi-cli\nscipy==1.10.1  # x86 PINNED\n    # via -r web_service/backend/requirements.in\nsecretstorage==3.3.3\n    # via keyring\nselectolax==0.4.0\n    # via -r web_service/backend/requirements.in\nshellingham==1.5.4\n    # via typer\nsix==1.16.0\n    # via python-dateutil\nslowapi==0.1.9\n    # via -r web_service/backend/requirements.in\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nsoupsieve==2.5\n    # via beautifulsoup4\nsqlalchemy==1.4.46  # x86 PINNED\n    # via -r web_service/backend/requirements.in\nstarlette==0.37.2\n    # via fastapi\nstructlog==24.2.0\n    # via -r web_service/backend/requirements.in\ntenacity==8.2.3\n    # via -r web_service/backend/requirements.in\ntomli==2.3.0\n    # via\n    #   black\n    #   build\n    #   fastapi-cli\n    #   pip-tools\n    #   pytest\ntyper==0.21.0\n    # via fastapi-cli\ntyping-extensions==4.12.2\n    # via\n    #   aiosqlite\n    #   black\n    #   exceptiongroup\n    #   fastapi\n    #   limits\n    #   pydantic\n    #   pydantic-core\n    #   rich-toolkit\n    #   starlette\n    #   typer\n    #   uvicorn\nujson==5.11.0\n    # via fastapi\nurllib3==2.6.2\n    # via\n    #   -r web_service/backend/requirements.in\n    #   requests\nuvicorn==0.30.1\n    # via\n    #   -r web_service/backend/requirements.in\n    #   fastapi\n    #   fastapi-cli\nhttptools==0.7.1\n    # via uvicorn\nwebsockets==15.0.1\n    # via uvicorn\nwatchfiles==1.1.1\n    # via uvicorn\nwebsockets==15.0.1\n    # via uvicorn\nwheel==0.43.0\n    # via\n    #   -r web_service/backend/requirements.in\n    #   pip-tools\nwrapt==1.16.0\n    # via deprecated\nzipp==3.23.0\n    # via importlib-metadata\n",
    "web_service/backend/user_friendly_errors.py": "# python_service/user_friendly_errors.py\n\n\"\"\"\nCentralized dictionary for mapping technical exceptions to user-friendly messages.\n\"\"\"\n\nERROR_MAP = {\n    \"AdapterHttpError\": {\n        \"message\": \"A data source is currently unavailable.\",\n        \"suggestion\": (\n            \"This is usually temporary. Please try again in a few minutes. \"\n            \"If the problem persists, the website may be down for maintenance.\"\n        ),\n    },\n    \"AdapterConfigError\": {\n        \"message\": \"A data adapter is misconfigured.\",\n        \"suggestion\": \"Please check that all required API keys and settings are present in your .env file.\",\n    },\n    \"default\": {\n        \"message\": \"An unexpected error occurred.\",\n        \"suggestion\": \"Please check the application logs for more details or contact support.\",\n    },\n}\n",
    "web_service/backend/version.py": "# web_service/backend/version.py\n\n__version__ = \"3.0.1\" # Default version\n\ndef get_version():\n    \"\"\"Returns the application version.\"\"\"\n    return __version__\n",
    "web_service/frontend/app/components/ErrorDisplay.tsx": "// web_platform/frontend/src/components/ErrorDisplay.tsx\n'use client';\n\nimport React from 'react';\n\ninterface ErrorInfo {\n  message: string;\n  suggestion: string;\n  details?: string;\n}\n\ninterface ErrorDisplayProps {\n  error: ErrorInfo;\n}\n\nexport const ErrorDisplay: React.FC<ErrorDisplayProps> = ({ error }) => {\n  return (\n    <div className=\"bg-red-900/20 border border-red-500/30 text-white rounded-lg p-6 max-w-2xl mx-auto my-8\">\n      <div className=\"flex items-center mb-4\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" className=\"h-8 w-8 text-red-400 mr-4\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n          <path fillRule=\"evenodd\" d=\"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z\" clipRule=\"evenodd\" />\n        </svg>\n        <h2 className=\"text-2xl font-bold text-red-400\">An Error Occurred</h2>\n      </div>\n      <p className=\"text-lg text-slate-300 mb-2\">{error.message}</p>\n      <p className=\"text-slate-400 mb-6\">{error.suggestion}</p>\n      {error.details && (\n        <details className=\"bg-slate-800/50 rounded-lg p-4\">\n          <summary className=\"cursor-pointer text-sm text-slate-500 hover:text-white\">\n            Technical Details\n          </summary>\n          <pre className=\"text-xs text-slate-400 mt-2 p-2 bg-black/30 rounded overflow-x-auto\">\n            <code>{error.details}</code>\n          </pre>\n        </details>\n      )}\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/RaceCardSkeleton.tsx": "// web_platform/frontend/src/components/RaceCardSkeleton.tsx\nimport React from 'react';\n\nexport const RaceCardSkeleton: React.FC = () => {\n  return (\n    <div className=\"race-card-skeleton border border-gray-700 rounded-lg p-4 bg-gray-800 shadow-lg animate-pulse\">\n      {/* Skeleton Header */}\n      <div className=\"flex items-center justify-between mb-4\">\n        <div className=\"flex items-center gap-3\">\n          <div>\n            <div className=\"h-7 w-28 bg-gray-700 rounded-md\"></div>\n            <div className=\"h-4 w-40 bg-gray-700 rounded-md mt-2\"></div>\n          </div>\n        </div>\n        <div className=\"h-16 w-16 bg-gray-700 rounded-full\"></div>\n      </div>\n\n      {/* Skeleton Info Grid */}\n      <div className=\"grid grid-cols-4 gap-2 mb-4 p-3 bg-gray-800/50 rounded-lg\">\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-12 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-8 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n        <div className=\"text-center\">\n          <div className=\"h-3 w-10 mx-auto bg-gray-700 rounded-md\"></div>\n          <div className=\"h-4 w-6 mx-auto bg-gray-700 rounded-md mt-2\"></div>\n        </div>\n      </div>\n\n      {/* Skeleton Runner Rows */}\n      <div className=\"space-y-2\">\n        {[...Array(3)].map((_, i) => (\n          <div key={i} className=\"runner-row rounded-md p-3\">\n            <div className=\"flex items-center justify-between\">\n              <div className=\"flex items-center gap-4 flex-1\">\n                <div className=\"w-10 h-10 rounded-full bg-gray-700\"></div>\n                <div className=\"flex flex-col space-y-2\">\n                  <div className=\"h-5 w-32 bg-gray-700 rounded-md\"></div>\n                  <div className=\"h-4 w-40 bg-gray-700 rounded-md\"></div>\n                </div>\n              </div>\n              <div className=\"text-right\">\n                <div className=\"h-6 w-16 bg-gray-700 rounded-md\"></div>\n                <div className=\"h-3 w-12 bg-gray-700 rounded-md mt-2\"></div>\n              </div>\n            </div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n};\n",
    "web_service/frontend/app/components/Tabs.tsx": "// src/components/Tabs.tsx\n'use client';\n\nimport React, { useState } from 'react';\n\ntype Tab = {\n  label: string;\n  content: React.ReactNode;\n};\n\ntype TabsProps = {\n  tabs: Tab[];\n};\n\nexport function Tabs({ tabs }: TabsProps) {\n  const [activeTab, setActiveTab] = useState(0);\n\n  return (\n    <div>\n      <div className=\"border-b border-slate-700\">\n        <nav className=\"-mb-px flex space-x-8\" aria-label=\"Tabs\">\n          {tabs.map((tab, index) => (\n            <button\n              key={tab.label}\n              onClick={() => setActiveTab(index)}\n              className={`${\n                activeTab === index\n                  ? 'border-blue-500 text-blue-400'\n                  : 'border-transparent text-slate-400 hover:text-slate-200 hover:border-slate-500'\n              } whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm transition-colors focus:outline-none`}\n            >\n              {tab.label}\n            </button>\n          ))}\n        </nav>\n      </div>\n      <div className=\"mt-8\">{tabs[activeTab].content}</div>\n    </div>\n  );\n}\n",
    "web_service/frontend/app/layout.tsx": "// web_platform/frontend/app/layout.tsx\nimport './globals.css';\nimport type { Metadata } from 'next';\nimport { Inter } from 'next/font/google';\nimport Providers from './Providers';\n\nconst inter = Inter({ subsets: ['latin'] });\n\nexport const metadata: Metadata = {\n  title: 'Fortuna',\n  description: 'Real-time horse racing analysis.',\n};\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  return (\n    <html lang=\"en\">\n      <body className={`${inter.className} bg-white text-gray-900 dark:bg-gray-900 dark:text-gray-100`}>\n        <Providers>{children}</Providers>\n      </body>\n    </html>\n  );\n}",
    "web_service/frontend/app/utils/exportManager.ts": "// web_platform/frontend/src/utils/exportManager.ts\n// import { saveAs } from 'file-saver';\n// import * as XLSX from 'xlsx';\n\nexport class ExportManager {\n  static exportToExcel(races: any[], filename: string = 'fortuna_races') {\n    //\n    // [JULES] - NOTE FOR JB AND AI EXPERTS:\n    // This feature has been temporarily disabled because the external dependency (xlsx)\n    // is hosted on a CDN (cdn.sheetjs.com) that is consistently failing during\n    // the CI/CD build process with 500 Internal Server Errors.\n    //\n    // To ensure the main application build is not blocked, I have commented out\n    // the implementation of this function. The 'xlsx' package remains in package.json,\n    // but this code will not be active until the dependency issue is resolved.\n    //\n\n    // const workbook = XLSX.utils.book_new();\n\n    // const summaryData = [\n    //   ['Total Qualified Races', races.length],\n    //   ['Generated At', new Date().toLocaleString()]\n    // ];\n    // const summarySheet = XLSX.utils.aoa_to_sheet(summaryData);\n    // XLSX.utils.book_append_sheet(workbook, summarySheet, 'Summary');\n\n    // const raceData = races.map(race => ({\n    //   'Venue': race.venue,\n    //   'Race Number': race.race_number,\n    //   'Post Time': new Date(race.start_time).toLocaleString(),\n    //   'Qualification Score': race.qualification_score || 0,\n    //   'Field Size': race.runners.filter(r => !r.scratched).length,\n    //   'Source': race.source\n    // }));\n    // const raceSheet = XLSX.utils.json_to_sheet(raceData);\n    // XLSX.utils.book_append_sheet(workbook, raceSheet, 'Races');\n\n    // XLSX.writeFile(workbook, `${filename}_${Date.now()}.xlsx`);\n    console.warn(\"Excel export is temporarily disabled due to an external dependency issue.\");\n    alert(\"The Excel export feature is temporarily disabled due to an unreliable external dependency. Please try again later.\");\n  }\n}\n",
    "windows_service.py": "# windows_service.py\nimport os\nimport socket\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n\nclass FortunaBackendService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"FortunaFaucetBackend\"\n    _svc_display_name_ = \"Fortuna Faucet Racing Analysis Service\"\n    _svc_description_ = \"Background service for continuous racing data monitoring.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.stop_event = win32event.CreateEvent(None, 0, 0, None)\n        self.backend_process = None\n        socket.setdefaulttimeout(60)\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        win32event.SetEvent(self.stop_event)\n        if self.backend_process:\n            self.backend_process.terminate()\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE,\n            servicemanager.PYS_SERVICE_STARTED,\n            (self._svc_name_, \"\"),\n        )\n        self.main()\n\n    def main(self):\n        install_dir = Path(__file__).parent.resolve()\n        venv_python = install_dir / \".venv\" / \"Scripts\" / \"python.exe\"\n        api_module_dir = install_dir / \"python_service\"\n\n        env = os.environ.copy()\n        env_file = install_dir / \".env\"\n        if env_file.exists():\n            with open(env_file) as f:\n                for line in f:\n                    if \"=\" in line and not line.startswith(\"#\"):\n                        key, value = line.strip().split(\"=\", 1)\n                        env[key] = value.strip('\"')\n\n        self.backend_process = subprocess.Popen(\n            [\n                str(venv_python),\n                \"-m\",\n                \"uvicorn\",\n                \"api:app\",\n                \"--host\",\n                \"127.0.0.1\",\n                \"--port\",\n                \"8000\",\n            ],\n            cwd=str(api_module_dir),\n            env=env,\n        )\n\n        win32event.WaitForSingleObject(self.stop_event, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(FortunaBackendService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(FortunaBackendService)\n"
}